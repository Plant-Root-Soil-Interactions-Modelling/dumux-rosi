10: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 11
10: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 11
10: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 11
10: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 11
10: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 11
10: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 10 11
10: communicator.hh::sendRecv, left MPI_Wait
10: leave BufferedCommunicator::sendRecv
10: communicator.hh::BufferedCommunicator::forward, left sendRecv
10 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), after communicator.free();
10void AMG<M,X,S,PI,A>::mgc(LevelContext& levelContext), finished 
85: communicator.hh::sendRecv, left MPI_Wait
85: leave BufferedCommunicator::sendRecv
85 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
85: communicator.hh::BufferedCommunicator::forward, enter sendRecv
85: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 11
85: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 11
85: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 11
85: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 11
85: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 11
85: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 11
85: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 11
85: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 11
85: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 11
85: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 11
85: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 10 11
85: communicator.hh::sendRecv, left MPI_Wait
85: leave BufferedCommunicator::sendRecv
85: communicator.hh::BufferedCommunicator::forward, left sendRecv
85 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), after communicator.free();
85void AMG<M,X,S,PI,A>::mgc(LevelContext& levelContext), finished 
98: communicator.hh::sendRecv, left MPI_Wait
98: leave BufferedCommunicator::sendRecv
98 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
98: communicator.hh::BufferedCommunicator::forward, enter sendRecv
98: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 18
98: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 18
98: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 18
98: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 18
98: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 18
98: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 18
98: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 18
98: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 18
98: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 18
98: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 18
98: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 10 18
98: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 11 18
98: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 12 18
98: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 13 18
98: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 14 18
98: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 15 18
98: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 16 18
98: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 17 18
98: communicator.hh::sendRecv, left MPI_Wait
47: communicator.hh::sendRecv, left MPI_Wait
47: leave BufferedCommunicator::sendRecv
47 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
47: communicator.hh::BufferedCommunicator::forward, enter sendRecv
47: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 6
47: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 6
47: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 6
47: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 6
47: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 6
47: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 6
47: communicator.hh::sendRecv, left MPI_Wait
47: leave BufferedCommunicator::sendRecv
47: communicator.hh::BufferedCommunicator::forward, left sendRecv
47 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), after communicator.free();
47void AMG<M,X,S,PI,A>::mgc(LevelContext& levelContext), finished 
45: communicator.hh::sendRecv, left MPI_Wait
45: leave BufferedCommunicator::sendRecv
45 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
45: communicator.hh::BufferedCommunicator::forward, enter sendRecv
45: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 17
45: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 17
45: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 17
45: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 17
45: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 17
45: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 17
45: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 17
45: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 17
45: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 17
45: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 17
45: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 10 17
45: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 11 17
45: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 12 17
45: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 13 17
45: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 14 17
45: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 15 17
45: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 16 17
45: communicator.hh::sendRecv, left MPI_Wait
45: leave BufferedCommunicator::sendRecv
45: communicator.hh::BufferedCommunicator::forward, left sendRecv
45 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), after communicator.free();
45void AMG<M,X,S,PI,A>::mgc(LevelContext& levelContext), finished 
17: communicator.hh::sendRecv, left MPI_Wait
17: leave BufferedCommunicator::sendRecv
17 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
17: communicator.hh::BufferedCommunicator::forward, enter sendRecv
17: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 3
17: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 3
17: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 3
17: communicator.hh::sendRecv, left MPI_Wait
17: leave BufferedCommunicator::sendRecv
17: communicator.hh::BufferedCommunicator::forward, left sendRecv
17 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), after communicator.free();
17void AMG<M,X,S,PI,A>::mgc(LevelContext& levelContext), finished 
116: communicator.hh::sendRecv, left MPI_Wait
116: leave BufferedCommunicator::sendRecv
116 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
116: communicator.hh::BufferedCommunicator::forward, enter sendRecv
116: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 7
116: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 7
116: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 7
116: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 7
116: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 7
116: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 7
116: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 7
116: communicator.hh::sendRecv, left MPI_Wait
66: communicator.hh::sendRecv, left MPI_Wait
66: leave BufferedCommunicator::sendRecv
66 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
66: communicator.hh::BufferedCommunicator::forward, enter sendRecv
66: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 15
66: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 15
66: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 15
66: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 15
66: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 15
66: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 15
66: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 15
66: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 15
66: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 15
66: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 15
66: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 10 15
66: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 11 15
66: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 12 15
66: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 13 15
66: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 14 15
66: communicator.hh::sendRecv, left MPI_Wait
64: communicator.hh::sendRecv, left MPI_Wait
64: leave BufferedCommunicator::sendRecv
64 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
64: communicator.hh::BufferedCommunicator::forward, enter sendRecv
64: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 14
64: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 14
64: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 14
64: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 14
64: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 14
64: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 14
64: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 14
64: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 14
64: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 14
64: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 14
64: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 10 14
64: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 11 14
64: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 12 14
64: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 13 14
64: communicator.hh::sendRecv, left MPI_Wait
64: leave BufferedCommunicator::sendRecv
64: communicator.hh::BufferedCommunicator::forward, left sendRecv
64 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), after communicator.free();
64void AMG<M,X,S,PI,A>::mgc(LevelContext& levelContext), finished 
89: communicator.hh::sendRecv, left MPI_Wait
89: leave BufferedCommunicator::sendRecv
89 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
89: communicator.hh::BufferedCommunicator::forward, enter sendRecv
89: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 11
89: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 11
89: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 11
89: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 11
89: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 11
89: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 11
89: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 11
89: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 11
89: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 11
89: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 11
89: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 10 11
89: communicator.hh::sendRecv, left MPI_Wait
89: leave BufferedCommunicator::sendRecv
89: communicator.hh::BufferedCommunicator::forward, left sendRecv
89 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), after communicator.free();
89void AMG<M,X,S,PI,A>::mgc(LevelContext& levelContext), finished 
22: communicator.hh::sendRecv, left MPI_Wait
22: leave BufferedCommunicator::sendRecv
22 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
22: communicator.hh::BufferedCommunicator::forward, enter sendRecv
22: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 20
22: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 20
22: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 20
22: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 20
22: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 20
22: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 20
22: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 20
22: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 20
22: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 20
22: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 20
22: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 10 20
22: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 11 20
22: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 12 20
22: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 13 20
22: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 14 20
22: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 15 20
22: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 16 20
22: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 17 20
22: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 18 20
22: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 19 20
22: communicator.hh::sendRecv, left MPI_Wait
22: leave BufferedCommunicator::sendRecv
22: communicator.hh::BufferedCommunicator::forward, left sendRecv
22 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), after communicator.free();
22void AMG<M,X,S,PI,A>::mgc(LevelContext& levelContext), finished 
105: communicator.hh::sendRecv, left MPI_Wait
105: leave BufferedCommunicator::sendRecv
105 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
105: communicator.hh::BufferedCommunicator::forward, enter sendRecv
105: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 9
105: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 9
105: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 9
105: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 9
105: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 9
105: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 9
105: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 9
105: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 9
105: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 9
105: communicator.hh::sendRecv, left MPI_Wait
105: leave BufferedCommunicator::sendRecv
105: communicator.hh::BufferedCommunicator::forward, left sendRecv
105 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), after communicator.free();
105void AMG<M,X,S,PI,A>::mgc(LevelContext& levelContext), finished 
58: communicator.hh::sendRecv, left MPI_Wait
58: leave BufferedCommunicator::sendRecv
58 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
58: communicator.hh::BufferedCommunicator::forward, enter sendRecv
58: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 21
58: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 21
58: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 21
58: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 21
58: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 21
58: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 21
58: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 21
58: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 21
58: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 21
58: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 21
58: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 10 21
58: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 11 21
58: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 12 21
58: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 13 21
58: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 14 21
58: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 15 21
58: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 16 21
58: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 17 21
58: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 18 21
58: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 19 21
58: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 20 21
58: communicator.hh::sendRecv, left MPI_Wait
71: communicator.hh::sendRecv, left MPI_Wait
71: leave BufferedCommunicator::sendRecv
71 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
71: communicator.hh::BufferedCommunicator::forward, enter sendRecv
71: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 20
71: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 20
71: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 20
71: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 20
71: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 20
71: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 20
71: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 20
71: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 20
71: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 20
71: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 20
71: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 10 20
71: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 11 20
71: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 12 20
71: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 13 20
71: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 14 20
71: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 15 20
71: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 16 20
71: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 17 20
71: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 18 20
71: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 19 20
71: communicator.hh::sendRecv, left MPI_Wait
71: leave BufferedCommunicator::sendRecv
71: communicator.hh::BufferedCommunicator::forward, left sendRecv
71 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), after communicator.free();
71void AMG<M,X,S,PI,A>::mgc(LevelContext& levelContext), finished 
32: communicator.hh::sendRecv, left MPI_Wait
32: leave BufferedCommunicator::sendRecv
32 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
32: communicator.hh::BufferedCommunicator::forward, enter sendRecv
32: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 16
32: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 16
32: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 16
32: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 16
32: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 16
32: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 16
32: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 16
32: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 16
32: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 16
32: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 16
32: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 10 16
32: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 11 16
32: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 12 16
32: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 13 16
32: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 14 16
32: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 15 16
32: communicator.hh::sendRecv, left MPI_Wait
32: leave BufferedCommunicator::sendRecv
32: communicator.hh::BufferedCommunicator::forward, left sendRecv
32 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), after communicator.free();
32void AMG<M,X,S,PI,A>::mgc(LevelContext& levelContext), finished 
33: communicator.hh::sendRecv, left MPI_Wait
33: leave BufferedCommunicator::sendRecv
33 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
33: communicator.hh::BufferedCommunicator::forward, enter sendRecv
33: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 9
33: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 9
33: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 9
33: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 9
33: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 9
33: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 9
33: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 9
33: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 9
33: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 9
33: communicator.hh::sendRecv, left MPI_Wait
33: leave BufferedCommunicator::sendRecv
33: communicator.hh::BufferedCommunicator::forward, left sendRecv
33 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), after communicator.free();
33void AMG<M,X,S,PI,A>::mgc(LevelContext& levelContext), finished 
35: communicator.hh::sendRecv, left MPI_Wait
35: leave BufferedCommunicator::sendRecv
35 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
35: communicator.hh::BufferedCommunicator::forward, enter sendRecv
35: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 9
35: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 9
35: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 9
35: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 9
35: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 9
35: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 9
35: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 9
35: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 9
35: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 9
35: communicator.hh::sendRecv, left MPI_Wait
35: leave BufferedCommunicator::sendRecv
35: communicator.hh::BufferedCommunicator::forward, left sendRecv
35 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), after communicator.free();
35void AMG<M,X,S,PI,A>::mgc(LevelContext& levelContext), finished 
38: communicator.hh::sendRecv, left MPI_Wait
38: leave BufferedCommunicator::sendRecv
38 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
38: communicator.hh::BufferedCommunicator::forward, enter sendRecv
38: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 24
38: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 24
38: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 24
38: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 24
38: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 24
38: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 24
38: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 24
38: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 24
38: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 24
38: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 24
38: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 10 24
38: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 11 24
38: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 12 24
38: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 13 24
38: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 14 24
38: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 15 24
38: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 16 24
38: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 17 24
38: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 18 24
38: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 19 24
38: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 20 24
38: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 21 24
38: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 22 24
38: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 23 24
38: communicator.hh::sendRecv, left MPI_Wait
38: leave BufferedCommunicator::sendRecv
38: communicator.hh::BufferedCommunicator::forward, left sendRecv
38 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), after communicator.free();
38void AMG<M,X,S,PI,A>::mgc(LevelContext& levelContext), finished 
20: communicator.hh::sendRecv, left MPI_Wait
20: leave BufferedCommunicator::sendRecv
20 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
20: communicator.hh::BufferedCommunicator::forward, enter sendRecv
20: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 10
20: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 10
20: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 10
20: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 10
20: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 10
20: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 10
20: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 10
20: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 10
20: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 10
20: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 10
20: communicator.hh::sendRecv, left MPI_Wait
20: leave BufferedCommunicator::sendRecv
20: communicator.hh::BufferedCommunicator::forward, left sendRecv
20 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), after communicator.free();
20void AMG<M,X,S,PI,A>::mgc(LevelContext& levelContext), finished 
54: communicator.hh::sendRecv, left MPI_Wait
54: leave BufferedCommunicator::sendRecv
54 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
54: communicator.hh::BufferedCommunicator::forward, enter sendRecv
54: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 18
54: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 18
54: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 18
54: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 18
54: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 18
54: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 18
54: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 18
54: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 18
54: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 18
54: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 18
54: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 10 18
54: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 11 18
54: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 12 18
54: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 13 18
54: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 14 18
54: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 15 18
54: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 16 18
54: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 17 18
54: communicator.hh::sendRecv, left MPI_Wait
104: communicator.hh::sendRecv, left MPI_Wait
104: leave BufferedCommunicator::sendRecv
104 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
104: communicator.hh::BufferedCommunicator::forward, enter sendRecv
104: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 9
104: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 9
104: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 9
104: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 9
104: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 9
104: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 9
104: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 9
104: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 9
104: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 9
104: communicator.hh::sendRecv, left MPI_Wait
104: leave BufferedCommunicator::sendRecv
104: communicator.hh::BufferedCommunicator::forward, left sendRecv
104 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), after communicator.free();
104void AMG<M,X,S,PI,A>::mgc(LevelContext& levelContext), finished 
117: communicator.hh::sendRecv, left MPI_Wait
117: leave BufferedCommunicator::sendRecv
117 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
117: communicator.hh::BufferedCommunicator::forward, enter sendRecv
117: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 22
117: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 22
117: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 22
117: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 22
117: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 22
117: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 22
117: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 22
117: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 22
117: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 22
117: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 22
117: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 10 22
117: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 11 22
117: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 12 22
117: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 13 22
117: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 14 22
117: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 15 22
117: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 16 22
117: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 17 22
117: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 18 22
117: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 19 22
117: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 20 22
117: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 21 22
117: communicator.hh::sendRecv, left MPI_Wait
67: communicator.hh::sendRecv, left MPI_Wait
67: leave BufferedCommunicator::sendRecv
67 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
67: communicator.hh::BufferedCommunicator::forward, enter sendRecv
67: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 13
67: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 13
67: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 13
67: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 13
67: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 13
67: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 13
67: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 13
67: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 13
67: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 13
67: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 13
67: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 10 13
67: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 11 13
67: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 12 13
67: communicator.hh::sendRecv, left MPI_Wait
59: communicator.hh::sendRecv, left MPI_Wait
59: leave BufferedCommunicator::sendRecv
59 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
59: communicator.hh::BufferedCommunicator::forward, enter sendRecv
59: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 11
59: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 11
59: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 11
59: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 11
59: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 11
59: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 11
59: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 11
59: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 11
59: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 11
59: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 11
59: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 10 11
59: communicator.hh::sendRecv, left MPI_Wait
59: leave BufferedCommunicator::sendRecv
59: communicator.hh::BufferedCommunicator::forward, left sendRecv
59 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), after communicator.free();
59void AMG<M,X,S,PI,A>::mgc(LevelContext& levelContext), finished 
90: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 6
90: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 6
90: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 6
90: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 6
90: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 6
90: communicator.hh::sendRecv, left MPI_Wait
90: leave BufferedCommunicator::sendRecv
90: communicator.hh::BufferedCommunicator::forward, left sendRecv
90 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), after communicator.free();
90void AMG<M,X,S,PI,A>::mgc(LevelContext& levelContext), finished 
25: communicator.hh::sendRecv, left MPI_Wait
25: leave BufferedCommunicator::sendRecv
25 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
25: communicator.hh::BufferedCommunicator::forward, enter sendRecv
25: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 7
25: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 7
25: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 7
25: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 7
25: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 7
25: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 7
25: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 7
25: communicator.hh::sendRecv, left MPI_Wait
25: leave BufferedCommunicator::sendRecv
25: communicator.hh::BufferedCommunicator::forward, left sendRecv
25 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), after communicator.free();
25void AMG<M,X,S,PI,A>::mgc(LevelContext& levelContext), finished 
113: communicator.hh::sendRecv, left MPI_Wait
113: leave BufferedCommunicator::sendRecv
113 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
113: communicator.hh::BufferedCommunicator::forward, enter sendRecv
113: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 18
113: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 18
113: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 18
113: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 18
113: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 18
113: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 18
113: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 18
113: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 18
113: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 18
113: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 18
113: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 10 18
113: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 11 18
113: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 12 18
113: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 13 18
113: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 14 18
113: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 15 18
113: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 16 18
113: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 17 18
113: communicator.hh::sendRecv, left MPI_Wait
4: communicator.hh::sendRecv, left MPI_Wait
4: leave BufferedCommunicator::sendRecv
4 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
4: communicator.hh::BufferedCommunicator::forward, enter sendRecv
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 14
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 14
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 14
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 14
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 14
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 14
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 14
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 14
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 14
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 14
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 10 14
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 11 14
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 12 14
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 13 14
4: communicator.hh::sendRecv, left MPI_Wait
4: leave BufferedCommunicator::sendRecv
4: communicator.hh::BufferedCommunicator::forward, left sendRecv
4 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), after communicator.free();
4void AMG<M,X,S,PI,A>::mgc(LevelContext& levelContext), finished 
74: communicator.hh::sendRecv, left MPI_Wait
74: leave BufferedCommunicator::sendRecv
74 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
74: communicator.hh::BufferedCommunicator::forward, enter sendRecv
74: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 11
74: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 11
74: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 11
74: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 11
74: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 11
74: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 11
74: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 11
74: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 11
74: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 11
74: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 11
74: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 10 11
74: communicator.hh::sendRecv, left MPI_Wait
74: leave BufferedCommunicator::sendRecv
74: communicator.hh::BufferedCommunicator::forward, left sendRecv
74 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), after communicator.free();
74void AMG<M,X,S,PI,A>::mgc(LevelContext& levelContext), finished 
1: communicator.hh::sendRecv, left MPI_Wait
1: leave BufferedCommunicator::sendRecv
1 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
1: communicator.hh::BufferedCommunicator::forward, enter sendRecv
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 5
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 5
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 5
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 5
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 5
1: communicator.hh::sendRecv, left MPI_Wait
1: leave BufferedCommunicator::sendRecv
1: communicator.hh::BufferedCommunicator::forward, left sendRecv
1 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), after communicator.free();
1void AMG<M,X,S,PI,A>::mgc(LevelContext& levelContext), finished 
112: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 5
112: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 5
112: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 5
112: communicator.hh::sendRecv, left MPI_Wait
112: leave BufferedCommunicator::sendRecv
112: communicator.hh::BufferedCommunicator::forward, left sendRecv
112 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), after communicator.free();
112void AMG<M,X,S,PI,A>::mgc(LevelContext& levelContext), finished 
112 schwarz.hh:virtual void apply (X& v, const Y& d), before preconditioner.apply(v,d);; 
112 schwarz.hh:virtual void apply (X& v, const Y& d), before communication.copyOwnerToAll(v,v); 
112 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
112: communicator.hh::BufferedCommunicator::forward, enter sendRecv
112: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 5
112: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 5
112: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 5
112: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 5
112: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 5
112: communicator.hh::sendRecv, left MPI_Wait
93: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 13
93: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 13
93: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 13
93: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 13
93: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 13
93: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 13
93: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 13
93: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 13
93: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 10 13
93: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 11 13
93: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 12 13
93: communicator.hh::sendRecv, left MPI_Wait
93: leave BufferedCommunicator::sendRecv
93: communicator.hh::BufferedCommunicator::forward, left sendRecv
93 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), after communicator.free();
93void AMG<M,X,S,PI,A>::mgc(LevelContext& levelContext), finished 
93 schwarz.hh:virtual void apply (X& v, const Y& d), before preconditioner.apply(v,d);; 
93 schwarz.hh:virtual void apply (X& v, const Y& d), before communication.copyOwnerToAll(v,v); 
93 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
93: communicator.hh::BufferedCommunicator::forward, enter sendRecv
93: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 13
93: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 13
93: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 13
93: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 13
93: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 13
93: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 13
93: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 13
93: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 13
93: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 13
93: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 13
93: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 10 13
93: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 11 13
93: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 12 13
93: communicator.hh::sendRecv, left MPI_Wait
93: leave BufferedCommunicator::sendRecv
93: communicator.hh::BufferedCommunicator::forward, left sendRecv
93 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), after communicator.free();
93 schwarz.hh:virtual void apply (X& v, const Y& d), AFTER communication.copyOwnerToAll(v,v); 
93 schwarz.hh:virtual void apply (X& v, const Y& d), before preconditioner.apply(v,d);; 
110: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 11
110: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 11
110: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 11
110: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 11
110: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 11
110: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 10 11
110: communicator.hh::sendRecv, left MPI_Wait
110: leave BufferedCommunicator::sendRecv
110: communicator.hh::BufferedCommunicator::forward, left sendRecv
110 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), after communicator.free();
110void AMG<M,X,S,PI,A>::mgc(LevelContext& levelContext), finished 
110 schwarz.hh:virtual void apply (X& v, const Y& d), before preconditioner.apply(v,d);; 
110 schwarz.hh:virtual void apply (X& v, const Y& d), before communication.copyOwnerToAll(v,v); 
110 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
110: communicator.hh::BufferedCommunicator::forward, enter sendRecv
110: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 11
110: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 11
110: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 11
110: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 11
110: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 11
110: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 11
110: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 11
110: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 11
110: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 11
110: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 11
110: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 10 11
21 schwarz.hh:virtual void apply (X& v, const Y& d), before preconditioner.apply(v,d);; 
21 schwarz.hh:virtual void apply (X& v, const Y& d), before communication.copyOwnerToAll(v,v); 
21 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
21: communicator.hh::BufferedCommunicator::forward, enter sendRecv
21: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 12
21: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 12
21: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 12
21: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 12
21: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 12
21: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 12
21: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 12
21: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 12
21: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 12
21: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 12
21: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 10 12
21: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 11 12
21: communicator.hh::sendRecv, left MPI_Wait
96: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 12
96: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 12
96: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 12
96: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 12
96: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 12
96: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 12
96: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 10 12
96: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 11 12
96: communicator.hh::sendRecv, left MPI_Wait
96: leave BufferedCommunicator::sendRecv
96: communicator.hh::BufferedCommunicator::forward, left sendRecv
96 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), after communicator.free();
96void AMG<M,X,S,PI,A>::mgc(LevelContext& levelContext), finished 
96 schwarz.hh:virtual void apply (X& v, const Y& d), before preconditioner.apply(v,d);; 
96 schwarz.hh:virtual void apply (X& v, const Y& d), before communication.copyOwnerToAll(v,v); 
96 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
96: communicator.hh::BufferedCommunicator::forward, enter sendRecv
96: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 12
96: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 12
96: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 12
96: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 12
96: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 12
96: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 12
96: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 12
96: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 12
96: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 12
96: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 12
96: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 10 12
96: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 11 12
96: communicator.hh::sendRecv, left MPI_Wait
73: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 11
73: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 11
73: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 11
73: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 11
73: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 11
73: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 10 11
73: communicator.hh::sendRecv, left MPI_Wait
73: leave BufferedCommunicator::sendRecv
73: communicator.hh::BufferedCommunicator::forward, left sendRecv
73 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), after communicator.free();
73void AMG<M,X,S,PI,A>::mgc(LevelContext& levelContext), finished 
73 schwarz.hh:virtual void apply (X& v, const Y& d), before preconditioner.apply(v,d);; 
73 schwarz.hh:virtual void apply (X& v, const Y& d), before communication.copyOwnerToAll(v,v); 
73 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
73: communicator.hh::BufferedCommunicator::forward, enter sendRecv
73: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 11
73: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 11
73: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 11
73: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 11
73: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 11
73: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 11
73: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 11
73: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 11
73: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 11
73: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 11
73: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 10 11
73: communicator.hh::sendRecv, left MPI_Wait
55: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 11
55: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 10 11
55: communicator.hh::sendRecv, left MPI_Wait
55: leave BufferedCommunicator::sendRecv
55: communicator.hh::BufferedCommunicator::forward, left sendRecv
55 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), after communicator.free();
55void AMG<M,X,S,PI,A>::mgc(LevelContext& levelContext), finished 
55 schwarz.hh:virtual void apply (X& v, const Y& d), before preconditioner.apply(v,d);; 
55 schwarz.hh:virtual void apply (X& v, const Y& d), before communication.copyOwnerToAll(v,v); 
55 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
55: communicator.hh::BufferedCommunicator::forward, enter sendRecv
55: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 11
55: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 11
55: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 11
55: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 11
55: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 11
55: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 11
55: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 11
55: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 11
55: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 11
55: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 11
55: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 10 11
55: communicator.hh::sendRecv, left MPI_Wait
78: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 15
78: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 15
78: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 10 15
78: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 11 15
78: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 12 15
78: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 13 15
78: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 14 15
78: communicator.hh::sendRecv, left MPI_Wait
78: leave BufferedCommunicator::sendRecv
78: communicator.hh::BufferedCommunicator::forward, left sendRecv
78 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), after communicator.free();
78void AMG<M,X,S,PI,A>::mgc(LevelContext& levelContext), finished 
78 schwarz.hh:virtual void apply (X& v, const Y& d), before preconditioner.apply(v,d);; 
78 schwarz.hh:virtual void apply (X& v, const Y& d), before communication.copyOwnerToAll(v,v); 
78 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
78: communicator.hh::BufferedCommunicator::forward, enter sendRecv
78: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 15
78: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 15
78: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 15
78: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 15
78: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 15
78: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 15
78: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 15
78: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 15
78: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 15
78: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 15
78: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 10 15
78: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 11 15
78: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 12 15
78: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 13 15
41: leave BufferedCommunicator::sendRecv
41: communicator.hh::BufferedCommunicator::forward, left sendRecv
41 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), after communicator.free();
41void AMG<M,X,S,PI,A>::mgc(LevelContext& levelContext), finished 
41 schwarz.hh:virtual void apply (X& v, const Y& d), before preconditioner.apply(v,d);; 
41 schwarz.hh:virtual void apply (X& v, const Y& d), before communication.copyOwnerToAll(v,v); 
41 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
41: communicator.hh::BufferedCommunicator::forward, enter sendRecv
41: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 20
41: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 20
41: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 20
41: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 20
41: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 20
41: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 20
41: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 20
41: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 20
41: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 20
41: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 20
41: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 10 20
41: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 11 20
41: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 12 20
41: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 13 20
41: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 14 20
41: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 15 20
41: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 16 20
41: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 17 20
41: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 18 20
41: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 19 20
41: communicator.hh::sendRecv, left MPI_Wait
2 schwarz.hh:virtual void apply (X& v, const Y& d), before preconditioner.apply(v,d);; 
2 schwarz.hh:virtual void apply (X& v, const Y& d), before communication.copyOwnerToAll(v,v); 
2 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
2: communicator.hh::BufferedCommunicator::forward, enter sendRecv
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 8
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 8
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 8
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 8
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 8
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 8
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 8
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 8
2: communicator.hh::sendRecv, left MPI_Wait
36: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 14
36: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 14
36: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 10 14
36: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 11 14
36: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 12 14
36: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 13 14
36: communicator.hh::sendRecv, left MPI_Wait
36: leave BufferedCommunicator::sendRecv
36: communicator.hh::BufferedCommunicator::forward, left sendRecv
36 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), after communicator.free();
36void AMG<M,X,S,PI,A>::mgc(LevelContext& levelContext), finished 
36 schwarz.hh:virtual void apply (X& v, const Y& d), before preconditioner.apply(v,d);; 
36 schwarz.hh:virtual void apply (X& v, const Y& d), before communication.copyOwnerToAll(v,v); 
36 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
36: communicator.hh::BufferedCommunicator::forward, enter sendRecv
36: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 14
36: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 14
36: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 14
36: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 14
36: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 14
36: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 14
36: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 14
36: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 14
36: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 14
36: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 14
36: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 10 14
36: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 11 14
36: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 12 14
36: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 13 14
36: communicator.hh::sendRecv, left MPI_Wait
40: communicator.hh::sendRecv, left MPI_Wait
40: leave BufferedCommunicator::sendRecv
40: communicator.hh::BufferedCommunicator::forward, left sendRecv
40 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), after communicator.free();
40void AMG<M,X,S,PI,A>::mgc(LevelContext& levelContext), finished 
40 schwarz.hh:virtual void apply (X& v, const Y& d), before preconditioner.apply(v,d);; 
40 schwarz.hh:virtual void apply (X& v, const Y& d), before communication.copyOwnerToAll(v,v); 
40 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
40: communicator.hh::BufferedCommunicator::forward, enter sendRecv
40: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 15
40: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 15
40: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 15
40: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 15
40: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 15
40: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 15
40: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 15
40: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 15
40: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 15
40: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 15
40: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 10 15
40: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 11 15
40: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 12 15
40: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 13 15
27 schwarz.hh:virtual void apply (X& v, const Y& d), before preconditioner.apply(v,d);; 
27 schwarz.hh:virtual void apply (X& v, const Y& d), before communication.copyOwnerToAll(v,v); 
27 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
27: communicator.hh::BufferedCommunicator::forward, enter sendRecv
27: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 17
27: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 17
27: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 17
27: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 17
27: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 17
27: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 17
27: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 17
27: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 17
27: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 17
27: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 17
27: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 10 17
27: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 11 17
27: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 12 17
27: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 13 17
27: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 14 17
27: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 15 17
27: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 16 17
27: communicator.hh::sendRecv, left MPI_Wait
70: communicator.hh::sendRecv, left MPI_Wait
70: leave BufferedCommunicator::sendRecv
70: communicator.hh::BufferedCommunicator::forward, left sendRecv
70 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), after communicator.free();
70void AMG<M,X,S,PI,A>::mgc(LevelContext& levelContext), finished 
70 schwarz.hh:virtual void apply (X& v, const Y& d), before preconditioner.apply(v,d);; 
70 schwarz.hh:virtual void apply (X& v, const Y& d), before communication.copyOwnerToAll(v,v); 
70 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
70: communicator.hh::BufferedCommunicator::forward, enter sendRecv
70: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 13
70: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 13
70: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 13
70: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 13
70: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 13
70: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 13
70: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 13
70: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 13
70: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 13
70: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 13
70: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 10 13
70: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 11 13
70: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 12 13
70: communicator.hh::sendRecv, left MPI_Wait
6: leave BufferedCommunicator::sendRecv
6: communicator.hh::BufferedCommunicator::forward, left sendRecv
6 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), after communicator.free();
6void AMG<M,X,S,PI,A>::mgc(LevelContext& levelContext), finished 
6 schwarz.hh:virtual void apply (X& v, const Y& d), before preconditioner.apply(v,d);; 
6 schwarz.hh:virtual void apply (X& v, const Y& d), before communication.copyOwnerToAll(v,v); 
6 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
6: communicator.hh::BufferedCommunicator::forward, enter sendRecv
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 17
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 17
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 17
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 17
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 17
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 17
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 17
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 17
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 17
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 17
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 10 17
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 11 17
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 12 17
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 13 17
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 14 17
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 15 17
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 16 17
6: communicator.hh::sendRecv, left MPI_Wait
11 schwarz.hh:virtual void apply (X& v, const Y& d), before preconditioner.apply(v,d);; 
11 schwarz.hh:virtual void apply (X& v, const Y& d), before communication.copyOwnerToAll(v,v); 
11 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
11: communicator.hh::BufferedCommunicator::forward, enter sendRecv
11: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 17
11: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 17
11: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 17
11: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 17
11: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 17
11: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 17
11: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 17
11: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 17
11: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 17
11: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 17
11: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 10 17
11: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 11 17
11: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 12 17
11: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 13 17
11: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 14 17
11: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 15 17
11: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 16 17
11: communicator.hh::sendRecv, left MPI_Wait
11: leave BufferedCommunicator::sendRecv
11: communicator.hh::BufferedCommunicator::forward, left sendRecv
11 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), after communicator.free();
11 schwarz.hh:virtual void apply (X& v, const Y& d), AFTER communication.copyOwnerToAll(v,v); 
11 schwarz.hh:virtual void apply (X& v, const Y& d), before preconditioner.apply(v,d);; 
11 schwarz.hh:virtual void apply (X& v, const Y& d), before communication.copyOwnerToAll(v,v); 
11 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
11: communicator.hh::BufferedCommunicator::forward, enter sendRecv
11: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 17
7: leave BufferedCommunicator::sendRecv
7: communicator.hh::BufferedCommunicator::forward, left sendRecv
7 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), after communicator.free();
7void AMG<M,X,S,PI,A>::mgc(LevelContext& levelContext), finished 
7 schwarz.hh:virtual void apply (X& v, const Y& d), before preconditioner.apply(v,d);; 
7 schwarz.hh:virtual void apply (X& v, const Y& d), before communication.copyOwnerToAll(v,v); 
7 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
7: communicator.hh::BufferedCommunicator::forward, enter sendRecv
7: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 13
7: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 13
7: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 13
7: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 13
7: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 13
7: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 13
7: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 13
7: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 13
7: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 13
7: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 13
7: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 10 13
7: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 11 13
7: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 12 13
7: communicator.hh::sendRecv, left MPI_Wait
7: leave BufferedCommunicator::sendRecv
7: communicator.hh::BufferedCommunicator::forward, left sendRecv
7 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), after communicator.free();
7 schwarz.hh:virtual void apply (X& v, const Y& d), AFTER communication.copyOwnerToAll(v,v); 
7 schwarz.hh:virtual void apply (X& v, const Y& d), before preconditioner.apply(v,d);; 
7 schwarz.hh:virtual void apply (X& v, const Y& d), before communication.copyOwnerToAll(v,v); 
7 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
7: communicator.hh::BufferedCommunicator::forward, enter sendRecv
7: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 13
7: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 13
99: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 11 13
99: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 12 13
99: communicator.hh::sendRecv, left MPI_Wait
99: leave BufferedCommunicator::sendRecv
99: communicator.hh::BufferedCommunicator::forward, left sendRecv
99 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), after communicator.free();
99void AMG<M,X,S,PI,A>::mgc(LevelContext& levelContext), finished 
99 schwarz.hh:virtual void apply (X& v, const Y& d), before preconditioner.apply(v,d);; 
99 schwarz.hh:virtual void apply (X& v, const Y& d), before communication.copyOwnerToAll(v,v); 
99 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
99: communicator.hh::BufferedCommunicator::forward, enter sendRecv
99: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 13
99: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 13
99: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 13
99: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 13
99: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 13
99: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 13
99: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 13
99: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 13
99: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 13
99: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 13
99: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 10 13
99: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 11 13
99: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 12 13
99: communicator.hh::sendRecv, left MPI_Wait
8 schwarz.hh:virtual void apply (X& v, const Y& d), before preconditioner.apply(v,d);; 
8 schwarz.hh:virtual void apply (X& v, const Y& d), before communication.copyOwnerToAll(v,v); 
8 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
8: communicator.hh::BufferedCommunicator::forward, enter sendRecv
8: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 20
8: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 20
8: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 20
8: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 20
8: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 20
8: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 20
8: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 20
8: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 20
8: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 20
8: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 20
8: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 10 20
8: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 11 20
8: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 12 20
8: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 13 20
8: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 14 20
8: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 15 20
8: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 16 20
8: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 17 20
8: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 18 20
8: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 19 20
8: communicator.hh::sendRecv, left MPI_Wait
68: leave BufferedCommunicator::sendRecv
68: communicator.hh::BufferedCommunicator::forward, left sendRecv
68 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), after communicator.free();
68void AMG<M,X,S,PI,A>::mgc(LevelContext& levelContext), finished 
68 schwarz.hh:virtual void apply (X& v, const Y& d), before preconditioner.apply(v,d);; 
68 schwarz.hh:virtual void apply (X& v, const Y& d), before communication.copyOwnerToAll(v,v); 
68 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
68: communicator.hh::BufferedCommunicator::forward, enter sendRecv
68: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 22
68: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 22
68: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 22
68: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 22
68: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 22
68: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 22
68: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 22
68: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 22
68: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 22
68: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 22
68: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 10 22
68: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 11 22
68: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 12 22
68: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 13 22
68: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 14 22
68: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 15 22
68: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 16 22
68: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 17 22
68: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 18 22
68: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 19 22
68: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 20 22
68: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 21 22
68: communicator.hh::sendRecv, left MPI_Wait
87: leave BufferedCommunicator::sendRecv
87: communicator.hh::BufferedCommunicator::forward, left sendRecv
87 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), after communicator.free();
87void AMG<M,X,S,PI,A>::mgc(LevelContext& levelContext), finished 
87 schwarz.hh:virtual void apply (X& v, const Y& d), before preconditioner.apply(v,d);; 
87 schwarz.hh:virtual void apply (X& v, const Y& d), before communication.copyOwnerToAll(v,v); 
87 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
87: communicator.hh::BufferedCommunicator::forward, enter sendRecv
87: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 19
87: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 19
87: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 19
87: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 19
87: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 19
87: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 19
87: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 19
87: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 19
87: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 19
87: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 19
87: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 10 19
87: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 11 19
87: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 12 19
87: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 13 19
87: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 14 19
87: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 15 19
87: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 16 19
87: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 17 19
87: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 18 19
87: communicator.hh::sendRecv, left MPI_Wait
57: leave BufferedCommunicator::sendRecv
57: communicator.hh::BufferedCommunicator::forward, left sendRecv
57 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), after communicator.free();
57void AMG<M,X,S,PI,A>::mgc(LevelContext& levelContext), finished 
57 schwarz.hh:virtual void apply (X& v, const Y& d), before preconditioner.apply(v,d);; 
57 schwarz.hh:virtual void apply (X& v, const Y& d), before communication.copyOwnerToAll(v,v); 
57 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
57: communicator.hh::BufferedCommunicator::forward, enter sendRecv
57: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 19
57: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 19
57: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 19
57: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 19
57: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 19
57: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 19
57: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 19
57: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 19
57: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 19
57: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 19
57: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 10 19
57: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 11 19
57: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 12 19
57: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 13 19
57: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 14 19
57: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 15 19
57: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 16 19
57: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 17 19
57: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 18 19
57: communicator.hh::sendRecv, left MPI_Wait
97: leave BufferedCommunicator::sendRecv
97: communicator.hh::BufferedCommunicator::forward, left sendRecv
97 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), after communicator.free();
97void AMG<M,X,S,PI,A>::mgc(LevelContext& levelContext), finished 
97 schwarz.hh:virtual void apply (X& v, const Y& d), before preconditioner.apply(v,d);; 
97 schwarz.hh:virtual void apply (X& v, const Y& d), before communication.copyOwnerToAll(v,v); 
97 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
97: communicator.hh::BufferedCommunicator::forward, enter sendRecv
97: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 9
97: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 9
97: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 9
97: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 9
97: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 9
97: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 9
97: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 9
97: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 9
97: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 9
97: communicator.hh::sendRecv, left MPI_Wait
97: leave BufferedCommunicator::sendRecv
97: communicator.hh::BufferedCommunicator::forward, left sendRecv
97 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), after communicator.free();
97 schwarz.hh:virtual void apply (X& v, const Y& d), AFTER communication.copyOwnerToAll(v,v); 
97 schwarz.hh:virtual void apply (X& v, const Y& d), before preconditioner.apply(v,d);; 
97 schwarz.hh:virtual void apply (X& v, const Y& d), before communication.copyOwnerToAll(v,v); 
82: communicator.hh::sendRecv, left MPI_Wait
82: leave BufferedCommunicator::sendRecv
82: communicator.hh::BufferedCommunicator::forward, left sendRecv
82 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), after communicator.free();
82void AMG<M,X,S,PI,A>::mgc(LevelContext& levelContext), finished 
82 schwarz.hh:virtual void apply (X& v, const Y& d), before preconditioner.apply(v,d);; 
82 schwarz.hh:virtual void apply (X& v, const Y& d), before communication.copyOwnerToAll(v,v); 
82 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
82: communicator.hh::BufferedCommunicator::forward, enter sendRecv
82: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 26
82: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 26
82: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 26
82: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 26
82: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 26
82: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 26
82: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 26
82: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 26
82: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 26
82: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 26
82: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 10 26
82: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 11 26
82: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 12 26
82: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 13 26
82: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 14 26
82: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 15 26
82: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 16 26
82: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 17 26
82: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 18 26
82: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 19 26
82: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 20 26
82: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 21 26
82: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 22 26
82: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 23 26
82: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 24 26
16 schwarz.hh:virtual void apply (X& v, const Y& d), before preconditioner.apply(v,d);; 
16 schwarz.hh:virtual void apply (X& v, const Y& d), before communication.copyOwnerToAll(v,v); 
16 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
16: communicator.hh::BufferedCommunicator::forward, enter sendRecv
16: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 6
16: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 6
16: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 6
16: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 6
16: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 6
16: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 6
16: communicator.hh::sendRecv, left MPI_Wait
31 schwarz.hh:virtual void apply (X& v, const Y& d), before preconditioner.apply(v,d);; 
31 schwarz.hh:virtual void apply (X& v, const Y& d), before communication.copyOwnerToAll(v,v); 
31 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
31: communicator.hh::BufferedCommunicator::forward, enter sendRecv
31: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 10
31: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 10
31: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 10
31: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 10
31: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 10
31: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 10
31: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 10
31: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 10
31: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 10
31: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 10
31: communicator.hh::sendRecv, left MPI_Wait
31: leave BufferedCommunicator::sendRecv
31: communicator.hh::BufferedCommunicator::forward, left sendRecv
31 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), after communicator.free();
31 schwarz.hh:virtual void apply (X& v, const Y& d), AFTER communication.copyOwnerToAll(v,v); 
31 schwarz.hh:virtual void apply (X& v, const Y& d), before preconditioner.apply(v,d);; 
31 schwarz.hh:virtual void apply (X& v, const Y& d), before communication.copyOwnerToAll(v,v); 
37 schwarz.hh:virtual void apply (X& v, const Y& d), before preconditioner.apply(v,d);; 
37 schwarz.hh:virtual void apply (X& v, const Y& d), before communication.copyOwnerToAll(v,v); 
37 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
37: communicator.hh::BufferedCommunicator::forward, enter sendRecv
37: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 15
37: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 15
37: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 15
37: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 15
37: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 15
37: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 15
37: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 15
37: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 15
37: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 15
37: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 15
37: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 10 15
37: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 11 15
37: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 12 15
37: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 13 15
37: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 14 15
37: communicator.hh::sendRecv, left MPI_Wait
37: leave BufferedCommunicator::sendRecv
37: communicator.hh::BufferedCommunicator::forward, left sendRecv
37 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), after communicator.free();
37 schwarz.hh:virtual void apply (X& v, const Y& d), AFTER communication.copyOwnerToAll(v,v); 
37 schwarz.hh:virtual void apply (X& v, const Y& d), before preconditioner.apply(v,d);; 
9 schwarz.hh:virtual void apply (X& v, const Y& d), before preconditioner.apply(v,d);; 
9 schwarz.hh:virtual void apply (X& v, const Y& d), before communication.copyOwnerToAll(v,v); 
9 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
9: communicator.hh::BufferedCommunicator::forward, enter sendRecv
9: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 7
9: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 7
9: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 7
9: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 7
9: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 7
9: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 7
9: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 7
9: communicator.hh::sendRecv, left MPI_Wait
9: leave BufferedCommunicator::sendRecv
9: communicator.hh::BufferedCommunicator::forward, left sendRecv
9 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), after communicator.free();
9 schwarz.hh:virtual void apply (X& v, const Y& d), AFTER communication.copyOwnerToAll(v,v); 
9 schwarz.hh:virtual void apply (X& v, const Y& d), before preconditioner.apply(v,d);; 
9 schwarz.hh:virtual void apply (X& v, const Y& d), before communication.copyOwnerToAll(v,v); 
9 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
9: communicator.hh::BufferedCommunicator::forward, enter sendRecv
9: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 7
62: leave BufferedCommunicator::sendRecv
62: communicator.hh::BufferedCommunicator::forward, left sendRecv
62 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), after communicator.free();
62void AMG<M,X,S,PI,A>::mgc(LevelContext& levelContext), finished 
62 schwarz.hh:virtual void apply (X& v, const Y& d), before preconditioner.apply(v,d);; 
62 schwarz.hh:virtual void apply (X& v, const Y& d), before communication.copyOwnerToAll(v,v); 
62 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
62: communicator.hh::BufferedCommunicator::forward, enter sendRecv
62: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 15
62: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 15
62: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 15
62: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 15
62: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 15
62: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 15
62: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 15
62: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 15
62: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 15
62: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 15
62: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 10 15
62: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 11 15
62: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 12 15
62: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 13 15
62: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 14 15
62: communicator.hh::sendRecv, left MPI_Wait
48 schwarz.hh:virtual void apply (X& v, const Y& d), before preconditioner.apply(v,d);; 
48 schwarz.hh:virtual void apply (X& v, const Y& d), before communication.copyOwnerToAll(v,v); 
48 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
48: communicator.hh::BufferedCommunicator::forward, enter sendRecv
48: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 15
48: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 15
48: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 15
48: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 15
48: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 15
48: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 15
48: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 15
48: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 15
48: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 15
48: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 15
48: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 10 15
48: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 11 15
48: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 12 15
48: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 13 15
48: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 14 15
48: communicator.hh::sendRecv, left MPI_Wait
43 schwarz.hh:virtual void apply (X& v, const Y& d), before preconditioner.apply(v,d);; 
43 schwarz.hh:virtual void apply (X& v, const Y& d), before communication.copyOwnerToAll(v,v); 
43 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
43: communicator.hh::BufferedCommunicator::forward, enter sendRecv
43: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 11
43: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 11
43: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 11
43: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 11
43: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 11
43: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 11
43: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 11
43: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 11
43: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 11
43: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 11
43: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 10 11
43: communicator.hh::sendRecv, left MPI_Wait
43: leave BufferedCommunicator::sendRecv
43: communicator.hh::BufferedCommunicator::forward, left sendRecv
43 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), after communicator.free();
43 schwarz.hh:virtual void apply (X& v, const Y& d), AFTER communication.copyOwnerToAll(v,v); 
43 schwarz.hh:virtual void apply (X& v, const Y& d), before preconditioner.apply(v,d);; 
77: leave BufferedCommunicator::sendRecv
77: communicator.hh::BufferedCommunicator::forward, left sendRecv
77 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), after communicator.free();
77void AMG<M,X,S,PI,A>::mgc(LevelContext& levelContext), finished 
77 schwarz.hh:virtual void apply (X& v, const Y& d), before preconditioner.apply(v,d);; 
77 schwarz.hh:virtual void apply (X& v, const Y& d), before communication.copyOwnerToAll(v,v); 
77 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
77: communicator.hh::BufferedCommunicator::forward, enter sendRecv
77: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 6
77: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 6
77: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 6
77: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 6
77: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 6
77: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 6
77: communicator.hh::sendRecv, left MPI_Wait
65: leave BufferedCommunicator::sendRecv
65: communicator.hh::BufferedCommunicator::forward, left sendRecv
65 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), after communicator.free();
65void AMG<M,X,S,PI,A>::mgc(LevelContext& levelContext), finished 
65 schwarz.hh:virtual void apply (X& v, const Y& d), before preconditioner.apply(v,d);; 
65 schwarz.hh:virtual void apply (X& v, const Y& d), before communication.copyOwnerToAll(v,v); 
65 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
65: communicator.hh::BufferedCommunicator::forward, enter sendRecv
65: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 9
65: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 9
65: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 9
65: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 9
65: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 9
65: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 9
65: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 9
65: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 9
65: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 9
65: communicator.hh::sendRecv, left MPI_Wait
39 schwarz.hh:virtual void apply (X& v, const Y& d), before preconditioner.apply(v,d);; 
39 schwarz.hh:virtual void apply (X& v, const Y& d), before communication.copyOwnerToAll(v,v); 
39 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
39: communicator.hh::BufferedCommunicator::forward, enter sendRecv
39: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 9
39: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 9
39: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 9
39: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 9
39: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 9
39: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 9
39: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 9
39: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 9
39: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 9
39: communicator.hh::sendRecv, left MPI_Wait
79: leave BufferedCommunicator::sendRecv
79: communicator.hh::BufferedCommunicator::forward, left sendRecv
79 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), after communicator.free();
79void AMG<M,X,S,PI,A>::mgc(LevelContext& levelContext), finished 
79 schwarz.hh:virtual void apply (X& v, const Y& d), before preconditioner.apply(v,d);; 
79 schwarz.hh:virtual void apply (X& v, const Y& d), before communication.copyOwnerToAll(v,v); 
79 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
79: communicator.hh::BufferedCommunicator::forward, enter sendRecv
79: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 16
79: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 16
79: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 16
79: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 16
79: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 16
79: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 16
79: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 16
79: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 16
79: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 16
79: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 16
79: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 10 16
79: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 11 16
79: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 12 16
79: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 13 16
79: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 14 16
79: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 15 16
79: communicator.hh::sendRecv, left MPI_Wait
80: leave BufferedCommunicator::sendRecv
80: communicator.hh::BufferedCommunicator::forward, left sendRecv
80 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), after communicator.free();
80void AMG<M,X,S,PI,A>::mgc(LevelContext& levelContext), finished 
80 schwarz.hh:virtual void apply (X& v, const Y& d), before preconditioner.apply(v,d);; 
80 schwarz.hh:virtual void apply (X& v, const Y& d), before communication.copyOwnerToAll(v,v); 
80 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
80: communicator.hh::BufferedCommunicator::forward, enter sendRecv
80: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 10
80: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 10
80: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 10
80: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 10
80: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 10
80: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 10
80: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 10
80: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 10
80: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 10
80: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 10
80: communicator.hh::sendRecv, left MPI_Wait
92: leave BufferedCommunicator::sendRecv
92: communicator.hh::BufferedCommunicator::forward, left sendRecv
92 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), after communicator.free();
92void AMG<M,X,S,PI,A>::mgc(LevelContext& levelContext), finished 
92 schwarz.hh:virtual void apply (X& v, const Y& d), before preconditioner.apply(v,d);; 
92 schwarz.hh:virtual void apply (X& v, const Y& d), before communication.copyOwnerToAll(v,v); 
92 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
92: communicator.hh::BufferedCommunicator::forward, enter sendRecv
92: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 16
92: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 16
92: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 16
92: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 16
92: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 16
92: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 16
92: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 16
92: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 16
92: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 16
92: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 16
92: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 10 16
92: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 11 16
92: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 12 16
92: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 13 16
92: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 14 16
92: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 15 16
92: communicator.hh::sendRecv, left MPI_Wait
83: leave BufferedCommunicator::sendRecv
83: communicator.hh::BufferedCommunicator::forward, left sendRecv
83 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), after communicator.free();
83void AMG<M,X,S,PI,A>::mgc(LevelContext& levelContext), finished 
83 schwarz.hh:virtual void apply (X& v, const Y& d), before preconditioner.apply(v,d);; 
83 schwarz.hh:virtual void apply (X& v, const Y& d), before communication.copyOwnerToAll(v,v); 
83 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
83: communicator.hh::BufferedCommunicator::forward, enter sendRecv
83: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 17
83: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 17
83: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 17
83: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 17
83: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 17
83: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 17
83: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 17
83: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 17
83: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 17
83: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 17
83: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 10 17
83: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 11 17
83: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 12 17
83: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 13 17
83: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 14 17
83: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 15 17
83: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 16 17
83: communicator.hh::sendRecv, left MPI_Wait
88: leave BufferedCommunicator::sendRecv
88: communicator.hh::BufferedCommunicator::forward, left sendRecv
88 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), after communicator.free();
88void AMG<M,X,S,PI,A>::mgc(LevelContext& levelContext), finished 
88 schwarz.hh:virtual void apply (X& v, const Y& d), before preconditioner.apply(v,d);; 
88 schwarz.hh:virtual void apply (X& v, const Y& d), before communication.copyOwnerToAll(v,v); 
88 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
88: communicator.hh::BufferedCommunicator::forward, enter sendRecv
88: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 21
88: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 21
88: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 21
88: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 21
88: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 21
88: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 21
88: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 21
88: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 21
88: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 21
88: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 21
88: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 10 21
88: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 11 21
88: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 12 21
88: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 13 21
88: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 14 21
88: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 15 21
88: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 16 21
88: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 17 21
88: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 18 21
88: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 19 21
88: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 20 21
88: communicator.hh::sendRecv, left MPI_Wait
69: leave BufferedCommunicator::sendRecv
69: communicator.hh::BufferedCommunicator::forward, left sendRecv
69 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), after communicator.free();
69void AMG<M,X,S,PI,A>::mgc(LevelContext& levelContext), finished 
69 schwarz.hh:virtual void apply (X& v, const Y& d), before preconditioner.apply(v,d);; 
69 schwarz.hh:virtual void apply (X& v, const Y& d), before communication.copyOwnerToAll(v,v); 
69 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
69: communicator.hh::BufferedCommunicator::forward, enter sendRecv
69: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 7
69: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 7
69: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 7
69: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 7
69: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 7
69: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 7
69: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 7
69: communicator.hh::sendRecv, left MPI_Wait
56: leave BufferedCommunicator::sendRecv
56: communicator.hh::BufferedCommunicator::forward, left sendRecv
56 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), after communicator.free();
56void AMG<M,X,S,PI,A>::mgc(LevelContext& levelContext), finished 
56 schwarz.hh:virtual void apply (X& v, const Y& d), before preconditioner.apply(v,d);; 
56 schwarz.hh:virtual void apply (X& v, const Y& d), before communication.copyOwnerToAll(v,v); 
56 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
56: communicator.hh::BufferedCommunicator::forward, enter sendRecv
56: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 31
56: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 31
56: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 31
56: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 31
56: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 31
56: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 31
56: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 31
56: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 31
56: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 31
56: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 31
56: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 10 31
56: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 11 31
56: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 12 31
56: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 13 31
56: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 14 31
56: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 15 31
56: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 16 31
56: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 17 31
56: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 18 31
56: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 19 31
56: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 20 31
56: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 21 31
56: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 22 31
56: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 23 31
56: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 24 31
56: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 25 31
56: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 26 31
56: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 27 31
56: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status);19 schwarz.hh:virtual void apply (X& v, const Y& d), before preconditioner.apply(v,d);; 
19 schwarz.hh:virtual void apply (X& v, const Y& d), before communication.copyOwnerToAll(v,v); 
19 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
19: communicator.hh::BufferedCommunicator::forward, enter sendRecv
19: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 17
19: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 17
19: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 17
19: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 17
19: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 17
19: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 17
19: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 17
19: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 17
19: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 17
19: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 17
19: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 10 17
19: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 11 17
19: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 12 17
19: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 13 17
19: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 14 17
19: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 15 17
19: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 16 17
19: communicator.hh::sendRecv, left MPI_Wait
120: leave BufferedCommunicator::sendRecv
120: communicator.hh::BufferedCommunicator::forward, left sendRecv
120 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), after communicator.free();
120void AMG<M,X,S,PI,A>::mgc(LevelContext& levelContext), finished 
120 schwarz.hh:virtual void apply (X& v, const Y& d), before preconditioner.apply(v,d);; 
120 schwarz.hh:virtual void apply (X& v, const Y& d), before communication.copyOwnerToAll(v,v); 
120 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
120: communicator.hh::BufferedCommunicator::forward, enter sendRecv
120: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 7
120: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 7
120: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 7
120: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 7
120: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 7
120: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 7
120: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 7
120: communicator.hh::sendRecv, left MPI_Wait
114: leave BufferedCommunicator::sendRecv
114: communicator.hh::BufferedCommunicator::forward, left sendRecv
114 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), after communicator.free();
114void AMG<M,X,S,PI,A>::mgc(LevelContext& levelContext), finished 
114 schwarz.hh:virtual void apply (X& v, const Y& d), before preconditioner.apply(v,d);; 
114 schwarz.hh:virtual void apply (X& v, const Y& d), before communication.copyOwnerToAll(v,v); 
114 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
114: communicator.hh::BufferedCommunicator::forward, enter sendRecv
114: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 13
114: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 13
114: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 13
114: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 13
114: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 13
114: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 13
114: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 13
114: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 13
114: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 13
114: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 13
114: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 10 13
114: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 11 13
114: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 12 13
114: communicator.hh::sendRecv, left MPI_Wait
26 schwarz.hh:virtual void apply (X& v, const Y& d), before preconditioner.apply(v,d);; 
26 schwarz.hh:virtual void apply (X& v, const Y& d), before communication.copyOwnerToAll(v,v); 
26 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
26: communicator.hh::BufferedCommunicator::forward, enter sendRecv
26: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 26
26: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 26
26: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 26
26: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 26
26: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 26
26: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 26
26: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 26
26: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 26
26: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 26
26: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 26
26: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 10 26
26: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 11 26
26: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 12 26
26: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 13 26
26: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 14 26
26: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 15 26
26: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 16 26
26: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 17 26
26: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 18 26
26: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 19 26
26: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 20 26
26: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 21 26
26: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 22 26
26: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 23 26
26: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 24 26
26: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 25 26
26: communicator.hh::sendRecv, left MPI_Wait
14 schwarz.hh:virtual void apply (X& v, const Y& d), before preconditioner.apply(v,d);; 
14 schwarz.hh:virtual void apply (X& v, const Y& d), before communication.copyOwnerToAll(v,v); 
14 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
14: communicator.hh::BufferedCommunicator::forward, enter sendRecv
14: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 17
14: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 17
14: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 17
14: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 17
14: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 17
14: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 17
14: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 17
14: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 17
14: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 17
14: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 17
14: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 10 17
14: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 11 17
14: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 12 17
14: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 13 17
14: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 14 17
14: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 15 17
14: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 16 17
14: communicator.hh::sendRecv, left MPI_Wait
94: leave BufferedCommunicator::sendRecv
94: communicator.hh::BufferedCommunicator::forward, left sendRecv
94 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), after communicator.free();
94void AMG<M,X,S,PI,A>::mgc(LevelContext& levelContext), finished 
94 schwarz.hh:virtual void apply (X& v, const Y& d), before preconditioner.apply(v,d);; 
94 schwarz.hh:virtual void apply (X& v, const Y& d), before communication.copyOwnerToAll(v,v); 
94 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
94: communicator.hh::BufferedCommunicator::forward, enter sendRecv
94: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 21
94: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 21
94: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 21
94: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 21
94: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 21
94: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 21
94: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 21
94: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 21
94: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 21
94: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 21
94: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 10 21
94: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 11 21
94: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 12 21
94: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 13 21
94: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 14 21
94: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 15 21
94: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 16 21
94: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 17 21
94: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 18 21
94: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 19 21
94: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 20 21
94: communicator.hh::sendRecv, left MPI_Wait
103: leave BufferedCommunicator::sendRecv
103: communicator.hh::BufferedCommunicator::forward, left sendRecv
103 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), after communicator.free();
103void AMG<M,X,S,PI,A>::mgc(LevelContext& levelContext), finished 
103 schwarz.hh:virtual void apply (X& v, const Y& d), before preconditioner.apply(v,d);; 
103 schwarz.hh:virtual void apply (X& v, const Y& d), before communication.copyOwnerToAll(v,v); 
103 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
103: communicator.hh::BufferedCommunicator::forward, enter sendRecv
103: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 16
103: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 16
103: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 16
103: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 16
103: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 16
103: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 16
103: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 16
103: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 16
103: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 16
103: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 16
103: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 10 16
103: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 11 16
103: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 12 16
103: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 13 16
103: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 14 16
103: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 15 16
103: communicator.hh::sendRecv, left MPI_Wait
34 schwarz.hh:virtual void apply (X& v, const Y& d), before preconditioner.apply(v,d);; 
34 schwarz.hh:virtual void apply (X& v, const Y& d), before communication.copyOwnerToAll(v,v); 
34 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
34: communicator.hh::BufferedCommunicator::forward, enter sendRecv
34: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 14
34: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 14
34: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 14
34: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 14
34: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 14
34: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 14
34: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 14
34: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 14
34: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 14
34: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 14
34: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 10 14
34: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 11 14
34: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 12 14
34: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 13 14
34: communicator.hh::sendRecv, left MPI_Wait
51 schwarz.hh:virtual void apply (X& v, const Y& d), before preconditioner.apply(v,d);; 
51 schwarz.hh:virtual void apply (X& v, const Y& d), before communication.copyOwnerToAll(v,v); 
51 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
51: communicator.hh::BufferedCommunicator::forward, enter sendRecv
51: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 10
51: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 10
51: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 10
51: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 10
51: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 10
51: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 10
51: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 10
51: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 10
51: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 10
51: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 10
51: communicator.hh::sendRecv, left MPI_Wait
95: leave BufferedCommunicator::sendRecv
95: communicator.hh::BufferedCommunicator::forward, left sendRecv
95 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), after communicator.free();
95void AMG<M,X,S,PI,A>::mgc(LevelContext& levelContext), finished 
95 schwarz.hh:virtual void apply (X& v, const Y& d), before preconditioner.apply(v,d);; 
95 schwarz.hh:virtual void apply (X& v, const Y& d), before communication.copyOwnerToAll(v,v); 
95 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
95: communicator.hh::BufferedCommunicator::forward, enter sendRecv
95: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 7
95: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 7
95: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 7
95: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 7
95: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 7
95: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 7
95: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 7
95: communicator.hh::sendRecv, left MPI_Wait
95: leave BufferedCommunicator::sendRecv
95: communicator.hh::BufferedCommunicator::forward, left sendRecv
95 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), after communicator.free();
95 schwarz.hh:virtual void apply (X& v, const Y& d), AFTER communication.copyOwnerToAll(v,v); 
95 schwarz.hh:virtual void apply (X& v, const Y& d), before preconditioner.apply(v,d);; 
95 schwarz.hh:virtual void apply (X& v, const Y& d), before communication.copyOwnerToAll(v,v); 
95 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
95: communicator.hh::BufferedCommunicator::forward, enter sendRecv
95: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 7
75: leave BufferedCommunicator::sendRecv
75: communicator.hh::BufferedCommunicator::forward, left sendRecv
75 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), after communicator.free();
75void AMG<M,X,S,PI,A>::mgc(LevelContext& levelContext), finished 
75 schwarz.hh:virtual void apply (X& v, const Y& d), before preconditioner.apply(v,d);; 
75 schwarz.hh:virtual void apply (X& v, const Y& d), before communication.copyOwnerToAll(v,v); 
75 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
75: communicator.hh::BufferedCommunicator::forward, enter sendRecv
75: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 17
75: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 17
75: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 17
75: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 17
75: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 17
75: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 17
75: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 17
75: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 17
75: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 17
75: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 17
75: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 10 17
75: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 11 17
75: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 12 17
75: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 13 17
75: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 14 17
75: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 15 17
75: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 16 17
75: communicator.hh::sendRecv, left MPI_Wait
75: leave BufferedCommunicator::sendRecv
75: communicator.hh::BufferedCommunicator::forward, left sendRecv
75 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), after communicator.free();
75 schwarz.hh:virtual void apply (X& v, const Y& d), AFTER communication.copyOwnerToAll(v,v); 
75 schwarz.hh:virtual void apply (X& v, const Y& d), before preconditioner.apply(v,d);; 
75 schwarz.hh:virtual void apply (X& v, const Y& d), before communication.copyOwnerToAll(v,v); 
75 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
75: communicator.hh::BufferedCommunicator::forward, enter sendRecv
75: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 17
75: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 17
63: leave BufferedCommunicator::sendRecv
63: communicator.hh::BufferedCommunicator::forward, left sendRecv
63 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), after communicator.free();
63void AMG<M,X,S,PI,A>::mgc(LevelContext& levelContext), finished 
63 schwarz.hh:virtual void apply (X& v, const Y& d), before preconditioner.apply(v,d);; 
63 schwarz.hh:virtual void apply (X& v, const Y& d), before communication.copyOwnerToAll(v,v); 
63 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
63: communicator.hh::BufferedCommunicator::forward, enter sendRecv
63: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 9
63: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 9
63: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 9
63: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 9
63: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 9
63: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 9
63: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 9
63: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 9
63: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 9
63: communicator.hh::sendRecv, left MPI_Wait
63: leave BufferedCommunicator::sendRecv
63: communicator.hh::BufferedCommunicator::forward, left sendRecv
63 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), after communicator.free();
63 schwarz.hh:virtual void apply (X& v, const Y& d), AFTER communication.copyOwnerToAll(v,v); 
63 schwarz.hh:virtual void apply (X& v, const Y& d), before preconditioner.apply(v,d);; 
63 schwarz.hh:virtual void apply (X& v, const Y& d), before communication.copyOwnerToAll(v,v); 
63 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
63: communicator.hh::BufferedCommunicator::forward, enter sendRecv
63: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 9
76: leave BufferedCommunicator::sendRecv
76: communicator.hh::BufferedCommunicator::forward, left sendRecv
76 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), after communicator.free();
76void AMG<M,X,S,PI,A>::mgc(LevelContext& levelContext), finished 
76 schwarz.hh:virtual void apply (X& v, const Y& d), before preconditioner.apply(v,d);; 
76 schwarz.hh:virtual void apply (X& v, const Y& d), before communication.copyOwnerToAll(v,v); 
76 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
76: communicator.hh::BufferedCommunicator::forward, enter sendRecv
76: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 12
76: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 12
76: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 12
76: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 12
76: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 12
76: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 12
76: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 12
76: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 12
76: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 12
76: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 12
76: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 10 12
76: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 11 12
76: communicator.hh::sendRecv, left MPI_Wait
115: leave BufferedCommunicator::sendRecv
115: communicator.hh::BufferedCommunicator::forward, left sendRecv
115 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), after communicator.free();
115void AMG<M,X,S,PI,A>::mgc(LevelContext& levelContext), finished 
115 schwarz.hh:virtual void apply (X& v, const Y& d), before preconditioner.apply(v,d);; 
115 schwarz.hh:virtual void apply (X& v, const Y& d), before communication.copyOwnerToAll(v,v); 
115 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
115: communicator.hh::BufferedCommunicator::forward, enter sendRecv
115: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 11
115: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 11
115: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 11
115: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 11
115: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 11
115: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 11
115: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 11
115: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 11
115: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 11
115: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 11
115: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 10 11
115: communicator.hh::sendRecv, left MPI_Wait
115: leave BufferedCommunicator::sendRecv
115: communicator.hh::BufferedCommunicator::forward, left sendRecv
115 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), after communicator.free();
115 schwarz.hh:virtual void apply (X& v, const Y& d), AFTER communication.copyOwnerToAll(v,v); 
115 schwarz.hh:virtual void apply (X& v, const Y& d), before preconditioner.apply(v,d);; 
115 schwarz.hh:virtual void apply (X& v, const Y& d), before communication.copyOwnerToAll(v,v); 
115 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
115: communicator.hh::BufferedCommunicator::forward, enter sendRecv
115: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 11
115: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 11
115: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 11
18 schwarz.hh:virtual void apply (X& v, const Y& d), before preconditioner.apply(v,d);; 
18 schwarz.hh:virtual void apply (X& v, const Y& d), before communication.copyOwnerToAll(v,v); 
18 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
18: communicator.hh::BufferedCommunicator::forward, enter sendRecv
18: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 14
18: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 14
18: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 14
18: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 14
18: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 14
18: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 14
18: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 14
18: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 14
18: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 14
18: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 14
18: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 10 14
18: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 11 14
18: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 12 14
18: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 13 14
18: communicator.hh::sendRecv, left MPI_Wait
53 schwarz.hh:virtual void apply (X& v, const Y& d), before preconditioner.apply(v,d);; 
53 schwarz.hh:virtual void apply (X& v, const Y& d), before communication.copyOwnerToAll(v,v); 
53 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
53: communicator.hh::BufferedCommunicator::forward, enter sendRecv
53: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 17
53: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 17
53: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 17
53: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 17
53: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 17
53: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 17
53: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 17
53: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 17
53: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 17
53: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 17
53: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 10 17
53: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 11 17
53: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 12 17
53: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 13 17
53: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 14 17
53: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 15 17
53: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 16 17
53: communicator.hh::sendRecv, left MPI_Wait
23 schwarz.hh:virtual void apply (X& v, const Y& d), before preconditioner.apply(v,d);; 
23 schwarz.hh:virtual void apply (X& v, const Y& d), before communication.copyOwnerToAll(v,v); 
23 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
23: communicator.hh::BufferedCommunicator::forward, enter sendRecv
23: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 11
23: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 11
23: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 11
23: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 11
23: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 11
23: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 11
23: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 11
23: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 11
23: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 11
23: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 11
23: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 10 11
23: communicator.hh::sendRecv, left MPI_Wait
23: leave BufferedCommunicator::sendRecv
23: communicator.hh::BufferedCommunicator::forward, left sendRecv
23 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), after communicator.free();
23 schwarz.hh:virtual void apply (X& v, const Y& d), AFTER communication.copyOwnerToAll(v,v); 
23 schwarz.hh:virtual void apply (X& v, const Y& d), before preconditioner.apply(v,d);; 
23 schwarz.hh:virtual void apply (X& v, const Y& d), before communication.copyOwnerToAll(v,v); 
23 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
23: communicator.hh::BufferedCommunicator::forward, enter sendRecv
23: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 11
119: leave BufferedCommunicator::sendRecv
119: communicator.hh::BufferedCommunicator::forward, left sendRecv
119 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), after communicator.free();
119void AMG<M,X,S,PI,A>::mgc(LevelContext& levelContext), finished 
119 schwarz.hh:virtual void apply (X& v, const Y& d), before preconditioner.apply(v,d);; 
119 schwarz.hh:virtual void apply (X& v, const Y& d), before communication.copyOwnerToAll(v,v); 
119 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
119: communicator.hh::BufferedCommunicator::forward, enter sendRecv
119: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 13
119: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 13
119: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 13
119: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 13
119: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 13
119: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 13
119: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 13
119: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 13
119: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 13
119: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 13
119: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 10 13
119: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 11 13
119: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 12 13
119: communicator.hh::sendRecv, left MPI_Wait
119: leave BufferedCommunicator::sendRecv
119: communicator.hh::BufferedCommunicator::forward, left sendRecv
119 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), after communicator.free();
119 schwarz.hh:virtual void apply (X& v, const Y& d), AFTER communication.copyOwnerToAll(v,v); 
119 schwarz.hh:virtual void apply (X& v, const Y& d), before preconditioner.apply(v,d);; 
119 schwarz.hh:virtual void apply (X& v, const Y& d), before communication.copyOwnerToAll(v,v); 
119 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
119: communicator.hh::BufferedCommunicator::forward, enter sendRecv
119: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 13
119: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 13
50 schwarz.hh:virtual void apply (X& v, const Y& d), before preconditioner.apply(v,d);; 
50 schwarz.hh:virtual void apply (X& v, const Y& d), before communication.copyOwnerToAll(v,v); 
50 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
50: communicator.hh::BufferedCommunicator::forward, enter sendRecv
50: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 10
50: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 10
50: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 10
50: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 10
50: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 10
50: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 10
50: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 10
50: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 10
50: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 10
50: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 10
50: communicator.hh::sendRecv, left MPI_Wait
84: leave BufferedCommunicator::sendRecv
84: communicator.hh::BufferedCommunicator::forward, left sendRecv
84 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), after communicator.free();
84void AMG<M,X,S,PI,A>::mgc(LevelContext& levelContext), finished 
84 schwarz.hh:virtual void apply (X& v, const Y& d), before preconditioner.apply(v,d);; 
84 schwarz.hh:virtual void apply (X& v, const Y& d), before communication.copyOwnerToAll(v,v); 
84 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
84: communicator.hh::BufferedCommunicator::forward, enter sendRecv
84: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 18
84: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 18
84: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 18
84: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 18
84: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 18
84: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 18
84: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 18
84: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 18
84: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 18
84: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 18
84: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 10 18
84: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 11 18
84: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 12 18
84: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 13 18
84: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 14 18
84: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 15 18
84: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 16 18
84: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 17 18
84: communicator.hh::sendRecv, left MPI_Wait
111: leave BufferedCommunicator::sendRecv
111: communicator.hh::BufferedCommunicator::forward, left sendRecv
111 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), after communicator.free();
111void AMG<M,X,S,PI,A>::mgc(LevelContext& levelContext), finished 
111 schwarz.hh:virtual void apply (X& v, const Y& d), before preconditioner.apply(v,d);; 
111 schwarz.hh:virtual void apply (X& v, const Y& d), before communication.copyOwnerToAll(v,v); 
111 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
111: communicator.hh::BufferedCommunicator::forward, enter sendRecv
111: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 11
111: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 11
111: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 11
111: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 11
111: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 11
111: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 11
111: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 11
111: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 11
111: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 11
111: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 11
111: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 10 11
111: communicator.hh::sendRecv, left MPI_Wait
111: leave BufferedCommunicator::sendRecv
111: communicator.hh::BufferedCommunicator::forward, left sendRecv
111 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), after communicator.free();
111 schwarz.hh:virtual void apply (X& v, const Y& d), AFTER communication.copyOwnerToAll(v,v); 
111 schwarz.hh:virtual void apply (X& v, const Y& d), before preconditioner.apply(v,d);; 
111 schwarz.hh:virtual void apply (X& v, const Y& d), before communication.copyOwnerToAll(v,v); 
111 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
111: communicator.hh::BufferedCommunicator::forward, enter sendRecv
111: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 11
111: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 11
111: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 11
72: leave BufferedCommunicator::sendRecv
72: communicator.hh::BufferedCommunicator::forward, left sendRecv
72 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), after communicator.free();
72void AMG<M,X,S,PI,A>::mgc(LevelContext& levelContext), finished 
72 schwarz.hh:virtual void apply (X& v, const Y& d), before preconditioner.apply(v,d);; 
72 schwarz.hh:virtual void apply (X& v, const Y& d), before communication.copyOwnerToAll(v,v); 
72 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
72: communicator.hh::BufferedCommunicator::forward, enter sendRecv
72: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 20
72: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 20
72: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 20
72: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 20
72: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 20
72: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 20
72: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 20
72: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 20
72: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 20
72: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 20
72: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 10 20
72: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 11 20
72: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 12 20
72: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 13 20
72: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 14 20
72: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 15 20
72: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 16 20
72: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 17 20
72: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 18 20
72: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 19 20
72: communicator.hh::sendRecv, left MPI_Wait
30 schwarz.hh:virtual void apply (X& v, const Y& d), before preconditioner.apply(v,d);; 
30 schwarz.hh:virtual void apply (X& v, const Y& d), before communication.copyOwnerToAll(v,v); 
30 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
30: communicator.hh::BufferedCommunicator::forward, enter sendRecv
30: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 6
30: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 6
30: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 6
30: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 6
30: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 6
30: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 6
30: communicator.hh::sendRecv, left MPI_Wait
30: leave BufferedCommunicator::sendRecv
30: communicator.hh::BufferedCommunicator::forward, left sendRecv
30 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), after communicator.free();
30 schwarz.hh:virtual void apply (X& v, const Y& d), AFTER communication.copyOwnerToAll(v,v); 
30 schwarz.hh:virtual void apply (X& v, const Y& d), before preconditioner.apply(v,d);; 
30 schwarz.hh:virtual void apply (X& v, const Y& d), before communication.copyOwnerToAll(v,v); 
30 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
30: communicator.hh::BufferedCommunicator::forward, enter sendRecv
30: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 6
30: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 6
61: leave BufferedCommunicator::sendRecv
61: communicator.hh::BufferedCommunicator::forward, left sendRecv
61 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), after communicator.free();
61void AMG<M,X,S,PI,A>::mgc(LevelContext& levelContext), finished 
61 schwarz.hh:virtual void apply (X& v, const Y& d), before preconditioner.apply(v,d);; 
61 schwarz.hh:virtual void apply (X& v, const Y& d), before communication.copyOwnerToAll(v,v); 
61 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
61: communicator.hh::BufferedCommunicator::forward, enter sendRecv
61: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 9
61: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 9
61: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 9
61: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 9
61: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 9
61: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 9
61: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 9
61: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 9
61: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 9
61: communicator.hh::sendRecv, left MPI_Wait
61: leave BufferedCommunicator::sendRecv
61: communicator.hh::BufferedCommunicator::forward, left sendRecv
61 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), after communicator.free();
61 schwarz.hh:virtual void apply (X& v, const Y& d), AFTER communication.copyOwnerToAll(v,v); 
61 schwarz.hh:virtual void apply (X& v, const Y& d), before preconditioner.apply(v,d);; 
61 schwarz.hh:virtual void apply (X& v, const Y& d), before communication.copyOwnerToAll(v,v); 
61 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
61: communicator.hh::BufferedCommunicator::forward, enter sendRecv
61: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 9
61: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 9
61: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 9
3: leave BufferedCommunicator::sendRecv
3: communicator.hh::BufferedCommunicator::forward, left sendRecv
3 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), after communicator.free();
3void AMG<M,X,S,PI,A>::mgc(LevelContext& levelContext), finished 
3 schwarz.hh:virtual void apply (X& v, const Y& d), before preconditioner.apply(v,d);; 
3 schwarz.hh:virtual void apply (X& v, const Y& d), before communication.copyOwnerToAll(v,v); 
3 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
3: communicator.hh::BufferedCommunicator::forward, enter sendRecv
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 9
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 9
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 9
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 9
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 9
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 9
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 9
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 9
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 9
3: communicator.hh::sendRecv, left MPI_Wait
3: leave BufferedCommunicator::sendRecv
3: communicator.hh::BufferedCommunicator::forward, left sendRecv
3 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), after communicator.free();
3 schwarz.hh:virtual void apply (X& v, const Y& d), AFTER communication.copyOwnerToAll(v,v); 
3 schwarz.hh:virtual void apply (X& v, const Y& d), before preconditioner.apply(v,d);; 
3 schwarz.hh:virtual void apply (X& v, const Y& d), before communication.copyOwnerToAll(v,v); 
3 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
3: communicator.hh::BufferedCommunicator::forward, enter sendRecv
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 9
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 9
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 9
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 9
52 schwarz.hh:virtual void apply (X& v, const Y& d), before preconditioner.apply(v,d);; 
52 schwarz.hh:virtual void apply (X& v, const Y& d), before communication.copyOwnerToAll(v,v); 
52 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
52: communicator.hh::BufferedCommunicator::forward, enter sendRecv
52: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 26
52: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 26
52: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 26
52: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 26
52: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 26
52: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 26
52: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 26
52: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 26
52: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 26
52: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 26
52: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 10 26
52: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 11 26
52: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 12 26
52: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 13 26
52: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 14 26
52: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 15 26
52: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 16 26
52: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 17 26
52: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 18 26
52: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 19 26
52: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 20 26
52: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 21 26
52: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 22 26
52: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 23 26
52: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 24 26
52: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 25 26
52: communicator.hh::sendRecv, left MPI_Wait
102: leave BufferedCommunicator::sendRecv
102: communicator.hh::BufferedCommunicator::forward, left sendRecv
102 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), after communicator.free();
102void AMG<M,X,S,PI,A>::mgc(LevelContext& levelContext), finished 
102 schwarz.hh:virtual void apply (X& v, const Y& d), before preconditioner.apply(v,d);; 
102 schwarz.hh:virtual void apply (X& v, const Y& d), before communication.copyOwnerToAll(v,v); 
102 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
102: communicator.hh::BufferedCommunicator::forward, enter sendRecv
102: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 15
102: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 15
102: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 15
102: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 15
102: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 15
102: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 15
102: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 15
102: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 15
102: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 15
102: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 15
102: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 10 15
102: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 11 15
102: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 12 15
102: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 13 15
102: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 14 15
102: communicator.hh::sendRecv, left MPI_Wait
1 schwarz.hh:virtual void apply (X& v, const Y& d), before preconditioner.apply(v,d);; 
1 schwarz.hh:virtual void apply (X& v, const Y& d), before communication.copyOwnerToAll(v,v); 
1 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
1: communicator.hh::BufferedCommunicator::forward, enter sendRecv
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 5
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 5
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 5
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 5
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 5
1: communicator.hh::sendRecv, left MPI_Wait
1: leave BufferedCommunicator::sendRecv
1: communicator.hh::BufferedCommunicator::forward, left sendRecv
1 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), after communicator.free();
1 schwarz.hh:virtual void apply (X& v, const Y& d), AFTER communication.copyOwnerToAll(v,v); 
1 schwarz.hh:virtual void apply (X& v, const Y& d), before preconditioner.apply(v,d);; 
1 schwarz.hh:virtual void apply (X& v, const Y& d), before communication.copyOwnerToAll(v,v); 
1 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
1: communicator.hh::BufferedCommunicator::forward, enter sendRecv
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 5
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 5
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 5
ize(), recvRequests, &finished, &status); 115 117
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 116 117
0: communicator.hh::sendRecv, left MPI_Wait
0: leave BufferedCommunicator::sendRecv
0: communicator.hh::BufferedCommunicator::forward, left sendRecv
0 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
0: communicator.hh::BufferedCommunicator::forward, enter sendRecv
0: communicator.hh::sendRecv, left MPI_Wait
0: leave BufferedCommunicator::sendRecv
0: communicator.hh::BufferedCommunicator::forward, left sendRecv
0 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), after communicator.free();
0: communicator.hh::sendRecv, left MPI_Wait
0: leave BufferedCommunicator::sendRecv
0 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
0: communicator.hh::BufferedCommunicator::forward, enter sendRecv
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 3
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 3
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 3
0: communicator.hh::sendRecv, left MPI_Wait
0: leave BufferedCommunicator::sendRecv
0: communicator.hh::BufferedCommunicator::forward, left sendRecv
0 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), after communicator.free();
0void AMG<M,X,S,PI,A>::mgc(LevelContext& levelContext), finished 
0 schwarz.hh:virtual void apply (X& v, const Y& d), before preconditioner.apply(v,d);; 
0 schwarz.hh:virtual void apply (X& v, const Y& d), before communication.copyOwnerToAll(v,v); 
0 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
0: communicator.hh::BufferedCommunicator::forward, enter sendRecv
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 3
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 3
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 3
0: communicator.hh::sendRecv, left MPI_Wait
0: leave BufferedCommunicator::sendRecv
0: communicator.hh::BufferedCommunicator::forward, left sendRecv
0 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), after communicator.free();
0 schwarz.hh:virtual void apply (X& v, const Y& d), AFTER communication.copyOwnerToAll(v,v); 
0 schwarz.hh:virtual void apply (X& v, const Y& d), before preconditioner.apply(v,d);; 
0 schwarz.hh:virtual void apply (X& v, const Y& d), before communication.copyOwnerToAll(v,v); 
0 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
0: communicator.hh::BufferedCommunicator::forward, enter sendRecv
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 3
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 3
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 3
0: communicator.hh::sendRecv, left MPI_Wait
0: leave BufferedCommunicator::sendRecv
0: communicator.hh::BufferedCommunicator::forward, left sendRecv
0 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), after communicator.free();
0 schwarz.hh:virtual void apply (X& v, const Y& d), AFTER communication.copyOwnerToAll(v,v); 
0void AMG<M,X,S,PI,A>::mgc(LevelContext& levelContext), finished 
0 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), AFTER mgc(levelContext) 
0 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), finished 
0   AFTER _prec->ap100: leave BufferedCommunicator::sendRecv
100: communicator.hh::BufferedCommunicator::forward, left sendRecv
100 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), after communicator.free();
100void AMG<M,X,S,PI,A>::mgc(LevelContext& levelContext), finished 
100 schwarz.hh:virtual void apply (X& v, const Y& d), before preconditioner.apply(v,d);; 
100 schwarz.hh:virtual void apply (X& v, const Y& d), before communication.copyOwnerToAll(v,v); 
100 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
100: communicator.hh::BufferedCommunicator::forward, enter sendRecv
100: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 6
100: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 6
100: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 6
100: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 6
100: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 6
100: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 6
100: communicator.hh::sendRecv, left MPI_Wait
100: leave BufferedCommunicator::sendRecv
100: communicator.hh::BufferedCommunicator::forward, left sendRecv
100 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), after communicator.free();
100 schwarz.hh:virtual void apply (X& v, const Y& d), AFTER communication.copyOwnerToAll(v,v); 
100 schwarz.hh:virtual void apply (X& v, const Y& d), before preconditioner.apply(v,d);; 
28 schwarz.hh:virtual void apply (X& v, const Y& d), before preconditioner.apply(v,d);; 
28 schwarz.hh:virtual void apply (X& v, const Y& d), before communication.copyOwnerToAll(v,v); 
28 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
28: communicator.hh::BufferedCommunicator::forward, enter sendRecv
28: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 18
28: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 18
28: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 18
28: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 18
28: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 18
28: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 18
28: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 18
28: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 18
28: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 18
28: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 18
28: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 10 18
28: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 11 18
28: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 12 18
28: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 13 18
28: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 14 18
28: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 15 18
28: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 16 18
28: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 17 18
28: communicator.hh::sendRecv, left MPI_Wait
12 schwarz.hh:virtual void apply (X& v, const Y& d), before preconditioner.apply(v,d);; 
12 schwarz.hh:virtual void apply (X& v, const Y& d), before communication.copyOwnerToAll(v,v); 
12 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
12: communicator.hh::BufferedCommunicator::forward, enter sendRecv
12: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 26
12: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 26
12: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 26
12: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 26
12: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 26
12: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 26
12: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 26
12: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 26
12: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 26
12: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 26
12: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 10 26
12: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 11 26
12: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 12 26
12: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 13 26
12: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 14 26
12: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 15 26
12: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 16 26
12: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 17 26
12: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 18 26
12: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 19 26
12: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 20 26
12: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 21 26
12: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 22 26
12: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 23 26
12: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 24 26
12: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 25 26
12: communicator.hh::sendRecv, left MPI_Wait
81: leave BufferedCommunicator::sendRecv
81: communicator.hh::BufferedCommunicator::forward, left sendRecv
81 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), after communicator.free();
81void AMG<M,X,S,PI,A>::mgc(LevelContext& levelContext), finished 
81 schwarz.hh:virtual void apply (X& v, const Y& d), before preconditioner.apply(v,d);; 
81 schwarz.hh:virtual void apply (X& v, const Y& d), before communication.copyOwnerToAll(v,v); 
81 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
81: communicator.hh::BufferedCommunicator::forward, enter sendRecv
81: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 11
81: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 11
81: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 11
81: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 11
81: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 11
81: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 11
81: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 11
81: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 11
81: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 11
81: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 11
81: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 10 11
81: communicator.hh::sendRecv, left MPI_Wait
86: leave BufferedCommunicator::sendRecv
86: communicator.hh::BufferedCommunicator::forward, left sendRecv
86 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), after communicator.free();
86void AMG<M,X,S,PI,A>::mgc(LevelContext& levelContext), finished 
86 schwarz.hh:virtual void apply (X& v, const Y& d), before preconditioner.apply(v,d);; 
86 schwarz.hh:virtual void apply (X& v, const Y& d), before communication.copyOwnerToAll(v,v); 
86 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
86: communicator.hh::BufferedCommunicator::forward, enter sendRecv
86: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 31
86: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 31
86: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 31
86: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 31
86: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 31
86: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 31
86: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 31
86: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 31
86: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 31
86: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 31
86: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 10 31
86: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 11 31
86: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 12 31
86: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 13 31
86: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 14 31
86: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 15 31
86: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 16 31
86: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 17 31
86: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 18 31
86: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 19 31
86: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 20 31
86: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 21 31
86: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 22 31
86: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 23 31
86: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 24 31
86: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 25 31
86: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 26 31
86: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 27 31
86: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status);13 schwarz.hh:virtual void apply (X& v, const Y& d), before preconditioner.apply(v,d);; 
13 schwarz.hh:virtual void apply (X& v, const Y& d), before communication.copyOwnerToAll(v,v); 
13 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
13: communicator.hh::BufferedCommunicator::forward, enter sendRecv
13: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 11
13: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 11
13: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 11
13: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 11
13: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 11
13: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 11
13: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 11
13: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 11
13: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 11
13: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 11
13: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 10 11
13: communicator.hh::sendRecv, left MPI_Wait
13: leave BufferedCommunicator::sendRecv
13: communicator.hh::BufferedCommunicator::forward, left sendRecv
13 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), after communicator.free();
13 schwarz.hh:virtual void apply (X& v, const Y& d), AFTER communication.copyOwnerToAll(v,v); 
13 schwarz.hh:virtual void apply (X& v, const Y& d), before preconditioner.apply(v,d);; 
13 schwarz.hh:virtual void apply (X& v, const Y& d), before communication.copyOwnerToAll(v,v); 
13 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
13: communicator.hh::BufferedCommunicator::forward, enter sendRecv
13: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 11
13: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 11
13: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 11
13: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 11
13: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 11
60 schwarz.hh:virtual void apply (X& v, const Y& d), before preconditioner.apply(v,d);; 
60 schwarz.hh:virtual void apply (X& v, const Y& d), before communication.copyOwnerToAll(v,v); 
60 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
60: communicator.hh::BufferedCommunicator::forward, enter sendRecv
60: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 6
60: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 6
60: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 6
60: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 6
60: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 6
60: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 6
60: communicator.hh::sendRecv, left MPI_Wait
60: leave BufferedCommunicator::sendRecv
60: communicator.hh::BufferedCommunicator::forward, left sendRecv
60 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), after communicator.free();
60 schwarz.hh:virtual void apply (X& v, const Y& d), AFTER communication.copyOwnerToAll(v,v); 
60 schwarz.hh:virtual void apply (X& v, const Y& d), before preconditioner.apply(v,d);; 
60 schwarz.hh:virtual void apply (X& v, const Y& d), before communication.copyOwnerToAll(v,v); 
60 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
60: communicator.hh::BufferedCommunicator::forward, enter sendRecv
60: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 6
60: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 6
60: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 6
29 schwarz.hh:virtual void apply (X& v, const Y& d), before preconditioner.apply(v,d);; 
29 schwarz.hh:virtual void apply (X& v, const Y& d), before communication.copyOwnerToAll(v,v); 
29 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
29: communicator.hh::BufferedCommunicator::forward, enter sendRecv
29: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 11
29: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 11
29: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 11
29: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 11
29: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 11
29: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 11
29: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 11
29: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 11
29: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 11
29: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 11
29: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 10 11
29: communicator.hh::sendRecv, left MPI_Wait
29: leave BufferedCommunicator::sendRecv
29: communicator.hh::BufferedCommunicator::forward, left sendRecv
29 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), after communicator.free();
5 schwarz.hh:virtual void apply (X& v, const Y& d), before preconditioner.apply(v,d);; 
5 schwarz.hh:virtual void apply (X& v, const Y& d), before communication.copyOwnerToAll(v,v); 
5 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
5: communicator.hh::BufferedCommunicator::forward, enter sendRecv
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 11
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 11
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 11
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 11
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 11
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 11
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 11
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 11
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 11
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 11
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 10 11
5: communicator.hh::sendRecv, left MPI_Wait
5: leave BufferedCommunicator::sendRecv
5: communicator.hh::BufferedCommunicator::forward, left sendRecv
5 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), after communicator.free();
5 schwarz.hh:virtual void apply (X& v, const Y& d), AFTER communication.copyOwnerToAll(v,v); 
5 schwarz.hh:virtual void apply (X& v, const Y& d), before preconditioner.apply(v,d);; 
5 schwarz.hh:virtual void apply (X& v, const Y& d), before communication.copyOwnerToAll(v,v); 
5 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
5: communicator.hh::BufferedCommunicator::forward, enter sendRecv
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 11
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 11
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 11
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 11
101: leave BufferedCommunicator::sendRecv
101: communicator.hh::BufferedCommunicator::forward, left sendRecv
101 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), after communicator.free();
101void AMG<M,X,S,PI,A>::mgc(LevelContext& levelContext), finished 
101 schwarz.hh:virtual void apply (X& v, const Y& d), before preconditioner.apply(v,d);; 
101 schwarz.hh:virtual void apply (X& v, const Y& d), before communication.copyOwnerToAll(v,v); 
101 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
101: communicator.hh::BufferedCommunicator::forward, enter sendRecv
101: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 24
101: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 24
101: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 24
101: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 24
101: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 24
101: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 24
101: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 24
101: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 24
101: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 24
101: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 24
101: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 10 24
101: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 11 24
101: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 12 24
101: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 13 24
101: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 14 24
101: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 15 24
101: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 16 24
101: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 17 24
101: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 18 24
101: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 19 24
101: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 20 24
101: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 21 24
101: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 22 24
101: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 23 24
101: communicator.hh::sendRecv, left MPI_Wait
101: leave BufferedCommunicator::sendRecv
101: communicator.hh::BufferedCommunicator::forward, left sendRecv
101 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), after communicator.free();
101 schwarz.hh:virtual void apply (X& v, const Y& d), AFTER communication.copyOwnerToAll(v,v); 
101 schwarz.hh:virtual void apply (X& v, const Y& d), before preconditioner.apply(v,d);; 
42 schwarz.hh:virtual void apply (X& v, const Y& d), before preconditioner.apply(v,d);; 
42 schwarz.hh:virtual void apply (X& v, const Y& d), before communication.copyOwnerToAll(v,v); 
42 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
42: communicator.hh::BufferedCommunicator::forward, enter sendRecv
42: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 20
42: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 20
42: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 20
42: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 20
42: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 20
42: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 20
42: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 20
42: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 20
42: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 20
42: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 20
42: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 10 20
42: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 11 20
42: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 12 20
42: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 13 20
42: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 14 20
42: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 15 20
42: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 16 20
42: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 17 20
42: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 18 20
42: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 19 20
42: communicator.hh::sendRecv, left MPI_Wait
42: leave BufferedCommunicator::sendRecv
42: communicator.hh::BufferedCommunicator::forward, left sendRecv
42 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), after communicator.free();
42 schwarz.hh:virtual void apply (X& v, const Y& d), AFTER communication.copyOwnerToAll(v,v); 
42 schwarz.hh:virtual void apply (X& v, const Y& d), before preconditioner.apply(v,d);; 
42 schwarz.hh:virtual void apply (X& v, const Y& d), before communication.copyOwnerToAll(v,v); 
42 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
42: communicator.hh::BufferedCommunicator::forward, enter sendRecv
118: leave BufferedCommunicator::sendRecv
118: communicator.hh::BufferedCommunicator::forward, left sendRecv
118 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), after communicator.free();
118void AMG<M,X,S,PI,A>::mgc(LevelContext& levelContext), finished 
118 schwarz.hh:virtual void apply (X& v, const Y& d), before preconditioner.apply(v,d);; 
118 schwarz.hh:virtual void apply (X& v, const Y& d), before communication.copyOwnerToAll(v,v); 
118 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
118: communicator.hh::BufferedCommunicator::forward, enter sendRecv
118: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 14
118: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 14
118: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 14
118: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 14
118: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 14
118: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 14
118: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 14
118: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 14
118: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 14
118: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 14
118: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 10 14
118: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 11 14
118: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 12 14
118: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 13 14
118: communicator.hh::sendRecv, left MPI_Wait
15 schwarz.hh:virtual void apply (X& v, const Y& d), before preconditioner.apply(v,d);; 
15 schwarz.hh:virtual void apply (X& v, const Y& d), before communication.copyOwnerToAll(v,v); 
15 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
15: communicator.hh::BufferedCommunicator::forward, enter sendRecv
15: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 8
15: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 8
15: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 8
15: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 8
15: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 8
15: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 8
15: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 8
15: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 8
15: communicator.hh::sendRecv, left MPI_Wait
15: leave BufferedCommunicator::sendRecv
15: communicator.hh::BufferedCommunicator::forward, left sendRecv
15 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), after communicator.free();
15 schwarz.hh:virtual void apply (X& v, const Y& d), AFTER communication.copyOwnerToAll(v,v); 
15 schwarz.hh:virtual void apply (X& v, const Y& d), before preconditioner.apply(v,d);; 
15 schwarz.hh:virtual void apply (X& v, const Y& d), before communication.copyOwnerToAll(v,v); 
15 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
15: communicator.hh::BufferedCommunicator::forward, enter sendRecv
15: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 8
15: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 8
10 schwarz.hh:virtual void apply (X& v, const Y& d), before preconditioner.apply(v,d);; 
10 schwarz.hh:virtual void apply (X& v, const Y& d), before communication.copyOwnerToAll(v,v); 
10 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
10: communicator.hh::BufferedCommunicator::forward, enter sendRecv
10: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 11
10: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 11
10: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 11
10: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 11
10: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 11
10: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 11
10: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 11
10: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 11
10: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 11
10: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 11
10: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 10 11
10: communicator.hh::sendRecv, left MPI_Wait
10: leave BufferedCommunicator::sendRecv
10: communicator.hh::BufferedCommunicator::forward, left sendRecv
10 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), after communicator.free();
10 schwarz.hh:virtual void apply (X& v, const Y& d), AFTER communication.copyOwnerToAll(v,v); 
10 schwarz.hh:virtual void apply (X& v, const Y& d), before preconditioner.apply(v,d);; 
85 schwarz.hh:virtual void apply (X& v, const Y& d), before preconditioner.apply(v,d);; 
85 schwarz.hh:virtual void apply (X& v, const Y& d), before communication.copyOwnerToAll(v,v); 
85 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
85: communicator.hh::BufferedCommunicator::forward, enter sendRecv
85: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 11
85: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 11
85: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 11
85: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 11
85: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 11
85: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 11
85: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 11
85: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 11
85: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 11
85: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 11
85: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 10 11
85: communicator.hh::sendRecv, left MPI_Wait
85: leave BufferedCommunicator::sendRecv
85: communicator.hh::BufferedCommunicator::forward, left sendRecv
85 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), after communicator.free();
98: leave BufferedCommunicator::sendRecv
98: communicator.hh::BufferedCommunicator::forward, left sendRecv
98 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), after communicator.free();
98void AMG<M,X,S,PI,A>::mgc(LevelContext& levelContext), finished 
98 schwarz.hh:virtual void apply (X& v, const Y& d), before preconditioner.apply(v,d);; 
98 schwarz.hh:virtual void apply (X& v, const Y& d), before communication.copyOwnerToAll(v,v); 
98 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
98: communicator.hh::BufferedCommunicator::forward, enter sendRecv
98: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 18
98: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 18
98: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 18
98: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 18
98: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 18
98: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 18
98: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 18
98: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 18
98: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 18
98: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 18
98: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 10 18
98: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 11 18
98: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 12 18
98: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 13 18
98: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 14 18
98: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 15 18
98: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 16 18
98: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 17 18
98: communicator.hh::sendRecv, left MPI_Wait
47 schwarz.hh:virtual void apply (X& v, const Y& d), before preconditioner.apply(v,d);; 
47 schwarz.hh:virtual void apply (X& v, const Y& d), before communication.copyOwnerToAll(v,v); 
47 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
47: communicator.hh::BufferedCommunicator::forward, enter sendRecv
47: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 6
47: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 6
47: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 6
47: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 6
47: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 6
47: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 6
47: communicator.hh::sendRecv, left MPI_Wait
47: leave BufferedCommunicator::sendRecv
47: communicator.hh::BufferedCommunicator::forward, left sendRecv
47 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), after communicator.free();
47 schwarz.hh:virtual void apply (X& v, const Y& d), AFTER communication.copyOwnerToAll(v,v); 
47 schwarz.hh:virtual void apply (X& v, const Y& d), before preconditioner.apply(v,d);; 
45 schwarz.hh:virtual void apply (X& v, const Y& d), before preconditioner.apply(v,d);; 
45 schwarz.hh:virtual void apply (X& v, const Y& d), before communication.copyOwnerToAll(v,v); 
45 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
45: communicator.hh::BufferedCommunicator::forward, enter sendRecv
45: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 17
45: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 17
45: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 17
45: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 17
45: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 17
45: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 17
45: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 17
45: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 17
45: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 17
45: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 17
45: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 10 17
45: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 11 17
45: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 12 17
45: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 13 17
45: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 14 17
45: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 15 17
45: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 16 17
45: communicator.hh::sendRecv, left MPI_Wait
45: leave BufferedCommunicator::sendRecv
45: communicator.hh::BufferedCommunicator::forward, left sendRecv
45 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), after communicator.free();
45 schwarz.hh:virtual void apply (X& v, const Y& d), AFTER communication.copyOwnerToAll(v,v); 
45 schwarz.hh:virtual void apply (X& v, const Y& d), before preconditioner.apply(v,d);; 
17 schwarz.hh:virtual void apply (X& v, const Y& d), before preconditioner.apply(v,d);; 
17 schwarz.hh:virtual void apply (X& v, const Y& d), before communication.copyOwnerToAll(v,v); 
17 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
17: communicator.hh::BufferedCommunicator::forward, enter sendRecv
17: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 3
17: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 3
17: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 3
17: communicator.hh::sendRecv, left MPI_Wait
17: leave BufferedCommunicator::sendRecv
17: communicator.hh::BufferedCommunicator::forward, left sendRecv
17 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), after communicator.free();
17 schwarz.hh:virtual void apply (X& v, const Y& d), AFTER communication.copyOwnerToAll(v,v); 
17 schwarz.hh:virtual void apply (X& v, const Y& d), before preconditioner.apply(v,d);; 
17 schwarz.hh:virtual void apply (X& v, const Y& d), before communication.copyOwnerToAll(v,v); 
17 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
17: communicator.hh::BufferedCommunicator::forward, enter sendRecv
17: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 3
116: leave BufferedCommunicator::sendRecv
116: communicator.hh::BufferedCommunicator::forward, left sendRecv
116 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), after communicator.free();
116void AMG<M,X,S,PI,A>::mgc(LevelContext& levelContext), finished 
116 schwarz.hh:virtual void apply (X& v, const Y& d), before preconditioner.apply(v,d);; 
116 schwarz.hh:virtual void apply (X& v, const Y& d), before communication.copyOwnerToAll(v,v); 
116 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
116: communicator.hh::BufferedCommunicator::forward, enter sendRecv
116: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 7
116: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 7
116: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 7
116: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 7
116: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 7
116: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 7
116: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 7
116: communicator.hh::sendRecv, left MPI_Wait
116: leave BufferedCommunicator::sendRecv
116: communicator.hh::BufferedCommunicator::forward, left sendRecv
116 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), after communicator.free();
116 schwarz.hh:virtual void apply (X& v, const Y& d), AFTER communication.copyOwnerToAll(v,v); 
116 schwarz.hh:virtual void apply (X& v, const Y& d), before preconditioner.apply(v,d);; 
116 schwarz.hh:virtual void apply (X& v, const Y& d), before communication.copyOwnerToAll(v,v); 
116 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
116: communicator.hh::BufferedCommunicator::forward, enter sendRecv
116: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 7
116: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 7
116: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 7
66: leave BufferedCommunicator::sendRecv
66: communicator.hh::BufferedCommunicator::forward, left sendRecv
66 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), after communicator.free();
66void AMG<M,X,S,PI,A>::mgc(LevelContext& levelContext), finished 
66 schwarz.hh:virtual void apply (X& v, const Y& d), before preconditioner.apply(v,d);; 
66 schwarz.hh:virtual void apply (X& v, const Y& d), before communication.copyOwnerToAll(v,v); 
66 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
66: communicator.hh::BufferedCommunicator::forward, enter sendRecv
66: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 15
66: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 15
66: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 15
66: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 15
66: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 15
66: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 15
66: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 15
66: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 15
66: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 15
66: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 15
66: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 10 15
66: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 11 15
66: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 12 15
66: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 13 15
66: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 14 15
66: communicator.hh::sendRecv, left MPI_Wait
66: leave BufferedCommunicator::sendRecv
66: communicator.hh::BufferedCommunicator::forward, left sendRecv
66 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), after communicator.free();
66 schwarz.hh:virtual void apply (X& v, const Y& d), AFTER communication.copyOwnerToAll(v,v); 
66 schwarz.hh:virtual void apply (X& v, const Y& d), before preconditioner.apply(v,d);; 
66 schwarz.hh:virtual void apply (X& v, const Y& d), before communication.copyOwnerToAll(v,v); 
66 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
66: communicator.hh::BufferedCommunicator::forward, enter sendRecv
66: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 15
66: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 15
66: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 15
66: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 15
66: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 15
64 schwarz.hh:virtual void apply (X& v, const Y& d), before preconditioner.apply(v,d);; 
64 schwarz.hh:virtual void apply (X& v, const Y& d), before communication.copyOwnerToAll(v,v); 
64 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
64: communicator.hh::BufferedCommunicator::forward, enter sendRecv
64: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 14
64: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 14
64: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 14
64: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 14
64: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 14
64: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 14
64: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 14
64: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 14
64: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 14
64: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 14
64: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 10 14
64: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 11 14
64: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 12 14
64: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 13 14
64: communicator.hh::sendRecv, left MPI_Wait
64: leave BufferedCommunicator::sendRecv
64: communicator.hh::BufferedCommunicator::forward, left sendRecv
64 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), after communicator.free();
64 schwarz.hh:virtual void apply (X& v, const Y& d), AFTER communication.copyOwnerToAll(v,v); 
64 schwarz.hh:virtual void apply (X& v, const Y& d), before preconditioner.apply(v,d);; 
64 schwarz.hh:virtual void apply (X& v, const Y& d), before communication.copyOwnerToAll(v,v); 
64 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
64: communicator.hh::BufferedCommunicator::forward, enter sendRecv
64: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 14
64: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 14
64: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 14
89 schwarz.hh:virtual void apply (X& v, const Y& d), before preconditioner.apply(v,d);; 
89 schwarz.hh:virtual void apply (X& v, const Y& d), before communication.copyOwnerToAll(v,v); 
89 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
89: communicator.hh::BufferedCommunicator::forward, enter sendRecv
89: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 11
89: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 11
89: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 11
89: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 11
89: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 11
89: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 11
89: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 11
89: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 11
89: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 11
89: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 11
89: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 10 11
89: communicator.hh::sendRecv, left MPI_Wait
89: leave BufferedCommunicator::sendRecv
89: communicator.hh::BufferedCommunicator::forward, left sendRecv
89 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), after communicator.free();
89 schwarz.hh:virtual void apply (X& v, const Y& d), AFTER communication.copyOwnerToAll(v,v); 
89 schwarz.hh:virtual void apply (X& v, const Y& d), before preconditioner.apply(v,d);; 
22 schwarz.hh:virtual void apply (X& v, const Y& d), before preconditioner.apply(v,d);; 
22 schwarz.hh:virtual void apply (X& v, const Y& d), before communication.copyOwnerToAll(v,v); 
22 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
22: communicator.hh::BufferedCommunicator::forward, enter sendRecv
22: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 20
22: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 20
22: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 20
22: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 20
22: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 20
22: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 20
22: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 20
22: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 20
22: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 20
22: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 20
22: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 10 20
22: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 11 20
22: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 12 20
22: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 13 20
22: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 14 20
22: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 15 20
22: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 16 20
22: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 17 20
22: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 18 20
22: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 19 20
22: communicator.hh::sendRecv, left MPI_Wait
22: leave BufferedCommunicator::sendRecv
22: communicator.hh::BufferedCommunicator::forward, left sendRecv
22 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), after communicator.free();
22 schwarz.hh:virtual void apply (X& v, const Y& d), AFTER communication.copyOwnerToAll(v,v); 
22 schwarz.hh:virtual void apply (X& v, const Y& d), before preconditioner.apply(v,d);; 
22 schwarz.hh:virtual void apply (X& v, const Y& d), before communication.copyOwnerToAll(v,v); 
22 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
22: communicator.hh::BufferedCommunicator::forward, enter sendRecv
22: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 20
22: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 20
22: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 20
22: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 20
105 schwarz.hh:virtual void apply (X& v, const Y& d), before preconditioner.apply(v,d);; 
105 schwarz.hh:virtual void apply (X& v, const Y& d), before communication.copyOwnerToAll(v,v); 
105 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
105: communicator.hh::BufferedCommunicator::forward, enter sendRecv
105: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 9
105: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 9
105: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 9
105: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 9
105: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 9
105: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 9
105: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 9
105: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 9
105: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 9
105: communicator.hh::sendRecv, left MPI_Wait
105: leave BufferedCommunicator::sendRecv
105: communicator.hh::BufferedCommunicator::forward, left sendRecv
105 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), after communicator.free();
105 schwarz.hh:virtual void apply (X& v, const Y& d), AFTER communication.copyOwnerToAll(v,v); 
105 schwarz.hh:virtual void apply (X& v, const Y& d), before preconditioner.apply(v,d);; 
105 schwarz.hh:virtual void apply (X& v, const Y& d), before communication.copyOwnerToAll(v,v); 
105 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
105: communicator.hh::BufferedCommunicator::forward, enter sendRecv
105: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 9
105: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 9
105: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 9
105: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 9
105: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 9
58: leave BufferedCommunicator::sendRecv
58: communicator.hh::BufferedCommunicator::forward, left sendRecv
58 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), after communicator.free();
58void AMG<M,X,S,PI,A>::mgc(LevelContext& levelContext), finished 
58 schwarz.hh:virtual void apply (X& v, const Y& d), before preconditioner.apply(v,d);; 
58 schwarz.hh:virtual void apply (X& v, const Y& d), before communication.copyOwnerToAll(v,v); 
58 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
58: communicator.hh::BufferedCommunicator::forward, enter sendRecv
58: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 21
58: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 21
58: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 21
58: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 21
58: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 21
58: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 21
58: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 21
58: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 21
58: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 21
58: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 21
58: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 10 21
58: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 11 21
58: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 12 21
58: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 13 21
58: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 14 21
58: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 15 21
58: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 16 21
58: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 17 21
58: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 18 21
58: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 19 21
58: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 20 21
58: communicator.hh::sendRecv, left MPI_Wait
58: leave BufferedCommunicator::sendRecv
58: communicator.hh::BufferedCommunicator::forward, left sendRecv
58 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), after communicator.free();
58 schwarz.hh:virtual void apply (X& v, const Y& d), AFTER communication.copyOwnerToAll(v,v); 
58 schwarz.hh:virtual void apply (X& v, const Y& d), before preconditioner.apply(v,d);; 
71 schwarz.hh:virtual void apply (X& v, const Y& d), before preconditioner.apply(v,d);; 
71 schwarz.hh:virtual void apply (X& v, const Y& d), before communication.copyOwnerToAll(v,v); 
71 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
71: communicator.hh::BufferedCommunicator::forward, enter sendRecv
71: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 20
71: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 20
71: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 20
71: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 20
71: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 20
71: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 20
71: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 20
71: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 20
71: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 20
71: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 20
71: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 10 20
71: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 11 20
71: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 12 20
71: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 13 20
71: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 14 20
71: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 15 20
71: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 16 20
71: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 17 20
71: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 18 20
71: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 19 20
71: communicator.hh::sendRecv, left MPI_Wait
71: leave BufferedCommunicator::sendRecv
71: communicator.hh::BufferedCommunicator::forward, left sendRecv
71 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), after communicator.free();
71 schwarz.hh:virtual void apply (X& v, const Y& d), AFTER communication.copyOwnerToAll(v,v); 
71 schwarz.hh:virtual void apply (X& v, const Y& d), before preconditioner.apply(v,d);; 
71 schwarz.hh:virtual void apply (X& v, const Y& d), before communication.copyOwnerToAll(v,v); 
71 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
71: communicator.hh::BufferedCommunicator::forward, enter sendRecv
71: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 20
71: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 20
71: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 20
71: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 20
71: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 20
71: comm32 schwarz.hh:virtual void apply (X& v, const Y& d), before preconditioner.apply(v,d);; 
32 schwarz.hh:virtual void apply (X& v, const Y& d), before communication.copyOwnerToAll(v,v); 
32 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
32: communicator.hh::BufferedCommunicator::forward, enter sendRecv
32: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 16
32: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 16
32: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 16
32: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 16
32: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 16
32: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 16
32: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 16
32: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 16
32: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 16
32: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 16
32: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 10 16
32: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 11 16
32: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 12 16
32: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 13 16
32: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 14 16
32: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 15 16
32: communicator.hh::sendRecv, left MPI_Wait
32: leave BufferedCommunicator::sendRecv
32: communicator.hh::BufferedCommunicator::forward, left sendRecv
32 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), after communicator.free();
32 schwarz.hh:virtual void apply (X& v, const Y& d), AFTER communication.copyOwnerToAll(v,v); 
32 schwarz.hh:virtual void apply (X& v, const Y& d), before preconditioner.apply(v,d);; 
32 schwarz.hh:virtual void apply (X& v, const Y& d), before communication.copyOwnerToAll(v,v); 
32 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
32: communicator.hh::BufferedCommunicator::forward, enter sendRecv
32: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 16
32: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 16
33 schwarz.hh:virtual void apply (X& v, const Y& d), before preconditioner.apply(v,d);; 
33 schwarz.hh:virtual void apply (X& v, const Y& d), before communication.copyOwnerToAll(v,v); 
33 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
33: communicator.hh::BufferedCommunicator::forward, enter sendRecv
33: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 9
33: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 9
33: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 9
33: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 9
33: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 9
33: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 9
33: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 9
33: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 9
33: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 9
33: communicator.hh::sendRecv, left MPI_Wait
33: leave BufferedCommunicator::sendRecv
33: communicator.hh::BufferedCommunicator::forward, left sendRecv
33 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), after communicator.free();
33 schwarz.hh:virtual void apply (X& v, const Y& d), AFTER communication.copyOwnerToAll(v,v); 
33 schwarz.hh:virtual void apply (X& v, const Y& d), before preconditioner.apply(v,d);; 
33 schwarz.hh:virtual void apply (X& v, const Y& d), before communication.copyOwnerToAll(v,v); 
33 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
33: communicator.hh::BufferedCommunicator::forward, enter sendRecv
33: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 9
33: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 9
33: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 9
33: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 9
33: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 9
35 schwarz.hh:virtual void apply (X& v, const Y& d), before preconditioner.apply(v,d);; 
35 schwarz.hh:virtual void apply (X& v, const Y& d), before communication.copyOwnerToAll(v,v); 
35 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
35: communicator.hh::BufferedCommunicator::forward, enter sendRecv
35: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 9
35: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 9
35: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 9
35: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 9
35: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 9
35: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 9
35: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 9
35: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 9
35: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 9
35: communicator.hh::sendRecv, left MPI_Wait
35: leave BufferedCommunicator::sendRecv
35: communicator.hh::BufferedCommunicator::forward, left sendRecv
35 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), after communicator.free();
35 schwarz.hh:virtual void apply (X& v, const Y& d), AFTER communication.copyOwnerToAll(v,v); 
35 schwarz.hh:virtual void apply (X& v, const Y& d), before preconditioner.apply(v,d);; 
35 schwarz.hh:virtual void apply (X& v, const Y& d), before communication.copyOwnerToAll(v,v); 
35 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
35: communicator.hh::BufferedCommunicator::forward, enter sendRecv
35: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 9
35: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 9
35: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 9
35: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 9
35: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 9
35: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 9
38 schwarz.hh:virtual void apply (X& v, const Y& d), before preconditioner.apply(v,d);; 
38 schwarz.hh:virtual void apply (X& v, const Y& d), before communication.copyOwnerToAll(v,v); 
38 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
38: communicator.hh::BufferedCommunicator::forward, enter sendRecv
38: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 24
38: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 24
38: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 24
38: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 24
38: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 24
38: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 24
38: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 24
38: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 24
38: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 24
38: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 24
38: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 10 24
38: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 11 24
38: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 12 24
38: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 13 24
38: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 14 24
38: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 15 24
38: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 16 24
38: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 17 24
38: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 18 24
38: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 19 24
38: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 20 24
38: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 21 24
38: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 22 24
38: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 23 24
38: communicator.hh::sendRecv, left MPI_Wait
38: leave BufferedCommunicator::sendRecv
38: communicator.hh::BufferedCommunicator::forward, left sendRecv
38 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), after communicator.free();
38 schwarz.hh:virtual void apply (X& v, const Y& d), AFTER communication.copyOwnerToAll(v,v); 
38 schwarz.hh:virtual void apply (X& v, const Y& d), before preconditioner.apply(v,d);; 
38 schwarz.hh:virtual void apply (X& v, const Y& d), before communication.copyOwnerToAll(v,v); 
38 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
38: communicator.hh::BufferedCommunicator::forward, enter sendRecv
38: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 24
20 schwarz.hh:virtual void apply (X& v, const Y& d), before preconditioner.apply(v,d);; 
20 schwarz.hh:virtual void apply (X& v, const Y& d), before communication.copyOwnerToAll(v,v); 
20 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
20: communicator.hh::BufferedCommunicator::forward, enter sendRecv
20: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 10
20: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 10
20: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 10
20: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 10
20: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 10
20: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 10
20: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 10
20: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 10
20: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 10
20: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 10
20: communicator.hh::sendRecv, left MPI_Wait
20: leave BufferedCommunicator::sendRecv
20: communicator.hh::BufferedCommunicator::forward, left sendRecv
20 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), after communicator.free();
20 schwarz.hh:virtual void apply (X& v, const Y& d), AFTER communication.copyOwnerToAll(v,v); 
20 schwarz.hh:virtual void apply (X& v, const Y& d), before preconditioner.apply(v,d);; 
20 schwarz.hh:virtual void apply (X& v, const Y& d), before communication.copyOwnerToAll(v,v); 
20 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
20: communicator.hh::BufferedCommunicator::forward, enter sendRecv
20: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 10
20: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 10
20: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 10
20: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 10
20: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 10
54: leave BufferedCommunicator::sendRecv
54: communicator.hh::BufferedCommunicator::forward, left sendRecv
54 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), after communicator.free();
54void AMG<M,X,S,PI,A>::mgc(LevelContext& levelContext), finished 
54 schwarz.hh:virtual void apply (X& v, const Y& d), before preconditioner.apply(v,d);; 
54 schwarz.hh:virtual void apply (X& v, const Y& d), before communication.copyOwnerToAll(v,v); 
54 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
54: communicator.hh::BufferedCommunicator::forward, enter sendRecv
54: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 18
54: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 18
54: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 18
54: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 18
54: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 18
54: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 18
54: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 18
54: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 18
54: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 18
54: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 18
54: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 10 18
54: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 11 18
54: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 12 18
54: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 13 18
54: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 14 18
54: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 15 18
54: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 16 18
54: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 17 18
54: communicator.hh::sendRecv, left MPI_Wait
54: leave BufferedCommunicator::sendRecv
54: communicator.hh::BufferedCommunicator::forward, left sendRecv
54 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), after communicator.free();
54 schwarz.hh:virtual void apply (X& v, const Y& d), AFTER communication.copyOwnerToAll(v,v); 
54 schwarz.hh:virtual void apply (X& v, const Y& d), before preconditioner.apply(v,d);; 
54 schwarz.hh:virtual void apply (X& v, const Y& d), before communication.copyOwnerToAll(v,v); 
54 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
54: communicator.hh::BufferedCommunicator::forward, enter sendRecv
54: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 18
104 schwarz.hh:virtual void apply (X& v, const Y& d), before preconditioner.apply(v,d);; 
104 schwarz.hh:virtual void apply (X& v, const Y& d), before communication.copyOwnerToAll(v,v); 
104 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
104: communicator.hh::BufferedCommunicator::forward, enter sendRecv
104: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 9
104: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 9
104: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 9
104: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 9
104: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 9
104: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 9
104: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 9
104: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 9
104: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 9
104: communicator.hh::sendRecv, left MPI_Wait
104: leave BufferedCommunicator::sendRecv
104: communicator.hh::BufferedCommunicator::forward, left sendRecv
104 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), after communicator.free();
104 schwarz.hh:virtual void apply (X& v, const Y& d), AFTER communication.copyOwnerToAll(v,v); 
104 schwarz.hh:virtual void apply (X& v, const Y& d), before preconditioner.apply(v,d);; 
104 schwarz.hh:virtual void apply (X& v, const Y& d), before communication.copyOwnerToAll(v,v); 
104 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
104: communicator.hh::BufferedCommunicator::forward, enter sendRecv
104: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 9
117: leave BufferedCommunicator::sendRecv
117: communicator.hh::BufferedCommunicator::forward, left sendRecv
117 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), after communicator.free();
117void AMG<M,X,S,PI,A>::mgc(LevelContext& levelContext), finished 
117 schwarz.hh:virtual void apply (X& v, const Y& d), before preconditioner.apply(v,d);; 
117 schwarz.hh:virtual void apply (X& v, const Y& d), before communication.copyOwnerToAll(v,v); 
117 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
117: communicator.hh::BufferedCommunicator::forward, enter sendRecv
117: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 22
117: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 22
117: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 22
117: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 22
117: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 22
117: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 22
117: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 22
117: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 22
117: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 22
117: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 22
117: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 10 22
117: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 11 22
117: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 12 22
117: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 13 22
117: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 14 22
117: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 15 22
117: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 16 22
117: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 17 22
117: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 18 22
117: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 19 22
117: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 20 22
117: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 21 22
117: communicator.hh::sendRecv, left MPI_Wait
117: leave BufferedCommunicator::sendRecv
117: communicator.hh::BufferedCommunicator::forward, left sendRecv
117 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), after communicator.free();
117 schwarz.hh:virtual void apply (X& v, const Y& d), AFTER communication.copyOwnerToAll(v,v); 
117 schwarz.hh:virtual void apply (X& v, const Y& d), before preconditioner.apply(v,d);; 
117 schwarz.hh:virtual void apply (X& v, const Y& d), before communication.copyOwnerToAll(v,v); 
117 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
117: communicator.hh::BufferedCommunicator::forward, enter sendRecv
117: communicator.hh::sendRecv, before MPI_Waitany(me67: leave BufferedCommunicator::sendRecv
67: communicator.hh::BufferedCommunicator::forward, left sendRecv
67 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), after communicator.free();
67void AMG<M,X,S,PI,A>::mgc(LevelContext& levelContext), finished 
67 schwarz.hh:virtual void apply (X& v, const Y& d), before preconditioner.apply(v,d);; 
67 schwarz.hh:virtual void apply (X& v, const Y& d), before communication.copyOwnerToAll(v,v); 
67 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
67: communicator.hh::BufferedCommunicator::forward, enter sendRecv
67: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 13
67: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 13
67: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 13
67: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 13
67: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 13
67: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 13
67: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 13
67: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 13
67: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 13
67: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 13
67: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 10 13
67: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 11 13
67: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 12 13
67: communicator.hh::sendRecv, left MPI_Wait
67: leave BufferedCommunicator::sendRecv
67: communicator.hh::BufferedCommunicator::forward, left sendRecv
67 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), after communicator.free();
67 schwarz.hh:virtual void apply (X& v, const Y& d), AFTER communication.copyOwnerToAll(v,v); 
67 schwarz.hh:virtual void apply (X& v, const Y& d), before preconditioner.apply(v,d);; 
67 schwarz.hh:virtual void apply (X& v, const Y& d), before communication.copyOwnerToAll(v,v); 
67 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
67: communicator.hh::BufferedCommunicator::forward, enter sendRecv
67: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 13
67: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 13
67: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 13
67: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 13
67: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 13
67: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 13
59 schwarz.hh:virtual void apply (X& v, const Y& d), before preconditioner.apply(v,d);; 
59 schwarz.hh:virtual void apply (X& v, const Y& d), before communication.copyOwnerToAll(v,v); 
59 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
59: communicator.hh::BufferedCommunicator::forward, enter sendRecv
59: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 11
59: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 11
59: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 11
59: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 11
59: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 11
59: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 11
59: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 11
59: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 11
59: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 11
59: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 11
59: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 10 11
59: communicator.hh::sendRecv, left MPI_Wait
59: leave BufferedCommunicator::sendRecv
59: communicator.hh::BufferedCommunicator::forward, left sendRecv
59 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), after communicator.free();
59 schwarz.hh:virtual void apply (X& v, const Y& d), AFTER communication.copyOwnerToAll(v,v); 
59 schwarz.hh:virtual void apply (X& v, const Y& d), before preconditioner.apply(v,d);; 
59 schwarz.hh:virtual void apply (X& v, const Y& d), before communication.copyOwnerToAll(v,v); 
59 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
59: communicator.hh::BufferedCommunicator::forward, enter sendRecv
59: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 11
90 schwarz.hh:virtual void apply (X& v, const Y& d), before preconditioner.apply(v,d);; 
90 schwarz.hh:virtual void apply (X& v, const Y& d), before communication.copyOwnerToAll(v,v); 
90 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
90: communicator.hh::BufferedCommunicator::forward, enter sendRecv
90: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 6
90: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 6
90: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 6
90: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 6
90: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 6
90: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 6
90: communicator.hh::sendRecv, left MPI_Wait
90: leave BufferedCommunicator::sendRecv
90: communicator.hh::BufferedCommunicator::forward, left sendRecv
90 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), after communicator.free();
90 schwarz.hh:virtual void apply (X& v, const Y& d), AFTER communication.copyOwnerToAll(v,v); 
90 schwarz.hh:virtual void apply (X& v, const Y& d), before preconditioner.apply(v,d);; 
90 schwarz.hh:virtual void apply (X& v, const Y& d), before communication.copyOwnerToAll(v,v); 
90 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
90: communicator.hh::BufferedCommunicator::forward, enter sendRecv
90: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 6
90: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 6
90: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 6
90: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 6
90: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 6
90: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 6
25 schwarz.hh:virtual void apply (X& v, const Y& d), before preconditioner.apply(v,d);; 
25 schwarz.hh:virtual void apply (X& v, const Y& d), before communication.copyOwnerToAll(v,v); 
25 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
25: communicator.hh::BufferedCommunicator::forward, enter sendRecv
25: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 7
25: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 7
25: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 7
25: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 7
25: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 7
25: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 7
25: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 7
25: communicator.hh::sendRecv, left MPI_Wait
25: leave BufferedCommunicator::sendRecv
25: communicator.hh::BufferedCommunicator::forward, left sendRecv
25 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), after communicator.free();
25 schwarz.hh:virtual void apply (X& v, const Y& d), AFTER communication.copyOwnerToAll(v,v); 
25 schwarz.hh:virtual void apply (X& v, const Y& d), before preconditioner.apply(v,d);; 
25 schwarz.hh:virtual void apply (X& v, const Y& d), before communication.copyOwnerToAll(v,v); 
25 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
25: communicator.hh::BufferedCommunicator::forward, enter sendRecv
25: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 7
25: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 7
25: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 7
113: leave BufferedCommunicator::sendRecv
113: communicator.hh::BufferedCommunicator::forward, left sendRecv
113 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), after communicator.free();
113void AMG<M,X,S,PI,A>::mgc(LevelContext& levelContext), finished 
113 schwarz.hh:virtual void apply (X& v, const Y& d), before preconditioner.apply(v,d);; 
113 schwarz.hh:virtual void apply (X& v, const Y& d), before communication.copyOwnerToAll(v,v); 
113 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
113: communicator.hh::BufferedCommunicator::forward, enter sendRecv
113: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 18
113: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 18
113: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 18
113: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 18
113: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 18
113: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 18
113: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 18
113: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 18
113: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 18
113: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 18
113: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 10 18
113: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 11 18
113: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 12 18
113: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 13 18
113: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 14 18
113: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 15 18
113: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 16 18
113: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 17 18
113: communicator.hh::sendRecv, left MPI_Wait
113: leave BufferedCommunicator::sendRecv
113: communicator.hh::BufferedCommunicator::forward, left sendRecv
113 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), after communicator.free();
113 schwarz.hh:virtual void apply (X& v, const Y& d), AFTER communication.copyOwnerToAll(v,v); 
113 schwarz.hh:virtual void apply (X& v, const Y& d), before preconditioner.apply(v,d);; 
113 schwarz.hh:virtual void apply (X& v, const Y& d), before communication.copyOwnerToAll(v,v); 
113 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
113: communicator.hh::BufferedCommunicator::forward, enter sendRecv
113: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 18
113: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 18
113: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 18
113: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 18
113: communicator.hh::sendRecv, before MPI_Waitany(messag4 schwarz.hh:virtual void apply (X& v, const Y& d), before preconditioner.apply(v,d);; 
4 schwarz.hh:virtual void apply (X& v, const Y& d), before communication.copyOwnerToAll(v,v); 
4 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
4: communicator.hh::BufferedCommunicator::forward, enter sendRecv
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 14
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 14
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 14
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 14
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 14
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 14
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 14
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 14
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 14
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 14
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 10 14
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 11 14
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 12 14
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 13 14
4: communicator.hh::sendRecv, left MPI_Wait
4: leave BufferedCommunicator::sendRecv
4: communicator.hh::BufferedCommunicator::forward, left sendRecv
4 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), after communicator.free();
4 schwarz.hh:virtual void apply (X& v, const Y& d), AFTER communication.copyOwnerToAll(v,v); 
4 schwarz.hh:virtual void apply (X& v, const Y& d), before preconditioner.apply(v,d);; 
4 schwarz.hh:virtual void apply (X& v, const Y& d), before communication.copyOwnerToAll(v,v); 
4 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
4: communicator.hh::BufferedCommunicator::forward, enter sendRecv
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 14
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 14
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 14
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 14
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 14
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 14
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 14
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 14
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 14
74 schwarz.hh:virtual void apply (X& v, const Y& d), before preconditioner.apply(v,d);; 
74 schwarz.hh:virtual void apply (X& v, const Y& d), before communication.copyOwnerToAll(v,v); 
74 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
74: communicator.hh::BufferedCommunicator::forward, enter sendRecv
74: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 11
74: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 11
74: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 11
74: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 11
74: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 11
74: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 11
74: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 11
74: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 11
74: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 11
74: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 11
74: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 10 11
74: communicator.hh::sendRecv, left MPI_Wait
74: leave BufferedCommunicator::sendRecv
74: communicator.hh::BufferedCommunicator::forward, left sendRecv
74 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), after communicator.free();
74 schwarz.hh:virtual void apply (X& v, const Y& d), AFTER communication.copyOwnerToAll(v,v); 
74 schwarz.hh:virtual void apply (X& v, const Y& d), before preconditioner.apply(v,d);; 
74 schwarz.hh:virtual void apply (X& v, const Y& d), before communication.copyOwnerToAll(v,v); 
74 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
74: communicator.hh::BufferedCommunicator::forward, enter sendRecv
74: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 11
74: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 11
74: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 11
74: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 11
74: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 11
91: leave BufferedCommunicator::sendRecv
91: communicator.hh::BufferedCommunicator::forward, left sendRecv
91 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), after communicator.free();
91void AMG<M,X,S,PI,A>::mgc(LevelContext& levelContext), finished 
91 schwarz.hh:virtual void apply (X& v, const Y& d), before preconditioner.apply(v,d);; 
91 schwarz.hh:virtual void apply (X& v, const Y& d), before communication.copyOwnerToAll(v,v); 
91 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
91: communicator.hh::BufferedCommunicator::forward, enter sendRecv
91: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 9
91: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 9
91: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 9
91: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 9
91: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 9
91: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 9
91: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 9
91: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 9
91: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 9
91: communicator.hh::sendRecv, left MPI_Wait
91: leave BufferedCommunicator::sendRecv
91: communicator.hh::BufferedCommunicator::forward, left sendRecv
91 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), after communicator.free();
91 schwarz.hh:virtual void apply (X& v, const Y& d), AFTER communication.copyOwnerToAll(v,v); 
91 schwarz.hh:virtual void apply (X& v, const Y& d), before preconditioner.apply(v,d);; 
91 schwarz.hh:virtual void apply (X& v, const Y& d), before communication.copyOwnerToAll(v,v); 
91 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
91: communicator.hh::BufferedCommunicator::forward, enter sendRecv
91: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 9
91: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 9
91: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 9
91: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 9
91: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 9
91: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 9
91: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 9
109: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 20
109: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 20
109: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 20
109: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 10 20
109: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 11 20
109: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 12 20
109: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 13 20
109: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 14 20
109: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 15 20
109: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 16 20
109: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 17 20
109: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 18 20
109: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 19 20
109: communicator.hh::sendRecv, left MPI_Wait
109: leave BufferedCommunicator::sendRecv
109: communicator.hh::BufferedCommunicator::forward, left sendRecv
109 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), after communicator.free();
109void AMG<M,X,S,PI,A>::mgc(LevelContext& levelContext), finished 
109 schwarz.hh:virtual void apply (X& v, const Y& d), before preconditioner.apply(v,d);; 
109 schwarz.hh:virtual void apply (X& v, const Y& d), before communication.copyOwnerToAll(v,v); 
109 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
109: communicator.hh::BufferedCommunicator::forward, enter sendRecv
109: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 20
109: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 20
109: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 20
109: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 20
109: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 20
109: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 20
109: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 20
109: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 20
109: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 20
109: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 20
109: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 10 20
109: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 11 20
109: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 12 20
109: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 13 20
109: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 14 20
109: communicator.hh::sendRecv, b44: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 11
44: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 11
44: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 11
44: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 11
44: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 11
44: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 10 11
44: communicator.hh::sendRecv, left MPI_Wait
44: leave BufferedCommunicator::sendRecv
44: communicator.hh::BufferedCommunicator::forward, left sendRecv
44 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), after communicator.free();
44void AMG<M,X,S,PI,A>::mgc(LevelContext& levelContext), finished 
44 schwarz.hh:virtual void apply (X& v, const Y& d), before preconditioner.apply(v,d);; 
44 schwarz.hh:virtual void apply (X& v, const Y& d), before communication.copyOwnerToAll(v,v); 
44 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
44: communicator.hh::BufferedCommunicator::forward, enter sendRecv
44: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 11
44: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 11
44: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 11
44: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 11
44: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 11
44: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 11
44: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 11
44: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 11
44: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 11
44: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 11
44: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 10 11
44: communicator.hh::sendRecv, left MPI_Wait
44: leave BufferedCommunicator::sendRecv
44: communicator.hh::BufferedCommunicator::forward, left sendRecv
44 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), after communicator.free();
44 schwarz.hh:virtual void apply (X& v, const Y& d), AFTER communication.copyOwnerToAll(v,v); 
44 schwarz.hh:virtual void apply (X& v, const Y& d), before preconditioner.apply(v,d);; 
44 schwarz.hh:virtual void apply (X& v, const Y& d), before communication.copyOwnerToAll(v,v); 
44 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
44: communicator.hh::BufferedCommunicator::forward, enter sendRecv
44: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 11
44: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 11
44: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 11
44: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 11
44: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 11
44: communicator.hh::sendRecv, before MPI_Waitany(message106: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 16
106: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 16
106: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 10 16
106: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 11 16
106: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 12 16
106: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 13 16
106: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 14 16
106: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 15 16
106: communicator.hh::sendRecv, left MPI_Wait
106: leave BufferedCommunicator::sendRecv
106: communicator.hh::BufferedCommunicator::forward, left sendRecv
106 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), after communicator.free();
106void AMG<M,X,S,PI,A>::mgc(LevelContext& levelContext), finished 
106 schwarz.hh:virtual void apply (X& v, const Y& d), before preconditioner.apply(v,d);; 
106 schwarz.hh:virtual void apply (X& v, const Y& d), before communication.copyOwnerToAll(v,v); 
106 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
106: communicator.hh::BufferedCommunicator::forward, enter sendRecv
106: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 16
106: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 16
106: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 16
106: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 16
106: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 16
106: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 16
106: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 16
106: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 16
106: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 16
106: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 16
106: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 10 16
106: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 11 16
106: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 12 16
106: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 13 16
106: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 14 16
106: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 15 16
106: communicator.hh::sendRecv, left MPI_Wait
106: leave BufferedCommunicator::sendRecv
106: communicator.hh::BufferedCommunicator::forward, left sendRecv
106 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), after communicator.free();
106 schwarz.hh:virtual void apply (X& v, const Y& d), AFTER communication.copyOwnerToAll(v,v); 
106 schwarz.hh:virtual void apply (X& v, const Y& d), before preconditioner.apply(v,d);; 
106 schwarz.hh:virtual void apply (X& v, const Y& d), before communication.c108: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 9
108: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 9
108: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 9
108: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 9
108: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 9
108: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 9
108: communicator.hh::sendRecv, left MPI_Wait
108: leave BufferedCommunicator::sendRecv
108: communicator.hh::BufferedCommunicator::forward, left sendRecv
108 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), after communicator.free();
108void AMG<M,X,S,PI,A>::mgc(LevelContext& levelContext), finished 
108 schwarz.hh:virtual void apply (X& v, const Y& d), before preconditioner.apply(v,d);; 
108 schwarz.hh:virtual void apply (X& v, const Y& d), before communication.copyOwnerToAll(v,v); 
108 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
108: communicator.hh::BufferedCommunicator::forward, enter sendRecv
108: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 9
108: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 9
108: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 9
108: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 9
108: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 9
108: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 9
108: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 9
108: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 9
108: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 9
108: communicator.hh::sendRecv, left MPI_Wait
108: leave BufferedCommunicator::sendRecv
108: communicator.hh::BufferedCommunicator::forward, left sendRecv
108 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), after communicator.free();
108 schwarz.hh:virtual void apply (X& v, const Y& d), AFTER communication.copyOwnerToAll(v,v); 
108 schwarz.hh:virtual void apply (X& v, const Y& d), before preconditioner.apply(v,d);; 
108 schwarz.hh:virtual void apply (X& v, const Y& d), before communication.copyOwnerToAll(v,v); 
108 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
108: communicator.hh::BufferedCommunicator::forward, enter sendRecv
108: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 9
108: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 9
108: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 9
108: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 9
108: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 9
108: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 9
108: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 9
108: communicator.hh::sendRecv, before MP46: communicator.hh::sendRecv, left MPI_Wait
46: leave BufferedCommunicator::sendRecv
46: communicator.hh::BufferedCommunicator::forward, left sendRecv
46 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), after communicator.free();
46void AMG<M,X,S,PI,A>::mgc(LevelContext& levelContext), finished 
46 schwarz.hh:virtual void apply (X& v, const Y& d), before preconditioner.apply(v,d);; 
46 schwarz.hh:virtual void apply (X& v, const Y& d), before communication.copyOwnerToAll(v,v); 
46 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
46: communicator.hh::BufferedCommunicator::forward, enter sendRecv
46: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 12
46: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 12
46: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 12
46: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 12
46: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 12
46: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 12
46: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 12
46: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 12
46: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 12
46: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 12
46: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 10 12
46: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 11 12
46: communicator.hh::sendRecv, left MPI_Wait
46: leave BufferedCommunicator::sendRecv
46: communicator.hh::BufferedCommunicator::forward, left sendRecv
46 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), after communicator.free();
46 schwarz.hh:virtual void apply (X& v, const Y& d), AFTER communication.copyOwnerToAll(v,v); 
46 schwarz.hh:virtual void apply (X& v, const Y& d), before preconditioner.apply(v,d);; 
46 schwarz.hh:virtual void apply (X& v, const Y& d), before communication.copyOwnerToAll(v,v); 
46 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
46: communicator.hh::BufferedCommunicator::forward, enter sendRecv
46: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 12
46: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 12
46: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 12
46: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 12
46: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 12
46: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 12
46: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 12
46: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 12
46: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 12
46: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 12
46: communicator.hh::sendRecv, before MPI_Waitany(message24 schwarz.hh:virtual void apply (X& v, const Y& d), before preconditioner.apply(v,d);; 
24 schwarz.hh:virtual void apply (X& v, const Y& d), before communication.copyOwnerToAll(v,v); 
24 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
24: communicator.hh::BufferedCommunicator::forward, enter sendRecv
24: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 14
24: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 14
24: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 14
24: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 14
24: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 14
24: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 14
24: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 14
24: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 14
24: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 14
24: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 14
24: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 10 14
24: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 11 14
24: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 12 14
24: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 13 14
24: communicator.hh::sendRecv, left MPI_Wait
24: leave BufferedCommunicator::sendRecv
24: communicator.hh::BufferedCommunicator::forward, left sendRecv
24 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), after communicator.free();
24 schwarz.hh:virtual void apply (X& v, const Y& d), AFTER communication.copyOwnerToAll(v,v); 
24 schwarz.hh:virtual void apply (X& v, const Y& d), before preconditioner.apply(v,d);; 
24 schwarz.hh:virtual void apply (X& v, const Y& d), before communication.copyOwnerToAll(v,v); 
24 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
24: communicator.hh::BufferedCommunicator::forward, enter sendRecv
24: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 14
24: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 14
24: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 14
24: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 14
24: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 14
24: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 14
24: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 14
24: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 14
24: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 14
24: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 14
24: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 10 14
24: communica107: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 5
107: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 5
107: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 5
107: communicator.hh::sendRecv, left MPI_Wait
107: leave BufferedCommunicator::sendRecv
107: communicator.hh::BufferedCommunicator::forward, left sendRecv
107 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), after communicator.free();
107void AMG<M,X,S,PI,A>::mgc(LevelContext& levelContext), finished 
107 schwarz.hh:virtual void apply (X& v, const Y& d), before preconditioner.apply(v,d);; 
107 schwarz.hh:virtual void apply (X& v, const Y& d), before communication.copyOwnerToAll(v,v); 
107 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
107: communicator.hh::BufferedCommunicator::forward, enter sendRecv
107: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 5
107: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 5
107: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 5
107: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 5
107: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 5
107: communicator.hh::sendRecv, left MPI_Wait
107: leave BufferedCommunicator::sendRecv
107: communicator.hh::BufferedCommunicator::forward, left sendRecv
107 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), after communicator.free();
107 schwarz.hh:virtual void apply (X& v, const Y& d), AFTER communication.copyOwnerToAll(v,v); 
107 schwarz.hh:virtual void apply (X& v, const Y& d), before preconditioner.apply(v,d);; 
107 schwarz.hh:virtual void apply (X& v, const Y& d), before communication.copyOwnerToAll(v,v); 
107 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
107: communicator.hh::BufferedCommunicator::forward, enter sendRecv
107: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 5
107: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 5
107: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 5
107: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 5
107: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 5
107: communicator.hh::sendRecv, left MPI_Wait
107: leave BufferedCommunicator::sendRecv
107: communicator.hh::BufferedCommunicator::forward, left sendRecv
107 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), after communicator.free();
107 schwarz.hh:virtual void apply (X& v, const Y& d), AFTER communication.copyOwnerToAll(v,v); 
107void AMG<M,X,S,PI,A>::mgc(LevelContext& levelContext), finished 
107 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), AFTER mgc(levelContext) 
107 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), finished 
107   AFTER _prec->apply(y,p), before v = A * y 
107  before  alpha = rho_new / < rt, v > 
49: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 10 15
49: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 11 15
49: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 12 15
49: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 13 15
49: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 14 15
49: communicator.hh::sendRecv, left MPI_Wait
49: leave BufferedCommunicator::sendRecv
49: communicator.hh::BufferedCommunicator::forward, left sendRecv
49 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), after communicator.free();
49void AMG<M,X,S,PI,A>::mgc(LevelContext& levelContext), finished 
49 schwarz.hh:virtual void apply (X& v, const Y& d), before preconditioner.apply(v,d);; 
49 schwarz.hh:virtual void apply (X& v, const Y& d), before communication.copyOwnerToAll(v,v); 
49 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
49: communicator.hh::BufferedCommunicator::forward, enter sendRecv
49: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 15
49: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 15
49: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 15
49: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 15
49: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 15
49: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 15
49: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 15
49: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 15
49: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 15
49: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 15
49: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 10 15
49: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 11 15
49: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 12 15
49: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 13 15
49: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 14 15
49: communicator.hh::sendRecv, left MPI_Wait
49: leave BufferedCommunicator::sendRecv
49: communicator.hh::BufferedCommunicator::forward, left sendRecv
49 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), after communicator.free();
49 schwarz.hh:virtual void apply (X& v, const Y& d), AFTER communication.copyOwnerToAll(v,v); 
49 schwarz.hh:virtual void apply (X& v, const Y& d), before preconditioner.apply(v,d);; 
49 schwarz.hh:virtual void apply (X& v, const Y& d), before communication.copyOwnerToAll(v,v); 
49 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
49: communicator.hh::BufferedCommunicator::forward, enter sendRecv
49: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 15
49: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 15
49: communicator.hh::sendRecv, before MPI_Waitany29 schwarz.hh:virtual void apply (X& v, const Y& d), AFTER communication.copyOwnerToAll(v,v); 
29 schwarz.hh:virtual void apply (X& v, const Y& d), before preconditioner.apply(v,d);; 
29 schwarz.hh:virtual void apply (X& v, const Y& d), before communication.copyOwnerToAll(v,v); 
29 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
29: communicator.hh::BufferedCommunicator::forward, enter sendRecv
29: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 11
29: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 11
29: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 11
29: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 11
29: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 11
29: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 11
29: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 11
29: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 11
29: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 11
29: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 11
29: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 10 11
29: communicator.hh::sendRecv, left MPI_Wait
29: leave BufferedCommunicator::sendRecv
29: communicator.hh::BufferedCommunicator::forward, left sendRecv
29 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), after communicator.free();
29 schwarz.hh:virtual void apply (X& v, const Y& d), AFTER communication.copyOwnerToAll(v,v); 
29void AMG<M,X,S,PI,A>::mgc(LevelContext& levelContext), finished 
29 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), AFTER mgc(levelContext) 
29 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), finished 
29   AFTER _prec->apply(y,p), before v = A * y 
29  before  alpha = rho_new / < rt, v > 
29  before alpha = rho_new / h; 
29  before apply first correction to x 
29  before r = r - alpha*v 
29  before test stop criteria 
29 istlsolver::printOutput 
  0.5      1.30064e-21      2.43002e-13
29 === rate=5.90498e-26, T=0.0266541, TIT=0.0533081, IT=0.5
29 finished solveLinearSystemImpl_ 1
29 converged value: 1 121 1 let s go get convergedRemote
29 to  comm_.min(converged) 
29 did  comm_.min(converged) convergedRemote: 1
29 final convergedRemote: 1
29 Update: x^(k+1) = x^k - deltax^k 
29 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
29 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
29 Newton iteration  1 done29 , maximum relative shift = 3.976e-03
29 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 11
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 11
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 11
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 11
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 11
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 11
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 10 11
5: communicator.hh::sendRecv, left MPI_Wait
5: leave BufferedCommunicator::sendRecv
5: communicator.hh::BufferedCommunicator::forward, left sendRecv
5 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), after communicator.free();
5 schwarz.hh:virtual void apply (X& v, const Y& d), AFTER communication.copyOwnerToAll(v,v); 
5void AMG<M,X,S,PI,A>::mgc(LevelContext& levelContext), finished 
5 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), AFTER mgc(levelContext) 
5 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), finished 
5   AFTER _prec->apply(y,p), before v = A * y 
5  before  alpha = rho_new / < rt, v > 
5  before alpha = rho_new / h; 
5  before apply first correction to x 
5  before r = r - alpha*v 
5  before test stop criteria 
5 istlsolver::printOutput 
  0.5      1.30064e-21      2.43002e-13
5 === rate=5.90498e-26, T=0.0266215, TIT=0.0532431, IT=0.5
5 finished solveLinearSystemImpl_ 1
5 converged value: 1 121 1 let s go get convergedRemote
5 to  comm_.min(converged) 
5 did  comm_.min(converged) convergedRemote: 1
5 final convergedRemote: 1
5 Update: x^(k+1) = x^k - deltax^k 
5 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
5 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
5 Newton iteration  1 done5 , maximum relative shift = 3.976e-03
5 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
101 schwarz.hh:virtual void apply (X& v, const Y& d), before communication.copyOwnerToAll(v,v); 
101 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
101: communicator.hh::BufferedCommunicator::forward, enter sendRecv
101: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 24
101: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 24
101: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 24
101: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 24
101: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 24
101: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 24
101: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 24
101: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 24
101: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 24
101: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 24
101: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 10 24
101: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 11 24
101: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 12 24
101: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 13 24
101: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 14 24
101: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 15 24
101: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 16 24
101: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 17 24
101: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 18 24
101: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 19 24
101: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 20 24
101: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 21 24
101: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 22 24
101: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 23 24
101: communicator.hh::sendRecv, left MPI_Wait
101: leave BufferedCommunicator::sendRecv
101: communicator.hh::BufferedCommunicator::forward, left sendRecv
101 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), after communicator.free();
101 schwarz.hh:virtual void apply (X& v, const Y& d), AFTER communication.copyOwnerToAll(v,v); 
101void AMG<M,X,S,PI,A>::mgc(LevelContext& levelContext), finished 
101 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), AFTER mgc(levelContext) 
101 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), finished 
101   AFTER _prec->apply(y,p), before v = A * y 
101  before  alpha = rho_new / < rt, v > 
101  before alpha = rho_new / h; 
101  before apply first correction to x 
101  before r = r - alpha*v 
101  before test stop criteria 
101 istlsolver::printOutput 
  0.5      1.30064e-21      2.43002e-13
101 === rate=5.90498e-26, T=0.0266411, TIT=0.0532821,42: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 20
42: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 20
42: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 20
42: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 20
42: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 20
42: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 20
42: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 20
42: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 20
42: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 20
42: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 20
42: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 10 20
42: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 11 20
42: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 12 20
42: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 13 20
42: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 14 20
42: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 15 20
42: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 16 20
42: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 17 20
42: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 18 20
42: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 19 20
42: communicator.hh::sendRecv, left MPI_Wait
42: leave BufferedCommunicator::sendRecv
42: communicator.hh::BufferedCommunicator::forward, left sendRecv
42 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), after communicator.free();
42 schwarz.hh:virtual void apply (X& v, const Y& d), AFTER communication.copyOwnerToAll(v,v); 
42void AMG<M,X,S,PI,A>::mgc(LevelContext& levelContext), finished 
42 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), AFTER mgc(levelContext) 
42 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), finished 
42   AFTER _prec->apply(y,p), before v = A * y 
42  before  alpha = rho_new / < rt, v > 
42  before alpha = rho_new / h; 
42  before apply first correction to x 
42  before r = r - alpha*v 
42  before test stop criteria 
42 istlsolver::printOutput 
  0.5      1.30064e-21      2.43002e-13
42 === rate=5.90498e-26, T=0.0266622, TIT=0.0533244, IT=0.5
42 finished solveLinearSystemImpl_ 1
42 converged value: 1 121 1 let s go get convergedRemote
42 to  comm_.min(converged) 
42 did  comm_.min(converged) convergedRemote: 1
42 final convergedRemote: 1
42 Update: x^(k+1) = x^k - deltax^k 
42 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
42 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
42 Newton iteration  1 done42 , maximum relative shift = 3.976e-03
42 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
118: leave BufferedCommunicator::sendRecv
118: communicator.hh::BufferedCommunicator::forward, left sendRecv
118 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), after communicator.free();
118 schwarz.hh:virtual void apply (X& v, const Y& d), AFTER communication.copyOwnerToAll(v,v); 
118 schwarz.hh:virtual void apply (X& v, const Y& d), before preconditioner.apply(v,d);; 
118 schwarz.hh:virtual void apply (X& v, const Y& d), before communication.copyOwnerToAll(v,v); 
118 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
118: communicator.hh::BufferedCommunicator::forward, enter sendRecv
118: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 14
118: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 14
118: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 14
118: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 14
118: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 14
118: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 14
118: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 14
118: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 14
118: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 14
118: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 14
118: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 10 14
118: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 11 14
118: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 12 14
118: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 13 14
118: communicator.hh::sendRecv, left MPI_Wait
118: leave BufferedCommunicator::sendRecv
118: communicator.hh::BufferedCommunicator::forward, left sendRecv
118 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), after communicator.free();
118 schwarz.hh:virtual void apply (X& v, const Y& d), AFTER communication.copyOwnerToAll(v,v); 
118void AMG<M,X,S,PI,A>::mgc(LevelContext& levelContext), finished 
118 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), AFTER mgc(levelContext) 
118 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), finished 
118   AFTER _prec->apply(y,p), before v = A * y 
118  before  alpha = rho_new / < rt, v > 
118  before alpha = rho_new / h; 
118  before apply first correction to x 
118  before r = r - alpha*v 
118  before test stop criteria 
118 istlsolver::printOutput 
  0.5      1.30064e-21      2.43002e-13
118 === rate=5.90498e-26, T=0.0266885, TIT=0.0533769, IT=0.5
118 finished solveLinearSystemImpl_ 1
118 converged value: 1 121 1 let s go get convergedRemote
118 to  comm_.min(converged) 
118 did  comm_.min(converged) convergedRemote: 1
118 final convergedRemote: 1
118 Update: x^(k+1) = x^k - deltax^k 
118 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
118 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
118 Newton iteration  1 done118 , maximum relative shift = 3.976e-03
118 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
15: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 8
15: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 8
15: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 8
15: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 8
15: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 8
15: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 8
15: communicator.hh::sendRecv, left MPI_Wait
15: leave BufferedCommunicator::sendRecv
15: communicator.hh::BufferedCommunicator::forward, left sendRecv
15 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), after communicator.free();
15 schwarz.hh:virtual void apply (X& v, const Y& d), AFTER communication.copyOwnerToAll(v,v); 
15void AMG<M,X,S,PI,A>::mgc(LevelContext& levelContext), finished 
15 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), AFTER mgc(levelContext) 
15 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), finished 
15   AFTER _prec->apply(y,p), before v = A * y 
15  before  alpha = rho_new / < rt, v > 
15  before alpha = rho_new / h; 
15  before apply first correction to x 
15  before r = r - alpha*v 
15  before test stop criteria 
15 istlsolver::printOutput 
  0.5      1.30064e-21      2.43002e-13
15 === rate=5.90498e-26, T=0.0266976, TIT=0.0533952, IT=0.5
15 finished solveLinearSystemImpl_ 1
15 converged value: 1 121 1 let s go get convergedRemote
15 to  comm_.min(converged) 
15 did  comm_.min(converged) convergedRemote: 1
15 final convergedRemote: 1
15 Update: x^(k+1) = x^k - deltax^k 
15 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
15 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
15 Newton iteration  1 done15 , maximum relative shift = 3.976e-03
15 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
10 schwarz.hh:virtual void apply (X& v, const Y& d), before communication.copyOwnerToAll(v,v); 
10 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
10: communicator.hh::BufferedCommunicator::forward, enter sendRecv
10: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 11
10: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 11
10: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 11
10: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 11
10: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 11
10: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 11
10: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 11
10: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 11
10: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 11
10: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 11
10: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 10 11
10: communicator.hh::sendRecv, left MPI_Wait
10: leave BufferedCommunicator::sendRecv
10: communicator.hh::BufferedCommunicator::forward, left sendRecv
10 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), after communicator.free();
10 schwarz.hh:virtual void apply (X& v, const Y& d), AFTER communication.copyOwnerToAll(v,v); 
10void AMG<M,X,S,PI,A>::mgc(LevelContext& levelContext), finished 
10 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), AFTER mgc(levelContext) 
10 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), finished 
10   AFTER _prec->apply(y,p), before v = A * y 
10  before  alpha = rho_new / < rt, v > 
10  before alpha = rho_new / h; 
10  before apply first correction to x 
10  before r = r - alpha*v 
10  before test stop criteria 
10 istlsolver::printOutput 
  0.5      1.30064e-21      2.43002e-13
10 === rate=5.90498e-26, T=0.0266602, TIT=0.0533204, IT=0.5
10 finished solveLinearSystemImpl_ 1
10 converged value: 1 121 1 let s go get convergedRemote
10 to  comm_.min(converged) 
10 did  comm_.min(converged) convergedRemote: 1
10 final convergedRemote: 1
10 Update: x^(k+1) = x^k - deltax^k 
10 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
10 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
10 Newton iteration  1 done10 , maximum relative shift = 3.976e-03
10 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
85 schwarz.hh:virtual void apply (X& v, const Y& d), AFTER communication.copyOwnerToAll(v,v); 
85 schwarz.hh:virtual void apply (X& v, const Y& d), before preconditioner.apply(v,d);; 
85 schwarz.hh:virtual void apply (X& v, const Y& d), before communication.copyOwnerToAll(v,v); 
85 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
85: communicator.hh::BufferedCommunicator::forward, enter sendRecv
85: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 11
85: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 11
85: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 11
85: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 11
85: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 11
85: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 11
85: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 11
85: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 11
85: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 11
85: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 11
85: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 10 11
85: communicator.hh::sendRecv, left MPI_Wait
85: leave BufferedCommunicator::sendRecv
85: communicator.hh::BufferedCommunicator::forward, left sendRecv
85 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), after communicator.free();
85 schwarz.hh:virtual void apply (X& v, const Y& d), AFTER communication.copyOwnerToAll(v,v); 
85void AMG<M,X,S,PI,A>::mgc(LevelContext& levelContext), finished 
85 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), AFTER mgc(levelContext) 
85 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), finished 
85   AFTER _prec->apply(y,p), before v = A * y 
85  before  alpha = rho_new / < rt, v > 
85  before alpha = rho_new / h; 
85  before apply first correction to x 
85  before r = r - alpha*v 
85  before test stop criteria 
85 istlsolver::printOutput 
  0.5      1.30064e-21      2.43002e-13
85 === rate=5.90498e-26, T=0.0266055, TIT=0.053211, IT=0.5
85 finished solveLinearSystemImpl_ 1
85 converged value: 1 121 1 let s go get convergedRemote
85 to  comm_.min(converged) 
85 did  comm_.min(converged) convergedRemote: 1
85 final convergedRemote: 1
85 Update: x^(k+1) = x^k - deltax^k 
85 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
85 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
85 Newton iteration  1 done85 , maximum relative shift = 3.976e-03
85 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
98: leave BufferedCommunicator::sendRecv
98: communicator.hh::BufferedCommunicator::forward, left sendRecv
98 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), after communicator.free();
98 schwarz.hh:virtual void apply (X& v, const Y& d), AFTER communication.copyOwnerToAll(v,v); 
98 schwarz.hh:virtual void apply (X& v, const Y& d), before preconditioner.apply(v,d);; 
98 schwarz.hh:virtual void apply (X& v, const Y& d), before communication.copyOwnerToAll(v,v); 
98 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
98: communicator.hh::BufferedCommunicator::forward, enter sendRecv
98: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 18
98: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 18
98: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 18
98: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 18
98: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 18
98: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 18
98: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 18
98: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 18
98: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 18
98: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 18
98: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 10 18
98: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 11 18
98: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 12 18
98: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 13 18
98: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 14 18
98: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 15 18
98: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 16 18
98: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 17 18
98: communicator.hh::sendRecv, left MPI_Wait
98: leave BufferedCommunicator::sendRecv
98: communicator.hh::BufferedCommunicator::forward, left sendRecv
98 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), after communicator.free();
98 schwarz.hh:virtual void apply (X& v, const Y& d), AFTER communication.copyOwnerToAll(v,v); 
98void AMG<M,X,S,PI,A>::mgc(LevelContext& levelContext), finished 
98 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), AFTER mgc(levelContext) 
98 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), finished 
98   AFTER _prec->apply(y,p), before v = A * y 
98  before  alpha = rho_new / < rt, v > 
98  before alpha = rho_new / h; 
98  before apply first correction to x 
98  before r = r - alpha*v 
98  before test stop criteria 
98 istlsolver::printOutput 
  0.5      1.30064e-21      2.43002e-13
98 === rate=5.90498e-26, T=0.026625, TIT=0.0532499, IT=0.5
98 finished solveLinearSystemImpl_ 1
98 converged value: 1 121 1 let s go get convergedRemote
98 to  comm_.min(converged) 
98 did  comm_.min(converged) convergedRemote: 1
98 final convergedRemote: 1
98 Update: x^(k+1) = x^k - deltax^k 
98 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
98 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
98 Newton iteration47 schwarz.hh:virtual void apply (X& v, const Y& d), before communication.copyOwnerToAll(v,v); 
47 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
47: communicator.hh::BufferedCommunicator::forward, enter sendRecv
47: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 6
47: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 6
47: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 6
47: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 6
47: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 6
47: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 6
47: communicator.hh::sendRecv, left MPI_Wait
47: leave BufferedCommunicator::sendRecv
47: communicator.hh::BufferedCommunicator::forward, left sendRecv
47 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), after communicator.free();
47 schwarz.hh:virtual void apply (X& v, const Y& d), AFTER communication.copyOwnerToAll(v,v); 
47void AMG<M,X,S,PI,A>::mgc(LevelContext& levelContext), finished 
47 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), AFTER mgc(levelContext) 
47 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), finished 
47   AFTER _prec->apply(y,p), before v = A * y 
47  before  alpha = rho_new / < rt, v > 
47  before alpha = rho_new / h; 
47  before apply first correction to x 
47  before r = r - alpha*v 
47  before test stop criteria 
47 istlsolver::printOutput 
  0.5      1.30064e-21      2.43002e-13
47 === rate=5.90498e-26, T=0.0266846, TIT=0.0533692, IT=0.5
47 finished solveLinearSystemImpl_ 1
47 converged value: 1 121 1 let s go get convergedRemote
47 to  comm_.min(converged) 
47 did  comm_.min(converged) convergedRemote: 1
47 final convergedRemote: 1
47 Update: x^(k+1) = x^k - deltax^k 
47 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
47 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
47 Newton iteration  1 done47 , maximum relative shift = 3.976e-03
47 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
45 schwarz.hh:virtual void apply (X& v, const Y& d), before communication.copyOwnerToAll(v,v); 
45 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
45: communicator.hh::BufferedCommunicator::forward, enter sendRecv
45: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 17
45: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 17
45: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 17
45: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 17
45: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 17
45: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 17
45: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 17
45: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 17
45: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 17
45: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 17
45: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 10 17
45: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 11 17
45: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 12 17
45: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 13 17
45: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 14 17
45: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 15 17
45: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 16 17
45: communicator.hh::sendRecv, left MPI_Wait
45: leave BufferedCommunicator::sendRecv
45: communicator.hh::BufferedCommunicator::forward, left sendRecv
45 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), after communicator.free();
45 schwarz.hh:virtual void apply (X& v, const Y& d), AFTER communication.copyOwnerToAll(v,v); 
45void AMG<M,X,S,PI,A>::mgc(LevelContext& levelContext), finished 
45 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), AFTER mgc(levelContext) 
45 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), finished 
45   AFTER _prec->apply(y,p), before v = A * y 
45  before  alpha = rho_new / < rt, v > 
45  before alpha = rho_new / h; 
45  before apply first correction to x 
45  before r = r - alpha*v 
45  before test stop criteria 
45 istlsolver::printOutput 
  0.5      1.30064e-21      2.43002e-13
45 === rate=5.90498e-26, T=0.0266493, TIT=0.0532986, IT=0.5
45 finished solveLinearSystemImpl_ 1
45 converged value: 1 121 1 let s go get convergedRemote
45 to  comm_.min(converged) 
45 did  comm_.min(converged) convergedRemote: 1
45 final convergedRemote: 1
45 Update: x^(k+1) = x^k - deltax^k 
45 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
45 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
45 Newton iteration  1 done45 , maximum relative shift = 3.976e-03
45 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
17: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 3
17: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 3
17: communicator.hh::sendRecv, left MPI_Wait
17: leave BufferedCommunicator::sendRecv
17: communicator.hh::BufferedCommunicator::forward, left sendRecv
17 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), after communicator.free();
17 schwarz.hh:virtual void apply (X& v, const Y& d), AFTER communication.copyOwnerToAll(v,v); 
17void AMG<M,X,S,PI,A>::mgc(LevelContext& levelContext), finished 
17 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), AFTER mgc(levelContext) 
17 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), finished 
17   AFTER _prec->apply(y,p), before v = A * y 
17  before  alpha = rho_new / < rt, v > 
17  before alpha = rho_new / h; 
17  before apply first correction to x 
17  before r = r - alpha*v 
17  before test stop criteria 
17 istlsolver::printOutput 
  0.5      1.30064e-21      2.43002e-13
17 === rate=5.90498e-26, T=0.0266006, TIT=0.0532013, IT=0.5
17 finished solveLinearSystemImpl_ 1
17 converged value: 1 121 1 let s go get convergedRemote
17 to  comm_.min(converged) 
17 did  comm_.min(converged) convergedRemote: 1
17 final convergedRemote: 1
17 Update: x^(k+1) = x^k - deltax^k 
17 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
17 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
17 Newton iteration  1 done17 , maximum relative shift = 3.976e-03
17 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
116: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 7
116: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 7
116: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 7
116: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 7
116: communicator.hh::sendRecv, left MPI_Wait
116: leave BufferedCommunicator::sendRecv
116: communicator.hh::BufferedCommunicator::forward, left sendRecv
116 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), after communicator.free();
116 schwarz.hh:virtual void apply (X& v, const Y& d), AFTER communication.copyOwnerToAll(v,v); 
116void AMG<M,X,S,PI,A>::mgc(LevelContext& levelContext), finished 
116 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), AFTER mgc(levelContext) 
116 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), finished 
116   AFTER _prec->apply(y,p), before v = A * y 
116  before  alpha = rho_new / < rt, v > 
116  before alpha = rho_new / h; 
116  before apply first correction to x 
116  before r = r - alpha*v 
116  before test stop criteria 
116 istlsolver::printOutput 
  0.5      1.30064e-21      2.43002e-13
116 === rate=5.90498e-26, T=0.0267188, TIT=0.0534375, IT=0.5
116 finished solveLinearSystemImpl_ 1
116 converged value: 1 121 1 let s go get convergedRemote
116 to  comm_.min(converged) 
116 did  comm_.min(converged) convergedRemote: 1
116 final convergedRemote: 1
116 Update: x^(k+1) = x^k - deltax^k 
116 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
116 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
116 Newton iteration  1 done116 , maximum relative shift = 3.976e-03
116 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
66: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 15
66: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 15
66: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 15
66: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 15
66: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 15
66: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 10 15
66: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 11 15
66: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 12 15
66: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 13 15
66: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 14 15
66: communicator.hh::sendRecv, left MPI_Wait
66: leave BufferedCommunicator::sendRecv
66: communicator.hh::BufferedCommunicator::forward, left sendRecv
66 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), after communicator.free();
66 schwarz.hh:virtual void apply (X& v, const Y& d), AFTER communication.copyOwnerToAll(v,v); 
66void AMG<M,X,S,PI,A>::mgc(LevelContext& levelContext), finished 
66 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), AFTER mgc(levelContext) 
66 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), finished 
66   AFTER _prec->apply(y,p), before v = A * y 
66  before  alpha = rho_new / < rt, v > 
66  before alpha = rho_new / h; 
66  before apply first correction to x 
66  before r = r - alpha*v 
66  before test stop criteria 
66 istlsolver::printOutput 
  0.5      1.30064e-21      2.43002e-13
66 === rate=5.90498e-26, T=0.0265869, TIT=0.0531737, IT=0.5
66 finished solveLinearSystemImpl_ 1
66 converged value: 1 121 1 let s go get convergedRemote
66 to  comm_.min(converged) 
66 did  comm_.min(converged) convergedRemote: 1
66 final convergedRemote: 1
66 Update: x^(k+1) = x^k - deltax^k 
66 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
66 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
66 Newton iteration  1 done66 , maximum relative shift = 3.976e-03
66 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
64: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 14
64: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 14
64: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 14
64: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 14
64: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 14
64: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 14
64: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 14
64: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 10 14
64: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 11 14
64: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 12 14
64: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 13 14
64: communicator.hh::sendRecv, left MPI_Wait
64: leave BufferedCommunicator::sendRecv
64: communicator.hh::BufferedCommunicator::forward, left sendRecv
64 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), after communicator.free();
64 schwarz.hh:virtual void apply (X& v, const Y& d), AFTER communication.copyOwnerToAll(v,v); 
64void AMG<M,X,S,PI,A>::mgc(LevelContext& levelContext), finished 
64 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), AFTER mgc(levelContext) 
64 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), finished 
64   AFTER _prec->apply(y,p), before v = A * y 
64  before  alpha = rho_new / < rt, v > 
64  before alpha = rho_new / h; 
64  before apply first correction to x 
64  before r = r - alpha*v 
64  before test stop criteria 
64 istlsolver::printOutput 
  0.5      1.30064e-21      2.43002e-13
64 === rate=5.90498e-26, T=0.0265926, TIT=0.0531852, IT=0.5
64 finished solveLinearSystemImpl_ 1
64 converged value: 1 121 1 let s go get convergedRemote
64 to  comm_.min(converged) 
64 did  comm_.min(converged) convergedRemote: 1
64 final convergedRemote: 1
64 Update: x^(k+1) = x^k - deltax^k 
64 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
64 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
64 Newton iteration  1 done64 , maximum relative shift = 3.976e-03
64 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
89 schwarz.hh:virtual void apply (X& v, const Y& d), before communication.copyOwnerToAll(v,v); 
89 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
89: communicator.hh::BufferedCommunicator::forward, enter sendRecv
89: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 11
89: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 11
89: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 11
89: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 11
89: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 11
89: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 11
89: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 11
89: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 11
89: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 11
89: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 11
89: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 10 11
89: communicator.hh::sendRecv, left MPI_Wait
89: leave BufferedCommunicator::sendRecv
89: communicator.hh::BufferedCommunicator::forward, left sendRecv
89 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), after communicator.free();
89 schwarz.hh:virtual void apply (X& v, const Y& d), AFTER communication.copyOwnerToAll(v,v); 
89void AMG<M,X,S,PI,A>::mgc(LevelContext& levelContext), finished 
89 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), AFTER mgc(levelContext) 
89 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), finished 
89   AFTER _prec->apply(y,p), before v = A * y 
89  before  alpha = rho_new / < rt, v > 
89  before alpha = rho_new / h; 
89  before apply first correction to x 
89  before r = r - alpha*v 
89  before test stop criteria 
89 istlsolver::printOutput 
  0.5      1.30064e-21      2.43002e-13
89 === rate=5.90498e-26, T=0.0266261, TIT=0.0532522, IT=0.5
89 finished solveLinearSystemImpl_ 1
89 converged value: 1 121 1 let s go get convergedRemote
89 to  comm_.min(converged) 
89 did  comm_.min(converged) convergedRemote: 1
89 final convergedRemote: 1
89 Update: x^(k+1) = x^k - deltax^k 
89 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
89 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
89 Newton iteration  1 done89 , maximum relative shift = 3.976e-03
89 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
22: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 20
22: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 20
22: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 20
22: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 20
22: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 20
22: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 20
22: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 10 20
22: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 11 20
22: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 12 20
22: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 13 20
22: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 14 20
22: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 15 20
22: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 16 20
22: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 17 20
22: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 18 20
22: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 19 20
22: communicator.hh::sendRecv, left MPI_Wait
22: leave BufferedCommunicator::sendRecv
22: communicator.hh::BufferedCommunicator::forward, left sendRecv
22 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), after communicator.free();
22 schwarz.hh:virtual void apply (X& v, const Y& d), AFTER communication.copyOwnerToAll(v,v); 
22void AMG<M,X,S,PI,A>::mgc(LevelContext& levelContext), finished 
22 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), AFTER mgc(levelContext) 
22 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), finished 
22   AFTER _prec->apply(y,p), before v = A * y 
22  before  alpha = rho_new / < rt, v > 
22  before alpha = rho_new / h; 
22  before apply first correction to x 
22  before r = r - alpha*v 
22  before test stop criteria 
22 istlsolver::printOutput 
  0.5      1.30064e-21      2.43002e-13
22 === rate=5.90498e-26, T=0.0267023, TIT=0.0534047, IT=0.5
22 finished solveLinearSystemImpl_ 1
22 converged value: 1 121 1 let s go get convergedRemote
22 to  comm_.min(converged) 
22 did  comm_.min(converged) convergedRemote: 1
22 final convergedRemote: 1
22 Update: x^(k+1) = x^k - deltax^k 
22 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
22 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
22 Newton iteration  1 done22 , maximum relative shift = 3.976e-03
22 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
105: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 9
105: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 9
105: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 9
105: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 9
105: communicator.hh::sendRecv, left MPI_Wait
105: leave BufferedCommunicator::sendRecv
105: communicator.hh::BufferedCommunicator::forward, left sendRecv
105 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), after communicator.free();
105 schwarz.hh:virtual void apply (X& v, const Y& d), AFTER communication.copyOwnerToAll(v,v); 
105void AMG<M,X,S,PI,A>::mgc(LevelContext& levelContext), finished 
105 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), AFTER mgc(levelContext) 
105 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), finished 
105   AFTER _prec->apply(y,p), before v = A * y 
105  before  alpha = rho_new / < rt, v > 
105  before alpha = rho_new / h; 
105  before apply first correction to x 
105  before r = r - alpha*v 
105  before test stop criteria 
105 istlsolver::printOutput 
  0.5      1.30064e-21      2.43002e-13
105 === rate=5.90498e-26, T=0.0266354, TIT=0.0532708, IT=0.5
105 finished solveLinearSystemImpl_ 1
105 converged value: 1 121 1 let s go get convergedRemote
105 to  comm_.min(converged) 
105 did  comm_.min(converged) convergedRemote: 1
105 final convergedRemote: 1
105 Update: x^(k+1) = x^k - deltax^k 
105 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
105 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
105 Newton iteration  1 done105 , maximum relative shift = 3.976e-03
105 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
58 schwarz.hh:virtual void apply (X& v, const Y& d), before communication.copyOwnerToAll(v,v); 
58 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
58: communicator.hh::BufferedCommunicator::forward, enter sendRecv
58: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 21
58: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 21
58: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 21
58: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 21
58: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 21
58: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 21
58: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 21
58: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 21
58: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 21
58: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 21
58: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 10 21
58: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 11 21
58: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 12 21
58: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 13 21
58: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 14 21
58: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 15 21
58: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 16 21
58: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 17 21
58: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 18 21
58: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 19 21
58: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 20 21
58: communicator.hh::sendRecv, left MPI_Wait
58: leave BufferedCommunicator::sendRecv
58: communicator.hh::BufferedCommunicator::forward, left sendRecv
58 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), after communicator.free();
58 schwarz.hh:virtual void apply (X& v, const Y& d), AFTER communication.copyOwnerToAll(v,v); 
58void AMG<M,X,S,PI,A>::mgc(LevelContext& levelContext), finished 
58 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), AFTER mgc(levelContext) 
58 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), finished 
58   AFTER _prec->apply(y,p), before v = A * y 
58  before  alpha = rho_new / < rt, v > 
58  before alpha = rho_new / h; 
58  before apply first correction to x 
58  before r = r - alpha*v 
58  before test stop criteria 
58 istlsolver::printOutput 
  0.5      1.30064e-21      2.43002e-13
58 === rate=5.90498e-26, T=0.0266727, TIT=0.0533455, IT=0.5
58 finished solveLinearSystemImpl_ 1
58 converged value: 1 121 1 let s go get convergedRemote
58 to  comm_.min(converged) 
58 did  comm_.min(converged) convergedRemote: 1
58 final convergedRemote: 1
58 Update: x^(k+1) = x^k - deltax^k 
58 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
58 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
58 Newton iteration  1 done58 , maximum relatiunicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 20
71: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 20
71: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 20
71: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 20
71: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 20
71: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 10 20
71: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 11 20
71: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 12 20
71: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 13 20
71: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 14 20
71: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 15 20
71: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 16 20
71: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 17 20
71: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 18 20
71: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 19 20
71: communicator.hh::sendRecv, left MPI_Wait
71: leave BufferedCommunicator::sendRecv
71: communicator.hh::BufferedCommunicator::forward, left sendRecv
71 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), after communicator.free();
71 schwarz.hh:virtual void apply (X& v, const Y& d), AFTER communication.copyOwnerToAll(v,v); 
71void AMG<M,X,S,PI,A>::mgc(LevelContext& levelContext), finished 
71 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), AFTER mgc(levelContext) 
71 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), finished 
71   AFTER _prec->apply(y,p), before v = A * y 
71  before  alpha = rho_new / < rt, v > 
71  before alpha = rho_new / h; 
71  before apply first correction to x 
71  before r = r - alpha*v 
71  before test stop criteria 
71 istlsolver::printOutput 
  0.5      1.30064e-21      2.43002e-13
71 === rate=5.90498e-26, T=0.0266233, TIT=0.0532466, IT=0.5
71 finished solveLinearSystemImpl_ 1
71 converged value: 1 121 1 let s go get convergedRemote
71 to  comm_.min(converged) 
71 did  comm_.min(converged) convergedRemote: 1
71 final convergedRemote: 1
71 Update: x^(k+1) = x^k - deltax^k 
71 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
71 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
71 Newton iteration  1 done71 , maximum relative shift = 3.976e-03
71 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
32: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 16
32: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 16
32: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 16
32: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 16
32: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 16
32: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 16
32: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 16
32: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 16
32: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 10 16
32: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 11 16
32: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 12 16
32: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 13 16
32: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 14 16
32: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 15 16
32: communicator.hh::sendRecv, left MPI_Wait
32: leave BufferedCommunicator::sendRecv
32: communicator.hh::BufferedCommunicator::forward, left sendRecv
32 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), after communicator.free();
32 schwarz.hh:virtual void apply (X& v, const Y& d), AFTER communication.copyOwnerToAll(v,v); 
32void AMG<M,X,S,PI,A>::mgc(LevelContext& levelContext), finished 
32 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), AFTER mgc(levelContext) 
32 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), finished 
32   AFTER _prec->apply(y,p), before v = A * y 
32  before  alpha = rho_new / < rt, v > 
32  before alpha = rho_new / h; 
32  before apply first correction to x 
32  before r = r - alpha*v 
32  before test stop criteria 
32 istlsolver::printOutput 
  0.5      1.30064e-21      2.43002e-13
32 === rate=5.90498e-26, T=0.0265927, TIT=0.0531855, IT=0.5
32 finished solveLinearSystemImpl_ 1
32 converged value: 1 121 1 let s go get convergedRemote
32 to  comm_.min(converged) 
32 did  comm_.min(converged) convergedRemote: 1
32 final convergedRemote: 1
32 Update: x^(k+1) = x^k - deltax^k 
32 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
32 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
32 Newton iteration  1 done32 , maximum relative shift = 3.976e-03
32 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
33: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 9
33: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 9
33: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 9
33: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 9
33: communicator.hh::sendRecv, left MPI_Wait
33: leave BufferedCommunicator::sendRecv
33: communicator.hh::BufferedCommunicator::forward, left sendRecv
33 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), after communicator.free();
33 schwarz.hh:virtual void apply (X& v, const Y& d), AFTER communication.copyOwnerToAll(v,v); 
33void AMG<M,X,S,PI,A>::mgc(LevelContext& levelContext), finished 
33 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), AFTER mgc(levelContext) 
33 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), finished 
33   AFTER _prec->apply(y,p), before v = A * y 
33  before  alpha = rho_new / < rt, v > 
33  before alpha = rho_new / h; 
33  before apply first correction to x 
33  before r = r - alpha*v 
33  before test stop criteria 
33 istlsolver::printOutput 
  0.5      1.30064e-21      2.43002e-13
33 === rate=5.90498e-26, T=0.0266101, TIT=0.0532202, IT=0.5
33 finished solveLinearSystemImpl_ 1
33 converged value: 1 121 1 let s go get convergedRemote
33 to  comm_.min(converged) 
33 did  comm_.min(converged) convergedRemote: 1
33 final convergedRemote: 1
33 Update: x^(k+1) = x^k - deltax^k 
33 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
33 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
33 Newton iteration  1 done33 , maximum relative shift = 3.976e-03
33 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
35: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 9
35: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 9
35: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 9
35: communicator.hh::sendRecv, left MPI_Wait
35: leave BufferedCommunicator::sendRecv
35: communicator.hh::BufferedCommunicator::forward, left sendRecv
35 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), after communicator.free();
35 schwarz.hh:virtual void apply (X& v, const Y& d), AFTER communication.copyOwnerToAll(v,v); 
35void AMG<M,X,S,PI,A>::mgc(LevelContext& levelContext), finished 
35 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), AFTER mgc(levelContext) 
35 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), finished 
35   AFTER _prec->apply(y,p), before v = A * y 
35  before  alpha = rho_new / < rt, v > 
35  before alpha = rho_new / h; 
35  before apply first correction to x 
35  before r = r - alpha*v 
35  before test stop criteria 
35 istlsolver::printOutput 
  0.5      1.30064e-21      2.43002e-13
35 === rate=5.90498e-26, T=0.0266221, TIT=0.0532441, IT=0.5
35 finished solveLinearSystemImpl_ 1
35 converged value: 1 121 1 let s go get convergedRemote
35 to  comm_.min(converged) 
35 did  comm_.min(converged) convergedRemote: 1
35 final convergedRemote: 1
35 Update: x^(k+1) = x^k - deltax^k 
35 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
35 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
35 Newton iteration  1 done35 , maximum relative shift = 3.976e-03
35 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
38: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 24
38: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 24
38: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 24
38: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 24
38: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 24
38: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 24
38: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 24
38: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 24
38: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 24
38: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 10 24
38: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 11 24
38: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 12 24
38: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 13 24
38: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 14 24
38: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 15 24
38: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 16 24
38: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 17 24
38: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 18 24
38: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 19 24
38: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 20 24
38: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 21 24
38: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 22 24
38: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 23 24
38: communicator.hh::sendRecv, left MPI_Wait
38: leave BufferedCommunicator::sendRecv
38: communicator.hh::BufferedCommunicator::forward, left sendRecv
38 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), after communicator.free();
38 schwarz.hh:virtual void apply (X& v, const Y& d), AFTER communication.copyOwnerToAll(v,v); 
38void AMG<M,X,S,PI,A>::mgc(LevelContext& levelContext), finished 
38 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), AFTER mgc(levelContext) 
38 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), finished 
38   AFTER _prec->apply(y,p), before v = A * y 
38  before  alpha = rho_new / < rt, v > 
38  before alpha = rho_new / h; 
38  before apply first correction to x 
38  before r = r - alpha*v 
38  before test stop criteria 
38 istlsolver::printOutput 
  0.5      1.30064e-21      2.43002e-13
38 === rate=5.90498e-26, T=0.0266591, TIT=0.0533182, IT=0.5
38 finished solveLinearSystemImpl_ 1
38 converged value: 1 121 1 let s go get convergedRemote
38 to  comm_.min(converged) 
38 did  comm_.min(converged) convergedRemote: 1
38 final convergedRemote: 1
38 Update: x^(k+1) = x^k - deltax^k 
38 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
38 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
38 Newton iteration  1 done38 , maximum relative shift = 3.976e-03
38 Assemble: r(x^k) = dS/dt + div F - q;   M20: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 10
20: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 10
20: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 10
20: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 10
20: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 10
20: communicator.hh::sendRecv, left MPI_Wait
20: leave BufferedCommunicator::sendRecv
20: communicator.hh::BufferedCommunicator::forward, left sendRecv
20 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), after communicator.free();
20 schwarz.hh:virtual void apply (X& v, const Y& d), AFTER communication.copyOwnerToAll(v,v); 
20void AMG<M,X,S,PI,A>::mgc(LevelContext& levelContext), finished 
20 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), AFTER mgc(levelContext) 
20 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), finished 
20   AFTER _prec->apply(y,p), before v = A * y 
20  before  alpha = rho_new / < rt, v > 
20  before alpha = rho_new / h; 
20  before apply first correction to x 
20  before r = r - alpha*v 
20  before test stop criteria 
20 istlsolver::printOutput 
  0.5      1.30064e-21      2.43002e-13
20 === rate=5.90498e-26, T=0.0266039, TIT=0.0532079, IT=0.5
20 finished solveLinearSystemImpl_ 1
20 converged value: 1 121 1 let s go get convergedRemote
20 to  comm_.min(converged) 
20 did  comm_.min(converged) convergedRemote: 1
20 final convergedRemote: 1
20 Update: x^(k+1) = x^k - deltax^k 
20 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
20 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
20 Newton iteration  1 done20 , maximum relative shift = 3.976e-03
20 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
54: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 18
54: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 18
54: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 18
54: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 18
54: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 18
54: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 18
54: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 18
54: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 18
54: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 18
54: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 10 18
54: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 11 18
54: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 12 18
54: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 13 18
54: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 14 18
54: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 15 18
54: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 16 18
54: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 17 18
54: communicator.hh::sendRecv, left MPI_Wait
54: leave BufferedCommunicator::sendRecv
54: communicator.hh::BufferedCommunicator::forward, left sendRecv
54 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), after communicator.free();
54 schwarz.hh:virtual void apply (X& v, const Y& d), AFTER communication.copyOwnerToAll(v,v); 
54void AMG<M,X,S,PI,A>::mgc(LevelContext& levelContext), finished 
54 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), AFTER mgc(levelContext) 
54 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), finished 
54   AFTER _prec->apply(y,p), before v = A * y 
54  before  alpha = rho_new / < rt, v > 
54  before alpha = rho_new / h; 
54  before apply first correction to x 
54  before r = r - alpha*v 
54  before test stop criteria 
54 istlsolver::printOutput 
  0.5      1.30064e-21      2.43002e-13
54 === rate=5.90498e-26, T=0.02669, TIT=0.05338, IT=0.5
54 finished solveLinearSystemImpl_ 1
54 converged value: 1 121 1 let s go get convergedRemote
54 to  comm_.min(converged) 
54 did  comm_.min(converged) convergedRemote: 1
54 final convergedRemote: 1
54 Update: x^(k+1) = x^k - deltax^k 
54 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
54 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
54 Newton iteration  1 done54 , maximum relative shift = 3.976e-03
54 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
104: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 9
104: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 9
104: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 9
104: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 9
104: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 9
104: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 9
104: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 9
104: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 9
104: communicator.hh::sendRecv, left MPI_Wait
104: leave BufferedCommunicator::sendRecv
104: communicator.hh::BufferedCommunicator::forward, left sendRecv
104 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), after communicator.free();
104 schwarz.hh:virtual void apply (X& v, const Y& d), AFTER communication.copyOwnerToAll(v,v); 
104void AMG<M,X,S,PI,A>::mgc(LevelContext& levelContext), finished 
104 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), AFTER mgc(levelContext) 
104 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), finished 
104   AFTER _prec->apply(y,p), before v = A * y 
104  before  alpha = rho_new / < rt, v > 
104  before alpha = rho_new / h; 
104  before apply first correction to x 
104  before r = r - alpha*v 
104  before test stop criteria 
104 istlsolver::printOutput 
  0.5      1.30064e-21      2.43002e-13
104 === rate=5.90498e-26, T=0.0266431, TIT=0.0532862, IT=0.5
104 finished solveLinearSystemImpl_ 1
104 converged value: 1 121 1 let s go get convergedRemote
104 to  comm_.min(converged) 
104 did  comm_.min(converged) convergedRemote: 1
104 final convergedRemote: 1
104 Update: x^(k+1) = x^k - deltax^k 
104 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
104 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
104 Newton iteration  1 done104 , maximum relative shift = 3.976e-03
104 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
ssageInformation_.size(), recvRequests, &finished, &status); 0 22
117: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 22
117: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 22
117: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 22
117: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 22
117: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 22
117: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 22
117: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 22
117: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 22
117: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 22
117: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 10 22
117: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 11 22
117: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 12 22
117: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 13 22
117: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 14 22
117: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 15 22
117: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 16 22
117: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 17 22
117: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 18 22
117: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 19 22
117: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 20 22
117: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 21 22
117: communicator.hh::sendRecv, left MPI_Wait
117: leave BufferedCommunicator::sendRecv
117: communicator.hh::BufferedCommunicator::forward, left sendRecv
117 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), after communicator.free();
117 schwarz.hh:virtual void apply (X& v, const Y& d), AFTER communication.copyOwnerToAll(v,v); 
117void AMG<M,X,S,PI,A>::mgc(LevelContext& levelContext), finished 
117 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), AFTER mgc(levelContext) 
117 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), finished 
117   AFTER _prec->apply(y,p), before v = A * y 
117  before  alpha = rho_new / < rt, v > 
117  before alpha = rho_new / h; 
117  before apply first correction to x 
117  before r = r - alpha*v 
117  before test stop criteria 
117 istlsolver::printOutput 
  0.5      1.30064e-21      2.43002e-13
117 === rate=5.90498e-26, T=0.0266761, TIT=0.0533523, IT=0.5
117 finished solveLinearSystemImpl_ 1
117 converged value: 1 121 1 let s go get convergedRemote
117 to  comm_.min(converged) 
117 did  comm_.min(converged) convergedRemote: 1
117 final convergedRemote: 1
117 Update: x^(k+1) = x^k - deltax^k 
117 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
117 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
117 Newton iteration  1 done117 , maximum relative shift = 3.976e-03
117 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
67: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 13
67: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 13
67: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 13
67: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 13
67: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 10 13
67: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 11 13
67: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 12 13
67: communicator.hh::sendRecv, left MPI_Wait
67: leave BufferedCommunicator::sendRecv
67: communicator.hh::BufferedCommunicator::forward, left sendRecv
67 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), after communicator.free();
67 schwarz.hh:virtual void apply (X& v, const Y& d), AFTER communication.copyOwnerToAll(v,v); 
67void AMG<M,X,S,PI,A>::mgc(LevelContext& levelContext), finished 
67 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), AFTER mgc(levelContext) 
67 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), finished 
67   AFTER _prec->apply(y,p), before v = A * y 
67  before  alpha = rho_new / < rt, v > 
67  before alpha = rho_new / h; 
67  before apply first correction to x 
67  before r = r - alpha*v 
67  before test stop criteria 
67 istlsolver::printOutput 
  0.5      1.30064e-21      2.43002e-13
67 === rate=5.90498e-26, T=0.026601, TIT=0.0532019, IT=0.5
67 finished solveLinearSystemImpl_ 1
67 converged value: 1 121 1 let s go get convergedRemote
67 to  comm_.min(converged) 
67 did  comm_.min(converged) convergedRemote: 1
67 final convergedRemote: 1
67 Update: x^(k+1) = x^k - deltax^k 
67 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
67 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
67 Newton iteration  1 done67 , maximum relative shift = 3.976e-03
67 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
59: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 11
59: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 11
59: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 11
59: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 11
59: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 11
59: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 11
59: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 11
59: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 11
59: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 11
59: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 10 11
59: communicator.hh::sendRecv, left MPI_Wait
59: leave BufferedCommunicator::sendRecv
59: communicator.hh::BufferedCommunicator::forward, left sendRecv
59 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), after communicator.free();
59 schwarz.hh:virtual void apply (X& v, const Y& d), AFTER communication.copyOwnerToAll(v,v); 
59void AMG<M,X,S,PI,A>::mgc(LevelContext& levelContext), finished 
59 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), AFTER mgc(levelContext) 
59 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), finished 
59   AFTER _prec->apply(y,p), before v = A * y 
59  before  alpha = rho_new / < rt, v > 
59  before alpha = rho_new / h; 
59  before apply first correction to x 
59  before r = r - alpha*v 
59  before test stop criteria 
59 istlsolver::printOutput 
  0.5      1.30064e-21      2.43002e-13
59 === rate=5.90498e-26, T=0.0266674, TIT=0.0533349, IT=0.5
59 finished solveLinearSystemImpl_ 1
59 converged value: 1 121 1 let s go get convergedRemote
59 to  comm_.min(converged) 
59 did  comm_.min(converged) convergedRemote: 1
59 final convergedRemote: 1
59 Update: x^(k+1) = x^k - deltax^k 
59 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
59 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
59 Newton iteration  1 done59 , maximum relative shift = 3.976e-03
59 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
90: communicator.hh::sendRecv, left MPI_Wait
90: leave BufferedCommunicator::sendRecv
90: communicator.hh::BufferedCommunicator::forward, left sendRecv
90 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), after communicator.free();
90 schwarz.hh:virtual void apply (X& v, const Y& d), AFTER communication.copyOwnerToAll(v,v); 
90void AMG<M,X,S,PI,A>::mgc(LevelContext& levelContext), finished 
90 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), AFTER mgc(levelContext) 
90 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), finished 
90   AFTER _prec->apply(y,p), before v = A * y 
90  before  alpha = rho_new / < rt, v > 
90  before alpha = rho_new / h; 
90  before apply first correction to x 
90  before r = r - alpha*v 
90  before test stop criteria 
90 istlsolver::printOutput 
  0.5      1.30064e-21      2.43002e-13
90 === rate=5.90498e-26, T=0.0266752, TIT=0.0533503, IT=0.5
90 finished solveLinearSystemImpl_ 1
90 converged value: 1 121 1 let s go get convergedRemote
90 to  comm_.min(converged) 
90 did  comm_.min(converged) convergedRemote: 1
90 final convergedRemote: 1
90 Update: x^(k+1) = x^k - deltax^k 
90 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
90 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
90 Newton iteration  1 done90 , maximum relative shift = 3.976e-03
90 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
25: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 7
25: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 7
25: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 7
25: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 7
25: communicator.hh::sendRecv, left MPI_Wait
25: leave BufferedCommunicator::sendRecv
25: communicator.hh::BufferedCommunicator::forward, left sendRecv
25 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), after communicator.free();
25 schwarz.hh:virtual void apply (X& v, const Y& d), AFTER communication.copyOwnerToAll(v,v); 
25void AMG<M,X,S,PI,A>::mgc(LevelContext& levelContext), finished 
25 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), AFTER mgc(levelContext) 
25 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), finished 
25   AFTER _prec->apply(y,p), before v = A * y 
25  before  alpha = rho_new / < rt, v > 
25  before alpha = rho_new / h; 
25  before apply first correction to x 
25  before r = r - alpha*v 
25  before test stop criteria 
25 istlsolver::printOutput 
  0.5      1.30064e-21      2.43002e-13
25 === rate=5.90498e-26, T=0.0266063, TIT=0.0532125, IT=0.5
25 finished solveLinearSystemImpl_ 1
25 converged value: 1 121 1 let s go get convergedRemote
25 to  comm_.min(converged) 
25 did  comm_.min(converged) convergedRemote: 1
25 final convergedRemote: 1
25 Update: x^(k+1) = x^k - deltax^k 
25 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
25 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
25 Newton iteration  1 done25 , maximum relative shift = 3.976e-03
25 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
eInformation_.size(), recvRequests, &finished, &status); 4 18
113: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 18
113: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 18
113: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 18
113: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 18
113: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 18
113: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 10 18
113: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 11 18
113: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 12 18
113: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 13 18
113: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 14 18
113: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 15 18
113: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 16 18
113: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 17 18
113: communicator.hh::sendRecv, left MPI_Wait
113: leave BufferedCommunicator::sendRecv
113: communicator.hh::BufferedCommunicator::forward, left sendRecv
113 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), after communicator.free();
113 schwarz.hh:virtual void apply (X& v, const Y& d), AFTER communication.copyOwnerToAll(v,v); 
113void AMG<M,X,S,PI,A>::mgc(LevelContext& levelContext), finished 
113 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), AFTER mgc(levelContext) 
113 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), finished 
113   AFTER _prec->apply(y,p), before v = A * y 
113  before  alpha = rho_new / < rt, v > 
113  before alpha = rho_new / h; 
113  before apply first correction to x 
113  before r = r - alpha*v 
113  before test stop criteria 
113 istlsolver::printOutput 
  0.5      1.30064e-21      2.43002e-13
113 === rate=5.90498e-26, T=0.0266151, TIT=0.0532302, IT=0.5
113 finished solveLinearSystemImpl_ 1
113 converged value: 1 121 1 let s go get convergedRemote
113 to  comm_.min(converged) 
113 did  comm_.min(converged) convergedRemote: 1
113 final convergedRemote: 1
113 Update: x^(k+1) = x^k - deltax^k 
113 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
113 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
113 Newton iteration  1 done113 , maximum relative shift = 3.976e-03
113 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 14
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 10 14
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 11 14
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 12 14
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 13 14
4: communicator.hh::sendRecv, left MPI_Wait
4: leave BufferedCommunicator::sendRecv
4: communicator.hh::BufferedCommunicator::forward, left sendRecv
4 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), after communicator.free();
4 schwarz.hh:virtual void apply (X& v, const Y& d), AFTER communication.copyOwnerToAll(v,v); 
4void AMG<M,X,S,PI,A>::mgc(LevelContext& levelContext), finished 
4 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), AFTER mgc(levelContext) 
4 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), finished 
4   AFTER _prec->apply(y,p), before v = A * y 
4  before  alpha = rho_new / < rt, v > 
4  before alpha = rho_new / h; 
4  before apply first correction to x 
4  before r = r - alpha*v 
4  before test stop criteria 
4 istlsolver::printOutput 
  0.5      1.30064e-21      2.43002e-13
4 === rate=5.90498e-26, T=0.0266198, TIT=0.0532396, IT=0.5
4 finished solveLinearSystemImpl_ 1
4 converged value: 1 121 1 let s go get convergedRemote
4 to  comm_.min(converged) 
4 did  comm_.min(converged) convergedRemote: 1
4 final convergedRemote: 1
4 Update: x^(k+1) = x^k - deltax^k 
4 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
4 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
4 Newton iteration  1 done4 , maximum relative shift = 3.976e-03
4 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
74: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 11
74: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 11
74: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 11
74: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 11
74: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 11
74: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 10 11
74: communicator.hh::sendRecv, left MPI_Wait
74: leave BufferedCommunicator::sendRecv
74: communicator.hh::BufferedCommunicator::forward, left sendRecv
74 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), after communicator.free();
74 schwarz.hh:virtual void apply (X& v, const Y& d), AFTER communication.copyOwnerToAll(v,v); 
74void AMG<M,X,S,PI,A>::mgc(LevelContext& levelContext), finished 
74 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), AFTER mgc(levelContext) 
74 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), finished 
74   AFTER _prec->apply(y,p), before v = A * y 
74  before  alpha = rho_new / < rt, v > 
74  before alpha = rho_new / h; 
74  before apply first correction to x 
74  before r = r - alpha*v 
74  before test stop criteria 
74 istlsolver::printOutput 
  0.5      1.30064e-21      2.43002e-13
74 === rate=5.90498e-26, T=0.0266108, TIT=0.0532217, IT=0.5
74 finished solveLinearSystemImpl_ 1
74 converged value: 1 121 1 let s go get convergedRemote
74 to  comm_.min(converged) 
74 did  comm_.min(converged) convergedRemote: 1
74 final convergedRemote: 1
74 Update: x^(k+1) = x^k - deltax^k 
74 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
74 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
74 Newton iteration  1 done74 , maximum relative shift = 3.976e-03
74 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
91: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 9
91: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 9
91: communicator.hh::sendRecv, left MPI_Wait
91: leave BufferedCommunicator::sendRecv
91: communicator.hh::BufferedCommunicator::forward, left sendRecv
91 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), after communicator.free();
91 schwarz.hh:virtual void apply (X& v, const Y& d), AFTER communication.copyOwnerToAll(v,v); 
91void AMG<M,X,S,PI,A>::mgc(LevelContext& levelContext), finished 
91 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), AFTER mgc(levelContext) 
91 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), finished 
91   AFTER _prec->apply(y,p), before v = A * y 
91  before  alpha = rho_new / < rt, v > 
91  before alpha = rho_new / h; 
91  before apply first correction to x 
91  before r = r - alpha*v 
91  before test stop criteria 
91 istlsolver::printOutput 
  0.5      1.30064e-21      2.43002e-13
91 === rate=5.90498e-26, T=0.0266404, TIT=0.0532808, IT=0.5
91 finished solveLinearSystemImpl_ 1
91 converged value: 1 121 1 let s go get convergedRemote
91 to  comm_.min(converged) 
91 did  comm_.min(converged) convergedRemote: 1
91 final convergedRemote: 1
91 Update: x^(k+1) = x^k - deltax^k 
91 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
91 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
91 Newton iteration  1 done91 , maximum relative shift = 3.976e-03
91 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
efore MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 15 20
109: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 16 20
109: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 17 20
109: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 18 20
109: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 19 20
109: communicator.hh::sendRecv, left MPI_Wait
109: leave BufferedCommunicator::sendRecv
109: communicator.hh::BufferedCommunicator::forward, left sendRecv
109 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), after communicator.free();
109 schwarz.hh:virtual void apply (X& v, const Y& d), AFTER communication.copyOwnerToAll(v,v); 
109 schwarz.hh:virtual void apply (X& v, const Y& d), before preconditioner.apply(v,d);; 
109 schwarz.hh:virtual void apply (X& v, const Y& d), before communication.copyOwnerToAll(v,v); 
109 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
109: communicator.hh::BufferedCommunicator::forward, enter sendRecv
109: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 20
109: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 20
109: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 20
109: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 20
109: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 20
109: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 20
109: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 20
109: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 20
109: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 20
109: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 20
109: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 10 20
109: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 11 20
109: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 12 20
109: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 13 20
109: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 14 20
109: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 15 20
109: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 16 20
109: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 17 20
109: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 18 20
109: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 19 20
109: communicator.hh::sendRecv, left MPI_Wait
109: leave BufferedCommunicator::sendRecv
109: communicator.hh::BufferedCommunicator::forward, left sendRecv
109 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), after communicator.free();
109 schwarz.hh:virtual void apply (X& v, const Y& d), AFTER communication.copyOwnerToAll(v,v); 
109void AMG<M,X,S,PI,A>::mgc(LevelContext& levelCInformation_.size(), recvRequests, &finished, &status); 5 11
44: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 11
44: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 11
44: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 11
44: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 11
44: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 10 11
44: communicator.hh::sendRecv, left MPI_Wait
44: leave BufferedCommunicator::sendRecv
44: communicator.hh::BufferedCommunicator::forward, left sendRecv
44 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), after communicator.free();
44 schwarz.hh:virtual void apply (X& v, const Y& d), AFTER communication.copyOwnerToAll(v,v); 
44void AMG<M,X,S,PI,A>::mgc(LevelContext& levelContext), finished 
44 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), AFTER mgc(levelContext) 
44 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), finished 
44   AFTER _prec->apply(y,p), before v = A * y 
44  before  alpha = rho_new / < rt, v > 
44  before alpha = rho_new / h; 
44  before apply first correction to x 
44  before r = r - alpha*v 
44  before test stop criteria 
44 istlsolver::printOutput 
  0.5      1.30064e-21      2.43002e-13
44 === rate=5.90498e-26, T=0.0266557, TIT=0.0533113, IT=0.5
44 finished solveLinearSystemImpl_ 1
44 converged value: 1 121 1 let s go get convergedRemote
44 to  comm_.min(converged) 
44 did  comm_.min(converged) convergedRemote: 1
44 final convergedRemote: 1
44 Update: x^(k+1) = x^k - deltax^k 
44 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
44 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
44 Newton iteration  1 done44 , maximum relative shift = 3.976e-03
44 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
opyOwnerToAll(v,v); 
106 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
106: communicator.hh::BufferedCommunicator::forward, enter sendRecv
106: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 16
106: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 16
106: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 16
106: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 16
106: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 16
106: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 16
106: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 16
106: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 16
106: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 16
106: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 16
106: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 10 16
106: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 11 16
106: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 12 16
106: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 13 16
106: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 14 16
106: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 15 16
106: communicator.hh::sendRecv, left MPI_Wait
106: leave BufferedCommunicator::sendRecv
106: communicator.hh::BufferedCommunicator::forward, left sendRecv
106 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), after communicator.free();
106 schwarz.hh:virtual void apply (X& v, const Y& d), AFTER communication.copyOwnerToAll(v,v); 
106void AMG<M,X,S,PI,A>::mgc(LevelContext& levelContext), finished 
106 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), AFTER mgc(levelContext) 
106 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), finished 
106   AFTER _prec->apply(y,p), before v = A * y 
106  before  alpha = rho_new / < rt, v > 
106  before alpha = rho_new / h; 
106  before apply first correction to x 
106  before r = r - alpha*v 
106  before test stop criteria 
106 istlsolver::printOutput 
  0.5      1.30064e-21      2.43002e-13
106 === rate=5.90498e-26, T=0.0266729, TIT=0.0533459, IT=0.5
106 finished solveLinearSystemImpl_ 1
106 converged value: 1 121 1 let s go get convergedRemote
106 to  comm_.min(converged) 
106 did  comm_.min(converged) convergedRemote: 1
106 final convergedRemote: 1
106 Update: x^(k+1) = x^k - deltax^k 
106 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
106 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
106 Newton iteration  1 done106 , maximum relative shift = 3.976e-03
106 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
I_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 9
108: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 9
108: communicator.hh::sendRecv, left MPI_Wait
108: leave BufferedCommunicator::sendRecv
108: communicator.hh::BufferedCommunicator::forward, left sendRecv
108 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), after communicator.free();
108 schwarz.hh:virtual void apply (X& v, const Y& d), AFTER communication.copyOwnerToAll(v,v); 
108void AMG<M,X,S,PI,A>::mgc(LevelContext& levelContext), finished 
108 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), AFTER mgc(levelContext) 
108 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), finished 
108   AFTER _prec->apply(y,p), before v = A * y 
108  before  alpha = rho_new / < rt, v > 
108  before alpha = rho_new / h; 
108  before apply first correction to x 
108  before r = r - alpha*v 
108  before test stop criteria 
108 istlsolver::printOutput 
  0.5      1.30064e-21      2.43002e-13
108 === rate=5.90498e-26, T=0.0266894, TIT=0.0533789, IT=0.5
108 finished solveLinearSystemImpl_ 1
108 converged value: 1 121 1 let s go get convergedRemote
108 to  comm_.min(converged) 
108 did  comm_.min(converged) convergedRemote: 1
108 final convergedRemote: 1
108 Update: x^(k+1) = x^k - deltax^k 
108 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
108 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
108 Newton iteration  1 done108 , maximum relative shift = 3.976e-03
108 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
Information_.size(), recvRequests, &finished, &status); 10 12
46: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 11 12
46: communicator.hh::sendRecv, left MPI_Wait
46: leave BufferedCommunicator::sendRecv
46: communicator.hh::BufferedCommunicator::forward, left sendRecv
46 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), after communicator.free();
46 schwarz.hh:virtual void apply (X& v, const Y& d), AFTER communication.copyOwnerToAll(v,v); 
46void AMG<M,X,S,PI,A>::mgc(LevelContext& levelContext), finished 
46 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), AFTER mgc(levelContext) 
46 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), finished 
46   AFTER _prec->apply(y,p), before v = A * y 
46  before  alpha = rho_new / < rt, v > 
46  before alpha = rho_new / h; 
46  before apply first correction to x 
46  before r = r - alpha*v 
46  before test stop criteria 
46 istlsolver::printOutput 
  0.5      1.30064e-21      2.43002e-13
46 === rate=5.90498e-26, T=0.0266954, TIT=0.0533909, IT=0.5
46 finished solveLinearSystemImpl_ 1
46 converged value: 1 121 1 let s go get convergedRemote
46 to  comm_.min(converged) 
46 did  comm_.min(converged) convergedRemote: 1
46 final convergedRemote: 1
46 Update: x^(k+1) = x^k - deltax^k 
46 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
46 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
46 Newton iteration  1 done46 , maximum relative shift = 3.976e-03
46 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
tor.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 11 14
24: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 12 14
24: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 13 14
24: communicator.hh::sendRecv, left MPI_Wait
24: leave BufferedCommunicator::sendRecv
24: communicator.hh::BufferedCommunicator::forward, left sendRecv
24 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), after communicator.free();
24 schwarz.hh:virtual void apply (X& v, const Y& d), AFTER communication.copyOwnerToAll(v,v); 
24void AMG<M,X,S,PI,A>::mgc(LevelContext& levelContext), finished 
24 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), AFTER mgc(levelContext) 
24 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), finished 
24   AFTER _prec->apply(y,p), before v = A * y 
24  before  alpha = rho_new / < rt, v > 
24  before alpha = rho_new / h; 
24  before apply first correction to x 
24  before r = r - alpha*v 
24  before test stop criteria 
24 istlsolver::printOutput 
  0.5      1.30064e-21      2.43002e-13
24 === rate=5.90498e-26, T=0.0266111, TIT=0.0532221, IT=0.5
24 finished solveLinearSystemImpl_ 1
24 converged value: 1 121 1 let s go get convergedRemote
24 to  comm_.min(converged) 
24 did  comm_.min(converged) convergedRemote: 1
24 final convergedRemote: 1
24 Update: x^(k+1) = x^k - deltax^k 
24 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
24 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
24 Newton iteration  1 done24 , maximum relative shift = 3.976e-03
24 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
107  before alpha = rho_new / h; 
107  before apply first correction to x 
107  before r = r - alpha*v 
107  before test stop criteria 
107 istlsolver::printOutput 
  0.5      1.30064e-21      2.43002e-13
107 === rate=5.90498e-26, T=0.0266733, TIT=0.0533466, IT=0.5
107 finished solveLinearSystemImpl_ 1
107 converged value: 1 121 1 let s go get convergedRemote
107 to  comm_.min(converged) 
107 did  comm_.min(converged) convergedRemote: 1
107 final convergedRemote: 1
107 Update: x^(k+1) = x^k - deltax^k 
107 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
107 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
107 Newton iteration  1 done107 , maximum relative shift = 3.976e-03
107 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
102: leave BufferedCommunicator::sendRecv
102: communicator.hh::BufferedCommunicator::forward, left sendRecv
102 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), after communicator.free();
102 schwarz.hh:virtual void apply (X& v, const Y& d), AFTER communication.copyOwnerToAll(v,v); 
102 schwarz.hh:virtual void apply (X& v, const Y& d), before preconditioner.apply(v,d);; 
102 schwarz.hh:virtual void apply (X& v, const Y& d), before communication.copyOwnerToAll(v,v); 
102 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
102: communicator.hh::BufferedCommunicator::forward, enter sendRecv
102: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 15
102: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 15
102: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 15
102: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 15
102: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 15
102: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 15
102: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 15
102: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 15
102: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 15
102: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 15
102: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 10 15
102: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 11 15
102: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 12 15
102: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 13 15
102: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 14 15
102: communicator.hh::sendRecv, left MPI_Wait
102: leave BufferedCommunicator::sendRecv
102: communicator.hh::BufferedCommunicator::forward, left sendRecv
102 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), after communicator.free();
102 schwarz.hh:virtual void apply (X& v, const Y& d), AFTER communication.copyOwnerToAll(v,v); 
102void AMG<M,X,S,PI,A>::mgc(LevelContext& levelContext), finished 
102 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), AFTER mgc(levelContext) 
102 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), finished 
102   AFTER _prec->apply(y,p), before v = A * y 
102  before  alpha = rho_new / < rt, v > 
102  before alpha = rho_new / h; 
102  before apply first correction to x 
102  before r = r - alpha*v 
102  before test stop criteria 
102 istlsolver::printOutput 
  0.5      1.30064e-21      2.43002e-13
102 === rate=5.90498e-26, T=0.026681, TIT=0.0533619, IT=0.5
102 finished solveLinearSystemImpl_ 1
102 converged value: 1 121 1 let s go get convergedRemote
102 to  comm_.min(converged) 
102 did  comm_.min(converged) convergedRemote: 1
102 final convergedRemote: 1
102 Update: x^(k+1) = x^k - deltax^k 
102 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
102 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
102 Newton iteration  1 done102 , maximum relative shift = 3.976e-03
102 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
112: leave BufferedCommunicator::sendRecv
112: communicator.hh::BufferedCommunicator::forward, left sendRecv
112 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), after communicator.free();
112 schwarz.hh:virtual void apply (X& v, const Y& d), AFTER communication.copyOwnerToAll(v,v); 
112 schwarz.hh:virtual void apply (X& v, const Y& d), before preconditioner.apply(v,d);; 
112 schwarz.hh:virtual void apply (X& v, const Y& d), before communication.copyOwnerToAll(v,v); 
112 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
112: communicator.hh::BufferedCommunicator::forward, enter sendRecv
112: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 5
112: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 5
112: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 5
112: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 5
112: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 5
112: communicator.hh::sendRecv, left MPI_Wait
112: leave BufferedCommunicator::sendRecv
112: communicator.hh::BufferedCommunicator::forward, left sendRecv
112 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), after communicator.free();
112 schwarz.hh:virtual void apply (X& v, const Y& d), AFTER communication.copyOwnerToAll(v,v); 
112void AMG<M,X,S,PI,A>::mgc(LevelContext& levelContext), finished 
112 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), AFTER mgc(levelContext) 
112 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), finished 
112   AFTER _prec->apply(y,p), before v = A * y 
112  before  alpha = rho_new / < rt, v > 
112  before alpha = rho_new / h; 
112  before apply first correction to x 
112  before r = r - alpha*v 
112  before test stop criteria 
112 istlsolver::printOutput 
  0.5      1.30064e-21      2.43002e-13
112 === rate=5.90498e-26, T=0.0266134, TIT=0.0532268, IT=0.5
112 finished solveLinearSystemImpl_ 1
112 converged value: 1 121 1 let s go get convergedRemote
112 to  comm_.min(converged) 
112 did  comm_.min(converged) convergedRemote: 1
112 final convergedRemote: 1
112 Update: x^(k+1) = x^k - deltax^k 
112 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
112 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
112 Newton iteration  1 done112 , maximum relative shift = 3.976e-03
112 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
93 schwarz.hh:virtual void apply (X& v, const Y& d), before communication.copyOwnerToAll(v,v); 
93 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
93: communicator.hh::BufferedCommunicator::forward, enter sendRecv
93: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 13
93: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 13
93: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 13
93: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 13
93: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 13
93: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 13
93: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 13
93: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 13
93: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 13
93: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 13
93: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 10 13
93: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 11 13
93: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 12 13
93: communicator.hh::sendRecv, left MPI_Wait
93: leave BufferedCommunicator::sendRecv
93: communicator.hh::BufferedCommunicator::forward, left sendRecv
93 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), after communicator.free();
93 schwarz.hh:virtual void apply (X& v, const Y& d), AFTER communication.copyOwnerToAll(v,v); 
93void AMG<M,X,S,PI,A>::mgc(LevelContext& levelContext), finished 
93 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), AFTER mgc(levelContext) 
93 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), finished 
93   AFTER _prec->apply(y,p), before v = A * y 
93  before  alpha = rho_new / < rt, v > 
93  before alpha = rho_new / h; 
93  before apply first correction to x 
93  before r = r - alpha*v 
93  before test stop criteria 
93 istlsolver::printOutput 
  0.5      1.30064e-21      2.43002e-13
93 === rate=5.90498e-26, T=0.0266505, TIT=0.0533011, IT=0.5
93 finished solveLinearSystemImpl_ 1
93 converged value: 1 121 1 let s go get convergedRemote
93 to  comm_.min(converged) 
93 did  comm_.min(converged) convergedRemote: 1
93 final convergedRemote: 1
93 Update: x^(k+1) = x^k - deltax^k 
93 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
93 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
93 Newton iteration  1 done93 , maximum relative shift = 3.976e-03
93 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
110: communicator.hh::sendRecv, left MPI_Wait
110: leave BufferedCommunicator::sendRecv
110: communicator.hh::BufferedCommunicator::forward, left sendRecv
110 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), after communicator.free();
110 schwarz.hh:virtual void apply (X& v, const Y& d), AFTER communication.copyOwnerToAll(v,v); 
110 schwarz.hh:virtual void apply (X& v, const Y& d), before preconditioner.apply(v,d);; 
110 schwarz.hh:virtual void apply (X& v, const Y& d), before communication.copyOwnerToAll(v,v); 
110 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
110: communicator.hh::BufferedCommunicator::forward, enter sendRecv
110: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 11
110: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 11
110: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 11
110: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 11
110: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 11
110: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 11
110: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 11
110: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 11
110: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 11
110: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 11
110: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 10 11
110: communicator.hh::sendRecv, left MPI_Wait
110: leave BufferedCommunicator::sendRecv
110: communicator.hh::BufferedCommunicator::forward, left sendRecv
110 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), after communicator.free();
110 schwarz.hh:virtual void apply (X& v, const Y& d), AFTER communication.copyOwnerToAll(v,v); 
110void AMG<M,X,S,PI,A>::mgc(LevelContext& levelContext), finished 
110 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), AFTER mgc(levelContext) 
110 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), finished 
110   AFTER _prec->apply(y,p), before v = A * y 
110  before  alpha = rho_new / < rt, v > 
110  before alpha = rho_new / h; 
110  before apply first correction to x 
110  before r = r - alpha*v 
110  before test stop criteria 
110 istlsolver::printOutput 
  0.5      1.30064e-21      2.43002e-13
110 === rate=5.90498e-26, T=0.0267088, TIT=0.0534175, IT=0.5
110 finished solveLinearSystemImpl_ 1
110 converged value: 1 121 1 let s go get convergedRemote
110 to  comm_.min(converged) 
110 did  comm_.min(converged) convergedRemote: 1
110 final convergedRemote: 1
110 Update: x^(k+1) = x^k - deltax^k 
110 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
110 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
110 Newton iteration  1 done110 , maximum relative shift = 3.976e-03
110 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
21: leave BufferedCommunicator::sendRecv
21: communicator.hh::BufferedCommunicator::forward, left sendRecv
21 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), after communicator.free();
21 schwarz.hh:virtual void apply (X& v, const Y& d), AFTER communication.copyOwnerToAll(v,v); 
21 schwarz.hh:virtual void apply (X& v, const Y& d), before preconditioner.apply(v,d);; 
21 schwarz.hh:virtual void apply (X& v, const Y& d), before communication.copyOwnerToAll(v,v); 
21 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
21: communicator.hh::BufferedCommunicator::forward, enter sendRecv
21: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 12
21: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 12
21: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 12
21: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 12
21: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 12
21: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 12
21: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 12
21: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 12
21: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 12
21: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 12
21: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 10 12
21: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 11 12
21: communicator.hh::sendRecv, left MPI_Wait
21: leave BufferedCommunicator::sendRecv
21: communicator.hh::BufferedCommunicator::forward, left sendRecv
21 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), after communicator.free();
21 schwarz.hh:virtual void apply (X& v, const Y& d), AFTER communication.copyOwnerToAll(v,v); 
21void AMG<M,X,S,PI,A>::mgc(LevelContext& levelContext), finished 
21 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), AFTER mgc(levelContext) 
21 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), finished 
21   AFTER _prec->apply(y,p), before v = A * y 
21  before  alpha = rho_new / < rt, v > 
21  before alpha = rho_new / h; 
21  before apply first correction to x 
21  before r = r - alpha*v 
21  before test stop criteria 
21 istlsolver::printOutput 
  0.5      1.30064e-21      2.43002e-13
21 === rate=5.90498e-26, T=0.0266059, TIT=0.0532118, IT=0.5
21 finished solveLinearSystemImpl_ 1
21 converged value: 1 121 1 let s go get convergedRemote
21 to  comm_.min(converged) 
21 did  comm_.min(converged) convergedRemote: 1
21 final convergedRemote: 1
21 Update: x^(k+1) = x^k - deltax^k 
21 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
21 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
21 Newton iteration  1 done21 , maximum relative shift = 3.976e-03
21 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
96: leave BufferedCommunicator::sendRecv
96: communicator.hh::BufferedCommunicator::forward, left sendRecv
96 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), after communicator.free();
96 schwarz.hh:virtual void apply (X& v, const Y& d), AFTER communication.copyOwnerToAll(v,v); 
96 schwarz.hh:virtual void apply (X& v, const Y& d), before preconditioner.apply(v,d);; 
96 schwarz.hh:virtual void apply (X& v, const Y& d), before communication.copyOwnerToAll(v,v); 
96 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
96: communicator.hh::BufferedCommunicator::forward, enter sendRecv
96: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 12
96: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 12
96: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 12
96: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 12
96: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 12
96: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 12
96: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 12
96: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 12
96: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 12
96: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 12
96: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 10 12
96: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 11 12
96: communicator.hh::sendRecv, left MPI_Wait
96: leave BufferedCommunicator::sendRecv
96: communicator.hh::BufferedCommunicator::forward, left sendRecv
96 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), after communicator.free();
96 schwarz.hh:virtual void apply (X& v, const Y& d), AFTER communication.copyOwnerToAll(v,v); 
96void AMG<M,X,S,PI,A>::mgc(LevelContext& levelContext), finished 
96 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), AFTER mgc(levelContext) 
96 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), finished 
96   AFTER _prec->apply(y,p), before v = A * y 
96  before  alpha = rho_new / < rt, v > 
96  before alpha = rho_new / h; 
96  before apply first correction to x 
96  before r = r - alpha*v 
96  before test stop criteria 
96 istlsolver::printOutput 
  0.5      1.30064e-21      2.43002e-13
96 === rate=5.90498e-26, T=0.0265914, TIT=0.0531828, IT=0.5
96 finished solveLinearSystemImpl_ 1
96 converged value: 1 121 1 let s go get convergedRemote
96 to  comm_.min(converged) 
96 did  comm_.min(converged) convergedRemote: 1
96 final convergedRemote: 1
96 Update: x^(k+1) = x^k - deltax^k 
96 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
96 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
96 Newton iteration  1 done96 , maximum relative shift = 3.976e-03
96 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
73: leave BufferedCommunicator::sendRecv
73: communicator.hh::BufferedCommunicator::forward, left sendRecv
73 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), after communicator.free();
73 schwarz.hh:virtual void apply (X& v, const Y& d), AFTER communication.copyOwnerToAll(v,v); 
73 schwarz.hh:virtual void apply (X& v, const Y& d), before preconditioner.apply(v,d);; 
73 schwarz.hh:virtual void apply (X& v, const Y& d), before communication.copyOwnerToAll(v,v); 
73 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
73: communicator.hh::BufferedCommunicator::forward, enter sendRecv
73: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 11
73: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 11
73: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 11
73: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 11
73: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 11
73: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 11
73: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 11
73: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 11
73: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 11
73: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 11
73: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 10 11
73: communicator.hh::sendRecv, left MPI_Wait
73: leave BufferedCommunicator::sendRecv
73: communicator.hh::BufferedCommunicator::forward, left sendRecv
73 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), after communicator.free();
73 schwarz.hh:virtual void apply (X& v, const Y& d), AFTER communication.copyOwnerToAll(v,v); 
73void AMG<M,X,S,PI,A>::mgc(LevelContext& levelContext), finished 
73 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), AFTER mgc(levelContext) 
73 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), finished 
73   AFTER _prec->apply(y,p), before v = A * y 
73  before  alpha = rho_new / < rt, v > 
73  before alpha = rho_new / h; 
73  before apply first correction to x 
73  before r = r - alpha*v 
73  before test stop criteria 
73 istlsolver::printOutput 
  0.5      1.30064e-21      2.43002e-13
73 === rate=5.90498e-26, T=0.026625, TIT=0.0532499, IT=0.5
73 finished solveLinearSystemImpl_ 1
73 converged value: 1 121 1 let s go get convergedRemote
73 to  comm_.min(converged) 
73 did  comm_.min(converged) convergedRemote: 1
73 final convergedRemote: 1
73 Update: x^(k+1) = x^k - deltax^k 
73 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
73 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
73 Newton iteration  1 done73 , maximum relative shift = 3.976e-03
73 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
55: leave BufferedCommunicator::sendRecv
55: communicator.hh::BufferedCommunicator::forward, left sendRecv
55 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), after communicator.free();
55 schwarz.hh:virtual void apply (X& v, const Y& d), AFTER communication.copyOwnerToAll(v,v); 
55 schwarz.hh:virtual void apply (X& v, const Y& d), before preconditioner.apply(v,d);; 
55 schwarz.hh:virtual void apply (X& v, const Y& d), before communication.copyOwnerToAll(v,v); 
55 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
55: communicator.hh::BufferedCommunicator::forward, enter sendRecv
55: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 11
55: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 11
55: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 11
55: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 11
55: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 11
55: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 11
55: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 11
55: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 11
55: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 11
55: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 11
55: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 10 11
55: communicator.hh::sendRecv, left MPI_Wait
55: leave BufferedCommunicator::sendRecv
55: communicator.hh::BufferedCommunicator::forward, left sendRecv
55 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), after communicator.free();
55 schwarz.hh:virtual void apply (X& v, const Y& d), AFTER communication.copyOwnerToAll(v,v); 
55void AMG<M,X,S,PI,A>::mgc(LevelContext& levelContext), finished 
55 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), AFTER mgc(levelContext) 
55 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), finished 
55   AFTER _prec->apply(y,p), before v = A * y 
55  before  alpha = rho_new / < rt, v > 
55  before alpha = rho_new / h; 
55  before apply first correction to x 
55  before r = r - alpha*v 
55  before test stop criteria 
55 istlsolver::printOutput 
  0.5      1.30064e-21      2.43002e-13
55 === rate=5.90498e-26, T=0.0266861, TIT=0.0533723, IT=0.5
55 finished solveLinearSystemImpl_ 1
55 converged value: 1 121 1 let s go get convergedRemote
55 to  comm_.min(converged) 
55 did  comm_.min(converged) convergedRemote: 1
55 final convergedRemote: 1
55 Update: x^(k+1) = x^k - deltax^k 
55 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
55 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
55 Newton iteration  1 done55 , maximum relative shift = 3.976e-03
55 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
78: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 14 15
78: communicator.hh::sendRecv, left MPI_Wait
78: leave BufferedCommunicator::sendRecv
78: communicator.hh::BufferedCommunicator::forward, left sendRecv
78 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), after communicator.free();
78 schwarz.hh:virtual void apply (X& v, const Y& d), AFTER communication.copyOwnerToAll(v,v); 
78 schwarz.hh:virtual void apply (X& v, const Y& d), before preconditioner.apply(v,d);; 
78 schwarz.hh:virtual void apply (X& v, const Y& d), before communication.copyOwnerToAll(v,v); 
78 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
78: communicator.hh::BufferedCommunicator::forward, enter sendRecv
78: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 15
78: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 15
78: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 15
78: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 15
78: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 15
78: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 15
78: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 15
78: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 15
78: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 15
78: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 15
78: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 10 15
78: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 11 15
78: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 12 15
78: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 13 15
78: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 14 15
78: communicator.hh::sendRecv, left MPI_Wait
78: leave BufferedCommunicator::sendRecv
78: communicator.hh::BufferedCommunicator::forward, left sendRecv
78 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), after communicator.free();
78 schwarz.hh:virtual void apply (X& v, const Y& d), AFTER communication.copyOwnerToAll(v,v); 
78void AMG<M,X,S,PI,A>::mgc(LevelContext& levelContext), finished 
78 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), AFTER mgc(levelContext) 
78 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), finished 
78   AFTER _prec->apply(y,p), before v = A * y 
78  before  alpha = rho_new / < rt, v > 
78  before alpha = rho_new / h; 
78  before apply first correction to x 
78  before r = r - alpha*v 
78  before test stop criteria 
78 istlsolver::printOutput 
  0.5      1.30064e-21      2.43002e-13
78 === rate=5.90498e-26, T=0.0266898, TIT=0.0533797, IT=0.5
78 finished solveLinearSystemImpl_ 1
78 converged value: 1 121 1 let s go get convergedRemote
78 to  comm_.min(converged) 
78 did  comm_.min(converged) convergedRemote: 1
78 final convergedRemote: 1
78 Update: x^(k+1) = x^k - deltax^k 
78 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
78 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
78 Newton iteration  1 done78 , maximum relative shift = 3.976e-03
78 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
41: leave BufferedCommunicator::sendRecv
41: communicator.hh::BufferedCommunicator::forward, left sendRecv
41 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), after communicator.free();
41 schwarz.hh:virtual void apply (X& v, const Y& d), AFTER communication.copyOwnerToAll(v,v); 
41 schwarz.hh:virtual void apply (X& v, const Y& d), before preconditioner.apply(v,d);; 
41 schwarz.hh:virtual void apply (X& v, const Y& d), before communication.copyOwnerToAll(v,v); 
41 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
41: communicator.hh::BufferedCommunicator::forward, enter sendRecv
41: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 20
41: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 20
41: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 20
41: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 20
41: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 20
41: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 20
41: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 20
41: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 20
41: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 20
41: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 20
41: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 10 20
41: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 11 20
41: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 12 20
41: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 13 20
41: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 14 20
41: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 15 20
41: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 16 20
41: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 17 20
41: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 18 20
41: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 19 20
41: communicator.hh::sendRecv, left MPI_Wait
41: leave BufferedCommunicator::sendRecv
41: communicator.hh::BufferedCommunicator::forward, left sendRecv
41 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), after communicator.free();
41 schwarz.hh:virtual void apply (X& v, const Y& d), AFTER communication.copyOwnerToAll(v,v); 
41void AMG<M,X,S,PI,A>::mgc(LevelContext& levelContext), finished 
41 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), AFTER mgc(levelContext) 
41 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), finished 
41   AFTER _prec->apply(y,p), before v = A * y 
41  before  alpha = rho_new / < rt, v > 
41  before alpha = rho_new / h; 
41  before apply first correction to x 
41  before r = r - alpha*v 
41  before test stop criteria 
41 istlsolver::printOutput 
  0.5      1.30064e-21      2.43002e-13
41 === rate=5.90498e-26, T=0.0266144, TIT=0.0532289, IT=0.5
41 finished solveLinearSystemImpl_ 1
41 converged value: 1 121 1 let s go get convergedRemote
41 to  comm_.min(converged) 
41 2: leave BufferedCommunicator::sendRecv
2: communicator.hh::BufferedCommunicator::forward, left sendRecv
2 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), after communicator.free();
2 schwarz.hh:virtual void apply (X& v, const Y& d), AFTER communication.copyOwnerToAll(v,v); 
2 schwarz.hh:virtual void apply (X& v, const Y& d), before preconditioner.apply(v,d);; 
2 schwarz.hh:virtual void apply (X& v, const Y& d), before communication.copyOwnerToAll(v,v); 
2 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
2: communicator.hh::BufferedCommunicator::forward, enter sendRecv
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 8
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 8
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 8
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 8
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 8
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 8
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 8
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 8
2: communicator.hh::sendRecv, left MPI_Wait
2: leave BufferedCommunicator::sendRecv
2: communicator.hh::BufferedCommunicator::forward, left sendRecv
2 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), after communicator.free();
2 schwarz.hh:virtual void apply (X& v, const Y& d), AFTER communication.copyOwnerToAll(v,v); 
2void AMG<M,X,S,PI,A>::mgc(LevelContext& levelContext), finished 
2 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), AFTER mgc(levelContext) 
2 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), finished 
2   AFTER _prec->apply(y,p), before v = A * y 
2  before  alpha = rho_new / < rt, v > 
2  before alpha = rho_new / h; 
2  before apply first correction to x 
2  before r = r - alpha*v 
2  before test stop criteria 
2 istlsolver::printOutput 
  0.5      1.30064e-21      2.43002e-13
2 === rate=5.90498e-26, T=0.0266296, TIT=0.0532592, IT=0.5
2 finished solveLinearSystemImpl_ 1
2 converged value: 1 121 1 let s go get convergedRemote
2 to  comm_.min(converged) 
2 did  comm_.min(converged) convergedRemote: 1
2 final convergedRemote: 1
2 Update: x^(k+1) = x^k - deltax^k 
2 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
2 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
2 Newton iteration  1 done2 , maximum relative shift = 3.976e-03
2 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
36: leave BufferedCommunicator::sendRecv
36: communicator.hh::BufferedCommunicator::forward, left sendRecv
36 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), after communicator.free();
36 schwarz.hh:virtual void apply (X& v, const Y& d), AFTER communication.copyOwnerToAll(v,v); 
36 schwarz.hh:virtual void apply (X& v, const Y& d), before preconditioner.apply(v,d);; 
36 schwarz.hh:virtual void apply (X& v, const Y& d), before communication.copyOwnerToAll(v,v); 
36 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
36: communicator.hh::BufferedCommunicator::forward, enter sendRecv
36: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 14
36: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 14
36: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 14
36: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 14
36: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 14
36: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 14
36: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 14
36: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 14
36: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 14
36: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 14
36: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 10 14
36: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 11 14
36: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 12 14
36: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 13 14
36: communicator.hh::sendRecv, left MPI_Wait
36: leave BufferedCommunicator::sendRecv
36: communicator.hh::BufferedCommunicator::forward, left sendRecv
36 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), after communicator.free();
36 schwarz.hh:virtual void apply (X& v, const Y& d), AFTER communication.copyOwnerToAll(v,v); 
36void AMG<M,X,S,PI,A>::mgc(LevelContext& levelContext), finished 
36 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), AFTER mgc(levelContext) 
36 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), finished 
36   AFTER _prec->apply(y,p), before v = A * y 
36  before  alpha = rho_new / < rt, v > 
36  before alpha = rho_new / h; 
36  before apply first correction to x 
36  before r = r - alpha*v 
36  before test stop criteria 
36 istlsolver::printOutput 
  0.5      1.30064e-21      2.43002e-13
36 === rate=5.90498e-26, T=0.0266464, TIT=0.0532927, IT=0.5
36 finished solveLinearSystemImpl_ 1
36 converged value: 1 121 1 let s go get convergedRemote
36 to  comm_.min(converged) 
36 did  comm_.min(converged) convergedRemote: 1
36 final convergedRemote: 1
36 Update: x^(k+1) = x^k - deltax^k 
36 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
36 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
36 Newton iteration  1 done36 , maximum relative shift = 3.976e-03
36 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
40: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 14 15
40: communicator.hh::sendRecv, left MPI_Wait
40: leave BufferedCommunicator::sendRecv
40: communicator.hh::BufferedCommunicator::forward, left sendRecv
40 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), after communicator.free();
40 schwarz.hh:virtual void apply (X& v, const Y& d), AFTER communication.copyOwnerToAll(v,v); 
40 schwarz.hh:virtual void apply (X& v, const Y& d), before preconditioner.apply(v,d);; 
40 schwarz.hh:virtual void apply (X& v, const Y& d), before communication.copyOwnerToAll(v,v); 
40 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
40: communicator.hh::BufferedCommunicator::forward, enter sendRecv
40: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 15
40: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 15
40: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 15
40: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 15
40: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 15
40: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 15
40: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 15
40: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 15
40: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 15
40: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 15
40: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 10 15
40: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 11 15
40: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 12 15
40: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 13 15
40: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 14 15
40: communicator.hh::sendRecv, left MPI_Wait
40: leave BufferedCommunicator::sendRecv
40: communicator.hh::BufferedCommunicator::forward, left sendRecv
40 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), after communicator.free();
40 schwarz.hh:virtual void apply (X& v, const Y& d), AFTER communication.copyOwnerToAll(v,v); 
40void AMG<M,X,S,PI,A>::mgc(LevelContext& levelContext), finished 
40 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), AFTER mgc(levelContext) 
40 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), finished 
40   AFTER _prec->apply(y,p), before v = A * y 
40  before  alpha = rho_new / < rt, v > 
40  before alpha = rho_new / h; 
40  before apply first correction to x 
40  before r = r - alpha*v 
40  before test stop criteria 
40 istlsolver::printOutput 
  0.5      1.30064e-21      2.43002e-13
40 === rate=5.90498e-26, T=0.0265832, TIT=0.0531664, IT=0.5
40 finished solveLinearSystemImpl_ 1
40 converged value: 1 121 1 let s go get convergedRemote
40 to  comm_.min(converged) 
40 did  comm_.min(converged) convergedRemote: 1
40 final convergedRemote: 1
40 Update: x^(k+1) = x^k - deltax^k 
40 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
40 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
40 Newton iteration  1 done40 , maximum relative shift = 3.976e-03
40 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
27: leave BufferedCommunicator::sendRecv
27: communicator.hh::BufferedCommunicator::forward, left sendRecv
27 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), after communicator.free();
27 schwarz.hh:virtual void apply (X& v, const Y& d), AFTER communication.copyOwnerToAll(v,v); 
27 schwarz.hh:virtual void apply (X& v, const Y& d), before preconditioner.apply(v,d);; 
27 schwarz.hh:virtual void apply (X& v, const Y& d), before communication.copyOwnerToAll(v,v); 
27 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
27: communicator.hh::BufferedCommunicator::forward, enter sendRecv
27: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 17
27: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 17
27: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 17
27: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 17
27: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 17
27: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 17
27: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 17
27: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 17
27: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 17
27: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 17
27: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 10 17
27: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 11 17
27: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 12 17
27: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 13 17
27: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 14 17
27: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 15 17
27: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 16 17
27: communicator.hh::sendRecv, left MPI_Wait
27: leave BufferedCommunicator::sendRecv
27: communicator.hh::BufferedCommunicator::forward, left sendRecv
27 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), after communicator.free();
27 schwarz.hh:virtual void apply (X& v, const Y& d), AFTER communication.copyOwnerToAll(v,v); 
27void AMG<M,X,S,PI,A>::mgc(LevelContext& levelContext), finished 
27 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), AFTER mgc(levelContext) 
27 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), finished 
27   AFTER _prec->apply(y,p), before v = A * y 
27  before  alpha = rho_new / < rt, v > 
27  before alpha = rho_new / h; 
27  before apply first correction to x 
27  before r = r - alpha*v 
27  before test stop criteria 
27 istlsolver::printOutput 
  0.5      1.30064e-21      2.43002e-13
27 === rate=5.90498e-26, T=0.0266338, TIT=0.0532675, IT=0.5
27 finished solveLinearSystemImpl_ 1
27 converged value: 1 121 1 let s go get convergedRemote
27 to  comm_.min(converged) 
27 did  comm_.min(converged) convergedRemote: 1
27 final convergedRemote: 1
27 Update: x^(k+1) = x^k - deltax^k 
27 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
27 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
27 Newton iteration  1 done27 , maximum relative shift = 3.976e-03
27 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
70: leave BufferedCommunicator::sendRecv
70: communicator.hh::BufferedCommunicator::forward, left sendRecv
70 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), after communicator.free();
70 schwarz.hh:virtual void apply (X& v, const Y& d), AFTER communication.copyOwnerToAll(v,v); 
70 schwarz.hh:virtual void apply (X& v, const Y& d), before preconditioner.apply(v,d);; 
70 schwarz.hh:virtual void apply (X& v, const Y& d), before communication.copyOwnerToAll(v,v); 
70 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
70: communicator.hh::BufferedCommunicator::forward, enter sendRecv
70: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 13
70: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 13
70: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 13
70: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 13
70: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 13
70: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 13
70: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 13
70: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 13
70: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 13
70: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 13
70: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 10 13
70: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 11 13
70: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 12 13
70: communicator.hh::sendRecv, left MPI_Wait
70: leave BufferedCommunicator::sendRecv
70: communicator.hh::BufferedCommunicator::forward, left sendRecv
70 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), after communicator.free();
70 schwarz.hh:virtual void apply (X& v, const Y& d), AFTER communication.copyOwnerToAll(v,v); 
70void AMG<M,X,S,PI,A>::mgc(LevelContext& levelContext), finished 
70 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), AFTER mgc(levelContext) 
70 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), finished 
70   AFTER _prec->apply(y,p), before v = A * y 
70  before  alpha = rho_new / < rt, v > 
70  before alpha = rho_new / h; 
70  before apply first correction to x 
70  before r = r - alpha*v 
70  before test stop criteria 
70 istlsolver::printOutput 
  0.5      1.30064e-21      2.43002e-13
70 === rate=5.90498e-26, T=0.0266567, TIT=0.0533135, IT=0.5
70 finished solveLinearSystemImpl_ 1
70 converged value: 1 121 1 let s go get convergedRemote
70 to  comm_.min(converged) 
70 did  comm_.min(converged) convergedRemote: 1
70 final convergedRemote: 1
70 Update: x^(k+1) = x^k - deltax^k 
70 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
70 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
70 Newton iteration  1 done70 , maximum relative shift = 3.976e-03
70 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
6: leave BufferedCommunicator::sendRecv
6: communicator.hh::BufferedCommunicator::forward, left sendRecv
6 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), after communicator.free();
6 schwarz.hh:virtual void apply (X& v, const Y& d), AFTER communication.copyOwnerToAll(v,v); 
6 schwarz.hh:virtual void apply (X& v, const Y& d), before preconditioner.apply(v,d);; 
6 schwarz.hh:virtual void apply (X& v, const Y& d), before communication.copyOwnerToAll(v,v); 
6 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
6: communicator.hh::BufferedCommunicator::forward, enter sendRecv
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 17
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 17
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 17
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 17
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 17
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 17
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 17
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 17
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 17
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 17
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 10 17
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 11 17
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 12 17
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 13 17
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 14 17
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 15 17
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 16 17
6: communicator.hh::sendRecv, left MPI_Wait
6: leave BufferedCommunicator::sendRecv
6: communicator.hh::BufferedCommunicator::forward, left sendRecv
6 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), after communicator.free();
6 schwarz.hh:virtual void apply (X& v, const Y& d), AFTER communication.copyOwnerToAll(v,v); 
6void AMG<M,X,S,PI,A>::mgc(LevelContext& levelContext), finished 
6 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), AFTER mgc(levelContext) 
6 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), finished 
6   AFTER _prec->apply(y,p), before v = A * y 
6  before  alpha = rho_new / < rt, v > 
6  before alpha = rho_new / h; 
6  before apply first correction to x 
6  before r = r - alpha*v 
6  before test stop criteria 
6 istlsolver::printOutput 
  0.5      1.30064e-21      2.43002e-13
6 === rate=5.90498e-26, T=0.026659, TIT=0.0533181, IT=0.5
6 finished solveLinearSystemImpl_ 1
6 converged value: 1 121 1 let s go get convergedRemote
6 to  comm_.min(converged) 
6 did  comm_.min(converged) convergedRemote: 1
6 final convergedRemote: 1
6 Update: x^(k+1) = x^k - deltax^k 
6 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
6 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
6 Newton iteration  1 done6 , maximum relative shift = 3.976e-03
6 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
11: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 17
11: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 17
11: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 17
11: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 17
11: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 17
11: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 17
11: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 17
11: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 17
11: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 17
11: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 10 17
11: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 11 17
11: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 12 17
11: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 13 17
11: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 14 17
11: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 15 17
11: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 16 17
11: communicator.hh::sendRecv, left MPI_Wait
11: leave BufferedCommunicator::sendRecv
11: communicator.hh::BufferedCommunicator::forward, left sendRecv
11 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), after communicator.free();
11 schwarz.hh:virtual void apply (X& v, const Y& d), AFTER communication.copyOwnerToAll(v,v); 
11void AMG<M,X,S,PI,A>::mgc(LevelContext& levelContext), finished 
11 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), AFTER mgc(levelContext) 
11 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), finished 
11   AFTER _prec->apply(y,p), before v = A * y 
11  before  alpha = rho_new / < rt, v > 
11  before alpha = rho_new / h; 
11  before apply first correction to x 
11  before r = r - alpha*v 
11  before test stop criteria 
11 istlsolver::printOutput 
  0.5      1.30064e-21      2.43002e-13
11 === rate=5.90498e-26, T=0.0266708, TIT=0.0533416, IT=0.5
11 finished solveLinearSystemImpl_ 1
11 converged value: 1 121 1 let s go get convergedRemote
11 to  comm_.min(converged) 
11 did  comm_.min(converged) convergedRemote: 1
11 final convergedRemote: 1
11 Update: x^(k+1) = x^k - deltax^k 
11 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
11 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
11 Newton iteration  1 done11 , maximum relative shift = 3.976e-03
11 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
7: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 13
7: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 13
7: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 13
7: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 13
7: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 13
7: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 13
7: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 13
7: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 13
7: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 10 13
7: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 11 13
7: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 12 13
7: communicator.hh::sendRecv, left MPI_Wait
7: leave BufferedCommunicator::sendRecv
7: communicator.hh::BufferedCommunicator::forward, left sendRecv
7 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), after communicator.free();
7 schwarz.hh:virtual void apply (X& v, const Y& d), AFTER communication.copyOwnerToAll(v,v); 
7void AMG<M,X,S,PI,A>::mgc(LevelContext& levelContext), finished 
7 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), AFTER mgc(levelContext) 
7 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), finished 
7   AFTER _prec->apply(y,p), before v = A * y 
7  before  alpha = rho_new / < rt, v > 
7  before alpha = rho_new / h; 
7  before apply first correction to x 
7  before r = r - alpha*v 
7  before test stop criteria 
7 istlsolver::printOutput 
  0.5      1.30064e-21      2.43002e-13
7 === rate=5.90498e-26, T=0.0266465, TIT=0.053293, IT=0.5
7 finished solveLinearSystemImpl_ 1
7 converged value: 1 121 1 let s go get convergedRemote
7 to  comm_.min(converged) 
7 did  comm_.min(converged) convergedRemote: 1
7 final convergedRemote: 1
7 Update: x^(k+1) = x^k - deltax^k 
7 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
7 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
7 Newton iteration  1 done7 , maximum relative shift = 3.976e-03
7 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
99: leave BufferedCommunicator::sendRecv
99: communicator.hh::BufferedCommunicator::forward, left sendRecv
99 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), after communicator.free();
99 schwarz.hh:virtual void apply (X& v, const Y& d), AFTER communication.copyOwnerToAll(v,v); 
99 schwarz.hh:virtual void apply (X& v, const Y& d), before preconditioner.apply(v,d);; 
99 schwarz.hh:virtual void apply (X& v, const Y& d), before communication.copyOwnerToAll(v,v); 
99 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
99: communicator.hh::BufferedCommunicator::forward, enter sendRecv
99: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 13
99: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 13
99: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 13
99: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 13
99: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 13
99: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 13
99: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 13
99: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 13
99: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 13
99: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 13
99: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 10 13
99: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 11 13
99: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 12 13
99: communicator.hh::sendRecv, left MPI_Wait
99: leave BufferedCommunicator::sendRecv
99: communicator.hh::BufferedCommunicator::forward, left sendRecv
99 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), after communicator.free();
99 schwarz.hh:virtual void apply (X& v, const Y& d), AFTER communication.copyOwnerToAll(v,v); 
99void AMG<M,X,S,PI,A>::mgc(LevelContext& levelContext), finished 
99 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), AFTER mgc(levelContext) 
99 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), finished 
99   AFTER _prec->apply(y,p), before v = A * y 
99  before  alpha = rho_new / < rt, v > 
99  before alpha = rho_new / h; 
99  before apply first correction to x 
99  before r = r - alpha*v 
99  before test stop criteria 
99 istlsolver::printOutput 
  0.5      1.30064e-21      2.43002e-13
99 === rate=5.90498e-26, T=0.026621, TIT=0.0532419, IT=0.5
99 finished solveLinearSystemImpl_ 1
99 converged value: 1 121 1 let s go get convergedRemote
99 to  comm_.min(converged) 
99 did  comm_.min(converged) convergedRemote: 1
99 final convergedRemote: 1
99 Update: x^(k+1) = x^k - deltax^k 
99 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
99 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
99 Newton iteration  1 done99 , maximum relative shift = 3.976e-03
99 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
8: leave BufferedCommunicator::sendRecv
8: communicator.hh::BufferedCommunicator::forward, left sendRecv
8 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), after communicator.free();
8 schwarz.hh:virtual void apply (X& v, const Y& d), AFTER communication.copyOwnerToAll(v,v); 
8 schwarz.hh:virtual void apply (X& v, const Y& d), before preconditioner.apply(v,d);; 
8 schwarz.hh:virtual void apply (X& v, const Y& d), before communication.copyOwnerToAll(v,v); 
8 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
8: communicator.hh::BufferedCommunicator::forward, enter sendRecv
8: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 20
8: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 20
8: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 20
8: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 20
8: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 20
8: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 20
8: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 20
8: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 20
8: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 20
8: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 20
8: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 10 20
8: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 11 20
8: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 12 20
8: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 13 20
8: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 14 20
8: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 15 20
8: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 16 20
8: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 17 20
8: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 18 20
8: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 19 20
8: communicator.hh::sendRecv, left MPI_Wait
8: leave BufferedCommunicator::sendRecv
8: communicator.hh::BufferedCommunicator::forward, left sendRecv
8 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), after communicator.free();
8 schwarz.hh:virtual void apply (X& v, const Y& d), AFTER communication.copyOwnerToAll(v,v); 
8void AMG<M,X,S,PI,A>::mgc(LevelContext& levelContext), finished 
8 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), AFTER mgc(levelContext) 
8 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), finished 
8   AFTER _prec->apply(y,p), before v = A * y 
8  before  alpha = rho_new / < rt, v > 
8  before alpha = rho_new / h; 
8  before apply first correction to x 
8  before r = r - alpha*v 
8  before test stop criteria 
8 istlsolver::printOutput 
  0.5      1.30064e-21      2.43002e-13
8 === rate=5.90498e-26, T=0.0266146, TIT=0.0532292, IT=0.5
8 finished solveLinearSystemImpl_ 1
8 converged value: 1 121 1 let s go get convergedRemote
8 to  comm_.min(converged) 
8 did  comm_.min(converged) convergedRemote: 1
8 f68: leave BufferedCommunicator::sendRecv
68: communicator.hh::BufferedCommunicator::forward, left sendRecv
68 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), after communicator.free();
68 schwarz.hh:virtual void apply (X& v, const Y& d), AFTER communication.copyOwnerToAll(v,v); 
68 schwarz.hh:virtual void apply (X& v, const Y& d), before preconditioner.apply(v,d);; 
68 schwarz.hh:virtual void apply (X& v, const Y& d), before communication.copyOwnerToAll(v,v); 
68 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
68: communicator.hh::BufferedCommunicator::forward, enter sendRecv
68: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 22
68: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 22
68: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 22
68: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 22
68: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 22
68: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 22
68: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 22
68: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 22
68: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 22
68: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 22
68: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 10 22
68: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 11 22
68: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 12 22
68: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 13 22
68: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 14 22
68: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 15 22
68: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 16 22
68: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 17 22
68: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 18 22
68: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 19 22
68: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 20 22
68: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 21 22
68: communicator.hh::sendRecv, left MPI_Wait
68: leave BufferedCommunicator::sendRecv
68: communicator.hh::BufferedCommunicator::forward, left sendRecv
68 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), after communicator.free();
68 schwarz.hh:virtual void apply (X& v, const Y& d), AFTER communication.copyOwnerToAll(v,v); 
68void AMG<M,X,S,PI,A>::mgc(LevelContext& levelContext), finished 
68 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), AFTER mgc(levelContext) 
68 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), finished 
68   AFTER _prec->apply(y,p), before v = A * y 
68  before  alpha = rho_new / < rt, v > 
68  before alpha = rho_new / h; 
68  before apply first correction to x 
68  before r = r - alpha*v 
68  before test stop criteria 
68 istlsolver::p87: leave BufferedCommunicator::sendRecv
87: communicator.hh::BufferedCommunicator::forward, left sendRecv
87 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), after communicator.free();
87 schwarz.hh:virtual void apply (X& v, const Y& d), AFTER communication.copyOwnerToAll(v,v); 
87 schwarz.hh:virtual void apply (X& v, const Y& d), before preconditioner.apply(v,d);; 
87 schwarz.hh:virtual void apply (X& v, const Y& d), before communication.copyOwnerToAll(v,v); 
87 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
87: communicator.hh::BufferedCommunicator::forward, enter sendRecv
87: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 19
87: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 19
87: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 19
87: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 19
87: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 19
87: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 19
87: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 19
87: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 19
87: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 19
87: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 19
87: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 10 19
87: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 11 19
87: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 12 19
87: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 13 19
87: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 14 19
87: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 15 19
87: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 16 19
87: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 17 19
87: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 18 19
87: communicator.hh::sendRecv, left MPI_Wait
87: leave BufferedCommunicator::sendRecv
87: communicator.hh::BufferedCommunicator::forward, left sendRecv
87 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), after communicator.free();
87 schwarz.hh:virtual void apply (X& v, const Y& d), AFTER communication.copyOwnerToAll(v,v); 
87void AMG<M,X,S,PI,A>::mgc(LevelContext& levelContext), finished 
87 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), AFTER mgc(levelContext) 
87 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), finished 
87   AFTER _prec->apply(y,p), before v = A * y 
87  before  alpha = rho_new / < rt, v > 
87  before alpha = rho_new / h; 
87  before apply first correction to x 
87  before r = r - alpha*v 
87  before test stop criteria 
87 istlsolver::printOutput 
  0.5      1.30064e-21      2.43002e-13
87 === rate=5.90498e-26, T=0.0267119, TIT=0.0534238, IT=0.5
87 finished solveLinearSystemImpl_ 1
87 converged value: 1 121 1 let s go get convergedRemote
87 to  comm_.min(converged) 
87 did  comm_.min(converged) convergedRemote: 1
87 final convergedRemote: 1
87 Update: x^(k+1) = x^k - deltax^k 
87 newton57: leave BufferedCommunicator::sendRecv
57: communicator.hh::BufferedCommunicator::forward, left sendRecv
57 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), after communicator.free();
57 schwarz.hh:virtual void apply (X& v, const Y& d), AFTER communication.copyOwnerToAll(v,v); 
57 schwarz.hh:virtual void apply (X& v, const Y& d), before preconditioner.apply(v,d);; 
57 schwarz.hh:virtual void apply (X& v, const Y& d), before communication.copyOwnerToAll(v,v); 
57 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
57: communicator.hh::BufferedCommunicator::forward, enter sendRecv
57: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 19
57: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 19
57: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 19
57: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 19
57: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 19
57: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 19
57: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 19
57: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 19
57: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 19
57: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 19
57: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 10 19
57: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 11 19
57: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 12 19
57: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 13 19
57: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 14 19
57: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 15 19
57: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 16 19
57: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 17 19
57: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 18 19
57: communicator.hh::sendRecv, left MPI_Wait
57: leave BufferedCommunicator::sendRecv
57: communicator.hh::BufferedCommunicator::forward, left sendRecv
57 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), after communicator.free();
57 schwarz.hh:virtual void apply (X& v, const Y& d), AFTER communication.copyOwnerToAll(v,v); 
57void AMG<M,X,S,PI,A>::mgc(LevelContext& levelContext), finished 
57 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), AFTER mgc(levelContext) 
57 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), finished 
57   AFTER _prec->apply(y,p), before v = A * y 
57  before  alpha = rho_new / < rt, v > 
57  before alpha = rho_new / h; 
57  before apply first correction to x 
57  before r = r - alpha*v 
57  before test stop criteria 
57 istlsolver::printOutput 
  0.5      1.30064e-21      2.43002e-13
57 === rate=5.90498e-26, T=0.0266393, TIT=0.0532786, IT=0.5
57 finished solveLinearSystemImpl_ 1
57 converged value: 1 121 1 let s go get convergedRemote
57 to  comm_.min(converged) 
57 did  comm_.min(converged) convergedRemote: 1
57 final convergedRemote: 1
57 Update: x^(k+1) = x^k - deltax^k 
57 newton97 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
97: communicator.hh::BufferedCommunicator::forward, enter sendRecv
97: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 9
97: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 9
97: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 9
97: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 9
97: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 9
97: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 9
97: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 9
97: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 9
97: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 9
97: communicator.hh::sendRecv, left MPI_Wait
97: leave BufferedCommunicator::sendRecv
97: communicator.hh::BufferedCommunicator::forward, left sendRecv
97 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), after communicator.free();
97 schwarz.hh:virtual void apply (X& v, const Y& d), AFTER communication.copyOwnerToAll(v,v); 
97void AMG<M,X,S,PI,A>::mgc(LevelContext& levelContext), finished 
97 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), AFTER mgc(levelContext) 
97 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), finished 
97   AFTER _prec->apply(y,p), before v = A * y 
97  before  alpha = rho_new / < rt, v > 
97  before alpha = rho_new / h; 
97  before apply first correction to x 
97  before r = r - alpha*v 
97  before test stop criteria 
97 istlsolver::printOutput 
  0.5      1.30064e-21      2.43002e-13
97 === rate=5.90498e-26, T=0.0266014, TIT=0.0532027, IT=0.5
97 finished solveLinearSystemImpl_ 1
97 converged value: 1 121 1 let s go get convergedRemote
97 to  comm_.min(converged) 
97 did  comm_.min(converged) convergedRemote: 1
97 final convergedRemote: 1
97 Update: x^(k+1) = x^k - deltax^k 
97 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
97 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
97 Newton iteration  1 done97 , maximum relative shift = 3.976e-03
97 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
82: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 25 26
82: communicator.hh::sendRecv, left MPI_Wait
82: leave BufferedCommunicator::sendRecv
82: communicator.hh::BufferedCommunicator::forward, left sendRecv
82 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), after communicator.free();
82 schwarz.hh:virtual void apply (X& v, const Y& d), AFTER communication.copyOwnerToAll(v,v); 
82 schwarz.hh:virtual void apply (X& v, const Y& d), before preconditioner.apply(v,d);; 
82 schwarz.hh:virtual void apply (X& v, const Y& d), before communication.copyOwnerToAll(v,v); 
82 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
82: communicator.hh::BufferedCommunicator::forward, enter sendRecv
82: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 26
82: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 26
82: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 26
82: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 26
82: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 26
82: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 26
82: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 26
82: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 26
82: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 26
82: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 26
82: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 10 26
82: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 11 26
82: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 12 26
82: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 13 26
82: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 14 26
82: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 15 26
82: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 16 26
82: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 17 26
82: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 18 26
82: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 19 26
82: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 20 26
82: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 21 26
82: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 22 26
82: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 23 26
82: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 24 26
82: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 25 26
82: communicator.hh::sendRecv, left MPI_Wait
82: leave BufferedCommunicator::sendRecv
82: communicator.hh::BufferedCommunicator::forward, left sendRecv
82 own16: leave BufferedCommunicator::sendRecv
16: communicator.hh::BufferedCommunicator::forward, left sendRecv
16 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), after communicator.free();
16 schwarz.hh:virtual void apply (X& v, const Y& d), AFTER communication.copyOwnerToAll(v,v); 
16 schwarz.hh:virtual void apply (X& v, const Y& d), before preconditioner.apply(v,d);; 
16 schwarz.hh:virtual void apply (X& v, const Y& d), before communication.copyOwnerToAll(v,v); 
16 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
16: communicator.hh::BufferedCommunicator::forward, enter sendRecv
16: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 6
16: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 6
16: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 6
16: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 6
16: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 6
16: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 6
16: communicator.hh::sendRecv, left MPI_Wait
16: leave BufferedCommunicator::sendRecv
16: communicator.hh::BufferedCommunicator::forward, left sendRecv
16 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), after communicator.free();
16 schwarz.hh:virtual void apply (X& v, const Y& d), AFTER communication.copyOwnerToAll(v,v); 
16void AMG<M,X,S,PI,A>::mgc(LevelContext& levelContext), finished 
16 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), AFTER mgc(levelContext) 
16 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), finished 
16   AFTER _prec->apply(y,p), before v = A * y 
16  before  alpha = rho_new / < rt, v > 
16  before alpha = rho_new / h; 
16  before apply first correction to x 
16  before r = r - alpha*v 
16  before test stop criteria 
16 istlsolver::printOutput 
  0.5      1.30064e-21      2.43002e-13
16 === rate=5.90498e-26, T=0.026589, TIT=0.053178, IT=0.5
16 finished solveLinearSystemImpl_ 1
16 converged value: 1 121 1 let s go get convergedRemote
16 to  comm_.min(converged) 
16 did  comm_.min(converged) convergedRemote: 1
16 final convergedRemote: 1
16 Update: x^(k+1) = x^k - deltax^k 
16 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
16 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
16 Newton iteration  1 done16 , maximum relative shift = 3.976e-03
16 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
31 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
31: communicator.hh::BufferedCommunicator::forward, enter sendRecv
31: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 10
31: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 10
31: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 10
31: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 10
31: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 10
31: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 10
31: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 10
31: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 10
31: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 10
31: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 10
31: communicator.hh::sendRecv, left MPI_Wait
31: leave BufferedCommunicator::sendRecv
31: communicator.hh::BufferedCommunicator::forward, left sendRecv
31 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), after communicator.free();
31 schwarz.hh:virtual void apply (X& v, const Y& d), AFTER communication.copyOwnerToAll(v,v); 
31void AMG<M,X,S,PI,A>::mgc(LevelContext& levelContext), finished 
31 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), AFTER mgc(levelContext) 
31 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), finished 
31   AFTER _prec->apply(y,p), before v = A * y 
31  before  alpha = rho_new / < rt, v > 
31  before alpha = rho_new / h; 
31  before apply first correction to x 
31  before r = r - alpha*v 
31  before test stop criteria 
31 istlsolver::printOutput 
  0.5      1.30064e-21      2.43002e-13
31 === rate=5.90498e-26, T=0.0266634, TIT=0.0533268, IT=0.5
31 finished solveLinearSystemImpl_ 1
31 converged value: 1 121 1 let s go get convergedRemote
31 to  comm_.min(converged) 
31 did  comm_.min(converged) convergedRemote: 1
31 final convergedRemote: 1
31 Update: x^(k+1) = x^k - deltax^k 
31 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
31 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
31 Newton iteration  1 done31 , maximum relative shift = 3.976e-03
31 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
37 schwarz.hh:virtual void apply (X& v, const Y& d), before communication.copyOwnerToAll(v,v); 
37 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
37: communicator.hh::BufferedCommunicator::forward, enter sendRecv
37: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 15
37: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 15
37: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 15
37: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 15
37: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 15
37: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 15
37: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 15
37: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 15
37: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 15
37: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 15
37: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 10 15
37: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 11 15
37: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 12 15
37: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 13 15
37: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 14 15
37: communicator.hh::sendRecv, left MPI_Wait
37: leave BufferedCommunicator::sendRecv
37: communicator.hh::BufferedCommunicator::forward, left sendRecv
37 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), after communicator.free();
37 schwarz.hh:virtual void apply (X& v, const Y& d), AFTER communication.copyOwnerToAll(v,v); 
37void AMG<M,X,S,PI,A>::mgc(LevelContext& levelContext), finished 
37 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), AFTER mgc(levelContext) 
37 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), finished 
37   AFTER _prec->apply(y,p), before v = A * y 
37  before  alpha = rho_new / < rt, v > 
37  before alpha = rho_new / h; 
37  before apply first correction to x 
37  before r = r - alpha*v 
37  before test stop criteria 
37 istlsolver::printOutput 
  0.5      1.30064e-21      2.43002e-13
37 === rate=5.90498e-26, T=0.0266337, TIT=0.0532673, IT=0.5
37 finished solveLinearSystemImpl_ 1
37 converged value: 1 121 1 let s go get convergedRemote
37 to  comm_.min(converged) 
37 did  comm_.min(converged) convergedRemote: 1
37 final convergedRemote: 1
37 Update: x^(k+1) = x^k - deltax^k 
37 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
37 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
37 Newton iteration  1 done37 , maximum relative shift = 3.976e-03
37 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
9: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 7
9: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 7
9: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 7
9: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 7
9: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 7
9: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 7
9: communicator.hh::sendRecv, left MPI_Wait
9: leave BufferedCommunicator::sendRecv
9: communicator.hh::BufferedCommunicator::forward, left sendRecv
9 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), after communicator.free();
9 schwarz.hh:virtual void apply (X& v, const Y& d), AFTER communication.copyOwnerToAll(v,v); 
9void AMG<M,X,S,PI,A>::mgc(LevelContext& levelContext), finished 
9 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), AFTER mgc(levelContext) 
9 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), finished 
9   AFTER _prec->apply(y,p), before v = A * y 
9  before  alpha = rho_new / < rt, v > 
9  before alpha = rho_new / h; 
9  before apply first correction to x 
9  before r = r - alpha*v 
9  before test stop criteria 
9 istlsolver::printOutput 
  0.5      1.30064e-21      2.43002e-13
9 === rate=5.90498e-26, T=0.0266207, TIT=0.0532413, IT=0.5
9 finished solveLinearSystemImpl_ 1
9 converged value: 1 121 1 let s go get convergedRemote
9 to  comm_.min(converged) 
9 did  comm_.min(converged) convergedRemote: 1
9 final convergedRemote: 1
9 Update: x^(k+1) = x^k - deltax^k 
9 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
9 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
9 Newton iteration  1 done9 , maximum relative shift = 3.976e-03
9 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
62: leave BufferedCommunicator::sendRecv
62: communicator.hh::BufferedCommunicator::forward, left sendRecv
62 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), after communicator.free();
62 schwarz.hh:virtual void apply (X& v, const Y& d), AFTER communication.copyOwnerToAll(v,v); 
62 schwarz.hh:virtual void apply (X& v, const Y& d), before preconditioner.apply(v,d);; 
62 schwarz.hh:virtual void apply (X& v, const Y& d), before communication.copyOwnerToAll(v,v); 
62 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
62: communicator.hh::BufferedCommunicator::forward, enter sendRecv
62: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 15
62: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 15
62: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 15
62: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 15
62: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 15
62: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 15
62: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 15
62: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 15
62: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 15
62: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 15
62: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 10 15
62: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 11 15
62: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 12 15
62: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 13 15
62: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 14 15
62: communicator.hh::sendRecv, left MPI_Wait
62: leave BufferedCommunicator::sendRecv
62: communicator.hh::BufferedCommunicator::forward, left sendRecv
62 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), after communicator.free();
62 schwarz.hh:virtual void apply (X& v, const Y& d), AFTER communication.copyOwnerToAll(v,v); 
62void AMG<M,X,S,PI,A>::mgc(LevelContext& levelContext), finished 
62 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), AFTER mgc(levelContext) 
62 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), finished 
62   AFTER _prec->apply(y,p), before v = A * y 
62  before  alpha = rho_new / < rt, v > 
62  before alpha = rho_new / h; 
62  before apply first correction to x 
62  before r = r - alpha*v 
62  before test stop criteria 
62 istlsolver::printOutput 
  0.5      1.30064e-21      2.43002e-13
62 === rate=5.90498e-26, T=0.0266952, TIT=0.0533903, IT=0.5
62 finished solveLinearSystemImpl_ 1
62 converged value: 1 121 1 let s go get convergedRemote
62 to  comm_.min(converged) 
62 did  comm_.min(converged) convergedRemote: 1
62 final convergedRemote: 1
62 Update: x^(k+1) = x^k - deltax^k 
62 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
62 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
62 Newton iteration  1 done62 , maximum relative shift = 3.976e-03
62 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
48: leave BufferedCommunicator::sendRecv
48: communicator.hh::BufferedCommunicator::forward, left sendRecv
48 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), after communicator.free();
48 schwarz.hh:virtual void apply (X& v, const Y& d), AFTER communication.copyOwnerToAll(v,v); 
48 schwarz.hh:virtual void apply (X& v, const Y& d), before preconditioner.apply(v,d);; 
48 schwarz.hh:virtual void apply (X& v, const Y& d), before communication.copyOwnerToAll(v,v); 
48 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
48: communicator.hh::BufferedCommunicator::forward, enter sendRecv
48: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 15
48: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 15
48: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 15
48: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 15
48: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 15
48: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 15
48: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 15
48: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 15
48: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 15
48: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 15
48: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 10 15
48: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 11 15
48: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 12 15
48: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 13 15
48: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 14 15
48: communicator.hh::sendRecv, left MPI_Wait
48: leave BufferedCommunicator::sendRecv
48: communicator.hh::BufferedCommunicator::forward, left sendRecv
48 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), after communicator.free();
48 schwarz.hh:virtual void apply (X& v, const Y& d), AFTER communication.copyOwnerToAll(v,v); 
48void AMG<M,X,S,PI,A>::mgc(LevelContext& levelContext), finished 
48 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), AFTER mgc(levelContext) 
48 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), finished 
48   AFTER _prec->apply(y,p), before v = A * y 
48  before  alpha = rho_new / < rt, v > 
48  before alpha = rho_new / h; 
48  before apply first correction to x 
48  before r = r - alpha*v 
48  before test stop criteria 
48 istlsolver::printOutput 
  0.5      1.30064e-21      2.43002e-13
48 === rate=5.90498e-26, T=0.0266013, TIT=0.0532027, IT=0.5
48 finished solveLinearSystemImpl_ 1
48 converged value: 1 121 1 let s go get convergedRemote
48 to  comm_.min(converged) 
48 did  comm_.min(converged) convergedRemote: 1
48 final convergedRemote: 1
48 Update: x^(k+1) = x^k - deltax^k 
48 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
48 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
48 Newton iteration  1 done48 , maximum relative shift = 3.976e-03
48 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
43 schwarz.hh:virtual void apply (X& v, const Y& d), before communication.copyOwnerToAll(v,v); 
43 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
43: communicator.hh::BufferedCommunicator::forward, enter sendRecv
43: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 11
43: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 11
43: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 11
43: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 11
43: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 11
43: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 11
43: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 11
43: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 11
43: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 11
43: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 11
43: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 10 11
43: communicator.hh::sendRecv, left MPI_Wait
43: leave BufferedCommunicator::sendRecv
43: communicator.hh::BufferedCommunicator::forward, left sendRecv
43 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), after communicator.free();
43 schwarz.hh:virtual void apply (X& v, const Y& d), AFTER communication.copyOwnerToAll(v,v); 
43void AMG<M,X,S,PI,A>::mgc(LevelContext& levelContext), finished 
43 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), AFTER mgc(levelContext) 
43 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), finished 
43   AFTER _prec->apply(y,p), before v = A * y 
43  before  alpha = rho_new / < rt, v > 
43  before alpha = rho_new / h; 
43  before apply first correction to x 
43  before r = r - alpha*v 
43  before test stop criteria 
43 istlsolver::printOutput 
  0.5      1.30064e-21      2.43002e-13
43 === rate=5.90498e-26, T=0.0266668, TIT=0.0533336, IT=0.5
43 finished solveLinearSystemImpl_ 1
43 converged value: 1 121 1 let s go get convergedRemote
43 to  comm_.min(converged) 
43 did  comm_.min(converged) convergedRemote: 1
43 final convergedRemote: 1
43 Update: x^(k+1) = x^k - deltax^k 
43 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
43 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
43 Newton iteration  1 done43 , maximum relative shift = 3.976e-03
43 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
77: leave BufferedCommunicator::sendRecv
77: communicator.hh::BufferedCommunicator::forward, left sendRecv
77 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), after communicator.free();
77 schwarz.hh:virtual void apply (X& v, const Y& d), AFTER communication.copyOwnerToAll(v,v); 
77 schwarz.hh:virtual void apply (X& v, const Y& d), before preconditioner.apply(v,d);; 
77 schwarz.hh:virtual void apply (X& v, const Y& d), before communication.copyOwnerToAll(v,v); 
77 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
77: communicator.hh::BufferedCommunicator::forward, enter sendRecv
77: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 6
77: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 6
77: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 6
77: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 6
77: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 6
77: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 6
77: communicator.hh::sendRecv, left MPI_Wait
77: leave BufferedCommunicator::sendRecv
77: communicator.hh::BufferedCommunicator::forward, left sendRecv
77 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), after communicator.free();
77 schwarz.hh:virtual void apply (X& v, const Y& d), AFTER communication.copyOwnerToAll(v,v); 
77void AMG<M,X,S,PI,A>::mgc(LevelContext& levelContext), finished 
77 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), AFTER mgc(levelContext) 
77 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), finished 
77   AFTER _prec->apply(y,p), before v = A * y 
77  before  alpha = rho_new / < rt, v > 
77  before alpha = rho_new / h; 
77  before apply first correction to x 
77  before r = r - alpha*v 
77  before test stop criteria 
77 istlsolver::printOutput 
  0.5      1.30064e-21      2.43002e-13
77 === rate=5.90498e-26, T=0.0266513, TIT=0.0533025, IT=0.5
77 finished solveLinearSystemImpl_ 1
77 converged value: 1 121 1 let s go get convergedRemote
77 to  comm_.min(converged) 
77 did  comm_.min(converged) convergedRemote: 1
77 final convergedRemote: 1
77 Update: x^(k+1) = x^k - deltax^k 
77 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
77 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
77 Newton iteration  1 done77 , maximum relative shift = 3.976e-03
77 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
65: leave BufferedCommunicator::sendRecv
65: communicator.hh::BufferedCommunicator::forward, left sendRecv
65 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), after communicator.free();
65 schwarz.hh:virtual void apply (X& v, const Y& d), AFTER communication.copyOwnerToAll(v,v); 
65 schwarz.hh:virtual void apply (X& v, const Y& d), before preconditioner.apply(v,d);; 
65 schwarz.hh:virtual void apply (X& v, const Y& d), before communication.copyOwnerToAll(v,v); 
65 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
65: communicator.hh::BufferedCommunicator::forward, enter sendRecv
65: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 9
65: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 9
65: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 9
65: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 9
65: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 9
65: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 9
65: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 9
65: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 9
65: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 9
65: communicator.hh::sendRecv, left MPI_Wait
65: leave BufferedCommunicator::sendRecv
65: communicator.hh::BufferedCommunicator::forward, left sendRecv
65 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), after communicator.free();
65 schwarz.hh:virtual void apply (X& v, const Y& d), AFTER communication.copyOwnerToAll(v,v); 
65void AMG<M,X,S,PI,A>::mgc(LevelContext& levelContext), finished 
65 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), AFTER mgc(levelContext) 
65 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), finished 
65   AFTER _prec->apply(y,p), before v = A * y 
65  before  alpha = rho_new / < rt, v > 
65  before alpha = rho_new / h; 
65  before apply first correction to x 
65  before r = r - alpha*v 
65  before test stop criteria 
65 istlsolver::printOutput 
  0.5      1.30064e-21      2.43002e-13
65 === rate=5.90498e-26, T=0.026598, TIT=0.053196, IT=0.5
65 finished solveLinearSystemImpl_ 1
65 converged value: 1 121 1 let s go get convergedRemote
65 to  comm_.min(converged) 
65 did  comm_.min(converged) convergedRemote: 1
65 final convergedRemote: 1
65 Update: x^(k+1) = x^k - deltax^k 
65 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
65 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
65 Newton iteration  1 done65 , maximum relative shift = 3.976e-03
65 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
39: leave BufferedCommunicator::sendRecv
39: communicator.hh::BufferedCommunicator::forward, left sendRecv
39 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), after communicator.free();
39 schwarz.hh:virtual void apply (X& v, const Y& d), AFTER communication.copyOwnerToAll(v,v); 
39 schwarz.hh:virtual void apply (X& v, const Y& d), before preconditioner.apply(v,d);; 
39 schwarz.hh:virtual void apply (X& v, const Y& d), before communication.copyOwnerToAll(v,v); 
39 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
39: communicator.hh::BufferedCommunicator::forward, enter sendRecv
39: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 9
39: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 9
39: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 9
39: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 9
39: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 9
39: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 9
39: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 9
39: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 9
39: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 9
39: communicator.hh::sendRecv, left MPI_Wait
39: leave BufferedCommunicator::sendRecv
39: communicator.hh::BufferedCommunicator::forward, left sendRecv
39 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), after communicator.free();
39 schwarz.hh:virtual void apply (X& v, const Y& d), AFTER communication.copyOwnerToAll(v,v); 
39void AMG<M,X,S,PI,A>::mgc(LevelContext& levelContext), finished 
39 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), AFTER mgc(levelContext) 
39 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), finished 
39   AFTER _prec->apply(y,p), before v = A * y 
39  before  alpha = rho_new / < rt, v > 
39  before alpha = rho_new / h; 
39  before apply first correction to x 
39  before r = r - alpha*v 
39  before test stop criteria 
39 istlsolver::printOutput 
  0.5      1.30064e-21      2.43002e-13
39 === rate=5.90498e-26, T=0.0266537, TIT=0.0533073, IT=0.5
39 finished solveLinearSystemImpl_ 1
39 converged value: 1 121 1 let s go get convergedRemote
39 to  comm_.min(converged) 
39 did  comm_.min(converged) convergedRemote: 1
39 final convergedRemote: 1
39 Update: x^(k+1) = x^k - deltax^k 
39 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
39 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
39 Newton iteration  1 done39 , maximum relative shift = 3.976e-03
39 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
79: leave BufferedCommunicator::sendRecv
79: communicator.hh::BufferedCommunicator::forward, left sendRecv
79 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), after communicator.free();
79 schwarz.hh:virtual void apply (X& v, const Y& d), AFTER communication.copyOwnerToAll(v,v); 
79 schwarz.hh:virtual void apply (X& v, const Y& d), before preconditioner.apply(v,d);; 
79 schwarz.hh:virtual void apply (X& v, const Y& d), before communication.copyOwnerToAll(v,v); 
79 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
79: communicator.hh::BufferedCommunicator::forward, enter sendRecv
79: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 16
79: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 16
79: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 16
79: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 16
79: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 16
79: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 16
79: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 16
79: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 16
79: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 16
79: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 16
79: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 10 16
79: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 11 16
79: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 12 16
79: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 13 16
79: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 14 16
79: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 15 16
79: communicator.hh::sendRecv, left MPI_Wait
79: leave BufferedCommunicator::sendRecv
79: communicator.hh::BufferedCommunicator::forward, left sendRecv
79 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), after communicator.free();
79 schwarz.hh:virtual void apply (X& v, const Y& d), AFTER communication.copyOwnerToAll(v,v); 
79void AMG<M,X,S,PI,A>::mgc(LevelContext& levelContext), finished 
79 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), AFTER mgc(levelContext) 
79 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), finished 
79   AFTER _prec->apply(y,p), before v = A * y 
79  before  alpha = rho_new / < rt, v > 
79  before alpha = rho_new / h; 
79  before apply first correction to x 
79  before r = r - alpha*v 
79  before test stop criteria 
79 istlsolver::printOutput 
  0.5      1.30064e-21      2.43002e-13
79 === rate=5.90498e-26, T=0.0266848, TIT=0.0533696, IT=0.5
79 finished solveLinearSystemImpl_ 1
79 converged value: 1 121 1 let s go get convergedRemote
79 to  comm_.min(converged) 
79 did  comm_.min(converged) convergedRemote: 1
79 final convergedRemote: 1
79 Update: x^(k+1) = x^k - deltax^k 
79 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
79 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
79 Newton iteration  1 done79 , maximum relative shift = 3.976e-03
79 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
80: leave BufferedCommunicator::sendRecv
80: communicator.hh::BufferedCommunicator::forward, left sendRecv
80 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), after communicator.free();
80 schwarz.hh:virtual void apply (X& v, const Y& d), AFTER communication.copyOwnerToAll(v,v); 
80 schwarz.hh:virtual void apply (X& v, const Y& d), before preconditioner.apply(v,d);; 
80 schwarz.hh:virtual void apply (X& v, const Y& d), before communication.copyOwnerToAll(v,v); 
80 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
80: communicator.hh::BufferedCommunicator::forward, enter sendRecv
80: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 10
80: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 10
80: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 10
80: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 10
80: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 10
80: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 10
80: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 10
80: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 10
80: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 10
80: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 10
80: communicator.hh::sendRecv, left MPI_Wait
80: leave BufferedCommunicator::sendRecv
80: communicator.hh::BufferedCommunicator::forward, left sendRecv
80 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), after communicator.free();
80 schwarz.hh:virtual void apply (X& v, const Y& d), AFTER communication.copyOwnerToAll(v,v); 
80void AMG<M,X,S,PI,A>::mgc(LevelContext& levelContext), finished 
80 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), AFTER mgc(levelContext) 
80 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), finished 
80   AFTER _prec->apply(y,p), before v = A * y 
80  before  alpha = rho_new / < rt, v > 
80  before alpha = rho_new / h; 
80  before apply first correction to x 
80  before r = r - alpha*v 
80  before test stop criteria 
80 istlsolver::printOutput 
  0.5      1.30064e-21      2.43002e-13
80 === rate=5.90498e-26, T=0.026618, TIT=0.053236, IT=0.5
80 finished solveLinearSystemImpl_ 1
80 converged value: 1 121 1 let s go get convergedRemote
80 to  comm_.min(converged) 
80 did  comm_.min(converged) convergedRemote: 1
80 final convergedRemote: 1
80 Update: x^(k+1) = x^k - deltax^k 
80 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
80 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
80 Newton iteration  1 done80 , maximum relative shift = 3.976e-03
80 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
92: leave BufferedCommunicator::sendRecv
92: communicator.hh::BufferedCommunicator::forward, left sendRecv
92 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), after communicator.free();
92 schwarz.hh:virtual void apply (X& v, const Y& d), AFTER communication.copyOwnerToAll(v,v); 
92 schwarz.hh:virtual void apply (X& v, const Y& d), before preconditioner.apply(v,d);; 
92 schwarz.hh:virtual void apply (X& v, const Y& d), before communication.copyOwnerToAll(v,v); 
92 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
92: communicator.hh::BufferedCommunicator::forward, enter sendRecv
92: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 16
92: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 16
92: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 16
92: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 16
92: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 16
92: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 16
92: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 16
92: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 16
92: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 16
92: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 16
92: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 10 16
92: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 11 16
92: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 12 16
92: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 13 16
92: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 14 16
92: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 15 16
92: communicator.hh::sendRecv, left MPI_Wait
92: leave BufferedCommunicator::sendRecv
92: communicator.hh::BufferedCommunicator::forward, left sendRecv
92 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), after communicator.free();
92 schwarz.hh:virtual void apply (X& v, const Y& d), AFTER communication.copyOwnerToAll(v,v); 
92void AMG<M,X,S,PI,A>::mgc(LevelContext& levelContext), finished 
92 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), AFTER mgc(levelContext) 
92 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), finished 
92   AFTER _prec->apply(y,p), before v = A * y 
92  before  alpha = rho_new / < rt, v > 
92  before alpha = rho_new / h; 
92  before apply first correction to x 
92  before r = r - alpha*v 
92  before test stop criteria 
92 istlsolver::printOutput 
  0.5      1.30064e-21      2.43002e-13
92 === rate=5.90498e-26, T=0.0266932, TIT=0.0533865, IT=0.5
92 finished solveLinearSystemImpl_ 1
92 converged value: 1 121 1 let s go get convergedRemote
92 to  comm_.min(converged) 
92 did  comm_.min(converged) convergedRemote: 1
92 final convergedRemote: 1
92 Update: x^(k+1) = x^k - deltax^k 
92 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
92 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
92 Newton iteration  1 done92 , maximum relative shift = 3.976e-03
92 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
83: leave BufferedCommunicator::sendRecv
83: communicator.hh::BufferedCommunicator::forward, left sendRecv
83 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), after communicator.free();
83 schwarz.hh:virtual void apply (X& v, const Y& d), AFTER communication.copyOwnerToAll(v,v); 
83 schwarz.hh:virtual void apply (X& v, const Y& d), before preconditioner.apply(v,d);; 
83 schwarz.hh:virtual void apply (X& v, const Y& d), before communication.copyOwnerToAll(v,v); 
83 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
83: communicator.hh::BufferedCommunicator::forward, enter sendRecv
83: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 17
83: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 17
83: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 17
83: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 17
83: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 17
83: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 17
83: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 17
83: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 17
83: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 17
83: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 17
83: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 10 17
83: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 11 17
83: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 12 17
83: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 13 17
83: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 14 17
83: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 15 17
83: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 16 17
83: communicator.hh::sendRecv, left MPI_Wait
83: leave BufferedCommunicator::sendRecv
83: communicator.hh::BufferedCommunicator::forward, left sendRecv
83 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), after communicator.free();
83 schwarz.hh:virtual void apply (X& v, const Y& d), AFTER communication.copyOwnerToAll(v,v); 
83void AMG<M,X,S,PI,A>::mgc(LevelContext& levelContext), finished 
83 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), AFTER mgc(levelContext) 
83 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), finished 
83   AFTER _prec->apply(y,p), before v = A * y 
83  before  alpha = rho_new / < rt, v > 
83  before alpha = rho_new / h; 
83  before apply first correction to x 
83  before r = r - alpha*v 
83  before test stop criteria 
83 istlsolver::printOutput 
  0.5      1.30064e-21      2.43002e-13
83 === rate=5.90498e-26, T=0.0266224, TIT=0.0532448, IT=0.5
83 finished solveLinearSystemImpl_ 1
83 converged value: 1 121 1 let s go get convergedRemote
83 to  comm_.min(converged) 
83 did  comm_.min(converged) convergedRemote: 1
83 final convergedRemote: 1
83 Update: x^(k+1) = x^k - deltax^k 
83 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
83 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
83 Newton iteration  1 done83 , maximum relative shift = 3.976e-03
83 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
88: leave BufferedCommunicator::sendRecv
88: communicator.hh::BufferedCommunicator::forward, left sendRecv
88 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), after communicator.free();
88 schwarz.hh:virtual void apply (X& v, const Y& d), AFTER communication.copyOwnerToAll(v,v); 
88 schwarz.hh:virtual void apply (X& v, const Y& d), before preconditioner.apply(v,d);; 
88 schwarz.hh:virtual void apply (X& v, const Y& d), before communication.copyOwnerToAll(v,v); 
88 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
88: communicator.hh::BufferedCommunicator::forward, enter sendRecv
88: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 21
88: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 21
88: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 21
88: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 21
88: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 21
88: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 21
88: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 21
88: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 21
88: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 21
88: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 21
88: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 10 21
88: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 11 21
88: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 12 21
88: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 13 21
88: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 14 21
88: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 15 21
88: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 16 21
88: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 17 21
88: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 18 21
88: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 19 21
88: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 20 21
88: communicator.hh::sendRecv, left MPI_Wait
88: leave BufferedCommunicator::sendRecv
88: communicator.hh::BufferedCommunicator::forward, left sendRecv
88 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), after communicator.free();
88 schwarz.hh:virtual void apply (X& v, const Y& d), AFTER communication.copyOwnerToAll(v,v); 
88void AMG<M,X,S,PI,A>::mgc(LevelContext& levelContext), finished 
88 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), AFTER mgc(levelContext) 
88 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), finished 
88   AFTER _prec->apply(y,p), before v = A * y 
88  before  alpha = rho_new / < rt, v > 
88  before alpha = rho_new / h; 
88  before apply first correction to x 
88  before r = r - alpha*v 
88  before test stop criteria 
88 istlsolver::printOutput 
  0.5      1.30064e-21      2.43002e-13
88 === rate=5.90498e-26, T=0.02663, TIT=0.05326, IT=0.5
88 finished69: leave BufferedCommunicator::sendRecv
69: communicator.hh::BufferedCommunicator::forward, left sendRecv
69 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), after communicator.free();
69 schwarz.hh:virtual void apply (X& v, const Y& d), AFTER communication.copyOwnerToAll(v,v); 
69 schwarz.hh:virtual void apply (X& v, const Y& d), before preconditioner.apply(v,d);; 
69 schwarz.hh:virtual void apply (X& v, const Y& d), before communication.copyOwnerToAll(v,v); 
69 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
69: communicator.hh::BufferedCommunicator::forward, enter sendRecv
69: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 7
69: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 7
69: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 7
69: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 7
69: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 7
69: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 7
69: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 7
69: communicator.hh::sendRecv, left MPI_Wait
69: leave BufferedCommunicator::sendRecv
69: communicator.hh::BufferedCommunicator::forward, left sendRecv
69 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), after communicator.free();
69 schwarz.hh:virtual void apply (X& v, const Y& d), AFTER communication.copyOwnerToAll(v,v); 
69void AMG<M,X,S,PI,A>::mgc(LevelContext& levelContext), finished 
69 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), AFTER mgc(levelContext) 
69 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), finished 
69   AFTER _prec->apply(y,p), before v = A * y 
69  before  alpha = rho_new / < rt, v > 
69  before alpha = rho_new / h; 
69  before apply first correction to x 
69  before r = r - alpha*v 
69  before test stop criteria 
69 istlsolver::printOutput 
  0.5      1.30064e-21      2.43002e-13
69 === rate=5.90498e-26, T=0.0265971, TIT=0.0531941, IT=0.5
69 finished solveLinearSystemImpl_ 1
69 converged value: 1 121 1 let s go get convergedRemote
69 to  comm_.min(converged) 
69 did  comm_.min(converged) convergedRemote: 1
69 final convergedRemote: 1
69 Update: x^(k+1) = x^k - deltax^k 
69 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
69 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
69 Newton iteration  1 done69 , maximum relative shift = 3.976e-03
69 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
 28 31
56: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 29 31
56: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 30 31
56: communicator.hh::sendRecv, left MPI_Wait
56: leave BufferedCommunicator::sendRecv
56: communicator.hh::BufferedCommunicator::forward, left sendRecv
56 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), after communicator.free();
56 schwarz.hh:virtual void apply (X& v, const Y& d), AFTER communication.copyOwnerToAll(v,v); 
56 schwarz.hh:virtual void apply (X& v, const Y& d), before preconditioner.apply(v,d);; 
56 schwarz.hh:virtual void apply (X& v, const Y& d), before communication.copyOwnerToAll(v,v); 
56 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
56: communicator.hh::BufferedCommunicator::forward, enter sendRecv
56: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 31
56: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 31
56: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 31
56: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 31
56: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 31
56: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 31
56: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 31
56: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 31
56: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 31
56: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 31
56: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 10 31
56: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 11 31
56: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 12 31
56: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 13 31
56: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 14 31
56: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 15 31
56: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 16 31
56: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 17 31
56: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 18 31
56: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 19 31
56: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 20 31
56: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 21 31
56: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 22 31
56: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 23 31
56: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 24 31
56: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 25 31
56: communicator.hh::sendRecv, b19: leave BufferedCommunicator::sendRecv
19: communicator.hh::BufferedCommunicator::forward, left sendRecv
19 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), after communicator.free();
19 schwarz.hh:virtual void apply (X& v, const Y& d), AFTER communication.copyOwnerToAll(v,v); 
19 schwarz.hh:virtual void apply (X& v, const Y& d), before preconditioner.apply(v,d);; 
19 schwarz.hh:virtual void apply (X& v, const Y& d), before communication.copyOwnerToAll(v,v); 
19 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
19: communicator.hh::BufferedCommunicator::forward, enter sendRecv
19: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 17
19: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 17
19: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 17
19: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 17
19: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 17
19: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 17
19: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 17
19: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 17
19: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 17
19: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 17
19: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 10 17
19: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 11 17
19: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 12 17
19: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 13 17
19: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 14 17
19: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 15 17
19: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 16 17
19: communicator.hh::sendRecv, left MPI_Wait
19: leave BufferedCommunicator::sendRecv
19: communicator.hh::BufferedCommunicator::forward, left sendRecv
19 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), after communicator.free();
19 schwarz.hh:virtual void apply (X& v, const Y& d), AFTER communication.copyOwnerToAll(v,v); 
19void AMG<M,X,S,PI,A>::mgc(LevelContext& levelContext), finished 
19 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), AFTER mgc(levelContext) 
19 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), finished 
19   AFTER _prec->apply(y,p), before v = A * y 
19  before  alpha = rho_new / < rt, v > 
19  before alpha = rho_new / h; 
19  before apply first correction to x 
19  before r = r - alpha*v 
19  before test stop criteria 
19 istlsolver::printOutput 
  0.5      1.30064e-21      2.43002e-13
19 === rate=5.90498e-26, T=0.0266224, TIT=0.0532447, IT=0.5
19 finished solveLinearSystemImpl_ 1
19 converged value: 1 121 1 let s go get convergedRemote
19 to  comm_.min(converged) 
19 did  comm_.min(converged) convergedRemote: 1
19 final convergedRemote: 1
19 Update: x^(k+1) = x^k - deltax^k 
19 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
19 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
19 Newton iteration  1 done19 , maximum relative shift = 3.976e-03
19 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
120: leave BufferedCommunicator::sendRecv
120: communicator.hh::BufferedCommunicator::forward, left sendRecv
120 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), after communicator.free();
120 schwarz.hh:virtual void apply (X& v, const Y& d), AFTER communication.copyOwnerToAll(v,v); 
120 schwarz.hh:virtual void apply (X& v, const Y& d), before preconditioner.apply(v,d);; 
120 schwarz.hh:virtual void apply (X& v, const Y& d), before communication.copyOwnerToAll(v,v); 
120 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
120: communicator.hh::BufferedCommunicator::forward, enter sendRecv
120: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 7
120: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 7
120: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 7
120: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 7
120: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 7
120: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 7
120: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 7
120: communicator.hh::sendRecv, left MPI_Wait
120: leave BufferedCommunicator::sendRecv
120: communicator.hh::BufferedCommunicator::forward, left sendRecv
120 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), after communicator.free();
120 schwarz.hh:virtual void apply (X& v, const Y& d), AFTER communication.copyOwnerToAll(v,v); 
120void AMG<M,X,S,PI,A>::mgc(LevelContext& levelContext), finished 
120 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), AFTER mgc(levelContext) 
120 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), finished 
120   AFTER _prec->apply(y,p), before v = A * y 
120  before  alpha = rho_new / < rt, v > 
120  before alpha = rho_new / h; 
120  before apply first correction to x 
120  before r = r - alpha*v 
120  before test stop criteria 
120 istlsolver::printOutput 
  0.5      1.30064e-21      2.43002e-13
120 === rate=5.90498e-26, T=0.0267217, TIT=0.0534434, IT=0.5
120 finished solveLinearSystemImpl_ 1
120 converged value: 1 121 1 let s go get convergedRemote
120 to  comm_.min(converged) 
120 did  comm_.min(converged) convergedRemote: 1
120 final convergedRemote: 1
120 Update: x^(k+1) = x^k - deltax^k 
120 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
120 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
120 Newton iteration  1 done120 , maximum relative shift = 3.976e-03
120 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
114: leave BufferedCommunicator::sendRecv
114: communicator.hh::BufferedCommunicator::forward, left sendRecv
114 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), after communicator.free();
114 schwarz.hh:virtual void apply (X& v, const Y& d), AFTER communication.copyOwnerToAll(v,v); 
114 schwarz.hh:virtual void apply (X& v, const Y& d), before preconditioner.apply(v,d);; 
114 schwarz.hh:virtual void apply (X& v, const Y& d), before communication.copyOwnerToAll(v,v); 
114 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
114: communicator.hh::BufferedCommunicator::forward, enter sendRecv
114: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 13
114: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 13
114: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 13
114: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 13
114: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 13
114: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 13
114: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 13
114: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 13
114: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 13
114: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 13
114: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 10 13
114: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 11 13
114: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 12 13
114: communicator.hh::sendRecv, left MPI_Wait
114: leave BufferedCommunicator::sendRecv
114: communicator.hh::BufferedCommunicator::forward, left sendRecv
114 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), after communicator.free();
114 schwarz.hh:virtual void apply (X& v, const Y& d), AFTER communication.copyOwnerToAll(v,v); 
114void AMG<M,X,S,PI,A>::mgc(LevelContext& levelContext), finished 
114 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), AFTER mgc(levelContext) 
114 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), finished 
114   AFTER _prec->apply(y,p), before v = A * y 
114  before  alpha = rho_new / < rt, v > 
114  before alpha = rho_new / h; 
114  before apply first correction to x 
114  before r = r - alpha*v 
114  before test stop criteria 
114 istlsolver::printOutput 
  0.5      1.30064e-21      2.43002e-13
114 === rate=5.90498e-26, T=0.0266938, TIT=0.0533877, IT=0.5
114 finished solveLinearSystemImpl_ 1
114 converged value: 1 121 1 let s go get convergedRemote
114 to  comm_.min(converged) 
114 did  comm_.min(converged) convergedRemote: 1
114 final convergedRemote: 1
114 Update: x^(k+1) = x^k - deltax^k 
114 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
114 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
114 Newton iteration  1 done114 , maximum relative shift = 3.976e-03
114 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
26: leave BufferedCommunicator::sendRecv
26: communicator.hh::BufferedCommunicator::forward, left sendRecv
26 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), after communicator.free();
26 schwarz.hh:virtual void apply (X& v, const Y& d), AFTER communication.copyOwnerToAll(v,v); 
26 schwarz.hh:virtual void apply (X& v, const Y& d), before preconditioner.apply(v,d);; 
26 schwarz.hh:virtual void apply (X& v, const Y& d), before communication.copyOwnerToAll(v,v); 
26 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
26: communicator.hh::BufferedCommunicator::forward, enter sendRecv
26: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 26
26: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 26
26: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 26
26: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 26
26: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 26
26: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 26
26: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 26
26: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 26
26: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 26
26: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 26
26: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 10 26
26: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 11 26
26: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 12 26
26: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 13 26
26: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 14 26
26: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 15 26
26: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 16 26
26: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 17 26
26: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 18 26
26: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 19 26
26: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 20 26
26: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 21 26
26: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 22 26
26: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 23 26
26: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 24 26
26: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 25 26
26: communicator.hh::sendRecv, left MPI_Wait
26: leave BufferedCommunicator::sendRecv
26: communicator.hh::BufferedCommunicator::forward, left sendRecv
26 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), after communicator.free();
26 schwarz.hh:virtual void apply (X& v, const Y& d), AFTER communication.cop14: leave BufferedCommunicator::sendRecv
14: communicator.hh::BufferedCommunicator::forward, left sendRecv
14 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), after communicator.free();
14 schwarz.hh:virtual void apply (X& v, const Y& d), AFTER communication.copyOwnerToAll(v,v); 
14 schwarz.hh:virtual void apply (X& v, const Y& d), before preconditioner.apply(v,d);; 
14 schwarz.hh:virtual void apply (X& v, const Y& d), before communication.copyOwnerToAll(v,v); 
14 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
14: communicator.hh::BufferedCommunicator::forward, enter sendRecv
14: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 17
14: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 17
14: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 17
14: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 17
14: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 17
14: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 17
14: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 17
14: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 17
14: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 17
14: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 17
14: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 10 17
14: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 11 17
14: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 12 17
14: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 13 17
14: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 14 17
14: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 15 17
14: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 16 17
14: communicator.hh::sendRecv, left MPI_Wait
14: leave BufferedCommunicator::sendRecv
14: communicator.hh::BufferedCommunicator::forward, left sendRecv
14 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), after communicator.free();
14 schwarz.hh:virtual void apply (X& v, const Y& d), AFTER communication.copyOwnerToAll(v,v); 
14void AMG<M,X,S,PI,A>::mgc(LevelContext& levelContext), finished 
14 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), AFTER mgc(levelContext) 
14 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), finished 
14   AFTER _prec->apply(y,p), before v = A * y 
14  before  alpha = rho_new / < rt, v > 
14  before alpha = rho_new / h; 
14  before apply first correction to x 
14  before r = r - alpha*v 
14  before test stop criteria 
14 istlsolver::printOutput 
  0.5      1.30064e-21      2.43002e-13
14 === rate=5.90498e-26, T=0.0267006, TIT=0.0534013, IT=0.5
14 finished solveLinearSystemImpl_ 1
14 converged value: 1 121 1 let s go get convergedRemote
14 to  comm_.min(converged) 
14 did  comm_.min(converged) convergedRemote: 1
14 final convergedRemote: 1
14 Update: x^(k+1) = x^k - deltax^k 
14 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
14 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
14 Newton iteration  1 done14 , maximum relative shift = 3.976e-03
14 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
94: leave BufferedCommunicator::sendRecv
94: communicator.hh::BufferedCommunicator::forward, left sendRecv
94 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), after communicator.free();
94 schwarz.hh:virtual void apply (X& v, const Y& d), AFTER communication.copyOwnerToAll(v,v); 
94 schwarz.hh:virtual void apply (X& v, const Y& d), before preconditioner.apply(v,d);; 
94 schwarz.hh:virtual void apply (X& v, const Y& d), before communication.copyOwnerToAll(v,v); 
94 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
94: communicator.hh::BufferedCommunicator::forward, enter sendRecv
94: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 21
94: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 21
94: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 21
94: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 21
94: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 21
94: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 21
94: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 21
94: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 21
94: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 21
94: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 21
94: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 10 21
94: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 11 21
94: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 12 21
94: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 13 21
94: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 14 21
94: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 15 21
94: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 16 21
94: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 17 21
94: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 18 21
94: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 19 21
94: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 20 21
94: communicator.hh::sendRecv, left MPI_Wait
94: leave BufferedCommunicator::sendRecv
94: communicator.hh::BufferedCommunicator::forward, left sendRecv
94 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), after communicator.free();
94 schwarz.hh:virtual void apply (X& v, const Y& d), AFTER communication.copyOwnerToAll(v,v); 
94void AMG<M,X,S,PI,A>::mgc(LevelContext& levelContext), finished 
94 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), AFTER mgc(levelContext) 
94 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), finished 
94   AFTER _prec->apply(y,p), before v = A * y 
94  before  alpha = rho_new / < rt, v > 
94  before alpha = rho_new / h; 
94  before apply first correction to x 
94  before r = r - alpha*v 
94  before test stop criteria 
94 istlsolver::printOutput 
  0.5      1.30064e-21      2.43002e-13
94 === rate=5.90498e-26, T=0.0266773, TIT=0.0533546, IT=0.5
94 fini103: leave BufferedCommunicator::sendRecv
103: communicator.hh::BufferedCommunicator::forward, left sendRecv
103 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), after communicator.free();
103 schwarz.hh:virtual void apply (X& v, const Y& d), AFTER communication.copyOwnerToAll(v,v); 
103 schwarz.hh:virtual void apply (X& v, const Y& d), before preconditioner.apply(v,d);; 
103 schwarz.hh:virtual void apply (X& v, const Y& d), before communication.copyOwnerToAll(v,v); 
103 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
103: communicator.hh::BufferedCommunicator::forward, enter sendRecv
103: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 16
103: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 16
103: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 16
103: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 16
103: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 16
103: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 16
103: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 16
103: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 16
103: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 16
103: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 16
103: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 10 16
103: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 11 16
103: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 12 16
103: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 13 16
103: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 14 16
103: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 15 16
103: communicator.hh::sendRecv, left MPI_Wait
103: leave BufferedCommunicator::sendRecv
103: communicator.hh::BufferedCommunicator::forward, left sendRecv
103 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), after communicator.free();
103 schwarz.hh:virtual void apply (X& v, const Y& d), AFTER communication.copyOwnerToAll(v,v); 
103void AMG<M,X,S,PI,A>::mgc(LevelContext& levelContext), finished 
103 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), AFTER mgc(levelContext) 
103 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), finished 
103   AFTER _prec->apply(y,p), before v = A * y 
103  before  alpha = rho_new / < rt, v > 
103  before alpha = rho_new / h; 
103  before apply first correction to x 
103  before r = r - alpha*v 
103  before test stop criteria 
103 istlsolver::printOutput 
  0.5      1.30064e-21      2.43002e-13
103 === rate=5.90498e-26, T=0.0266662, TIT=0.0533323, IT=0.5
103 finished solveLinearSystemImpl_ 1
103 converged value: 1 121 1 let s go get convergedRemote
103 to  comm_.min(converged) 
103 did  comm_.min(converged) convergedRemote: 1
103 final convergedRemote: 1
103 Update: x^(k+1) = x^k - deltax^k 
103 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
103 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
103 Newton iteration  1 done103 , maximum relative shift = 3.976e-03
103 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
34: leave BufferedCommunicator::sendRecv
34: communicator.hh::BufferedCommunicator::forward, left sendRecv
34 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), after communicator.free();
34 schwarz.hh:virtual void apply (X& v, const Y& d), AFTER communication.copyOwnerToAll(v,v); 
34 schwarz.hh:virtual void apply (X& v, const Y& d), before preconditioner.apply(v,d);; 
34 schwarz.hh:virtual void apply (X& v, const Y& d), before communication.copyOwnerToAll(v,v); 
34 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
34: communicator.hh::BufferedCommunicator::forward, enter sendRecv
34: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 14
34: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 14
34: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 14
34: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 14
34: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 14
34: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 14
34: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 14
34: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 14
34: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 14
34: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 14
34: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 10 14
34: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 11 14
34: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 12 14
34: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 13 14
34: communicator.hh::sendRecv, left MPI_Wait
34: leave BufferedCommunicator::sendRecv
34: communicator.hh::BufferedCommunicator::forward, left sendRecv
34 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), after communicator.free();
34 schwarz.hh:virtual void apply (X& v, const Y& d), AFTER communication.copyOwnerToAll(v,v); 
34void AMG<M,X,S,PI,A>::mgc(LevelContext& levelContext), finished 
34 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), AFTER mgc(levelContext) 
34 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), finished 
34   AFTER _prec->apply(y,p), before v = A * y 
34  before  alpha = rho_new / < rt, v > 
34  before alpha = rho_new / h; 
34  before apply first correction to x 
34  before r = r - alpha*v 
34  before test stop criteria 
34 istlsolver::printOutput 
  0.5      1.30064e-21      2.43002e-13
34 === rate=5.90498e-26, T=0.026613, TIT=0.053226, IT=0.5
34 finished solveLinearSystemImpl_ 1
34 converged value: 1 121 1 let s go get convergedRemote
34 to  comm_.min(converged) 
34 did  comm_.min(converged) convergedRemote: 1
34 final convergedRemote: 1
34 Update: x^(k+1) = x^k - deltax^k 
34 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
34 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
34 Newton iteration  1 done34 , maximum relative shift = 3.976e-03
34 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
51: leave BufferedCommunicator::sendRecv
51: communicator.hh::BufferedCommunicator::forward, left sendRecv
51 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), after communicator.free();
51 schwarz.hh:virtual void apply (X& v, const Y& d), AFTER communication.copyOwnerToAll(v,v); 
51 schwarz.hh:virtual void apply (X& v, const Y& d), before preconditioner.apply(v,d);; 
51 schwarz.hh:virtual void apply (X& v, const Y& d), before communication.copyOwnerToAll(v,v); 
51 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
51: communicator.hh::BufferedCommunicator::forward, enter sendRecv
51: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 10
51: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 10
51: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 10
51: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 10
51: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 10
51: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 10
51: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 10
51: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 10
51: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 10
51: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 10
51: communicator.hh::sendRecv, left MPI_Wait
51: leave BufferedCommunicator::sendRecv
51: communicator.hh::BufferedCommunicator::forward, left sendRecv
51 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), after communicator.free();
51 schwarz.hh:virtual void apply (X& v, const Y& d), AFTER communication.copyOwnerToAll(v,v); 
51void AMG<M,X,S,PI,A>::mgc(LevelContext& levelContext), finished 
51 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), AFTER mgc(levelContext) 
51 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), finished 
51   AFTER _prec->apply(y,p), before v = A * y 
51  before  alpha = rho_new / < rt, v > 
51  before alpha = rho_new / h; 
51  before apply first correction to x 
51  before r = r - alpha*v 
51  before test stop criteria 
51 istlsolver::printOutput 
  0.5      1.30064e-21      2.43002e-13
51 === rate=5.90498e-26, T=0.0266384, TIT=0.0532768, IT=0.5
51 finished solveLinearSystemImpl_ 1
51 converged value: 1 121 1 let s go get convergedRemote
51 to  comm_.min(converged) 
51 did  comm_.min(converged) convergedRemote: 1
51 final convergedRemote: 1
51 Update: x^(k+1) = x^k - deltax^k 
51 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
51 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
51 Newton iteration  1 done51 , maximum relative shift = 3.976e-03
51 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
95: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 7
95: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 7
95: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 7
95: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 7
95: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 7
95: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 7
95: communicator.hh::sendRecv, left MPI_Wait
95: leave BufferedCommunicator::sendRecv
95: communicator.hh::BufferedCommunicator::forward, left sendRecv
95 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), after communicator.free();
95 schwarz.hh:virtual void apply (X& v, const Y& d), AFTER communication.copyOwnerToAll(v,v); 
95void AMG<M,X,S,PI,A>::mgc(LevelContext& levelContext), finished 
95 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), AFTER mgc(levelContext) 
95 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), finished 
95   AFTER _prec->apply(y,p), before v = A * y 
95  before  alpha = rho_new / < rt, v > 
95  before alpha = rho_new / h; 
95  before apply first correction to x 
95  before r = r - alpha*v 
95  before test stop criteria 
95 istlsolver::printOutput 
  0.5      1.30064e-21      2.43002e-13
95 === rate=5.90498e-26, T=0.0266883, TIT=0.0533765, IT=0.5
95 finished solveLinearSystemImpl_ 1
95 converged value: 1 121 1 let s go get convergedRemote
95 to  comm_.min(converged) 
95 did  comm_.min(converged) convergedRemote: 1
95 final convergedRemote: 1
95 Update: x^(k+1) = x^k - deltax^k 
95 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
95 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
95 Newton iteration  1 done95 , maximum relative shift = 3.976e-03
95 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
75: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 17
75: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 17
75: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 17
75: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 17
75: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 17
75: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 17
75: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 17
75: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 17
75: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 10 17
75: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 11 17
75: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 12 17
75: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 13 17
75: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 14 17
75: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 15 17
75: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 16 17
75: communicator.hh::sendRecv, left MPI_Wait
75: leave BufferedCommunicator::sendRecv
75: communicator.hh::BufferedCommunicator::forward, left sendRecv
75 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), after communicator.free();
75 schwarz.hh:virtual void apply (X& v, const Y& d), AFTER communication.copyOwnerToAll(v,v); 
75void AMG<M,X,S,PI,A>::mgc(LevelContext& levelContext), finished 
75 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), AFTER mgc(levelContext) 
75 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), finished 
75   AFTER _prec->apply(y,p), before v = A * y 
75  before  alpha = rho_new / < rt, v > 
75  before alpha = rho_new / h; 
75  before apply first correction to x 
75  before r = r - alpha*v 
75  before test stop criteria 
75 istlsolver::printOutput 
  0.5      1.30064e-21      2.43002e-13
75 === rate=5.90498e-26, T=0.0266261, TIT=0.0532522, IT=0.5
75 finished solveLinearSystemImpl_ 1
75 converged value: 1 121 1 let s go get convergedRemote
75 to  comm_.min(converged) 
75 did  comm_.min(converged) convergedRemote: 1
75 final convergedRemote: 1
75 Update: x^(k+1) = x^k - deltax^k 
75 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
75 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
75 Newton iteration  1 done75 , maximum relative shift = 3.976e-03
75 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
63: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 9
63: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 9
63: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 9
63: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 9
63: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 9
63: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 9
63: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 9
63: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 9
63: communicator.hh::sendRecv, left MPI_Wait
63: leave BufferedCommunicator::sendRecv
63: communicator.hh::BufferedCommunicator::forward, left sendRecv
63 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), after communicator.free();
63 schwarz.hh:virtual void apply (X& v, const Y& d), AFTER communication.copyOwnerToAll(v,v); 
63void AMG<M,X,S,PI,A>::mgc(LevelContext& levelContext), finished 
63 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), AFTER mgc(levelContext) 
63 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), finished 
63   AFTER _prec->apply(y,p), before v = A * y 
63  before  alpha = rho_new / < rt, v > 
63  before alpha = rho_new / h; 
63  before apply first correction to x 
63  before r = r - alpha*v 
63  before test stop criteria 
63 istlsolver::printOutput 
  0.5      1.30064e-21      2.43002e-13
63 === rate=5.90498e-26, T=0.0266917, TIT=0.0533835, IT=0.5
63 finished solveLinearSystemImpl_ 1
63 converged value: 1 121 1 let s go get convergedRemote
63 to  comm_.min(converged) 
63 did  comm_.min(converged) convergedRemote: 1
63 final convergedRemote: 1
63 Update: x^(k+1) = x^k - deltax^k 
63 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
63 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
63 Newton iteration  1 done63 , maximum relative shift = 3.976e-03
63 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
76: leave BufferedCommunicator::sendRecv
76: communicator.hh::BufferedCommunicator::forward, left sendRecv
76 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), after communicator.free();
76 schwarz.hh:virtual void apply (X& v, const Y& d), AFTER communication.copyOwnerToAll(v,v); 
76 schwarz.hh:virtual void apply (X& v, const Y& d), before preconditioner.apply(v,d);; 
76 schwarz.hh:virtual void apply (X& v, const Y& d), before communication.copyOwnerToAll(v,v); 
76 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
76: communicator.hh::BufferedCommunicator::forward, enter sendRecv
76: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 12
76: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 12
76: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 12
76: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 12
76: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 12
76: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 12
76: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 12
76: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 12
76: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 12
76: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 12
76: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 10 12
76: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 11 12
76: communicator.hh::sendRecv, left MPI_Wait
76: leave BufferedCommunicator::sendRecv
76: communicator.hh::BufferedCommunicator::forward, left sendRecv
76 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), after communicator.free();
76 schwarz.hh:virtual void apply (X& v, const Y& d), AFTER communication.copyOwnerToAll(v,v); 
76void AMG<M,X,S,PI,A>::mgc(LevelContext& levelContext), finished 
76 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), AFTER mgc(levelContext) 
76 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), finished 
76   AFTER _prec->apply(y,p), before v = A * y 
76  before  alpha = rho_new / < rt, v > 
76  before alpha = rho_new / h; 
76  before apply first correction to x 
76  before r = r - alpha*v 
76  before test stop criteria 
76 istlsolver::printOutput 
  0.5      1.30064e-21      2.43002e-13
76 === rate=5.90498e-26, T=0.0266576, TIT=0.0533152, IT=0.5
76 finished solveLinearSystemImpl_ 1
76 converged value: 1 121 1 let s go get convergedRemote
76 to  comm_.min(converged) 
76 did  comm_.min(converged) convergedRemote: 1
76 final convergedRemote: 1
76 Update: x^(k+1) = x^k - deltax^k 
76 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
76 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
76 Newton iteration  1 done76 , maximum relative shift = 3.976e-03
76 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
115: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 11
115: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 11
115: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 11
115: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 11
115: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 11
115: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 11
115: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 11
115: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 10 11
115: communicator.hh::sendRecv, left MPI_Wait
115: leave BufferedCommunicator::sendRecv
115: communicator.hh::BufferedCommunicator::forward, left sendRecv
115 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), after communicator.free();
115 schwarz.hh:virtual void apply (X& v, const Y& d), AFTER communication.copyOwnerToAll(v,v); 
115void AMG<M,X,S,PI,A>::mgc(LevelContext& levelContext), finished 
115 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), AFTER mgc(levelContext) 
115 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), finished 
115   AFTER _prec->apply(y,p), before v = A * y 
115  before  alpha = rho_new / < rt, v > 
115  before alpha = rho_new / h; 
115  before apply first correction to x 
115  before r = r - alpha*v 
115  before test stop criteria 
115 istlsolver::printOutput 
  0.5      1.30064e-21      2.43002e-13
115 === rate=5.90498e-26, T=0.0266436, TIT=0.0532873, IT=0.5
115 finished solveLinearSystemImpl_ 1
115 converged value: 1 121 1 let s go get convergedRemote
115 to  comm_.min(converged) 
115 did  comm_.min(converged) convergedRemote: 1
115 final convergedRemote: 1
115 Update: x^(k+1) = x^k - deltax^k 
115 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
115 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
115 Newton iteration  1 done115 , maximum relative shift = 3.976e-03
115 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
18: leave BufferedCommunicator::sendRecv
18: communicator.hh::BufferedCommunicator::forward, left sendRecv
18 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), after communicator.free();
18 schwarz.hh:virtual void apply (X& v, const Y& d), AFTER communication.copyOwnerToAll(v,v); 
18 schwarz.hh:virtual void apply (X& v, const Y& d), before preconditioner.apply(v,d);; 
18 schwarz.hh:virtual void apply (X& v, const Y& d), before communication.copyOwnerToAll(v,v); 
18 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
18: communicator.hh::BufferedCommunicator::forward, enter sendRecv
18: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 14
18: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 14
18: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 14
18: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 14
18: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 14
18: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 14
18: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 14
18: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 14
18: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 14
18: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 14
18: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 10 14
18: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 11 14
18: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 12 14
18: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 13 14
18: communicator.hh::sendRecv, left MPI_Wait
18: leave BufferedCommunicator::sendRecv
18: communicator.hh::BufferedCommunicator::forward, left sendRecv
18 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), after communicator.free();
18 schwarz.hh:virtual void apply (X& v, const Y& d), AFTER communication.copyOwnerToAll(v,v); 
18void AMG<M,X,S,PI,A>::mgc(LevelContext& levelContext), finished 
18 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), AFTER mgc(levelContext) 
18 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), finished 
18   AFTER _prec->apply(y,p), before v = A * y 
18  before  alpha = rho_new / < rt, v > 
18  before alpha = rho_new / h; 
18  before apply first correction to x 
18  before r = r - alpha*v 
18  before test stop criteria 
18 istlsolver::printOutput 
  0.5      1.30064e-21      2.43002e-13
18 === rate=5.90498e-26, T=0.0266232, TIT=0.0532464, IT=0.5
18 finished solveLinearSystemImpl_ 1
18 converged value: 1 121 1 let s go get convergedRemote
18 to  comm_.min(converged) 
18 did  comm_.min(converged) convergedRemote: 1
18 final convergedRemote: 1
18 Update: x^(k+1) = x^k - deltax^k 
18 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
18 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
18 Newton iteration  1 done18 , maximum relative shift = 3.976e-03
18 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
53: leave BufferedCommunicator::sendRecv
53: communicator.hh::BufferedCommunicator::forward, left sendRecv
53 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), after communicator.free();
53 schwarz.hh:virtual void apply (X& v, const Y& d), AFTER communication.copyOwnerToAll(v,v); 
53 schwarz.hh:virtual void apply (X& v, const Y& d), before preconditioner.apply(v,d);; 
53 schwarz.hh:virtual void apply (X& v, const Y& d), before communication.copyOwnerToAll(v,v); 
53 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
53: communicator.hh::BufferedCommunicator::forward, enter sendRecv
53: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 17
53: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 17
53: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 17
53: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 17
53: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 17
53: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 17
53: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 17
53: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 17
53: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 17
53: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 17
53: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 10 17
53: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 11 17
53: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 12 17
53: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 13 17
53: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 14 17
53: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 15 17
53: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 16 17
53: communicator.hh::sendRecv, left MPI_Wait
53: leave BufferedCommunicator::sendRecv
53: communicator.hh::BufferedCommunicator::forward, left sendRecv
53 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), after communicator.free();
53 schwarz.hh:virtual void apply (X& v, const Y& d), AFTER communication.copyOwnerToAll(v,v); 
53void AMG<M,X,S,PI,A>::mgc(LevelContext& levelContext), finished 
53 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), AFTER mgc(levelContext) 
53 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), finished 
53   AFTER _prec->apply(y,p), before v = A * y 
53  before  alpha = rho_new / < rt, v > 
53  before alpha = rho_new / h; 
53  before apply first correction to x 
53  before r = r - alpha*v 
53  before test stop criteria 
53 istlsolver::printOutput 
  0.5      1.30064e-21      2.43002e-13
53 === rate=5.90498e-26, T=0.0266433, TIT=0.0532867, IT=0.5
53 finished solveLinearSystemImpl_ 1
53 converged value: 1 121 1 let s go get convergedRemote
53 to  comm_.min(converged) 
53 did  comm_.min(converged) convergedRemote: 1
53 final convergedRemote: 1
53 Update: x^(k+1) = x^k - deltax^k 
53 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
53 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
53 Newton iteration  1 done53 , maximum relative shift = 3.976e-03
53 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
23: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 11
23: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 11
23: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 11
23: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 11
23: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 11
23: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 11
23: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 11
23: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 11
23: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 11
23: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 10 11
23: communicator.hh::sendRecv, left MPI_Wait
23: leave BufferedCommunicator::sendRecv
23: communicator.hh::BufferedCommunicator::forward, left sendRecv
23 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), after communicator.free();
23 schwarz.hh:virtual void apply (X& v, const Y& d), AFTER communication.copyOwnerToAll(v,v); 
23void AMG<M,X,S,PI,A>::mgc(LevelContext& levelContext), finished 
23 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), AFTER mgc(levelContext) 
23 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), finished 
23   AFTER _prec->apply(y,p), before v = A * y 
23  before  alpha = rho_new / < rt, v > 
23  before alpha = rho_new / h; 
23  before apply first correction to x 
23  before r = r - alpha*v 
23  before test stop criteria 
23 istlsolver::printOutput 
  0.5      1.30064e-21      2.43002e-13
23 === rate=5.90498e-26, T=0.0266864, TIT=0.0533729, IT=0.5
23 finished solveLinearSystemImpl_ 1
23 converged value: 1 121 1 let s go get convergedRemote
23 to  comm_.min(converged) 
23 did  comm_.min(converged) convergedRemote: 1
23 final convergedRemote: 1
23 Update: x^(k+1) = x^k - deltax^k 
23 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
23 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
23 Newton iteration  1 done23 , maximum relative shift = 3.976e-03
23 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
119: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 13
119: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 13
119: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 13
119: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 13
119: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 13
119: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 13
119: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 13
119: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 13
119: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 10 13
119: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 11 13
119: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 12 13
119: communicator.hh::sendRecv, left MPI_Wait
119: leave BufferedCommunicator::sendRecv
119: communicator.hh::BufferedCommunicator::forward, left sendRecv
119 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), after communicator.free();
119 schwarz.hh:virtual void apply (X& v, const Y& d), AFTER communication.copyOwnerToAll(v,v); 
119void AMG<M,X,S,PI,A>::mgc(LevelContext& levelContext), finished 
119 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), AFTER mgc(levelContext) 
119 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), finished 
119   AFTER _prec->apply(y,p), before v = A * y 
119  before  alpha = rho_new / < rt, v > 
119  before alpha = rho_new / h; 
119  before apply first correction to x 
119  before r = r - alpha*v 
119  before test stop criteria 
119 istlsolver::printOutput 
  0.5      1.30064e-21      2.43002e-13
119 === rate=5.90498e-26, T=0.0267067, TIT=0.0534134, IT=0.5
119 finished solveLinearSystemImpl_ 1
119 converged value: 1 121 1 let s go get convergedRemote
119 to  comm_.min(converged) 
119 did  comm_.min(converged) convergedRemote: 1
119 final convergedRemote: 1
119 Update: x^(k+1) = x^k - deltax^k 
119 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
119 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
119 Newton iteration  1 done119 , maximum relative shift = 3.976e-03
119 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
50: leave BufferedCommunicator::sendRecv
50: communicator.hh::BufferedCommunicator::forward, left sendRecv
50 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), after communicator.free();
50 schwarz.hh:virtual void apply (X& v, const Y& d), AFTER communication.copyOwnerToAll(v,v); 
50 schwarz.hh:virtual void apply (X& v, const Y& d), before preconditioner.apply(v,d);; 
50 schwarz.hh:virtual void apply (X& v, const Y& d), before communication.copyOwnerToAll(v,v); 
50 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
50: communicator.hh::BufferedCommunicator::forward, enter sendRecv
50: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 10
50: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 10
50: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 10
50: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 10
50: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 10
50: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 10
50: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 10
50: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 10
50: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 10
50: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 10
50: communicator.hh::sendRecv, left MPI_Wait
50: leave BufferedCommunicator::sendRecv
50: communicator.hh::BufferedCommunicator::forward, left sendRecv
50 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), after communicator.free();
50 schwarz.hh:virtual void apply (X& v, const Y& d), AFTER communication.copyOwnerToAll(v,v); 
50void AMG<M,X,S,PI,A>::mgc(LevelContext& levelContext), finished 
50 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), AFTER mgc(levelContext) 
50 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), finished 
50   AFTER _prec->apply(y,p), before v = A * y 
50  before  alpha = rho_new / < rt, v > 
50  before alpha = rho_new / h; 
50  before apply first correction to x 
50  before r = r - alpha*v 
50  before test stop criteria 
50 istlsolver::printOutput 
  0.5      1.30064e-21      2.43002e-13
50 === rate=5.90498e-26, T=0.0266479, TIT=0.0532959, IT=0.5
50 finished solveLinearSystemImpl_ 1
50 converged value: 1 121 1 let s go get convergedRemote
50 to  comm_.min(converged) 
50 did  comm_.min(converged) convergedRemote: 1
50 final convergedRemote: 1
50 Update: x^(k+1) = x^k - deltax^k 
50 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
50 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
50 Newton iteration  1 done50 , maximum relative shift = 3.976e-03
50 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
84: leave BufferedCommunicator::sendRecv
84: communicator.hh::BufferedCommunicator::forward, left sendRecv
84 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), after communicator.free();
84 schwarz.hh:virtual void apply (X& v, const Y& d), AFTER communication.copyOwnerToAll(v,v); 
84 schwarz.hh:virtual void apply (X& v, const Y& d), before preconditioner.apply(v,d);; 
84 schwarz.hh:virtual void apply (X& v, const Y& d), before communication.copyOwnerToAll(v,v); 
84 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
84: communicator.hh::BufferedCommunicator::forward, enter sendRecv
84: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 18
84: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 18
84: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 18
84: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 18
84: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 18
84: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 18
84: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 18
84: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 18
84: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 18
84: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 18
84: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 10 18
84: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 11 18
84: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 12 18
84: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 13 18
84: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 14 18
84: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 15 18
84: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 16 18
84: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 17 18
84: communicator.hh::sendRecv, left MPI_Wait
84: leave BufferedCommunicator::sendRecv
84: communicator.hh::BufferedCommunicator::forward, left sendRecv
84 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), after communicator.free();
84 schwarz.hh:virtual void apply (X& v, const Y& d), AFTER communication.copyOwnerToAll(v,v); 
84void AMG<M,X,S,PI,A>::mgc(LevelContext& levelContext), finished 
84 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), AFTER mgc(levelContext) 
84 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), finished 
84   AFTER _prec->apply(y,p), before v = A * y 
84  before  alpha = rho_new / < rt, v > 
84  before alpha = rho_new / h; 
84  before apply first correction to x 
84  before r = r - alpha*v 
84  before test stop criteria 
84 istlsolver::printOutput 
  0.5      1.30064e-21      2.43002e-13
84 === rate=5.90498e-26, T=0.026605, TIT=0.0532099, IT=0.5
84 finished solveLinearSystemImpl_ 1
84 converged value: 1 121 1 let s go get convergedRemote
84 to  comm_.min(converged) 
84 did  comm_.min(converged) convergedRemote: 1
84 final convergedRemote: 1
84 Update: x^(k+1) = x^k - deltax^k 
84 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
84 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
84 Newton iteration111: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 11
111: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 11
111: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 11
111: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 11
111: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 11
111: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 11
111: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 11
111: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 10 11
111: communicator.hh::sendRecv, left MPI_Wait
111: leave BufferedCommunicator::sendRecv
111: communicator.hh::BufferedCommunicator::forward, left sendRecv
111 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), after communicator.free();
111 schwarz.hh:virtual void apply (X& v, const Y& d), AFTER communication.copyOwnerToAll(v,v); 
111void AMG<M,X,S,PI,A>::mgc(LevelContext& levelContext), finished 
111 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), AFTER mgc(levelContext) 
111 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), finished 
111   AFTER _prec->apply(y,p), before v = A * y 
111  before  alpha = rho_new / < rt, v > 
111  before alpha = rho_new / h; 
111  before apply first correction to x 
111  before r = r - alpha*v 
111  before test stop criteria 
111 istlsolver::printOutput 
  0.5      1.30064e-21      2.43002e-13
111 === rate=5.90498e-26, T=0.0267083, TIT=0.0534166, IT=0.5
111 finished solveLinearSystemImpl_ 1
111 converged value: 1 121 1 let s go get convergedRemote
111 to  comm_.min(converged) 
111 did  comm_.min(converged) convergedRemote: 1
111 final convergedRemote: 1
111 Update: x^(k+1) = x^k - deltax^k 
111 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
111 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
111 Newton iteration  1 done111 , maximum relative shift = 3.976e-03
111 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
72: leave BufferedCommunicator::sendRecv
72: communicator.hh::BufferedCommunicator::forward, left sendRecv
72 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), after communicator.free();
72 schwarz.hh:virtual void apply (X& v, const Y& d), AFTER communication.copyOwnerToAll(v,v); 
72 schwarz.hh:virtual void apply (X& v, const Y& d), before preconditioner.apply(v,d);; 
72 schwarz.hh:virtual void apply (X& v, const Y& d), before communication.copyOwnerToAll(v,v); 
72 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
72: communicator.hh::BufferedCommunicator::forward, enter sendRecv
72: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 20
72: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 20
72: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 20
72: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 20
72: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 20
72: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 20
72: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 20
72: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 20
72: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 20
72: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 20
72: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 10 20
72: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 11 20
72: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 12 20
72: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 13 20
72: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 14 20
72: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 15 20
72: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 16 20
72: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 17 20
72: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 18 20
72: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 19 20
72: communicator.hh::sendRecv, left MPI_Wait
72: leave BufferedCommunicator::sendRecv
72: communicator.hh::BufferedCommunicator::forward, left sendRecv
72 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), after communicator.free();
72 schwarz.hh:virtual void apply (X& v, const Y& d), AFTER communication.copyOwnerToAll(v,v); 
72void AMG<M,X,S,PI,A>::mgc(LevelContext& levelContext), finished 
72 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), AFTER mgc(levelContext) 
72 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), finished 
72   AFTER _prec->apply(y,p), before v = A * y 
72  before  alpha = rho_new / < rt, v > 
72  before alpha = rho_new / h; 
72  before apply first correction to x 
72  before r = r - alpha*v 
72  before test stop criteria 
72 istlsolver::printOutput 
  0.5      1.30064e-21      2.43002e-13
72 === rate=5.90498e-26, T=0.0266297, TIT=0.0532595, IT=0.5
72 finished solveLinearSystemImpl_ 1
72 converged value: 1 121 1 let s go get convergedRemote
72 to  comm_.min(converged) 
72 30: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 6
30: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 6
30: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 6
30: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 6
30: communicator.hh::sendRecv, left MPI_Wait
30: leave BufferedCommunicator::sendRecv
30: communicator.hh::BufferedCommunicator::forward, left sendRecv
30 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), after communicator.free();
30 schwarz.hh:virtual void apply (X& v, const Y& d), AFTER communication.copyOwnerToAll(v,v); 
30void AMG<M,X,S,PI,A>::mgc(LevelContext& levelContext), finished 
30 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), AFTER mgc(levelContext) 
30 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), finished 
30   AFTER _prec->apply(y,p), before v = A * y 
30  before  alpha = rho_new / < rt, v > 
30  before alpha = rho_new / h; 
30  before apply first correction to x 
30  before r = r - alpha*v 
30  before test stop criteria 
30 istlsolver::printOutput 
  0.5      1.30064e-21      2.43002e-13
30 === rate=5.90498e-26, T=0.0266433, TIT=0.0532866, IT=0.5
30 finished solveLinearSystemImpl_ 1
30 converged value: 1 121 1 let s go get convergedRemote
30 to  comm_.min(converged) 
30 did  comm_.min(converged) convergedRemote: 1
30 final convergedRemote: 1
30 Update: x^(k+1) = x^k - deltax^k 
30 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
30 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
30 Newton iteration  1 done30 , maximum relative shift = 3.976e-03
30 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
61: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 9
61: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 9
61: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 9
61: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 9
61: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 9
61: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 9
61: communicator.hh::sendRecv, left MPI_Wait
61: leave BufferedCommunicator::sendRecv
61: communicator.hh::BufferedCommunicator::forward, left sendRecv
61 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), after communicator.free();
61 schwarz.hh:virtual void apply (X& v, const Y& d), AFTER communication.copyOwnerToAll(v,v); 
61void AMG<M,X,S,PI,A>::mgc(LevelContext& levelContext), finished 
61 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), AFTER mgc(levelContext) 
61 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), finished 
61   AFTER _prec->apply(y,p), before v = A * y 
61  before  alpha = rho_new / < rt, v > 
61  before alpha = rho_new / h; 
61  before apply first correction to x 
61  before r = r - alpha*v 
61  before test stop criteria 
61 istlsolver::printOutput 
  0.5      1.30064e-21      2.43002e-13
61 === rate=5.90498e-26, T=0.0266767, TIT=0.0533535, IT=0.5
61 finished solveLinearSystemImpl_ 1
61 converged value: 1 121 1 let s go get convergedRemote
61 to  comm_.min(converged) 
61 did  comm_.min(converged) convergedRemote: 1
61 final convergedRemote: 1
61 Update: x^(k+1) = x^k - deltax^k 
61 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
61 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
61 Newton iteration  1 done61 , maximum relative shift = 3.976e-03
61 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 9
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 9
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 9
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 9
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 9
3: communicator.hh::sendRecv, left MPI_Wait
3: leave BufferedCommunicator::sendRecv
3: communicator.hh::BufferedCommunicator::forward, left sendRecv
3 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), after communicator.free();
3 schwarz.hh:virtual void apply (X& v, const Y& d), AFTER communication.copyOwnerToAll(v,v); 
3void AMG<M,X,S,PI,A>::mgc(LevelContext& levelContext), finished 
3 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), AFTER mgc(levelContext) 
3 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), finished 
3   AFTER _prec->apply(y,p), before v = A * y 
3  before  alpha = rho_new / < rt, v > 
3  before alpha = rho_new / h; 
3  before apply first correction to x 
3  before r = r - alpha*v 
3  before test stop criteria 
3 istlsolver::printOutput 
  0.5      1.30064e-21      2.43002e-13
3 === rate=5.90498e-26, T=0.0266132, TIT=0.0532264, IT=0.5
3 finished solveLinearSystemImpl_ 1
3 converged value: 1 121 1 let s go get convergedRemote
3 to  comm_.min(converged) 
3 did  comm_.min(converged) convergedRemote: 1
3 final convergedRemote: 1
3 Update: x^(k+1) = x^k - deltax^k 
3 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
3 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
3 Newton iteration  1 done3 , maximum relative shift = 3.976e-03
3 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
52: leave BufferedCommunicator::sendRecv
52: communicator.hh::BufferedCommunicator::forward, left sendRecv
52 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), after communicator.free();
52 schwarz.hh:virtual void apply (X& v, const Y& d), AFTER communication.copyOwnerToAll(v,v); 
52 schwarz.hh:virtual void apply (X& v, const Y& d), before preconditioner.apply(v,d);; 
52 schwarz.hh:virtual void apply (X& v, const Y& d), before communication.copyOwnerToAll(v,v); 
52 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
52: communicator.hh::BufferedCommunicator::forward, enter sendRecv
52: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 26
52: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 26
52: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 26
52: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 26
52: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 26
52: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 26
52: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 26
52: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 26
52: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 26
52: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 26
52: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 10 26
52: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 11 26
52: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 12 26
52: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 13 26
52: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 14 26
52: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 15 26
52: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 16 26
52: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 17 26
52: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 18 26
52: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 19 26
52: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 20 26
52: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 21 26
52: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 22 26
52: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 23 26
52: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 24 26
52: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 25 26
52: communicator.hh::sendRecv, left MPI_Wait
52: leave BufferedCommunicator::sendRecv
52: communicator.hh::BufferedCommunicator::forward, left sendRecv
52 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), after communicator.free();
52 schwarz.hh:virtual void apply (X& v, const Y& d), AFTER communication.cop(messageInformation_.size(), recvRequests, &finished, &status); 2 15
49: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 15
49: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 15
49: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 15
49: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 15
49: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 15
49: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 15
49: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 15
49: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 10 15
49: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 11 15
49: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 12 15
49: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 13 15
49: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 14 15
49: communicator.hh::sendRecv, left MPI_Wait
49: leave BufferedCommunicator::sendRecv
49: communicator.hh::BufferedCommunicator::forward, left sendRecv
49 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), after communicator.free();
49 schwarz.hh:virtual void apply (X& v, const Y& d), AFTER communication.copyOwnerToAll(v,v); 
49void AMG<M,X,S,PI,A>::mgc(LevelContext& levelContext), finished 
49 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), AFTER mgc(levelContext) 
49 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), finished 
49   AFTER _prec->apply(y,p), before v = A * y 
49  before  alpha = rho_new / < rt, v > 
49  before alpha = rho_new / h; 
49  before apply first correction to x 
49  before r = r - alpha*v 
49  before test stop criteria 
49 istlsolver::printOutput 
  0.5      1.30064e-21      2.43002e-13
49 === rate=5.90498e-26, T=0.0266195, TIT=0.053239, IT=0.5
49 finished solveLinearSystemImpl_ 1
49 converged value: 1 121 1 let s go get convergedRemote
49 to  comm_.min(converged) 
49 did  comm_.min(converged) convergedRemote: 1
49 final convergedRemote: 1
49 Update: x^(k+1) = x^k - deltax^k 
49 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
49 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
49 Newton iteration  1 done49 , maximum relative shift = 3.976e-03
49 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 5
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 5
1: communicator.hh::sendRecv, left MPI_Wait
1: leave BufferedCommunicator::sendRecv
1: communicator.hh::BufferedCommunicator::forward, left sendRecv
1 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), after communicator.free();
1 schwarz.hh:virtual void apply (X& v, const Y& d), AFTER communication.copyOwnerToAll(v,v); 
1void AMG<M,X,S,PI,A>::mgc(LevelContext& levelContext), finished 
1 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), AFTER mgc(levelContext) 
1 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), finished 
1   AFTER _prec->apply(y,p), before v = A * y 
1  before  alpha = rho_new / < rt, v > 
1  before alpha = rho_new / h; 
1  before apply first correction to x 
1  before r = r - alpha*v 
1  before test stop criteria 
1 istlsolver::printOutput 
  0.5      1.30064e-21      2.43002e-13
1 === rate=5.90498e-26, T=0.026594, TIT=0.0531881, IT=0.5
1 finished solveLinearSystemImpl_ 1
1 converged value: 1 121 1 let s go get convergedRemote
1 to  comm_.min(converged) 
1 did  comm_.min(converged) convergedRemote: 1
1 final convergedRemote: 1
1 Update: x^(k+1) = x^k - deltax^k 
1 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
1 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
1 Newton iteration  1 done1 , maximum relative shift = 3.976e-03
1 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
ply(y,p), before v = A * y 
0  before  alpha = rho_new / < rt, v > 
0  before alpha = rho_new / h; 
0  before apply first correction to x 
0  before r = r - alpha*v 
0  before test stop criteria 
0 istlsolver::printOutput 
  0.5      1.30064e-21      2.43002e-13
0 === rate=5.90498e-26, T=0.0120008, TIT=0.0240015, IT=0.5
0 finished solveLinearSystemImpl_ 1
0 converged value: 1 121 1 let s go get convergedRemote
0 to  comm_.min(converged) 
0 did  comm_.min(converged) convergedRemote: 1
0 final convergedRemote: 1
0 Update: x^(k+1) = x^k - deltax^k 
0 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
0 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
0 Newton iteration  1 done0 , maximum relative shift = 3.976e-03
0 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
100 schwarz.hh:virtual void apply (X& v, const Y& d), before communication.copyOwnerToAll(v,v); 
100 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
100: communicator.hh::BufferedCommunicator::forward, enter sendRecv
100: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 6
100: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 6
100: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 6
100: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 6
100: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 6
100: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 6
100: communicator.hh::sendRecv, left MPI_Wait
100: leave BufferedCommunicator::sendRecv
100: communicator.hh::BufferedCommunicator::forward, left sendRecv
100 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), after communicator.free();
100 schwarz.hh:virtual void apply (X& v, const Y& d), AFTER communication.copyOwnerToAll(v,v); 
100void AMG<M,X,S,PI,A>::mgc(LevelContext& levelContext), finished 
100 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), AFTER mgc(levelContext) 
100 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), finished 
100   AFTER _prec->apply(y,p), before v = A * y 
100  before  alpha = rho_new / < rt, v > 
100  before alpha = rho_new / h; 
100  before apply first correction to x 
100  before r = r - alpha*v 
100  before test stop criteria 
100 istlsolver::printOutput 
  0.5      1.30064e-21      2.43002e-13
100 === rate=5.90498e-26, T=0.0266347, TIT=0.0532693, IT=0.5
100 finished solveLinearSystemImpl_ 1
100 converged value: 1 121 1 let s go get convergedRemote
100 to  comm_.min(converged) 
100 did  comm_.min(converged) convergedRemote: 1
100 final convergedRemote: 1
100 Update: x^(k+1) = x^k - deltax^k 
100 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
100 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
100 Newton iteration  1 done100 , maximum relative shift = 3.976e-03
100 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
28: leave BufferedCommunicator::sendRecv
28: communicator.hh::BufferedCommunicator::forward, left sendRecv
28 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), after communicator.free();
28 schwarz.hh:virtual void apply (X& v, const Y& d), AFTER communication.copyOwnerToAll(v,v); 
28 schwarz.hh:virtual void apply (X& v, const Y& d), before preconditioner.apply(v,d);; 
28 schwarz.hh:virtual void apply (X& v, const Y& d), before communication.copyOwnerToAll(v,v); 
28 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
28: communicator.hh::BufferedCommunicator::forward, enter sendRecv
28: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 18
28: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 18
28: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 18
28: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 18
28: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 18
28: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 18
28: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 18
28: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 18
28: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 18
28: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 18
28: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 10 18
28: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 11 18
28: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 12 18
28: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 13 18
28: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 14 18
28: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 15 18
28: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 16 18
28: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 17 18
28: communicator.hh::sendRecv, left MPI_Wait
28: leave BufferedCommunicator::sendRecv
28: communicator.hh::BufferedCommunicator::forward, left sendRecv
28 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), after communicator.free();
28 schwarz.hh:virtual void apply (X& v, const Y& d), AFTER communication.copyOwnerToAll(v,v); 
28void AMG<M,X,S,PI,A>::mgc(LevelContext& levelContext), finished 
28 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), AFTER mgc(levelContext) 
28 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), finished 
28   AFTER _prec->apply(y,p), before v = A * y 
28  before  alpha = rho_new / < rt, v > 
28  before alpha = rho_new / h; 
28  before apply first correction to x 
28  before r = r - alpha*v 
28  before test stop criteria 
28 istlsolver::printOutput 
  0.5      1.30064e-21      2.43002e-13
28 === rate=5.90498e-26, T=0.0266409, TIT=0.0532819, IT=0.5
28 finished solveLinearSystemImpl_ 1
28 converged value: 1 121 1 let s go get convergedRemote
28 to  comm_.min(converged) 
28 did  comm_.min(converged) convergedRemote: 1
28 final convergedRemote: 1
28 Update: x^(k+1) = x^k - deltax^k 
28 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
28 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
28 Newton iteratio12: leave BufferedCommunicator::sendRecv
12: communicator.hh::BufferedCommunicator::forward, left sendRecv
12 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), after communicator.free();
12 schwarz.hh:virtual void apply (X& v, const Y& d), AFTER communication.copyOwnerToAll(v,v); 
12 schwarz.hh:virtual void apply (X& v, const Y& d), before preconditioner.apply(v,d);; 
12 schwarz.hh:virtual void apply (X& v, const Y& d), before communication.copyOwnerToAll(v,v); 
12 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
12: communicator.hh::BufferedCommunicator::forward, enter sendRecv
12: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 26
12: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 26
12: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 26
12: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 26
12: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 26
12: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 26
12: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 26
12: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 26
12: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 26
12: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 26
12: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 10 26
12: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 11 26
12: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 12 26
12: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 13 26
12: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 14 26
12: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 15 26
12: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 16 26
12: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 17 26
12: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 18 26
12: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 19 26
12: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 20 26
12: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 21 26
12: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 22 26
12: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 23 26
12: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 24 26
12: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 25 26
12: communicator.hh::sendRecv, left MPI_Wait
12: leave BufferedCommunicator::sendRecv
12: communicator.hh::BufferedCommunicator::forward, left sendRecv
12 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), after communicator.free();
12 schwarz.hh:virtual void apply (X& v, const Y& d), AFTER communication.cop81: leave BufferedCommunicator::sendRecv
81: communicator.hh::BufferedCommunicator::forward, left sendRecv
81 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), after communicator.free();
81 schwarz.hh:virtual void apply (X& v, const Y& d), AFTER communication.copyOwnerToAll(v,v); 
81 schwarz.hh:virtual void apply (X& v, const Y& d), before preconditioner.apply(v,d);; 
81 schwarz.hh:virtual void apply (X& v, const Y& d), before communication.copyOwnerToAll(v,v); 
81 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
81: communicator.hh::BufferedCommunicator::forward, enter sendRecv
81: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 11
81: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 11
81: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 11
81: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 11
81: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 11
81: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 11
81: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 11
81: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 11
81: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 11
81: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 11
81: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 10 11
81: communicator.hh::sendRecv, left MPI_Wait
81: leave BufferedCommunicator::sendRecv
81: communicator.hh::BufferedCommunicator::forward, left sendRecv
81 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), after communicator.free();
81 schwarz.hh:virtual void apply (X& v, const Y& d), AFTER communication.copyOwnerToAll(v,v); 
81void AMG<M,X,S,PI,A>::mgc(LevelContext& levelContext), finished 
81 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), AFTER mgc(levelContext) 
81 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), finished 
81   AFTER _prec->apply(y,p), before v = A * y 
81  before  alpha = rho_new / < rt, v > 
81  before alpha = rho_new / h; 
81  before apply first correction to x 
81  before r = r - alpha*v 
81  before test stop criteria 
81 istlsolver::printOutput 
  0.5      1.30064e-21      2.43002e-13
81 === rate=5.90498e-26, T=0.0266131, TIT=0.0532261, IT=0.5
81 finished solveLinearSystemImpl_ 1
81 converged value: 1 121 1 let s go get convergedRemote
81 to  comm_.min(converged) 
81 did  comm_.min(converged) convergedRemote: 1
81 final convergedRemote: 1
81 Update: x^(k+1) = x^k - deltax^k 
81 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
81 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
81 Newton iteration  1 done81 , maximum relative shift = 3.976e-03
81 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
 28 31
86: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 29 31
86: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 30 31
86: communicator.hh::sendRecv, left MPI_Wait
86: leave BufferedCommunicator::sendRecv
86: communicator.hh::BufferedCommunicator::forward, left sendRecv
86 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), after communicator.free();
86 schwarz.hh:virtual void apply (X& v, const Y& d), AFTER communication.copyOwnerToAll(v,v); 
86 schwarz.hh:virtual void apply (X& v, const Y& d), before preconditioner.apply(v,d);; 
86 schwarz.hh:virtual void apply (X& v, const Y& d), before communication.copyOwnerToAll(v,v); 
86 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
86: communicator.hh::BufferedCommunicator::forward, enter sendRecv
86: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 31
86: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 31
86: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 31
86: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 31
86: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 31
86: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 31
86: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 31
86: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 31
86: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 31
86: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 31
86: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 10 31
86: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 11 31
86: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 12 31
86: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 13 31
86: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 14 31
86: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 15 31
86: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 16 31
86: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 17 31
86: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 18 31
86: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 19 31
86: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 20 31
86: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 21 31
86: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 22 31
86: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 23 31
86: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 24 31
86: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 25 31
86: communicator.hh::sendRecv, b13: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 11
13: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 11
13: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 11
13: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 11
13: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 11
13: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 10 11
13: communicator.hh::sendRecv, left MPI_Wait
13: leave BufferedCommunicator::sendRecv
13: communicator.hh::BufferedCommunicator::forward, left sendRecv
13 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), after communicator.free();
13 schwarz.hh:virtual void apply (X& v, const Y& d), AFTER communication.copyOwnerToAll(v,v); 
13void AMG<M,X,S,PI,A>::mgc(LevelContext& levelContext), finished 
13 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), AFTER mgc(levelContext) 
13 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), finished 
13   AFTER _prec->apply(y,p), before v = A * y 
13  before  alpha = rho_new / < rt, v > 
13  before alpha = rho_new / h; 
13  before apply first correction to x 
13  before r = r - alpha*v 
13  before test stop criteria 
13 istlsolver::printOutput 
  0.5      1.30064e-21      2.43002e-13
13 === rate=5.90498e-26, T=0.0266549, TIT=0.0533097, IT=0.5
13 finished solveLinearSystemImpl_ 1
13 converged value: 1 121 1 let s go get convergedRemote
13 to  comm_.min(converged) 
13 did  comm_.min(converged) convergedRemote: 1
13 final convergedRemote: 1
13 Update: x^(k+1) = x^k - deltax^k 
13 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
13 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
13 Newton iteration  1 done13 , maximum relative shift = 3.976e-03
13 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
60: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 6
60: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 6
60: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 6
60: communicator.hh::sendRecv, left MPI_Wait
60: leave BufferedCommunicator::sendRecv
60: communicator.hh::BufferedCommunicator::forward, left sendRecv
60 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), after communicator.free();
60 schwarz.hh:virtual void apply (X& v, const Y& d), AFTER communication.copyOwnerToAll(v,v); 
60void AMG<M,X,S,PI,A>::mgc(LevelContext& levelContext), finished 
60 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), AFTER mgc(levelContext) 
60 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), finished 
60   AFTER _prec->apply(y,p), before v = A * y 
60  before  alpha = rho_new / < rt, v > 
60  before alpha = rho_new / h; 
60  before apply first correction to x 
60  before r = r - alpha*v 
60  before test stop criteria 
60 istlsolver::printOutput 
  0.5      1.30064e-21      2.43002e-13
60 === rate=5.90498e-26, T=0.0266724, TIT=0.0533448, IT=0.5
60 finished solveLinearSystemImpl_ 1
60 converged value: 1 121 1 let s go get convergedRemote
60 to  comm_.min(converged) 
60 did  comm_.min(converged) convergedRemote: 1
60 final convergedRemote: 1
60 Update: x^(k+1) = x^k - deltax^k 
60 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
60 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
60 Newton iteration  1 done60 , maximum relative shift = 3.976e-03
60 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
did  comm_.min(converged) convergedRemote: 1
41 final convergedRemote: 1
41 Update: x^(k+1) = x^k - deltax^k 
41 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
41 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
41 Newton iteration  1 done41 , maximum relative shift = 3.976e-03
41 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
inal convergedRemote: 1
8 Update: x^(k+1) = x^k - deltax^k 
8 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
8 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
8 Newton iteration  1 done8 , maximum relative shift = 3.976e-03
8 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
rintOutput 
  0.5      1.30064e-21      2.43002e-13
68 === rate=5.90498e-26, T=0.0265663, TIT=0.0531326, IT=0.5
68 finished solveLinearSystemImpl_ 1
68 converged value: 1 121 1 let s go get convergedRemote
68 to  comm_.min(converged) 
68 did  comm_.min(converged) convergedRemote: 1
68 final convergedRemote: 1
68 Update: x^(k+1) = x^k - deltax^k 
68 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
68 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
68 Newton iteration  1 done68 , maximum relative shift = 3.976e-03
68 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
UpdateShift_: to shift_ = comm_.max(shift_) 
87 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
87 Newton iteration  1 done87 , maximum relative shift = 3.976e-03
87 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
UpdateShift_: to shift_ = comm_.max(shift_) 
57 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
57 Newton iteration  1 done57 , maximum relative shift = 3.976e-03
57 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
eroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), after communicator.free();
82 schwarz.hh:virtual void apply (X& v, const Y& d), AFTER communication.copyOwnerToAll(v,v); 
82void AMG<M,X,S,PI,A>::mgc(LevelContext& levelContext), finished 
82 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), AFTER mgc(levelContext) 
82 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), finished 
82   AFTER _prec->apply(y,p), before v = A * y 
82  before  alpha = rho_new / < rt, v > 
82  before alpha = rho_new / h; 
82  before apply first correction to x 
82  before r = r - alpha*v 
82  before test stop criteria 
82 istlsolver::printOutput 
  0.5      1.30064e-21      2.43002e-13
82 === rate=5.90498e-26, T=0.0266212, TIT=0.0532424, IT=0.5
82 finished solveLinearSystemImpl_ 1
82 converged value: 1 121 1 let s go get convergedRemote
82 to  comm_.min(converged) 
82 did  comm_.min(converged) convergedRemote: 1
82 final convergedRemote: 1
82 Update: x^(k+1) = x^k - deltax^k 
82 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
82 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
82 Newton iteration  1 done82 , maximum relative shift = 3.976e-03
82 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
 solveLinearSystemImpl_ 1
88 converged value: 1 121 1 let s go get convergedRemote
88 to  comm_.min(converged) 
88 did  comm_.min(converged) convergedRemote: 1
88 final convergedRemote: 1
88 Update: x^(k+1) = x^k - deltax^k 
88 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
88 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
88 Newton iteration  1 done88 , maximum relative shift = 3.976e-03
88 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
efore MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 26 31
56: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 27 31
56: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 28 31
56: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 29 31
56: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 30 31
56: communicator.hh::sendRecv, left MPI_Wait
56: leave BufferedCommunicator::sendRecv
56: communicator.hh::BufferedCommunicator::forward, left sendRecv
56 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), after communicator.free();
56 schwarz.hh:virtual void apply (X& v, const Y& d), AFTER communication.copyOwnerToAll(v,v); 
56void AMG<M,X,S,PI,A>::mgc(LevelContext& levelContext), finished 
56 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), AFTER mgc(levelContext) 
56 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), finished 
56   AFTER _prec->apply(y,p), before v = A * y 
56  before  alpha = rho_new / < rt, v > 
56  before alpha = rho_new / h; 
56  before apply first correction to x 
56  before r = r - alpha*v 
56  before test stop criteria 
56 istlsolver::printOutput 
  0.5      1.30064e-21      2.43002e-13
56 === rate=5.90498e-26, T=0.0266389, TIT=0.0532777, IT=0.5
56 finished solveLinearSystemImpl_ 1
56 converged value: 1 121 1 let s go get convergedRemote
56 to  comm_.min(converged) 
56 did  comm_.min(converged) convergedRemote: 1
56 final convergedRemote: 1
56 Update: x^(k+1) = x^k - deltax^k 
56 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
56 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
56 Newton iteration  1 done56 , maximum relative shift = 3.976e-03
56 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
yOwnerToAll(v,v); 
26void AMG<M,X,S,PI,A>::mgc(LevelContext& levelContext), finished 
26 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), AFTER mgc(levelContext) 
26 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), finished 
26   AFTER _prec->apply(y,p), before v = A * y 
26  before  alpha = rho_new / < rt, v > 
26  before alpha = rho_new / h; 
26  before apply first correction to x 
26  before r = r - alpha*v 
26  before test stop criteria 
26 istlsolver::printOutput 
  0.5      1.30064e-21      2.43002e-13
26 === rate=5.90498e-26, T=0.0266529, TIT=0.0533058, IT=0.5
26 finished solveLinearSystemImpl_ 1
26 converged value: 1 121 1 let s go get convergedRemote
26 to  comm_.min(converged) 
26 did  comm_.min(converged) convergedRemote: 1
26 final convergedRemote: 1
26 Update: x^(k+1) = x^k - deltax^k 
26 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
26 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
26 Newton iteration  1 done26 , maximum relative shift = 3.976e-03
26 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
shed solveLinearSystemImpl_ 1
94 converged value: 1 121 1 let s go get convergedRemote
94 to  comm_.min(converged) 
94 did  comm_.min(converged) convergedRemote: 1
94 final convergedRemote: 1
94 Update: x^(k+1) = x^k - deltax^k 
94 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
94 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
94 Newton iteration  1 done94 , maximum relative shift = 3.976e-03
94 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
  1 done84 , maximum relative shift = 3.976e-03
84 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
did  comm_.min(converged) convergedRemote: 1
72 final convergedRemote: 1
72 Update: x^(k+1) = x^k - deltax^k 
72 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
72 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
72 Newton iteration  1 done72 , maximum relative shift = 3.976e-03
72 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
n  1 done28 , maximum relative shift = 3.976e-03
28 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
yOwnerToAll(v,v); 
12void AMG<M,X,S,PI,A>::mgc(LevelContext& levelContext), finished 
12 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), AFTER mgc(levelContext) 
12 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), finished 
12   AFTER _prec->apply(y,p), before v = A * y 
12  before  alpha = rho_new / < rt, v > 
12  before alpha = rho_new / h; 
12  before apply first correction to x 
12  before r = r - alpha*v 
12  before test stop criteria 
12 istlsolver::printOutput 
  0.5      1.30064e-21      2.43002e-13
12 === rate=5.90498e-26, T=0.0266686, TIT=0.0533373, IT=0.5
12 finished solveLinearSystemImpl_ 1
12 converged value: 1 121 1 let s go get convergedRemote
12 to  comm_.min(converged) 
12 did  comm_.min(converged) convergedRemote: 1
12 final convergedRemote: 1
12 Update: x^(k+1) = x^k - deltax^k 
12 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
12 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
12 Newton iteration  1 done12 , maximum relative shift = 3.976e-03
12 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
efore MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 26 31
86: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 27 31
86: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 28 31
86: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 29 31
86: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 30 31
86: communicator.hh::sendRecv, left MPI_Wait
86: leave BufferedCommunicator::sendRecv
86: communicator.hh::BufferedCommunicator::forward, left sendRecv
86 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), after communicator.free();
86 schwarz.hh:virtual void apply (X& v, const Y& d), AFTER communication.copyOwnerToAll(v,v); 
86void AMG<M,X,S,PI,A>::mgc(LevelContext& levelContext), finished 
86 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), AFTER mgc(levelContext) 
86 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), finished 
86   AFTER _prec->apply(y,p), before v = A * y 
86  before  alpha = rho_new / < rt, v > 
86  before alpha = rho_new / h; 
86  before apply first correction to x 
86  before r = r - alpha*v 
86  before test stop criteria 
86 istlsolver::printOutput 
  0.5      1.30064e-21      2.43002e-13
86 === rate=5.90498e-26, T=0.0267141, TIT=0.0534283, IT=0.5
86 finished solveLinearSystemImpl_ 1
86 converged value: 1 121 1 let s go get convergedRemote
86 to  comm_.min(converged) 
86 did  comm_.min(converged) convergedRemote: 1
86 final convergedRemote: 1
86 Update: x^(k+1) = x^k - deltax^k 
86 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
86 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
86 Newton iteration  1 done86 , maximum relative shift = 3.976e-03
86 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
yOwnerToAll(v,v); 
52void AMG<M,X,S,PI,A>::mgc(LevelContext& levelContext), finished 
52 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), AFTER mgc(levelContext) 
52 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), finished 
52   AFTER _prec->apply(y,p), before v = A * y 
52  before  alpha = rho_new / < rt, v > 
52  before alpha = rho_new / h; 
52  before apply first correction to x 
52  before r = r - alpha*v 
52  before test stop criteria 
52 istlsolver::printOutput 
  0.5      1.30064e-21      2.43002e-13
52 === rate=5.90498e-26, T=0.0266466, TIT=0.0532933, IT=0.5
52 finished solveLinearSystemImpl_ 1
52 converged value: 1 121 1 let s go get convergedRemote
52 to  comm_.min(converged) 
52 did  comm_.min(converged) convergedRemote: 1
52 final convergedRemote: 1
52 Update: x^(k+1) = x^k - deltax^k 
52 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
52 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
52 Newton iteration  1 done52 , maximum relative shift = 3.976e-03
52 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
 IT=0.5
101 finished solveLinearSystemImpl_ 1
101 converged value: 1 121 1 let s go get convergedRemote
101 to  comm_.min(converged) 
101 did  comm_.min(converged) convergedRemote: 1
101 final convergedRemote: 1
101 Update: x^(k+1) = x^k - deltax^k 
101 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
101 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
101 Newton iteration  1 done101 , maximum relative shift = 3.976e-03
101 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
  1 done98 , maximum relative shift = 3.976e-03
98 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
ve shift = 3.976e-03
58 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
 = grad r
ontext), finished 
109 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), AFTER mgc(levelContext) 
109 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), finished 
109   AFTER _prec->apply(y,p), before v = A * y 
109  before  alpha = rho_new / < rt, v > 
109  before alpha = rho_new / h; 
109  before apply first correction to x 
109  before r = r - alpha*v 
109  before test stop criteria 
109 istlsolver::printOutput 
  0.5      1.30064e-21      2.43002e-13
109 === rate=5.90498e-26, T=0.0266762, TIT=0.0533524, IT=0.5
109 finished solveLinearSystemImpl_ 1
109 converged value: 1 121 1 let s go get convergedRemote
109 to  comm_.min(converged) 
109 did  comm_.min(converged) convergedRemote: 1
109 final convergedRemote: 1
109 Update: x^(k+1) = x^k - deltax^k 
109 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
109 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
109 Newton iteration  1 done109 , maximum relative shift = 3.976e-03
109 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
107 Solve: M deltax^k = r 
107 Newton::solveLinearSystem : entering the try section 1
107 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
107 solveLinearSystemImpl_, before converged = ls.solve 
107 121 amgbackend::solve isParallel 1
102 Solve: M deltax^k = r 
102 Newton::solveLinearSystem : entering the try section 1
102 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
102 solveLinearSystemImpl_, before converged = ls.solve 
102 121 amgbackend::solve isParallel 1
112 Solve: M deltax^k = r 
112 Newton::solveLinearSystem : entering the try section 1
112 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
112 solveLinearSystemImpl_, before converged = ls.solve 
112 121 amgbackend::solve isParallel 1
93 Solve: M deltax^k = r 
93 Newton::solveLinearSystem : entering the try section 1
93 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
93 solveLinearSystemImpl_, before converged = ls.solve 
93 121 amgbackend::solve isParallel 1
110 Solve: M deltax^k = r 
110 Newton::solveLinearSystem : entering the try section 1
110 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
110 solveLinearSystemImpl_, before converged = ls.solve 
110 121 amgbackend::solve isParallel 1
21 Solve: M deltax^k = r 
21 Newton::solveLinearSystem : entering the try section 1
21 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
21 solveLinearSystemImpl_, before converged = ls.solve 
21 121 amgbackend::solve isParallel 1
96 Solve: M deltax^k = r 
96 Newton::solveLinearSystem : entering the try section 1
96 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
96 solveLinearSystemImpl_, before converged = ls.solve 
96 121 amgbackend::solve isParallel 1
73 Solve: M deltax^k = r 
73 Newton::solveLinearSystem : entering the try section 1
73 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
73 solveLinearSystemImpl_, before converged = ls.solve 
73 121 amgbackend::solve isParallel 1
55 Solve: M deltax^k = r 
55 Newton::solveLinearSystem : entering the try section 1
55 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
55 solveLinearSystemImpl_, before converged = ls.solve 
55 121 amgbackend::solve isParallel 1
78 Solve: M deltax^k = r 
78 Newton::solveLinearSystem : entering the try section 1
78 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
78 solveLinearSystemImpl_, before converged = ls.solve 
78 121 amgbackend::solve isParallel 1
60 Solve: M deltax^k = r 
60 Newton::solveLinearSystem : entering the try section 1
60 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
60 solveLinearSystemImpl_, before converged = ls.solve 
60 121 amgbackend::solve isParallel 1
2 Solve: M deltax^k = r 
2 Newton::solveLinearSystem : entering the try section 1
2 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
2 solveLinearSystemImpl_, before converged = ls.solve 
2 121 amgbackend::solve isParallel 1
36 Solve: M deltax^k = r 
36 Newton::solveLinearSystem : entering the try section 1
36 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
36 solveLinearSystemImpl_, before converged = ls.solve 
36 121 amgbackend::solve isParallel 1
40 Solve: M deltax^k = r 
40 Newton::solveLinearSystem : entering the try section 1
40 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
40 solveLinearSystemImpl_, before converged = ls.solve 
40 121 amgbackend::solve isParallel 1
27 Solve: M deltax^k = r 
27 Newton::solveLinearSystem : entering the try section 1
27 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
27 solveLinearSystemImpl_, before converged = ls.solve 
27 121 amgbackend::solve isParallel 1
70 Solve: M deltax^k = r 
70 Newton::solveLinearSystem : entering the try section 1
70 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
70 solveLinearSystemImpl_, before converged = ls.solve 
70 121 amgbackend::solve isParallel 1
6 Solve: M deltax^k = r 
6 Newton::solveLinearSystem : entering the try section 1
6 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
6 solveLinearSystemImpl_, before converged = ls.solve 
6 121 amgbackend::solve isParallel 1
11 Solve: M deltax^k = r 
11 Newton::solveLinearSystem : entering the try section 1
11 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
11 solveLinearSystemImpl_, before converged = ls.solve 
11 121 amgbackend::solve isParallel 1
7 Solve: M deltax^k = r 
7 Newton::solveLinearSystem : entering the try section 1
7 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
7 solveLinearSystemImpl_, before converged = ls.solve 
7 121 amgbackend::solve isParallel 1
99 Solve: M deltax^k = r 
99 Newton::solveLinearSystem : entering the try section 1
99 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
99 solveLinearSystemImpl_, before converged = ls.solve 
99 121 amgbackend::solve isParallel 1
41 Solve: M deltax^k = r 
41 Newton::solveLinearSystem : entering the try section 1
41 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
41 solveLinearSystemImpl_, before converged = ls.solve 
41 121 amgbackend::solve isParallel 1
8 Solve: M deltax^k = r 
8 Newton::solveLinearSystem : entering the try section 1
8 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
8 solveLinearSystemImpl_, before converged = ls.solve 
8 121 amgbackend::solve isParallel 1
68 Solve: M deltax^k = r 
68 Newton::solveLinearSystem : entering the try section 1
68 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
68 solveLinearSystemImpl_, before converged = ls.solve 
68 121 amgbackend::solve isParallel 1
87 Solve: M deltax^k = r 
87 Newton::solveLinearSystem : entering the try section 1
87 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
87 solveLinearSystemImpl_, before converged = ls.solve 
87 121 amgbackend::solve isParallel 1
97 Solve: M deltax^k = r 
97 Newton::solveLinearSystem : entering the try section 1
97 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
97 solveLinearSystemImpl_, before converged = ls.solve 
97 121 amgbackend::solve isParallel 1
57 Solve: M deltax^k = r 
57 Newton::solveLinearSystem : entering the try section 1
57 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
57 solveLinearSystemImpl_, before converged = ls.solve 
57 121 amgbackend::solve isParallel 1
16 Solve: M deltax^k = r 
16 Newton::solveLinearSystem : entering the try section 1
16 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
16 solveLinearSystemImpl_, before converged = ls.solve 
16 121 amgbackend::solve isParallel 1
31 Solve: M deltax^k = r 
31 Newton::solveLinearSystem : entering the try section 1
31 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
31 solveLinearSystemImpl_, before converged = ls.solve 
31 121 amgbackend::solve isParallel 1
37 Solve: M deltax^k = r 
37 Newton::solveLinearSystem : entering the try section 1
37 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
37 solveLinearSystemImpl_, before converged = ls.solve 
37 121 amgbackend::solve isParallel 1
9 Solve: M deltax^k = r 
9 Newton::solveLinearSystem : entering the try section 1
9 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
9 solveLinearSystemImpl_, before converged = ls.solve 
9 121 amgbackend::solve isParallel 1
62 Solve: M deltax^k = r 
62 Newton::solveLinearSystem : entering the try section 1
62 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
62 solveLinearSystemImpl_, before converged = ls.solve 
62 121 amgbackend::solve isParallel 1
48 Solve: M deltax^k = r 
48 Newton::solveLinearSystem : entering the try section 1
48 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
48 solveLinearSystemImpl_, before converged = ls.solve 
48 121 amgbackend::solve isParallel 1
43 Solve: M deltax^k = r 
43 Newton::solveLinearSystem : entering the try section 1
43 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
43 solveLinearSystemImpl_, before converged = ls.solve 
43 121 amgbackend::solve isParallel 1
77 Solve: M deltax^k = r 
77 Newton::solveLinearSystem : entering the try section 1
77 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
77 solveLinearSystemImpl_, before converged = ls.solve 
77 121 amgbackend::solve isParallel 1
65 Solve: M deltax^k = r 
65 Newton::solveLinearSystem : entering the try section 1
65 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
65 solveLinearSystemImpl_, before converged = ls.solve 
65 121 amgbackend::solve isParallel 1
39 Solve: M deltax^k = r 
39 Newton::solveLinearSystem : entering the try section 1
39 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
39 solveLinearSystemImpl_, before converged = ls.solve 
39 121 amgbackend::solve isParallel 1
79 Solve: M deltax^k = r 
79 Newton::solveLinearSystem : entering the try section 1
79 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
79 solveLinearSystemImpl_, before converged = ls.solve 
79 121 amgbackend::solve isParallel 1
80 Solve: M deltax^k = r 
80 Newton::solveLinearSystem : entering the try section 1
80 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
80 solveLinearSystemImpl_, before converged = ls.solve 
80 121 amgbackend::solve isParallel 1
92 Solve: M deltax^k = r 
92 Newton::solveLinearSystem : entering the try section 1
92 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
92 solveLinearSystemImpl_, before converged = ls.solve 
92 121 amgbackend::solve isParallel 1
83 Solve: M deltax^k = r 
83 Newton::solveLinearSystem : entering the try section 1
83 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
83 solveLinearSystemImpl_, before converged = ls.solve 
83 121 amgbackend::solve isParallel 1
82 Solve: M deltax^k = r 
82 Newton::solveLinearSystem : entering the try section 1
82 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
82 solveLinearSystemImpl_, before converged = ls.solve 
82 121 amgbackend::solve isParallel 1
69 Solve: M deltax^k = r 
69 Newton::solveLinearSystem : entering the try section 1
69 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
69 solveLinearSystemImpl_, before converged = ls.solve 
69 121 amgbackend::solve isParallel 1
88 Solve: M deltax^k = r 
88 Newton::solveLinearSystem : entering the try section 1
88 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
88 solveLinearSystemImpl_, before converged = ls.solve 
88 121 amgbackend::solve isParallel 1
19 Solve: M deltax^k = r 
19 Newton::solveLinearSystem : entering the try section 1
19 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
19 solveLinearSystemImpl_, before converged = ls.solve 
19 121 amgbackend::solve isParallel 1
120 Solve: M deltax^k = r 
120 Newton::solveLinearSystem : entering the try section 1
120 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
120 solveLinearSystemImpl_, before converged = ls.solve 
120 121 amgbackend::solve isParallel 1
114 Solve: M deltax^k = r 
114 Newton::solveLinearSystem : entering the try section 1
114 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
114 solveLinearSystemImpl_, before converged = ls.solve 
114 121 amgbackend::solve isParallel 1
56 Solve: M deltax^k = r 
56 Newton::solveLinearSystem : entering the try section 1
56 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
56 solveLinearSystemImpl_, before converged = ls.solve 
56 121 amgbackend::solve isParallel 1
14 Solve: M deltax^k = r 
14 Newton::solveLinearSystem : entering the try section 1
14 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
14 solveLinearSystemImpl_, before converged = ls.solve 
14 121 amgbackend::solve isParallel 1
26 Solve: M deltax^k = r 
26 Newton::solveLinearSystem : entering the try section 1
26 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
26 solveLinearSystemImpl_, before converged = ls.solve 
26 121 amgbackend::solve isParallel 1
103 Solve: M deltax^k = r 
103 Newton::solveLinearSystem : entering the try section 1
103 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
103 solveLinearSystemImpl_, before converged = ls.solve 
103 121 amgbackend::solve isParallel 1
34 Solve: M deltax^k = r 
34 Newton::solveLinearSystem : entering the try section 1
34 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
34 solveLinearSystemImpl_, before converged = ls.solve 
34 121 amgbackend::solve isParallel 1
51 Solve: M deltax^k = r 
51 Newton::solveLinearSystem : entering the try section 1
51 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
51 solveLinearSystemImpl_, before converged = ls.solve 
51 121 amgbackend::solve isParallel 1
95 Solve: M deltax^k = r 
95 Newton::solveLinearSystem : entering the try section 1
95 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
95 solveLinearSystemImpl_, before converged = ls.solve 
95 121 amgbackend::solve isParallel 1
75 Solve: M deltax^k = r 
75 Newton::solveLinearSystem : entering the try section 1
75 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
75 solveLinearSystemImpl_, before converged = ls.solve 
75 121 amgbackend::solve isParallel 1
63 Solve: M deltax^k = r 
63 Newton::solveLinearSystem : entering the try section 1
63 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
63 solveLinearSystemImpl_, before converged = ls.solve 
63 121 amgbackend::solve isParallel 1
76 Solve: M deltax^k = r 
76 Newton::solveLinearSystem : entering the try section 1
76 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
76 solveLinearSystemImpl_, before converged = ls.solve 
76 121 amgbackend::solve isParallel 1
115 Solve: M deltax^k = r 
115 Newton::solveLinearSystem : entering the try section 1
115 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
115 solveLinearSystemImpl_, before converged = ls.solve 
115 121 amgbackend::solve isParallel 1
18 Solve: M deltax^k = r 
18 Newton::solveLinearSystem : entering the try section 1
18 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
18 solveLinearSystemImpl_, before converged = ls.solve 
18 121 amgbackend::solve isParallel 1
53 Solve: M deltax^k = r 
53 Newton::solveLinearSystem : entering the try section 1
53 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
53 solveLinearSystemImpl_, before converged = ls.solve 
53 121 amgbackend::solve isParallel 1
23 Solve: M deltax^k = r 
23 Newton::solveLinearSystem : entering the try section 1
23 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
23 solveLinearSystemImpl_, before converged = ls.solve 
23 121 amgbackend::solve isParallel 1
119 Solve: M deltax^k = r 
119 Newton::solveLinearSystem : entering the try section 1
119 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
119 solveLinearSystemImpl_, before converged = ls.solve 
119 121 amgbackend::solve isParallel 1
50 Solve: M deltax^k = r 
50 Newton::solveLinearSystem : entering the try section 1
50 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
50 solveLinearSystemImpl_, before converged = ls.solve 
50 121 amgbackend::solve isParallel 1
94 Solve: M deltax^k = r 
94 Newton::solveLinearSystem : entering the try section 1
94 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
94 solveLinearSystemImpl_, before converged = ls.solve 
94 121 amgbackend::solve isParallel 1
111 Solve: M deltax^k = r 
111 Newton::solveLinearSystem : entering the try section 1
111 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
111 solveLinearSystemImpl_, before converged = ls.solve 
111 121 amgbackend::solve isParallel 1
84 Solve: M deltax^k = r 
84 Newton::solveLinearSystem : entering the try section 1
84 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
84 solveLinearSystemImpl_, before converged = ls.solve 
84 121 amgbackend::solve isParallel 1
30 Solve: M deltax^k = r 
30 Newton::solveLinearSystem : entering the try section 1
30 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
30 solveLinearSystemImpl_, before converged = ls.solve 
30 121 amgbackend::solve isParallel 1
61 Solve: M deltax^k = r 
61 Newton::solveLinearSystem : entering the try section 1
61 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
61 solveLinearSystemImpl_, before converged = ls.solve 
61 121 amgbackend::solve isParallel 1
3 Solve: M deltax^k = r 
3 Newton::solveLinearSystem : entering the try section 1
3 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
3 solveLinearSystemImpl_, before converged = ls.solve 
3 121 amgbackend::solve isParallel 1
109 Solve: M deltax^k = r 
109 Newton::solveLinearSystem : entering the try section 1
109 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
109 solveLinearSystemImpl_, before converged = ls.solve 
109 121 amgbackend::solve isParallel 1
49 Solve: M deltax^k = r 
49 Newton::solveLinearSystem : entering the try section 1
49 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
49 solveLinearSystemImpl_, before converged = ls.solve 
49 121 amgbackend::solve isParallel 1
1 Solve: M deltax^k = r 
1 Newton::solveLinearSystem : entering the try section 1
1 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
1 solveLinearSystemImpl_, before converged = ls.solve 
1 121 amgbackend::solve isParallel 1
0 Solve: M deltax^k = r 
0 Newton::solveLinearSystem : entering the try section 1
0 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
0 solveLinearSystemImpl_, before converged = ls.solve 
0 121 amgbackend::solve isParallel 1
100 Solve: M deltax^k = r 
100 Newton::solveLinearSystem : entering the try section 1
100 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
100 solveLinearSystemImpl_, before converged = ls.solve 
100 121 amgbackend::solve isParallel 1
72 Solve: M deltax^k = r 
72 Newton::solveLinearSystem : entering the try section 1
72 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
72 solveLinearSystemImpl_, before converged = ls.solve 
72 121 amgbackend::solve isParallel 1
28 Solve: M deltax^k = r 
28 Newton::solveLinearSystem : entering the try section 1
28 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
28 solveLinearSystemImpl_, before converged = ls.solve 
28 121 amgbackend::solve isParallel 1
81 Solve: M deltax^k = r 
81 Newton::solveLinearSystem : entering the try section 1
81 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
81 solveLinearSystemImpl_, before converged = ls.solve 
81 121 amgbackend::solve isParallel 1
12 Solve: M deltax^k = r 
12 Newton::solveLinearSystem : entering the try section 1
12 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
12 solveLinearSystemImpl_, before converged = ls.solve 
12 121 amgbackend::solve isParallel 1
13 Solve: M deltax^k = r 
13 Newton::solveLinearSystem : entering the try section 1
13 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
13 solveLinearSystemImpl_, before converged = ls.solve 
13 121 amgbackend::solve isParallel 1
86 Solve: M deltax^k = r 
86 Newton::solveLinearSystem : entering the try section 1
86 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
86 solveLinearSystemImpl_, before converged = ls.solve 
86 121 amgbackend::solve isParallel 1
29 Solve: M deltax^k = r 
29 Newton::solveLinearSystem : entering the try section 1
29 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
29 solveLinearSystemImpl_, before converged = ls.solve 
29 121 amgbackend::solve isParallel 1
5 Solve: M deltax^k = r 
5 Newton::solveLinearSystem : entering the try section 1
5 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
5 solveLinearSystemImpl_, before converged = ls.solve 
5 121 amgbackend::solve isParallel 1
52 Solve: M deltax^k = r 
52 Newton::solveLinearSystem : entering the try section 1
52 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
52 solveLinearSystemImpl_, before converged = ls.solve 
52 121 amgbackend::solve isParallel 1
42 Solve: M deltax^k = r 
42 Newton::solveLinearSystem : entering the try section 1
42 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
42 solveLinearSystemImpl_, before converged = ls.solve 
42 121 amgbackend::solve isParallel 1
118 Solve: M deltax^k = r 
118 Newton::solveLinearSystem : entering the try section 1
118 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
118 solveLinearSystemImpl_, before converged = ls.solve 
118 121 amgbackend::solve isParallel 1
15 Solve: M deltax^k = r 
15 Newton::solveLinearSystem : entering the try section 1
15 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
15 solveLinearSystemImpl_, before converged = ls.solve 
15 121 amgbackend::solve isParallel 1
10 Solve: M deltax^k = r 
10 Newton::solveLinearSystem : entering the try section 1
10 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
10 solveLinearSystemImpl_, before converged = ls.solve 
10 121 amgbackend::solve isParallel 1
85 Solve: M deltax^k = r 
85 Newton::solveLinearSystem : entering the try section 1
85 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
85 solveLinearSystemImpl_, before converged = ls.solve 
85 121 amgbackend::solve isParallel 1
101 Solve: M deltax^k = r 
101 Newton::solveLinearSystem : entering the try section 1
101 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
101 solveLinearSystemImpl_, before converged = ls.solve 
101 121 amgbackend::solve isParallel 1
47 Solve: M deltax^k = r 
47 Newton::solveLinearSystem : entering the try section 1
47 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
47 solveLinearSystemImpl_, before converged = ls.solve 
47 121 amgbackend::solve isParallel 1
45 Solve: M deltax^k = r 
45 Newton::solveLinearSystem : entering the try section 1
45 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
45 solveLinearSystemImpl_, before converged = ls.solve 
45 121 amgbackend::solve isParallel 1
17 Solve: M deltax^k = r 
17 Newton::solveLinearSystem : entering the try section 1
17 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
17 solveLinearSystemImpl_, before converged = ls.solve 
17 121 amgbackend::solve isParallel 1
116 Solve: M deltax^k = r 
116 Newton::solveLinearSystem : entering the try section 1
116 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
116 solveLinearSystemImpl_, before converged = ls.solve 
116 121 amgbackend::solve isParallel 1
66 Solve: M deltax^k = r 
66 Newton::solveLinearSystem : entering the try section 1
66 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
66 solveLinearSystemImpl_, before converged = ls.solve 
66 121 amgbackend::solve isParallel 1
64 Solve: M deltax^k = r 
64 Newton::solveLinearSystem : entering the try section 1
64 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
64 solveLinearSystemImpl_, before converged = ls.solve 
64 121 amgbackend::solve isParallel 1
89 Solve: M deltax^k = r 
89 Newton::solveLinearSystem : entering the try section 1
89 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
89 solveLinearSystemImpl_, before converged = ls.solve 
89 121 amgbackend::solve isParallel 1
22 Solve: M deltax^k = r 
22 Newton::solveLinearSystem : entering the try section 1
22 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
22 solveLinearSystemImpl_, before converged = ls.solve 
22 121 amgbackend::solve isParallel 1
105 Solve: M deltax^k = r 
105 Newton::solveLinearSystem : entering the try section 1
105 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
105 solveLinearSystemImpl_, before converged = ls.solve 
105 121 amgbackend::solve isParallel 1
98 Solve: M deltax^k = r 
98 Newton::solveLinearSystem : entering the try section 1
98 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
98 solveLinearSystemImpl_, before converged = ls.solve 
98 121 amgbackend::solve isParallel 1
71 Solve: M deltax^k = r 
71 Newton::solveLinearSystem : entering the try section 1
71 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
71 solveLinearSystemImpl_, before converged = ls.solve 
71 121 amgbackend::solve isParallel 1
32 Solve: M deltax^k = r 
32 Newton::solveLinearSystem : entering the try section 1
32 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
32 solveLinearSystemImpl_, before converged = ls.solve 
32 121 amgbackend::solve isParallel 1
33 Solve: M deltax^k = r 
33 Newton::solveLinearSystem : entering the try section 1
33 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
33 solveLinearSystemImpl_, before converged = ls.solve 
33 121 amgbackend::solve isParallel 1
35 Solve: M deltax^k = r 
35 Newton::solveLinearSystem : entering the try section 1
35 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
35 solveLinearSystemImpl_, before converged = ls.solve 
35 121 amgbackend::solve isParallel 1
58 Solve: M deltax^k = r 
58 Newton::solveLinearSystem : entering the try section 1
58 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
58 solveLinearSystemImpl_, before converged = ls.solve 
58 121 amgbackend::solve isParallel 1
20 Solve: M deltax^k = r 
20 Newton::solveLinearSystem : entering the try section 1
20 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
20 solveLinearSystemImpl_, before converged = ls.solve 
20 121 amgbackend::solve isParallel 1
54 Solve: M deltax^k = r 
54 Newton::solveLinearSystem : entering the try section 1
54 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
54 solveLinearSystemImpl_, before converged = ls.solve 
54 121 amgbackend::solve isParallel 1
104 Solve: M deltax^k = r 
104 Newton::solveLinearSystem : entering the try section 1
104 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
104 solveLinearSystemImpl_, before converged = ls.solve 
104 121 amgbackend::solve isParallel 1
117 Solve: M deltax^k = r 
117 Newton::solveLinearSystem : entering the try section 1
117 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
117 solveLinearSystemImpl_, before converged = ls.solve 
117 121 amgbackend::solve isParallel 1
67 Solve: M deltax^k = r 
67 Newton::solveLinearSystem : entering the try section 1
67 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
67 solveLinearSystemImpl_, before converged = ls.solve 
67 121 amgbackend::solve isParallel 1
59 Solve: M deltax^k = r 
59 Newton::solveLinearSystem : entering the try section 1
59 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
59 solveLinearSystemImpl_, before converged = ls.solve 
59 121 amgbackend::solve isParallel 1
90 Solve: M deltax^k = r 
90 Newton::solveLinearSystem : entering the try section 1
90 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
90 solveLinearSystemImpl_, before converged = ls.solve 
90 121 amgbackend::solve isParallel 1
25 Solve: M deltax^k = r 
25 Newton::solveLinearSystem : entering the try section 1
25 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
25 solveLinearSystemImpl_, before converged = ls.solve 
25 121 amgbackend::solve isParallel 1
113 Solve: M deltax^k = r 
113 Newton::solveLinearSystem : entering the try section 1
113 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
113 solveLinearSystemImpl_, before converged = ls.solve 
113 121 amgbackend::solve isParallel 1
4 Solve: M deltax^k = r 
4 Newton::solveLinearSystem : entering the try section 1
4 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
4 solveLinearSystemImpl_, before converged = ls.solve 
4 121 amgbackend::solve isParallel 1
74 Solve: M deltax^k = r 
74 Newton::solveLinearSystem : entering the try section 1
74 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
74 solveLinearSystemImpl_, before converged = ls.solve 
74 121 amgbackend::solve isParallel 1
91 Solve: M deltax^k = r 
91 Newton::solveLinearSystem : entering the try section 1
91 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
91 solveLinearSystemImpl_, before converged = ls.solve 
91 121 amgbackend::solve isParallel 1
38 Solve: M deltax^k = r 
38 Newton::solveLinearSystem : entering the try section 1
38 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
38 solveLinearSystemImpl_, before converged = ls.solve 
38 121 amgbackend::solve isParallel 1
44 Solve: M deltax^k = r 
44 Newton::solveLinearSystem : entering the try section 1
44 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
44 solveLinearSystemImpl_, before converged = ls.solve 
44 121 amgbackend::solve isParallel 1
106 Solve: M deltax^k = r 
106 Newton::solveLinearSystem : entering the try section 1
106 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
106 solveLinearSystemImpl_, before converged = ls.solve 
106 121 amgbackend::solve isParallel 1
108 Solve: M deltax^k = r 
108 Newton::solveLinearSystem : entering the try section 1
108 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
108 solveLinearSystemImpl_, before converged = ls.solve 
108 121 amgbackend::solve isParallel 1
46 Solve: M deltax^k = r 
46 Newton::solveLinearSystem : entering the try section 1
46 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
46 solveLinearSystemImpl_, before converged = ls.solve 
46 121 amgbackend::solve isParallel 1
24 Solve: M deltax^k = r 
24 Newton::solveLinearSystem : entering the try section 1
24 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
24 solveLinearSystemImpl_, before converged = ls.solve 
24 121 amgbackend::solve isParallel 1
95 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
95: communicator.hh::BufferedCommunicator::forward, enter sendRecv
95: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 7
111 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
111: communicator.hh::BufferedCommunicator::forward, enter sendRecv
111: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 11
0 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
0: communicator.hh::BufferedCommunicator::forward, enter sendRecv
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 3
17 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
17: communicator.hh::BufferedCommunicator::forward, enter sendRecv
17: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 3
116 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
116: communicator.hh::BufferedCommunicator::forward, enter sendRecv
116: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 7
90 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
90: communicator.hh::BufferedCommunicator::forward, enter sendRecv
90: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 6
3 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
3: communicator.hh::BufferedCommunicator::forward, enter sendRecv
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 9
107 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
107: communicator.hh::BufferedCommunicator::forward, enter sendRecv
107: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 5
77 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
77: communicator.hh::BufferedCommunicator::forward, enter sendRecv
77: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 6
120 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
120: communicator.hh::BufferedCommunicator::forward, enter sendRecv
120: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 7
120: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 7
120: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 7
120: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 7
120: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 7
120: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 7
120: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 7
120: communicator.hh::sendRecv, left MPI_Wait
114 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
114: communicator.hh::BufferedCommunicator::forward, enter sendRecv
114: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 13
114: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 13
114: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 13
114: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 13
114: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 13
114: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 13
114: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 13
114: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 13
114: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 13
114: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 13
114: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 10 13
114: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 11 13
114: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 12 13
56 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
56: communicator.hh::BufferedCommunicator::forward, enter sendRecv
56: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 31
56: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 31
56: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 31
56: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 31
56: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 31
56: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 31
56: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 31
56: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 31
56: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 31
56: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 31
56: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 10 31
56: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 11 31
56: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 12 31
56: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 13 31
56: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 14 31
56: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 15 31
56: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 16 31
56: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 17 31
56: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 18 31
56: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 19 31
56: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 20 31
56: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 21 31
56: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 22 31
14 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
14: communicator.hh::BufferedCommunicator::forward, enter sendRecv
14: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 17
14: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 17
14: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 17
14: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 17
14: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 17
14: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 17
14: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 17
14: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 17
14: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 17
14: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 17
14: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 10 17
14: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 11 17
26 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
26: communicator.hh::BufferedCommunicator::forward, enter sendRecv
26: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 26
26: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 26
26: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 26
26: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 26
26: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 26
26: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 26
26: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 26
26: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 26
26: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 26
26: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 26
26: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 10 26
26: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 11 26
26: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 12 26
26: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 13 26
26: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 14 26
26: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 15 26
26: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 16 26
26: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 17 26
26: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 18 26
26: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 19 26
26: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 20 26
26: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 21 26
26: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 22 26
103 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
103: communicator.hh::BufferedCommunicator::forward, enter sendRecv
103: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 16
103: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 16
103: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 16
103: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 16
103: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 16
103: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 16
103: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 16
103: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 16
103: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 16
103: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 16
103: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 10 16
103: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 11 16
103: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 12 16
103: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 13 16
34 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
34: communicator.hh::BufferedCommunicator::forward, enter sendRecv
34: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 14
34: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 14
34: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 14
34: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 14
34: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 14
34: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 14
34: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 14
34: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 14
34: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 14
34: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 14
51 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
51: communicator.hh::BufferedCommunicator::forward, enter sendRecv
51: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 10
51: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 10
51: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 10
51: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 10
51: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 10
51: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 10
51: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 10
51: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 10
51: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 10
51: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 10
24 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
24: communicator.hh::BufferedCommunicator::forward, enter sendRecv
24: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 14
24: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 14
24: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 14
24: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 14
24: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 14
24: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 14
24: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 14
24: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 14
24: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 14
24: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 14
24: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 10 14
75 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
75: communicator.hh::BufferedCommunicator::forward, enter sendRecv
75: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 17
75: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 17
75: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 17
75: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 17
75: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 17
75: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 17
75: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 17
75: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 17
75: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 17
75: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 17
75: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 10 17
75: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 11 17
75: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 12 17
75: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 13 17
75: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 14 17
75: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 15 17
63 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
63: communicator.hh::BufferedCommunicator::forward, enter sendRecv
63: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 9
63: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 9
63: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 9
63: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 9
63: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 9
63: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 9
76 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
76: communicator.hh::BufferedCommunicator::forward, enter sendRecv
76: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 12
76: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 12
76: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 12
76: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 12
76: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 12
76: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 12
76: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 12
76: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 12
76: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 12
115 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
115: communicator.hh::BufferedCommunicator::forward, enter sendRecv
115: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 11
115: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 11
115: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 11
115: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 11
115: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 11
115: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 11
115: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 11
115: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 11
115: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 11
115: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 11
115: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 10 11
18 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
18: communicator.hh::BufferedCommunicator::forward, enter sendRecv
18: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 14
18: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 14
18: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 14
18: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 14
18: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 14
18: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 14
18: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 14
18: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 14
18: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 14
53 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
53: communicator.hh::BufferedCommunicator::forward, enter sendRecv
53: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 17
53: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 17
53: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 17
53: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 17
53: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 17
53: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 17
53: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 17
53: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 17
53: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 17
53: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 17
53: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 10 17
53: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 11 17
53: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 12 17
23 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
23: communicator.hh::BufferedCommunicator::forward, enter sendRecv
23: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 11
23: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 11
23: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 11
23: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 11
23: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 11
23: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 11
23: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 11
23: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 11
23: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 11
23: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 11
/var/spool/slurmd/job13991/slurm_script: line 19: 3565280 Killed                  DUMUX_NUM_THREADS=121 mpirun -n 121 python3 testError3d.py
