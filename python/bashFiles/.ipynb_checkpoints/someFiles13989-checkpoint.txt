1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
1  before v = A * y 
0 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
0  before v = A * y 
0  before  alpha = rho_new / < rt, v > 
0  before alpha = rho_new / h; 
0  before apply first correction to x 
0  before r = r - alpha*v 
0  before test stop criteria 
0 istlsolver::printOutput 
  0.5      1.37931e-32      1.26644e-16
0 === rate=1.60386e-32, T=0.0305093, TIT=0.0610186, IT=0.5
1  before  alpha = rho_new / < rt, v > 
1  before alpha = rho_new / h; 
1  before apply first correction to x 
1  before r = r - alpha*v 
1  before test stop criteria 
1 istlsolver::printOutput 
  0.5      1.37931e-32      1.26644e-16
1 === rate=1.60386e-32, T=0.230917, TIT=0.461835, IT=0.5
1 finished solveLinearSystemImpl_ 1
1 converged value: 1 7 1 let s go get convergedRemote
1 to  comm_.min(converged) 
6  before alpha = rho_new / h; 
6  before apply first correction to x 
6  before r = r - alpha*v 
6  before test stop criteria 
6 istlsolver::printOutput 
  0.5      1.37931e-32      1.26644e-16
6 === rate=1.60386e-32, T=0.230889, TIT=0.461778, IT=0.5
6 finished solveLinearSystemImpl_ 1
6 converged value: 1 7 1 let s go get convergedRemote
6 to  comm_.min(converged) 
4  before alpha = rho_new / h; 
4  before apply first correction to x 
4  before r = r - alpha*v 
4  before test stop criteria 
4 istlsolver::printOutput 
  0.5      1.37931e-32      1.26644e-16
4 === rate=1.60386e-32, T=0.23089, TIT=0.461779, IT=0.5
4 finished solveLinearSystemImpl_ 1
4 converged value: 1 7 1 let s go get convergedRemote
4 to  comm_.min(converged) 
5  before alpha = rho_new / h; 
5  before apply first correction to x 
5  before r = r - alpha*v 
5  before test stop criteria 
5 istlsolver::printOutput 
  0.5      1.37931e-32      1.26644e-16
5 === rate=1.60386e-32, T=0.230917, TIT=0.461835, IT=0.5
5 finished solveLinearSystemImpl_ 1
5 converged value: 1 7 1 let s go get convergedRemote
5 to  comm_.min(converged) 
2  before alpha = rho_new / h; 
2  before apply first correction to x 
2  before r = r - alpha*v 
2  before test stop criteria 
2 istlsolver::printOutput 
  0.5      1.37931e-32      1.26644e-16
2 === rate=1.60386e-32, T=0.230882, TIT=0.461765, IT=0.5
2 finished solveLinearSystemImpl_ 1
2 converged value: 1 7 1 let s go get convergedRemote
2 to  comm_.min(converged) 
3  before alpha = rho_new / h; 
3  before apply first correction to x 
3  before r = r - alpha*v 
3  before test stop criteria 
3 istlsolver::printOutput 
  0.5      1.37931e-32      1.26644e-16
3 === rate=1.60386e-32, T=0.230919, TIT=0.461839, IT=0.5
3 finished solveLinearSystemImpl_ 1
3 converged value: 1 7 1 let s go get convergedRemote
3 to  comm_.min(converged) 
0 finished solveLinearSystemImpl_ 1
0 converged value: 1 7 1 let s go get convergedRemote
0 to  comm_.min(converged) 
0 did  comm_.min(converged) convergedRemote: 1
0 final convergedRemote: 1
0 Update: x^(k+1) = x^k - deltax^k 
0 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
1 did  comm_.min(converged) convergedRemote: 1
1 final convergedRemote: 1
1 Update: x^(k+1) = x^k - deltax^k 
1 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
1 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
1 Newton iteration  4 done1 , maximum relative shift = 1.407e-09
6 did  comm_.min(converged) convergedRemote: 1
6 final convergedRemote: 1
6 Update: x^(k+1) = x^k - deltax^k 
6 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
6 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
6 Newton iteration  4 done6 , maximum relative shift = 1.407e-09
4 did  comm_.min(converged) convergedRemote: 1
4 final convergedRemote: 1
4 Update: x^(k+1) = x^k - deltax^k 
4 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
4 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
4 Newton iteration  4 done4 , maximum relative shift = 1.407e-09
5 did  comm_.min(converged) convergedRemote: 1
5 final convergedRemote: 1
5 Update: x^(k+1) = x^k - deltax^k 
5 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
5 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
5 Newton iteration  4 done5 , maximum relative shift = 1.407e-09
2 did  comm_.min(converged) convergedRemote: 1
2 final convergedRemote: 1
2 Update: x^(k+1) = x^k - deltax^k 
2 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
2 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
2 Newton iteration  4 done2 , maximum relative shift = 1.407e-09
3 did  comm_.min(converged) convergedRemote: 1
3 final convergedRemote: 1
3 Update: x^(k+1) = x^k - deltax^k 
3 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
3 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
3 Newton iteration  4 done3 , maximum relative shift = 1.407e-09
1 Assemble/solve/update time: 0.298582(24.0243%)/0.943586(75.9222%)/0.000665258(0.0535275%)
1 gridVariables->advanceTimeStep
1 before, nonLinearSolver->suggestTimeStepSize, current time: 173.703 simTime 10800
1 before assembler->setPreviousSolution, ddt: 30
1 nonLinearSolver->solve
1 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
6 Assemble/solve/update time: 0.298267(23.9988%)/0.944081(75.9612%)/0.000497387(0.04002%)
6 gridVariables->advanceTimeStep
6 before, nonLinearSolver->suggestTimeStepSize, current time: 173.703 simTime 10800
6 before assembler->setPreviousSolution, ddt: 30
6 nonLinearSolver->solve
6 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
4 Assemble/solve/update time: 0.298416(24.0082%)/0.944061(75.9518%)/0.000496877(0.0399749%)
4 gridVariables->advanceTimeStep
4 before, nonLinearSolver->suggestTimeStepSize, current time: 173.703 simTime 10800
4 before assembler->setPreviousSolution, ddt: 30
4 nonLinearSolver->solve
4 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
5 Assemble/solve/update time: 0.298592(24.025%)/0.943584(75.9216%)/0.000663759(0.0534067%)
5 gridVariables->advanceTimeStep
5 before, nonLinearSolver->suggestTimeStepSize, current time: 173.703 simTime 10800
5 before assembler->setPreviousSolution, ddt: 30
5 nonLinearSolver->solve
5 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
3 Assemble/solve/update time: 0.298677(24.03%)/0.943597(75.9167%)/0.000662559(0.0533059%)
3 gridVariables->advanceTimeStep
3 before, nonLinearSolver->suggestTimeStepSize, current time: 173.703 simTime 10800
3 before assembler->setPreviousSolution, ddt: 30
3 nonLinearSolver->solve
3 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
2 Assemble/solve/update time: 0.298389(24.0063%)/0.944081(75.9542%)/0.000491276(0.0395246%)
2 gridVariables->advanceTimeStep
2 before, nonLinearSolver->suggestTimeStepSize, current time: 173.703 simTime 10800
2 before assembler->setPreviousSolution, ddt: 30
2 nonLinearSolver->solve
2 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
0 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
0 Newton iteration  4 done0 , maximum relative shift = 1.407e-09
0 Assemble/solve/update time: 0.297818(23.9716%)/0.944067(75.9887%)/0.000492247(0.0396214%)
0 gridVariables->advanceTimeStep
0 before, nonLinearSolver->suggestTimeStepSize, current time: 173.703 simTime 10800
0 before assembler->setPreviousSolution, ddt: 30
0 nonLinearSolver->solve
0 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
6 Solve: M deltax^k = r 
6 Newton::solveLinearSystem : entering the try section 0
6to  comm_.sum(norm2) 
4 Solve: M deltax^k = r 
4 Newton::solveLinearSystem : entering the try section 0
4to  comm_.sum(norm2) 
0 Solve: M deltax^k = r 
0 Newton::solveLinearSystem : entering the try section 0
0to  comm_.sum(norm2) 
3 Solve: M deltax^k = r 
3 Newton::solveLinearSystem : entering the try section 0
3to  comm_.sum(norm2) 
2 Solve: M deltax^k = r 
2 Newton::solveLinearSystem : entering the try section 0
2to  comm_.sum(norm2) 
5 Solve: M deltax^k = r 
5 Newton::solveLinearSystem : entering the try section 0
5to  comm_.sum(norm2) 
1 Solve: M deltax^k = r 
1 Newton::solveLinearSystem : entering the try section 0
1to  comm_.sum(norm2) 
1did  comm_.sum(norm2) 
1 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
1 solveLinearSystemImpl_, before converged = ls.solve 
1 7 amgbackend::solve isParallel 1
0did  comm_.sum(norm2) 
0 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
0 solveLinearSystemImpl_, before converged = ls.solve 
0 7 amgbackend::solve isParallel 1
3did  comm_.sum(norm2) 
3 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
3 solveLinearSystemImpl_, before converged = ls.solve 
3 7 amgbackend::solve isParallel 1
2did  comm_.sum(norm2) 
2 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
2 solveLinearSystemImpl_, before converged = ls.solve 
2 7 amgbackend::solve isParallel 1
5did  comm_.sum(norm2) 
5 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
5 solveLinearSystemImpl_, before converged = ls.solve 
5 7 amgbackend::solve isParallel 1
4did  comm_.sum(norm2) 
4 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
4 solveLinearSystemImpl_, before converged = ls.solve 
4 7 amgbackend::solve isParallel 1
6did  comm_.sum(norm2) 
6 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
6 solveLinearSystemImpl_, before converged = ls.solve 
6 7 amgbackend::solve isParallel 1
4 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
4 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
6 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
6 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
1 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
1 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
0 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
0 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 7
3 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
3 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
2 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
2 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
5 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
5 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 7
3 7 before Dune::BiCGSTABSolver 
3 Dune::BiCGSTABSolver.apply(x, b, result_) 
3 BiCGSTABSolver::apply 
3 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
2 7 before Dune::BiCGSTABSolver 
2 Dune::BiCGSTABSolver.apply(x, b, result_) 
2 BiCGSTABSolver::apply 
2 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
4 7 before Dune::BiCGSTABSolver 
4 Dune::BiCGSTABSolver.apply(x, b, result_) 
4 BiCGSTABSolver::apply 
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 7
6 7 before Dune::BiCGSTABSolver 
6 Dune::BiCGSTABSolver.apply(x, b, result_) 
6 BiCGSTABSolver::apply 
6 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
1 7 before Dune::BiCGSTABSolver 
1 Dune::BiCGSTABSolver.apply(x, b, result_) 
1 BiCGSTABSolver::apply 
1 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
5 7 before Dune::BiCGSTABSolver 
5 Dune::BiCGSTABSolver.apply(x, b, result_) 
5 BiCGSTABSolver::apply 
5 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
4 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
Using a direct coarse solver (SuperLU)
Building hierarchy of 1 levels (inclusive coarse solver) took 0.20173 seconds.
0 7 before Dune::BiCGSTABSolver 
0 Dune::BiCGSTABSolver.apply(x, b, result_) 
0 BiCGSTABSolver::apply 
0 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
4 === BiCGSTABSolver
 Iter          Defect            Rate
    0      4.48696e-09
4  (it = 0.5; it < _maxit; it+=.5) 0.5 250
4  before p = r 
4  before  y = W^-1 * p 
4  before _prec->apply(y,p); 0.5 250
4 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
6 === BiCGSTABSolver
 Iter          Defect            Rate
    0      4.48696e-09
6  (it = 0.5; it < _maxit; it+=.5) 0.5 250
6  before p = r 
6  before  y = W^-1 * p 
6  before _prec->apply(y,p); 0.5 250
6 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
1 === BiCGSTABSolver
 Iter          Defect            Rate
    0      4.48696e-09
1  (it = 0.5; it < _maxit; it+=.5) 0.5 250
1  before p = r 
1  before  y = W^-1 * p 
1  before _prec->apply(y,p); 0.5 250
1 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
5 === BiCGSTABSolver
 Iter          Defect            Rate
    0      4.48696e-09
5  (it = 0.5; it < _maxit; it+=.5) 0.5 250
5  before p = r 
5  before  y = W^-1 * p 
5  before _prec->apply(y,p); 0.5 250
5 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
3 === BiCGSTABSolver
 Iter          Defect            Rate
    0      4.48696e-09
3  (it = 0.5; it < _maxit; it+=.5) 0.5 250
3  before p = r 
3  before  y = W^-1 * p 
3  before _prec->apply(y,p); 0.5 250
3 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
2 === BiCGSTABSolver
 Iter          Defect            Rate
    0      4.48696e-09
2  (it = 0.5; it < _maxit; it+=.5) 0.5 250
2  before p = r 
2  before  y = W^-1 * p 
2  before _prec->apply(y,p); 0.5 250
2 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
0 === BiCGSTABSolver
 Iter          Defect            Rate
    0      4.48696e-09
0  (it = 0.5; it < _maxit; it+=.5) 0.5 250
0  before p = r 
0  before  y = W^-1 * p 
0  before _prec->apply(y,p); 0.5 250
0 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 7
0 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
3 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
2 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
2 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
4 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
4 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
6 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
6 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
1 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
5 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
5 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
5  before v = A * y 
0 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
0 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
1 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
3 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
3  before v = A * y 
3  before  alpha = rho_new / < rt, v > 
3  before alpha = rho_new / h; 
3  before apply first correction to x 
3  before r = r - alpha*v 
3  before test stop criteria 
3 istlsolver::printOutput 
  0.5      6.78157e-25       1.5114e-16
3 === rate=2.28432e-32, T=0.225198, TIT=0.450396, IT=0.5
3 finished solveLinearSystemImpl_ 1
3 converged value: 1 7 1 let s go get convergedRemote
3 to  comm_.min(converged) 
5  before  alpha = rho_new / < rt, v > 
5  before alpha = rho_new / h; 
5  before apply first correction to x 
5  before r = r - alpha*v 
5  before test stop criteria 
5 istlsolver::printOutput 
  0.5      6.78157e-25       1.5114e-16
5 === rate=2.28432e-32, T=0.225196, TIT=0.450392, IT=0.5
5 finished solveLinearSystemImpl_ 1
5 converged value: 1 7 1 let s go get convergedRemote
5 to  comm_.min(converged) 
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
0  before v = A * y 
0  before  alpha = rho_new / < rt, v > 
0  before alpha = rho_new / h; 
0  before apply first correction to x 
0  before r = r - alpha*v 
0  before test stop criteria 
0 istlsolver::printOutput 
  0.5      6.78157e-25       1.5114e-16
0 === rate=2.28432e-32, T=0.0299805, TIT=0.0599611, IT=0.5
2  before v = A * y 
2  before  alpha = rho_new / < rt, v > 
2  before alpha = rho_new / h; 
2  before apply first correction to x 
2  before r = r - alpha*v 
2  before test stop criteria 
2 istlsolver::printOutput 
  0.5      6.78157e-25       1.5114e-16
2 === rate=2.28432e-32, T=0.225083, TIT=0.450166, IT=0.5
2 finished solveLinearSystemImpl_ 1
2 converged value: 1 7 1 let s go get convergedRemote
2 to  comm_.min(converged) 
4  before v = A * y 
4  before  alpha = rho_new / < rt, v > 
4  before alpha = rho_new / h; 
4  before apply first correction to x 
4  before r = r - alpha*v 
4  before test stop criteria 
4 istlsolver::printOutput 
  0.5      6.78157e-25       1.5114e-16
4 === rate=2.28432e-32, T=0.22497, TIT=0.44994, IT=0.5
4 finished solveLinearSystemImpl_ 1
4 converged value: 1 7 1 let s go get convergedRemote
4 to  comm_.min(converged) 
6  before v = A * y 
6  before  alpha = rho_new / < rt, v > 
6  before alpha = rho_new / h; 
6  before apply first correction to x 
6  before r = r - alpha*v 
6  before test stop criteria 
6 istlsolver::printOutput 
  0.5      6.78157e-25       1.5114e-16
6 === rate=2.28432e-32, T=0.225085, TIT=0.450171, IT=0.5
6 finished solveLinearSystemImpl_ 1
6 converged value: 1 7 1 let s go get convergedRemote
6 to  comm_.min(converged) 
1  before v = A * y 
1  before  alpha = rho_new / < rt, v > 
1  before alpha = rho_new / h; 
1  before apply first correction to x 
1  before r = r - alpha*v 
1  before test stop criteria 
1 istlsolver::printOutput 
  0.5      6.78157e-25       1.5114e-16
1 === rate=2.28432e-32, T=0.225198, TIT=0.450396, IT=0.5
1 finished solveLinearSystemImpl_ 1
1 converged value: 1 7 1 let s go get convergedRemote
1 to  comm_.min(converged) 
3 did  comm_.min(converged) convergedRemote: 1
3 final convergedRemote: 1
3 Update: x^(k+1) = x^k - deltax^k 
3 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
3 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
3 Newton iteration  1 done3 , maximum relative shift = 1.978e-02
3 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
5 did  comm_.min(converged) convergedRemote: 1
5 final convergedRemote: 1
5 Update: x^(k+1) = x^k - deltax^k 
5 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
5 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
5 Newton iteration  1 done5 , maximum relative shift = 1.978e-02
5 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
0 finished solveLinearSystemImpl_ 1
0 converged value: 1 7 1 let s go get convergedRemote
0 to  comm_.min(converged) 
0 did  comm_.min(converged) convergedRemote: 1
0 final convergedRemote: 1
0 Update: x^(k+1) = x^k - deltax^k 
0 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
0 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
0 Newton iteration  1 done0 , maximum relative shift = 1.978e-02
0 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
2 did  comm_.min(converged) convergedRemote: 1
2 final convergedRemote: 1
2 Update: x^(k+1) = x^k - deltax^k 
2 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
2 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
2 Newton iteration  1 done2 , maximum relative shift = 1.978e-02
2 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
4 did  comm_.min(converged) convergedRemote: 1
4 final convergedRemote: 1
4 Update: x^(k+1) = x^k - deltax^k 
4 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
4 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
4 Newton iteration  1 done4 , maximum relative shift = 1.978e-02
4 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
6 did  comm_.min(converged) convergedRemote: 1
6 final convergedRemote: 1
6 Update: x^(k+1) = x^k - deltax^k 
6 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
6 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
6 Newton iteration  1 done6 , maximum relative shift = 1.978e-02
6 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
1 did  comm_.min(converged) convergedRemote: 1
1 final convergedRemote: 1
1 Update: x^(k+1) = x^k - deltax^k 
1 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
1 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
1 Newton iteration  1 done1 , maximum relative shift = 1.978e-02
1 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
6 Solve: M deltax^k = r 
6 Newton::solveLinearSystem : entering the try section 1
6 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
6 solveLinearSystemImpl_, before converged = ls.solve 
6 7 amgbackend::solve isParallel 1
3 Solve: M deltax^k = r 
3 Newton::solveLinearSystem : entering the try section 1
3 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
3 solveLinearSystemImpl_, before converged = ls.solve 
3 7 amgbackend::solve isParallel 1
5 Solve: M deltax^k = r 
5 Newton::solveLinearSystem : entering the try section 1
5 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
5 solveLinearSystemImpl_, before converged = ls.solve 
5 7 amgbackend::solve isParallel 1
0 Solve: M deltax^k = r 
0 Newton::solveLinearSystem : entering the try section 1
0 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
0 solveLinearSystemImpl_, before converged = ls.solve 
0 7 amgbackend::solve isParallel 1
2 Solve: M deltax^k = r 
2 Newton::solveLinearSystem : entering the try section 1
2 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
2 solveLinearSystemImpl_, before converged = ls.solve 
2 7 amgbackend::solve isParallel 1
4 Solve: M deltax^k = r 
4 Newton::solveLinearSystem : entering the try section 1
4 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
4 solveLinearSystemImpl_, before converged = ls.solve 
4 7 amgbackend::solve isParallel 1
1 Solve: M deltax^k = r 
1 Newton::solveLinearSystem : entering the try section 1
1 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
1 solveLinearSystemImpl_, before converged = ls.solve 
1 7 amgbackend::solve isParallel 1
0 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
0 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 7
2 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
2 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
1 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
1 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
4 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
4 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
6 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
6 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
3 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
3 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
5 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
5 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 7
2 7 before Dune::BiCGSTABSolver 
2 Dune::BiCGSTABSolver.apply(x, b, result_) 
2 BiCGSTABSolver::apply 
2 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
5 7 before Dune::BiCGSTABSolver 
5 Dune::BiCGSTABSolver.apply(x, b, result_) 
5 BiCGSTABSolver::apply 
5 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 7
1 7 before Dune::BiCGSTABSolver 
1 Dune::BiCGSTABSolver.apply(x, b, result_) 
1 BiCGSTABSolver::apply 
1 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
4 7 before Dune::BiCGSTABSolver 
4 Dune::BiCGSTABSolver.apply(x, b, result_) 
4 BiCGSTABSolver::apply 
4 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
6 7 before Dune::BiCGSTABSolver 
6 Dune::BiCGSTABSolver.apply(x, b, result_) 
6 BiCGSTABSolver::apply 
6 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
3 7 before Dune::BiCGSTABSolver 
3 Dune::BiCGSTABSolver.apply(x, b, result_) 
3 BiCGSTABSolver::apply 
3 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
Using a direct coarse solver (SuperLU)
Building hierarchy of 1 levels (inclusive coarse solver) took 0.201114 seconds.
0 7 before Dune::BiCGSTABSolver 
0 Dune::BiCGSTABSolver.apply(x, b, result_) 
0 BiCGSTABSolver::apply 
2 === BiCGSTABSolver
 Iter          Defect            Rate
    0      1.23652e-11
2  (it = 0.5; it < _maxit; it+=.5) 0.5 250
2  before p = r 
2  before  y = W^-1 * p 
2  before _prec->apply(y,p); 0.5 250
2 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
0 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
0 === BiCGSTABSolver
 Iter          Defect            Rate
    0      1.23652e-11
0  (it = 0.5; it < _maxit; it+=.5) 0.5 250
0  before p = r 
0  before  y = W^-1 * p 
0  before _prec->apply(y,p); 0.5 250
0 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 7
1 === BiCGSTABSolver
 Iter          Defect            Rate
    0      1.23652e-11
1  (it = 0.5; it < _maxit; it+=.5) 0.5 250
1  before p = r 
1  before  y = W^-1 * p 
1  before _prec->apply(y,p); 0.5 250
1 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
3 === BiCGSTABSolver
 Iter          Defect            Rate
    0      1.23652e-11
3  (it = 0.5; it < _maxit; it+=.5) 0.5 250
3  before p = r 
3  before  y = W^-1 * p 
3  before _prec->apply(y,p); 0.5 250
3 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
4 === BiCGSTABSolver
 Iter          Defect            Rate
    0      1.23652e-11
4  (it = 0.5; it < _maxit; it+=.5) 0.5 250
4  before p = r 
4  before  y = W^-1 * p 
4  before _prec->apply(y,p); 0.5 250
4 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
6 === BiCGSTABSolver
 Iter          Defect            Rate
    0      1.23652e-11
6  (it = 0.5; it < _maxit; it+=.5) 0.5 250
6  before p = r 
6  before  y = W^-1 * p 
6  before _prec->apply(y,p); 0.5 250
6 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
5 === BiCGSTABSolver
 Iter          Defect            Rate
    0      1.23652e-11
5  (it = 0.5; it < _maxit; it+=.5) 0.5 250
5  before p = r 
5  before  y = W^-1 * p 
5  before _prec->apply(y,p); 0.5 250
5 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 7
0 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
2 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
1 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
3 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
5 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
6 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
4 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
3 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
5 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
6 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
0 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
0 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
0  before v = A * y 
2 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
2  before v = A * y 
1 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
3  before v = A * y 
3  before  alpha = rho_new / < rt, v > 
3  before alpha = rho_new / h; 
3  before apply first correction to x 
3  before r = r - alpha*v 
3  before test stop criteria 
3 istlsolver::printOutput 
  0.5        2.268e-28      1.83417e-17
3 === rate=3.36419e-34, T=0.230794, TIT=0.461589, IT=0.5
3 finished solveLinearSystemImpl_ 1
3 converged value: 1 7 1 let s go get convergedRemote
3 to  comm_.min(converged) 
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
5  before v = A * y 
5  before  alpha = rho_new / < rt, v > 
5  before alpha = rho_new / h; 
5  before apply first correction to x 
5  before r = r - alpha*v 
5  before test stop criteria 
5 istlsolver::printOutput 
  0.5        2.268e-28      1.83417e-17
5 === rate=3.36419e-34, T=0.230791, TIT=0.461583, IT=0.5
5 finished solveLinearSystemImpl_ 1
5 converged value: 1 7 1 let s go get convergedRemote
5 to  comm_.min(converged) 
1  before v = A * y 
1  before  alpha = rho_new / < rt, v > 
1  before alpha = rho_new / h; 
1  before apply first correction to x 
1  before r = r - alpha*v 
1  before test stop criteria 
1 istlsolver::printOutput 
  0.5        2.268e-28      1.83417e-17
1 === rate=3.36419e-34, T=0.230791, TIT=0.461581, IT=0.5
1 finished solveLinearSystemImpl_ 1
1 converged value: 1 7 1 let s go get convergedRemote
1 to  comm_.min(converged) 
4 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
4  before v = A * y 
4  before  alpha = rho_new / < rt, v > 
4  before alpha = rho_new / h; 
4  before apply first correction to x 
4  before r = r - alpha*v 
4  before test stop criteria 
4 istlsolver::printOutput 
  0.5        2.268e-28      1.83417e-17
4 === rate=3.36419e-34, T=0.230732, TIT=0.461463, IT=0.5
4 finished solveLinearSystemImpl_ 1
4 converged value: 1 7 1 let s go get convergedRemote
4 to  comm_.min(converged) 
0  before  alpha = rho_new / < rt, v > 
0  before alpha = rho_new / h; 
0  before apply first correction to x 
0  before r = r - alpha*v 
0  before test stop criteria 
0 istlsolver::printOutput 
  0.5        2.268e-28      1.83417e-17
0 === rate=3.36419e-34, T=0.0355361, TIT=0.0710723, IT=0.5
2  before  alpha = rho_new / < rt, v > 
2  before alpha = rho_new / h; 
2  before apply first correction to x 
2  before r = r - alpha*v 
2  before test stop criteria 
2 istlsolver::printOutput 
  0.5        2.268e-28      1.83417e-17
2 === rate=3.36419e-34, T=0.230726, TIT=0.461452, IT=0.5
2 finished solveLinearSystemImpl_ 1
2 converged value: 1 7 1 let s go get convergedRemote
2 to  comm_.min(converged) 
6  before v = A * y 
6  before  alpha = rho_new / < rt, v > 
6  before alpha = rho_new / h; 
6  before apply first correction to x 
6  before r = r - alpha*v 
6  before test stop criteria 
6 istlsolver::printOutput 
  0.5        2.268e-28      1.83417e-17
6 === rate=3.36419e-34, T=0.230737, TIT=0.461473, IT=0.5
6 finished solveLinearSystemImpl_ 1
6 converged value: 1 7 1 let s go get convergedRemote
6 to  comm_.min(converged) 
6 did  comm_.min(converged) convergedRemote: 1
6 final convergedRemote: 1
6 Update: x^(k+1) = x^k - deltax^k 
6 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
6 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
6 Newton iteration  2 done6 , maximum relative shift = 3.742e-04
6 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
1 did  comm_.min(converged) convergedRemote: 1
1 final convergedRemote: 1
1 Update: x^(k+1) = x^k - deltax^k 
1 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
1 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
1 Newton iteration  2 done1 , maximum relative shift = 3.742e-04
1 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
4 did  comm_.min(converged) convergedRemote: 1
4 final convergedRemote: 1
4 Update: x^(k+1) = x^k - deltax^k 
4 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
4 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
4 Newton iteration  2 done4 , maximum relative shift = 3.742e-04
4 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
0 finished solveLinearSystemImpl_ 1
0 converged value: 1 7 1 let s go get convergedRemote
0 to  comm_.min(converged) 
0 did  comm_.min(converged) convergedRemote: 1
0 final convergedRemote: 1
0 Update: x^(k+1) = x^k - deltax^k 
0 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
0 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
0 Newton iteration  2 done0 , maximum relative shift = 3.742e-04
0 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
2 did  comm_.min(converged) convergedRemote: 1
2 final convergedRemote: 1
2 Update: x^(k+1) = x^k - deltax^k 
2 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
2 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
2 Newton iteration  2 done2 , maximum relative shift = 3.742e-04
2 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
5 did  comm_.min(converged) convergedRemote: 1
5 final convergedRemote: 1
5 Update: x^(k+1) = x^k - deltax^k 
5 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
5 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
5 Newton iteration  2 done5 , maximum relative shift = 3.742e-04
5 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
3 did  comm_.min(converged) convergedRemote: 1
3 final convergedRemote: 1
3 Update: x^(k+1) = x^k - deltax^k 
3 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
3 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
3 Newton iteration  2 done3 , maximum relative shift = 3.742e-04
3 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
0 Solve: M deltax^k = r 
0 Newton::solveLinearSystem : entering the try section 2
0 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
0 solveLinearSystemImpl_, before converged = ls.solve 
0 7 amgbackend::solve isParallel 1
2 Solve: M deltax^k = r 
2 Newton::solveLinearSystem : entering the try section 2
2 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
2 solveLinearSystemImpl_, before converged = ls.solve 
2 7 amgbackend::solve isParallel 1
5 Solve: M deltax^k = r 
5 Newton::solveLinearSystem : entering the try section 2
5 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
5 solveLinearSystemImpl_, before converged = ls.solve 
5 7 amgbackend::solve isParallel 1
3 Solve: M deltax^k = r 
3 Newton::solveLinearSystem : entering the try section 2
3 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
3 solveLinearSystemImpl_, before converged = ls.solve 
3 7 amgbackend::solve isParallel 1
6 Solve: M deltax^k = r 
6 Newton::solveLinearSystem : entering the try section 2
6 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
6 solveLinearSystemImpl_, before converged = ls.solve 
6 7 amgbackend::solve isParallel 1
1 Solve: M deltax^k = r 
1 Newton::solveLinearSystem : entering the try section 2
1 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
1 solveLinearSystemImpl_, before converged = ls.solve 
1 7 amgbackend::solve isParallel 1
4 Solve: M deltax^k = r 
4 Newton::solveLinearSystem : entering the try section 2
4 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
4 solveLinearSystemImpl_, before converged = ls.solve 
4 7 amgbackend::solve isParallel 1
3 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
3 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
6 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
6 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
1 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
1 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
5 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
5 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
0 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
0 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 7
2 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
2 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
4 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
4 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 7
5 7 before Dune::BiCGSTABSolver 
5 Dune::BiCGSTABSolver.apply(x, b, result_) 
5 BiCGSTABSolver::apply 
5 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
4 7 before Dune::BiCGSTABSolver 
4 Dune::BiCGSTABSolver.apply(x, b, result_) 
4 BiCGSTABSolver::apply 
4 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 7
2 7 before Dune::BiCGSTABSolver 
2 Dune::BiCGSTABSolver.apply(x, b, result_) 
2 BiCGSTABSolver::apply 
2 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
3 7 before Dune::BiCGSTABSolver 
3 Dune::BiCGSTABSolver.apply(x, b, result_) 
3 BiCGSTABSolver::apply 
3 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
6 7 before Dune::BiCGSTABSolver 
6 Dune::BiCGSTABSolver.apply(x, b, result_) 
6 BiCGSTABSolver::apply 
6 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
1 7 before Dune::BiCGSTABSolver 
1 Dune::BiCGSTABSolver.apply(x, b, result_) 
1 BiCGSTABSolver::apply 
1 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
Using a direct coarse solver (SuperLU)
Building hierarchy of 1 levels (inclusive coarse solver) took 0.20288 seconds.
0 7 before Dune::BiCGSTABSolver 
0 Dune::BiCGSTABSolver.apply(x, b, result_) 
0 BiCGSTABSolver::apply 
0 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
0 === BiCGSTABSolver
 Iter          Defect            Rate
    0      9.56037e-14
0  (it = 0.5; it < _maxit; it+=.5) 0.5 250
1 === BiCGSTABSolver
 Iter          Defect            Rate
    0      9.56037e-14
1  (it = 0.5; it < _maxit; it+=.5) 0.5 250
1  before p = r 
1  before  y = W^-1 * p 
1  before _prec->apply(y,p); 0.5 250
1 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
2 === BiCGSTABSolver
 Iter          Defect            Rate
    0      9.56037e-14
2  (it = 0.5; it < _maxit; it+=.5) 0.5 250
3 === BiCGSTABSolver
 Iter          Defect            Rate
    0      9.56037e-14
3  (it = 0.5; it < _maxit; it+=.5) 0.5 250
3  before p = r 
3  before  y = W^-1 * p 
3  before _prec->apply(y,p); 0.5 250
3 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
6 === BiCGSTABSolver
 Iter          Defect            Rate
    0      9.56037e-14
6  (it = 0.5; it < _maxit; it+=.5) 0.5 250
4 === BiCGSTABSolver
 Iter          Defect            Rate
    0      9.56037e-14
4  (it = 0.5; it < _maxit; it+=.5) 0.5 250
5 === BiCGSTABSolver
 Iter          Defect            Rate
    0      9.56037e-14
5  (it = 0.5; it < _maxit; it+=.5) 0.5 250
5  before p = r 
5  before  y = W^-1 * p 
5  before _prec->apply(y,p); 0.5 250
5 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
0  before p = r 
0  before  y = W^-1 * p 
0  before _prec->apply(y,p); 0.5 250
0 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
2  before p = r 
2  before  y = W^-1 * p 
2  before _prec->apply(y,p); 0.5 250
2 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
6  before p = r 
6  before  y = W^-1 * p 
6  before _prec->apply(y,p); 0.5 250
6 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
4  before p = r 
4  before  y = W^-1 * p 
4  before _prec->apply(y,p); 0.5 250
4 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 7
0 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
4 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
4 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
5 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
1 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
3 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
2 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
2 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
6 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
6 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
0 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
0 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
3 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
3  before v = A * y 
6  before v = A * y 
5 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
5  before v = A * y 
1 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
1  before v = A * y 
1  before  alpha = rho_new / < rt, v > 
1  before alpha = rho_new / h; 
1  before apply first correction to x 
1  before r = r - alpha*v 
1  before test stop criteria 
1 istlsolver::printOutput 
  0.5       1.2493e-31      1.30675e-18
1 === rate=1.70759e-36, T=0.226924, TIT=0.453849, IT=0.5
1 finished solveLinearSystemImpl_ 1
1 converged value: 1 7 1 let s go get convergedRemote
1 to  comm_.min(converged) 
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
2  before v = A * y 
2  before  alpha = rho_new / < rt, v > 
2  before alpha = rho_new / h; 
2  before apply first correction to x 
2  before r = r - alpha*v 
2  before test stop criteria 
2 istlsolver::printOutput 
  0.5       1.2493e-31      1.30675e-18
2 === rate=1.70759e-36, T=0.227007, TIT=0.454013, IT=0.5
2 finished solveLinearSystemImpl_ 1
2 converged value: 1 7 1 let s go get convergedRemote
2 to  comm_.min(converged) 
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
4  before v = A * y 
4  before  alpha = rho_new / < rt, v > 
4  before alpha = rho_new / h; 
4  before apply first correction to x 
4  before r = r - alpha*v 
4  before test stop criteria 
4 istlsolver::printOutput 
  0.5       1.2493e-31      1.30675e-18
4 === rate=1.70759e-36, T=0.227009, TIT=0.454018, IT=0.5
4 finished solveLinearSystemImpl_ 1
4 converged value: 1 7 1 let s go get convergedRemote
4 to  comm_.min(converged) 
6  before  alpha = rho_new / < rt, v > 
6  before alpha = rho_new / h; 
6  before apply first correction to x 
6  before r = r - alpha*v 
6  before test stop criteria 
6 istlsolver::printOutput 
  0.5       1.2493e-31      1.30675e-18
6 === rate=1.70759e-36, T=0.227011, TIT=0.454023, IT=0.5
6 finished solveLinearSystemImpl_ 1
6 converged value: 1 7 1 let s go get convergedRemote
6 to  comm_.min(converged) 
5  before  alpha = rho_new / < rt, v > 
5  before alpha = rho_new / h; 
5  before apply first correction to x 
5  before r = r - alpha*v 
5  before test stop criteria 
5 istlsolver::printOutput 
  0.5       1.2493e-31      1.30675e-18
5 === rate=1.70759e-36, T=0.226923, TIT=0.453847, IT=0.5
5 finished solveLinearSystemImpl_ 1
5 converged value: 1 7 1 let s go get convergedRemote
5 to  comm_.min(converged) 
3  before  alpha = rho_new / < rt, v > 
3  before alpha = rho_new / h; 
3  before apply first correction to x 
3  before r = r - alpha*v 
3  before test stop criteria 
3 istlsolver::printOutput 
  0.5       1.2493e-31      1.30675e-18
3 === rate=1.70759e-36, T=0.226927, TIT=0.453854, IT=0.5
3 finished solveLinearSystemImpl_ 1
3 converged value: 1 7 1 let s go get convergedRemote
3 to  comm_.min(converged) 
0  before v = A * y 
0  before  alpha = rho_new / < rt, v > 
0  before alpha = rho_new / h; 
0  before apply first correction to x 
0  before r = r - alpha*v 
0  before test stop criteria 
0 istlsolver::printOutput 
  0.5       1.2493e-31      1.30675e-18
0 === rate=1.70759e-36, T=0.0299385, TIT=0.059877, IT=0.5
1 did  comm_.min(converged) convergedRemote: 1
1 final convergedRemote: 1
1 Update: x^(k+1) = x^k - deltax^k 
1 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
1 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
1 Newton iteration  3 done1 , maximum relative shift = 3.321e-06
1 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
2 did  comm_.min(converged) convergedRemote: 1
2 final convergedRemote: 1
2 Update: x^(k+1) = x^k - deltax^k 
2 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
2 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
2 Newton iteration  3 done2 , maximum relative shift = 3.321e-06
2 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
4 did  comm_.min(converged) convergedRemote: 1
4 final convergedRemote: 1
4 Update: x^(k+1) = x^k - deltax^k 
4 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
4 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
4 Newton iteration  3 done4 , maximum relative shift = 3.321e-06
4 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
6 did  comm_.min(converged) convergedRemote: 1
6 final convergedRemote: 1
6 Update: x^(k+1) = x^k - deltax^k 
6 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
6 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
6 Newton iteration  3 done6 , maximum relative shift = 3.321e-06
6 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
5 did  comm_.min(converged) convergedRemote: 1
5 final convergedRemote: 1
5 Update: x^(k+1) = x^k - deltax^k 
5 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
5 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
5 Newton iteration  3 done5 , maximum relative shift = 3.321e-06
5 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
3 did  comm_.min(converged) convergedRemote: 1
3 final convergedRemote: 1
3 Update: x^(k+1) = x^k - deltax^k 
3 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
3 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
3 Newton iteration  3 done3 , maximum relative shift = 3.321e-06
3 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
0 finished solveLinearSystemImpl_ 1
0 converged value: 1 7 1 let s go get convergedRemote
0 to  comm_.min(converged) 
0 did  comm_.min(converged) convergedRemote: 1
0 final convergedRemote: 1
0 Update: x^(k+1) = x^k - deltax^k 
0 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
0 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
0 Newton iteration  3 done0 , maximum relative shift = 3.321e-06
0 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
1 Solve: M deltax^k = r 
1 Newton::solveLinearSystem : entering the try section 3
1 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
1 solveLinearSystemImpl_, before converged = ls.solve 
1 7 amgbackend::solve isParallel 1
2 Solve: M deltax^k = r 
2 Newton::solveLinearSystem : entering the try section 3
2 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
2 solveLinearSystemImpl_, before converged = ls.solve 
2 7 amgbackend::solve isParallel 1
4 Solve: M deltax^k = r 
4 Newton::solveLinearSystem : entering the try section 3
4 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
4 solveLinearSystemImpl_, before converged = ls.solve 
4 7 amgbackend::solve isParallel 1
6 Solve: M deltax^k = r 
6 Newton::solveLinearSystem : entering the try section 3
6 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
6 solveLinearSystemImpl_, before converged = ls.solve 
6 7 amgbackend::solve isParallel 1
5 Solve: M deltax^k = r 
5 Newton::solveLinearSystem : entering the try section 3
5 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
5 solveLinearSystemImpl_, before converged = ls.solve 
5 7 amgbackend::solve isParallel 1
0 Solve: M deltax^k = r 
0 Newton::solveLinearSystem : entering the try section 3
0 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
0 solveLinearSystemImpl_, before converged = ls.solve 
0 7 amgbackend::solve isParallel 1
3 Solve: M deltax^k = r 
3 Newton::solveLinearSystem : entering the try section 3
3 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
3 solveLinearSystemImpl_, before converged = ls.solve 
3 7 amgbackend::solve isParallel 1
0 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
0 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 7
5 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
5 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
1 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
1 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
2 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
2 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
4 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
4 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
6 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
6 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
3 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
3 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 7
6 7 before Dune::BiCGSTABSolver 
6 Dune::BiCGSTABSolver.apply(x, b, result_) 
6 BiCGSTABSolver::apply 
6 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
3 7 before Dune::BiCGSTABSolver 
3 Dune::BiCGSTABSolver.apply(x, b, result_) 
3 BiCGSTABSolver::apply 
3 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
5 7 before Dune::BiCGSTABSolver 
5 Dune::BiCGSTABSolver.apply(x, b, result_) 
5 BiCGSTABSolver::apply 
5 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
1 7 before Dune::BiCGSTABSolver 
1 Dune::BiCGSTABSolver.apply(x, b, result_) 
1 BiCGSTABSolver::apply 
1 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
2 7 before Dune::BiCGSTABSolver 
2 Dune::BiCGSTABSolver.apply(x, b, result_) 
2 BiCGSTABSolver::apply 
2 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
4 7 before Dune::BiCGSTABSolver 
4 Dune::BiCGSTABSolver.apply(x, b, result_) 
4 BiCGSTABSolver::apply 
4 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 7
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
Using a direct coarse solver (SuperLU)
Building hierarchy of 1 levels (inclusive coarse solver) took 0.202644 seconds.
0 7 before Dune::BiCGSTABSolver 
0 Dune::BiCGSTABSolver.apply(x, b, result_) 
0 BiCGSTABSolver::apply 
0 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
0 === BiCGSTABSolver
 Iter          Defect            Rate
    0      2.00518e-15
0  (it = 0.5; it < _maxit; it+=.5) 0.5 250
0  before p = r 
0  before  y = W^-1 * p 
0  before _prec->apply(y,p); 0.5 250
0 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 7
4 === BiCGSTABSolver
 Iter          Defect            Rate
    0      2.00518e-15
4  (it = 0.5; it < _maxit; it+=.5) 0.5 250
4  before p = r 
4  before  y = W^-1 * p 
4  before _prec->apply(y,p); 0.5 250
4 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
6 === BiCGSTABSolver
 Iter          Defect            Rate
    0      2.00518e-15
6  (it = 0.5; it < _maxit; it+=.5) 0.5 250
6  before p = r 
6  before  y = W^-1 * p 
6  before _prec->apply(y,p); 0.5 250
6 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
3 === BiCGSTABSolver
 Iter          Defect            Rate
    0      2.00518e-15
3  (it = 0.5; it < _maxit; it+=.5) 0.5 250
3  before p = r 
3  before  y = W^-1 * p 
3  before _prec->apply(y,p); 0.5 250
3 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
5 === BiCGSTABSolver
 Iter          Defect            Rate
    0      2.00518e-15
5  (it = 0.5; it < _maxit; it+=.5) 0.5 250
5  before p = r 
5  before  y = W^-1 * p 
5  before _prec->apply(y,p); 0.5 250
5 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
2 === BiCGSTABSolver
 Iter          Defect            Rate
    0      2.00518e-15
2  (it = 0.5; it < _maxit; it+=.5) 0.5 250
2  before p = r 
2  before  y = W^-1 * p 
2  before _prec->apply(y,p); 0.5 250
2 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
1 === BiCGSTABSolver
 Iter          Defect            Rate
    0      2.00518e-15
1  (it = 0.5; it < _maxit; it+=.5) 0.5 250
1  before p = r 
1  before  y = W^-1 * p 
1  before _prec->apply(y,p); 0.5 250
1 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 7
0 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
6 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
3 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
5 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
2 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
2 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
1 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
4 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
3 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
5 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
4 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
0 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
0 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
1 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
6 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
2  before v = A * y 
2  before  alpha = rho_new / < rt, v > 
2  before alpha = rho_new / h; 
2  before apply first correction to x 
2  before r = r - alpha*v 
2  before test stop criteria 
2 istlsolver::printOutput 
  0.5      5.08691e-32      2.53688e-17
2 === rate=6.43578e-34, T=0.226491, TIT=0.452982, IT=0.5
2 finished solveLinearSystemImpl_ 1
2 converged value: 1 7 1 let s go get convergedRemote
2 to  comm_.min(converged) 
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
5  before v = A * y 
5  before  alpha = rho_new / < rt, v > 
5  before alpha = rho_new / h; 
5  before apply first correction to x 
5  before r = r - alpha*v 
5  before test stop criteria 
5 istlsolver::printOutput 
  0.5      5.08691e-32      2.53688e-17
5 === rate=6.43578e-34, T=0.226644, TIT=0.453288, IT=0.5
5 finished solveLinearSystemImpl_ 1
5 converged value: 1 7 1 let s go get convergedRemote
5 to  comm_.min(converged) 
3  before v = A * y 
3  before  alpha = rho_new / < rt, v > 
3  before alpha = rho_new / h; 
3  before apply first correction to x 
3  before r = r - alpha*v 
3  before test stop criteria 
3 istlsolver::printOutput 
  0.5      5.08691e-32      2.53688e-17
3 === rate=6.43578e-34, T=0.226647, TIT=0.453293, IT=0.5
3 finished solveLinearSystemImpl_ 1
3 converged value: 1 7 1 let s go get convergedRemote
3 to  comm_.min(converged) 
6  before v = A * y 
6  before  alpha = rho_new / < rt, v > 
6  before alpha = rho_new / h; 
6  before apply first correction to x 
6  before r = r - alpha*v 
6  before test stop criteria 
6 istlsolver::printOutput 
  0.5      5.08691e-32      2.53688e-17
6 === rate=6.43578e-34, T=0.226497, TIT=0.452994, IT=0.5
6 finished solveLinearSystemImpl_ 1
6 converged value: 1 7 1 let s go get convergedRemote
6 to  comm_.min(converged) 
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
4  before v = A * y 
4  before  alpha = rho_new / < rt, v > 
4  before alpha = rho_new / h; 
4  before apply first correction to x 
4  before r = r - alpha*v 
4  before test stop criteria 
4 istlsolver::printOutput 
  0.5      5.08691e-32      2.53688e-17
4 === rate=6.43578e-34, T=0.226493, TIT=0.452987, IT=0.5
4 finished solveLinearSystemImpl_ 1
4 converged value: 1 7 1 let s go get convergedRemote
4 to  comm_.min(converged) 
0  before v = A * y 
0  before  alpha = rho_new / < rt, v > 
0  before alpha = rho_new / h; 
0  before apply first correction to x 
0  before r = r - alpha*v 
0  before test stop criteria 
0 istlsolver::printOutput 
  0.5      5.08691e-32      2.53688e-17
0 === rate=6.43578e-34, T=0.0297879, TIT=0.0595757, IT=0.5
1  before v = A * y 
1  before  alpha = rho_new / < rt, v > 
1  before alpha = rho_new / h; 
1  before apply first correction to x 
1  before r = r - alpha*v 
1  before test stop criteria 
1 istlsolver::printOutput 
  0.5      5.08691e-32      2.53688e-17
1 === rate=6.43578e-34, T=0.226646, TIT=0.453292, IT=0.5
1 finished solveLinearSystemImpl_ 1
1 converged value: 1 7 1 let s go get convergedRemote
1 to  comm_.min(converged) 
3 did  comm_.min(converged) convergedRemote: 1
3 final convergedRemote: 1
3 Update: x^(k+1) = x^k - deltax^k 
3 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
3 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
3 Newton iteration  4 done3 , maximum relative shift = 6.980e-08
3 Assemble/solve/update time: 0.29727(24.0005%)/0.940821(75.9585%)/0.000507258(0.0409542%)
3 gridVariables->advanceTimeStep
3 before, nonLinearSolver->suggestTimeStepSize, current time: 193.703 simTime 10800
3 before assembler->setPreviousSolution, ddt: 30
3 nonLinearSolver->solve
3 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
2 did  comm_.min(converged) convergedRemote: 1
2 final convergedRemote: 1
2 Update: x^(k+1) = x^k - deltax^k 
2 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
2 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
2 Newton iteration  4 done2 , maximum relative shift = 6.980e-08
2 Assemble/solve/update time: 0.297188(23.9901%)/0.941101(75.9692%)/0.000503246(0.0406239%)
2 gridVariables->advanceTimeStep
2 before, nonLinearSolver->suggestTimeStepSize, current time: 193.703 simTime 10800
2 before assembler->setPreviousSolution, ddt: 30
2 nonLinearSolver->solve
2 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
5 did  comm_.min(converged) convergedRemote: 1
5 final convergedRemote: 1
5 Update: x^(k+1) = x^k - deltax^k 
5 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
5 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
5 Newton iteration  4 done5 , maximum relative shift = 6.980e-08
5 Assemble/solve/update time: 0.297413(24.0096%)/0.940804(75.9493%)/0.000509408(0.0411235%)
5 gridVariables->advanceTimeStep
5 before, nonLinearSolver->suggestTimeStepSize, current time: 193.703 simTime 10800
5 before assembler->setPreviousSolution, ddt: 30
5 nonLinearSolver->solve
5 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
1 did  comm_.min(converged) convergedRemote: 1
1 final convergedRemote: 1
1 Update: x^(k+1) = x^k - deltax^k 
1 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
1 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
1 Newton iteration  4 done1 , maximum relative shift = 6.980e-08
1 Assemble/solve/update time: 0.297406(24.009%)/0.94081(75.9498%)/0.000510448(0.0412075%)
1 gridVariables->advanceTimeStep
1 before, nonLinearSolver->suggestTimeStepSize, current time: 193.703 simTime 10800
1 before assembler->setPreviousSolution, ddt: 30
1 nonLinearSolver->solve
1 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
6 did  comm_.min(converged) convergedRemote: 1
6 final convergedRemote: 1
6 Update: x^(k+1) = x^k - deltax^k 
6 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
6 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
6 Newton iteration  4 done6 , maximum relative shift = 6.980e-08
6 Assemble/solve/update time: 0.297211(23.9915%)/0.941104(75.968%)/0.000501496(0.0404819%)
6 gridVariables->advanceTimeStep
6 before, nonLinearSolver->suggestTimeStepSize, current time: 193.703 simTime 10800
6 before assembler->setPreviousSolution, ddt: 30
6 nonLinearSolver->solve
6 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
4 did  comm_.min(converged) convergedRemote: 1
4 final convergedRemote: 1
4 Update: x^(k+1) = x^k - deltax^k 
4 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
4 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
4 Newton iteration  4 done4 , maximum relative shift = 6.980e-08
4 Assemble/solve/update time: 0.297221(23.9924%)/0.941084(75.9668%)/0.000505156(0.0407775%)
4 gridVariables->advanceTimeStep
4 before, nonLinearSolver->suggestTimeStepSize, current time: 193.703 simTime 10800
4 before assembler->setPreviousSolution, ddt: 30
4 nonLinearSolver->solve
4 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
0 finished solveLinearSystemImpl_ 1
0 converged value: 1 7 1 let s go get convergedRemote
0 to  comm_.min(converged) 
0 did  comm_.min(converged) convergedRemote: 1
0 final convergedRemote: 1
0 Update: x^(k+1) = x^k - deltax^k 
0 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
0 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
0 Newton iteration  4 done0 , maximum relative shift = 6.980e-08
0 Assemble/solve/update time: 0.296719(23.9616%)/0.941092(75.998%)/0.000500146(0.0403894%)
0 gridVariables->advanceTimeStep
0 before, nonLinearSolver->suggestTimeStepSize, current time: 193.703 simTime 10800
0 before assembler->setPreviousSolution, ddt: 30
0 nonLinearSolver->solve
0 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
5 Solve: M deltax^k = r 
5 Newton::solveLinearSystem : entering the try section 0
5to  comm_.sum(norm2) 
5did  comm_.sum(norm2) 
5 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
5 solveLinearSystemImpl_, before converged = ls.solve 
5 7 amgbackend::solve isParallel 1
3 Solve: M deltax^k = r 
3 Newton::solveLinearSystem : entering the try section 0
3to  comm_.sum(norm2) 
3did  comm_.sum(norm2) 
3 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
3 solveLinearSystemImpl_, before converged = ls.solve 
3 7 amgbackend::solve isParallel 1
2 Solve: M deltax^k = r 
2 Newton::solveLinearSystem : entering the try section 0
2to  comm_.sum(norm2) 
2did  comm_.sum(norm2) 
2 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
2 solveLinearSystemImpl_, before converged = ls.solve 
0 Solve: M deltax^k = r 
0 Newton::solveLinearSystem : entering the try section 0
0to  comm_.sum(norm2) 
0did  comm_.sum(norm2) 
0 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
0 solveLinearSystemImpl_, before converged = ls.solve 
0 7 amgbackend::solve isParallel 1
1 Solve: M deltax^k = r 
1 Newton::solveLinearSystem : entering the try section 0
1to  comm_.sum(norm2) 
1did  comm_.sum(norm2) 
1 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
1 solveLinearSystemImpl_, before converged = ls.solve 
1 7 amgbackend::solve isParallel 1
6 Solve: M deltax^k = r 
6 Newton::solveLinearSystem : entering the try section 0
6to  comm_.sum(norm2) 
6did  comm_.sum(norm2) 
6 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
6 solveLinearSystemImpl_, before converged = ls.solve 
6 7 amgbackend::solve isParallel 1
4 Solve: M deltax^k = r 
4 Newton::solveLinearSystem : entering the try section 0
4to  comm_.sum(norm2) 
4did  comm_.sum(norm2) 
4 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
4 solveLinearSystemImpl_, before converged = ls.solve 
4 7 amgbackend::solve isParallel 1
2 7 amgbackend::solve isParallel 1
0 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
0 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 7
1 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
1 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
6 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
6 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
4 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
4 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
5 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
5 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
3 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
3 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
2 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
2 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 7
5 7 before Dune::BiCGSTABSolver 
5 Dune::BiCGSTABSolver.apply(x, b, result_) 
5 BiCGSTABSolver::apply 
5 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 7
3 7 before Dune::BiCGSTABSolver 
3 Dune::BiCGSTABSolver.apply(x, b, result_) 
3 BiCGSTABSolver::apply 
3 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
2 7 before Dune::BiCGSTABSolver 
2 Dune::BiCGSTABSolver.apply(x, b, result_) 
2 BiCGSTABSolver::apply 
2 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
1 7 before Dune::BiCGSTABSolver 
1 Dune::BiCGSTABSolver.apply(x, b, result_) 
1 BiCGSTABSolver::apply 
1 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
6 7 before Dune::BiCGSTABSolver 
6 Dune::BiCGSTABSolver.apply(x, b, result_) 
6 BiCGSTABSolver::apply 
6 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
4 7 before Dune::BiCGSTABSolver 
4 Dune::BiCGSTABSolver.apply(x, b, result_) 
4 BiCGSTABSolver::apply 
4 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
Using a direct coarse solver (SuperLU)
Building hierarchy of 1 levels (inclusive coarse solver) took 0.201458 seconds.
0 7 before Dune::BiCGSTABSolver 
0 Dune::BiCGSTABSolver.apply(x, b, result_) 
2 === BiCGSTABSolver
 Iter          Defect            Rate
    0      4.48476e-09
2  (it = 0.5; it < _maxit; it+=.5) 0.5 250
2  before p = r 
2  before  y = W^-1 * p 
2  before _prec->apply(y,p); 0.5 250
2 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
4 === BiCGSTABSolver
 Iter          Defect            Rate
    0      4.48476e-09
4  (it = 0.5; it < _maxit; it+=.5) 0.5 250
4  before p = r 
4  before  y = W^-1 * p 
4  before _prec->apply(y,p); 0.5 250
4 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
3 === BiCGSTABSolver
 Iter          Defect            Rate
    0      4.48476e-09
3  (it = 0.5; it < _maxit; it+=.5) 0.5 250
3  before p = r 
3  before  y = W^-1 * p 
3  before _prec->apply(y,p); 0.5 250
3 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
1 === BiCGSTABSolver
 Iter          Defect            Rate
    0      4.48476e-09
1  (it = 0.5; it < _maxit; it+=.5) 0.5 250
1  before p = r 
1  before  y = W^-1 * p 
1  before _prec->apply(y,p); 0.5 250
1 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
6 === BiCGSTABSolver
 Iter          Defect            Rate
    0      4.48476e-09
6  (it = 0.5; it < _maxit; it+=.5) 0.5 250
6  before p = r 
6  before  y = W^-1 * p 
6  before _prec->apply(y,p); 0.5 250
6 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
5 === BiCGSTABSolver
 Iter          Defect            Rate
    0      4.48476e-09
5  (it = 0.5; it < _maxit; it+=.5) 0.5 250
5  before p = r 
5  before  y = W^-1 * p 
5  before _prec->apply(y,p); 0.5 250
5 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
0 BiCGSTABSolver::apply 
0 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
0 === BiCGSTABSolver
 Iter          Defect            Rate
    0      4.48476e-09
0  (it = 0.5; it < _maxit; it+=.5) 0.5 250
0  before p = r 
0  before  y = W^-1 * p 
0  before _prec->apply(y,p); 0.5 250
0 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 7
0 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
5 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
2 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
4 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
4 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
3 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
1 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
6 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
5 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
2 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
3 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
1 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
0 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
0 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
2  before v = A * y 
2  before  alpha = rho_new / < rt, v > 
2  before alpha = rho_new / h; 
2  before apply first correction to x 
2  before r = r - alpha*v 
2  before test stop criteria 
2 istlsolver::printOutput 
  0.5      5.59771e-25      1.24816e-16
2 === rate=1.55791e-32, T=0.225039, TIT=0.450078, IT=0.5
2 finished solveLinearSystemImpl_ 1
2 converged value: 1 7 1 let s go get convergedRemote
2 to  comm_.min(converged) 
4  before v = A * y 
4  before  alpha = rho_new / < rt, v > 
4  before alpha = rho_new / h; 
4  before apply first correction to x 
4  before r = r - alpha*v 
4  before test stop criteria 
4 istlsolver::printOutput 
  0.5      5.59771e-25      1.24816e-16
4 === rate=1.55791e-32, T=0.225043, TIT=0.450085, IT=0.5
4 finished solveLinearSystemImpl_ 1
4 converged value: 1 7 1 let s go get convergedRemote
4 to  comm_.min(converged) 
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
3  before v = A * y 
3  before  alpha = rho_new / < rt, v > 
3  before alpha = rho_new / h; 
3  before apply first correction to x 
3  before r = r - alpha*v 
3  before test stop criteria 
3 istlsolver::printOutput 
  0.5      5.59771e-25      1.24816e-16
3 === rate=1.55791e-32, T=0.225106, TIT=0.450212, IT=0.5
3 finished solveLinearSystemImpl_ 1
3 converged value: 1 7 1 let s go get convergedRemote
3 to  comm_.min(converged) 
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
0  before v = A * y 
0  before  alpha = rho_new / < rt, v > 
0  before alpha = rho_new / h; 
0  before apply first correction to x 
0  before r = r - alpha*v 
0  before test stop criteria 
0 istlsolver::printOutput 
  0.5      5.59771e-25      1.24816e-16
0 === rate=1.55791e-32, T=0.0299343, TIT=0.0598687, IT=0.5
6 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
6  before v = A * y 
6  before  alpha = rho_new / < rt, v > 
6  before alpha = rho_new / h; 
6  before apply first correction to x 
6  before r = r - alpha*v 
6  before test stop criteria 
6 istlsolver::printOutput 
  0.5      5.59771e-25      1.24816e-16
6 === rate=1.55791e-32, T=0.225045, TIT=0.450089, IT=0.5
6 finished solveLinearSystemImpl_ 1
6 converged value: 1 7 1 let s go get convergedRemote
6 to  comm_.min(converged) 
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
1  before v = A * y 
1  before  alpha = rho_new / < rt, v > 
1  before alpha = rho_new / h; 
1  before apply first correction to x 
1  before r = r - alpha*v 
1  before test stop criteria 
1 istlsolver::printOutput 
  0.5      5.59771e-25      1.24816e-16
1 === rate=1.55791e-32, T=0.225105, TIT=0.45021, IT=0.5
1 finished solveLinearSystemImpl_ 1
1 converged value: 1 7 1 let s go get convergedRemote
1 to  comm_.min(converged) 
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
5  before v = A * y 
5  before  alpha = rho_new / < rt, v > 
5  before alpha = rho_new / h; 
5  before apply first correction to x 
5  before r = r - alpha*v 
5  before test stop criteria 
5 istlsolver::printOutput 
  0.5      5.59771e-25      1.24816e-16
5 === rate=1.55791e-32, T=0.225105, TIT=0.45021, IT=0.5
5 finished solveLinearSystemImpl_ 1
5 converged value: 1 7 1 let s go get convergedRemote
5 to  comm_.min(converged) 
5 did  comm_.min(converged) convergedRemote: 1
5 final convergedRemote: 1
5 Update: x^(k+1) = x^k - deltax^k 
5 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
5 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
5 Newton iteration  1 done5 , maximum relative shift = 2.098e-02
5 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
0 finished solveLinearSystemImpl_ 1
0 converged value: 1 7 1 let s go get convergedRemote
0 to  comm_.min(converged) 
0 did  comm_.min(converged) convergedRemote: 1
0 final convergedRemote: 1
0 Update: x^(k+1) = x^k - deltax^k 
0 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
0 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
0 Newton iteration  1 done0 , maximum relative shift = 2.098e-02
0 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
6 did  comm_.min(converged) convergedRemote: 1
6 final convergedRemote: 1
6 Update: x^(k+1) = x^k - deltax^k 
6 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
6 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
6 Newton iteration  1 done6 , maximum relative shift = 2.098e-02
6 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
1 did  comm_.min(converged) convergedRemote: 1
1 final convergedRemote: 1
1 Update: x^(k+1) = x^k - deltax^k 
1 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
1 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
1 Newton iteration  1 done1 , maximum relative shift = 2.098e-02
1 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
3 did  comm_.min(converged) convergedRemote: 1
3 final convergedRemote: 1
3 Update: x^(k+1) = x^k - deltax^k 
3 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
3 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
3 Newton iteration  1 done3 , maximum relative shift = 2.098e-02
3 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
2 did  comm_.min(converged) convergedRemote: 1
2 final convergedRemote: 1
2 Update: x^(k+1) = x^k - deltax^k 
2 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
2 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
2 Newton iteration  1 done2 , maximum relative shift = 2.098e-02
2 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
4 did  comm_.min(converged) convergedRemote: 1
4 final convergedRemote: 1
4 Update: x^(k+1) = x^k - deltax^k 
4 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
4 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
4 Newton iteration  1 done4 , maximum relative shift = 2.098e-02
4 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
5 Solve: M deltax^k = r 
5 Newton::solveLinearSystem : entering the try section 1
5 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
5 solveLinearSystemImpl_, before converged = ls.solve 
5 7 amgbackend::solve isParallel 1
0 Solve: M deltax^k = r 
0 Newton::solveLinearSystem : entering the try section 1
6 Solve: M deltax^k = r 
6 Newton::solveLinearSystem : entering the try section 1
6 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
6 solveLinearSystemImpl_, before converged = ls.solve 
6 7 amgbackend::solve isParallel 1
1 Solve: M deltax^k = r 
1 Newton::solveLinearSystem : entering the try section 1
1 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
1 solveLinearSystemImpl_, before converged = ls.solve 
1 7 amgbackend::solve isParallel 1
3 Solve: M deltax^k = r 
3 Newton::solveLinearSystem : entering the try section 1
3 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
3 solveLinearSystemImpl_, before converged = ls.solve 
3 7 amgbackend::solve isParallel 1
2 Solve: M deltax^k = r 
2 Newton::solveLinearSystem : entering the try section 1
2 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
2 solveLinearSystemImpl_, before converged = ls.solve 
2 7 amgbackend::solve isParallel 1
4 Solve: M deltax^k = r 
4 Newton::solveLinearSystem : entering the try section 1
4 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
4 solveLinearSystemImpl_, before converged = ls.solve 
4 7 amgbackend::solve isParallel 1
4 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
4 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
2 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
2 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
5 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
5 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
0 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
0 solveLinearSystemImpl_, before converged = ls.solve 
0 7 amgbackend::solve isParallel 1
0 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
0 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 7
6 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
6 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
1 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
1 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
3 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
3 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 7
1 7 before Dune::BiCGSTABSolver 
1 Dune::BiCGSTABSolver.apply(x, b, result_) 
1 BiCGSTABSolver::apply 
1 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 7
4 7 before Dune::BiCGSTABSolver 
4 Dune::BiCGSTABSolver.apply(x, b, result_) 
4 BiCGSTABSolver::apply 
4 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
2 7 before Dune::BiCGSTABSolver 
2 Dune::BiCGSTABSolver.apply(x, b, result_) 
2 BiCGSTABSolver::apply 
2 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
5 7 before Dune::BiCGSTABSolver 
5 Dune::BiCGSTABSolver.apply(x, b, result_) 
5 BiCGSTABSolver::apply 
5 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
3 7 before Dune::BiCGSTABSolver 
3 Dune::BiCGSTABSolver.apply(x, b, result_) 
3 BiCGSTABSolver::apply 
3 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
6 7 before Dune::BiCGSTABSolver 
6 Dune::BiCGSTABSolver.apply(x, b, result_) 
6 BiCGSTABSolver::apply 
6 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
Using a direct coarse solver (SuperLU)
Building hierarchy of 1 levels (inclusive coarse solver) took 0.200651 seconds.
0 7 before Dune::BiCGSTABSolver 
0 Dune::BiCGSTABSolver.apply(x, b, result_) 
0 BiCGSTABSolver::apply 
0 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
4 === BiCGSTABSolver
 Iter          Defect            Rate
    0       1.2833e-11
4  (it = 0.5; it < _maxit; it+=.5) 0.5 250
2 === BiCGSTABSolver
 Iter          Defect            Rate
    0       1.2833e-11
2  (it = 0.5; it < _maxit; it+=.5) 0.5 250
5 === BiCGSTABSolver
 Iter          Defect            Rate
    0       1.2833e-11
5  (it = 0.5; it < _maxit; it+=.5) 0.5 250
5  before p = r 
5  before  y = W^-1 * p 
5  before _prec->apply(y,p); 0.5 250
5 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
3 === BiCGSTABSolver
 Iter          Defect            Rate
    0       1.2833e-11
3  (it = 0.5; it < _maxit; it+=.5) 0.5 250
3  before p = r 
3  before  y = W^-1 * p 
3  before _prec->apply(y,p); 0.5 250
3 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
1 === BiCGSTABSolver
 Iter          Defect            Rate
    0       1.2833e-11
1  (it = 0.5; it < _maxit; it+=.5) 0.5 250
1  before p = r 
1  before  y = W^-1 * p 
1  before _prec->apply(y,p); 0.5 250
1 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
0 === BiCGSTABSolver
 Iter          Defect            Rate
    0       1.2833e-11
0  (it = 0.5; it < _maxit; it+=.5) 0.5 250
6 === BiCGSTABSolver
 Iter          Defect            Rate
    0       1.2833e-11
6  (it = 0.5; it < _maxit; it+=.5) 0.5 250
0  before p = r 
0  before  y = W^-1 * p 
0  before _prec->apply(y,p); 0.5 250
0 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 7
0 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
4  before p = r 
4  before  y = W^-1 * p 
4  before _prec->apply(y,p); 0.5 250
4 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
2  before p = r 
2  before  y = W^-1 * p 
2  before _prec->apply(y,p); 0.5 250
2 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
6  before p = r 
6  before  y = W^-1 * p 
6  before _prec->apply(y,p); 0.5 250
6 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
1 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
0 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
4 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
4 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
5 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
3 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
3 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
6 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
6 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
2 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
2 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
1 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
0 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
4  before v = A * y 
4  before  alpha = rho_new / < rt, v > 
5 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
5  before v = A * y 
5  before  alpha = rho_new / < rt, v > 
5  before alpha = rho_new / h; 
5  before apply first correction to x 
5  before r = r - alpha*v 
5  before test stop criteria 
5 istlsolver::printOutput 
  0.5      1.27477e-27      9.93355e-17
5 === rate=9.86754e-33, T=0.224393, TIT=0.448785, IT=0.5
5 finished solveLinearSystemImpl_ 1
5 converged value: 1 7 1 let s go get convergedRemote
5 to  comm_.min(converged) 
6  before v = A * y 
6  before  alpha = rho_new / < rt, v > 
6  before alpha = rho_new / h; 
6  before apply first correction to x 
6  before r = r - alpha*v 
6  before test stop criteria 
6 istlsolver::printOutput 
  0.5      1.27477e-27      9.93355e-17
6 === rate=9.86754e-33, T=0.224394, TIT=0.448788, IT=0.5
6 finished solveLinearSystemImpl_ 1
6 converged value: 1 7 1 let s go get convergedRemote
6 to  comm_.min(converged) 
2  before v = A * y 
2  before  alpha = rho_new / < rt, v > 
2  before alpha = rho_new / h; 
2  before apply first correction to x 
2  before r = r - alpha*v 
2  before test stop criteria 
2 istlsolver::printOutput 
  0.5      1.27477e-27      9.93355e-17
2 === rate=9.86754e-33, T=0.224395, TIT=0.448789, IT=0.5
2 finished solveLinearSystemImpl_ 1
2 converged value: 1 7 1 let s go get convergedRemote
2 to  comm_.min(converged) 
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
3  before v = A * y 
3  before  alpha = rho_new / < rt, v > 
3  before alpha = rho_new / h; 
3  before apply first correction to x 
3  before r = r - alpha*v 
3  before test stop criteria 
3 istlsolver::printOutput 
  0.5      1.27477e-27      9.93355e-17
3 === rate=9.86754e-33, T=0.224395, TIT=0.448791, IT=0.5
3 finished solveLinearSystemImpl_ 1
3 converged value: 1 7 1 let s go get convergedRemote
3 to  comm_.min(converged) 
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
1  before v = A * y 
1  before  alpha = rho_new / < rt, v > 
1  before alpha = rho_new / h; 
1  before apply first correction to x 
1  before r = r - alpha*v 
1  before test stop criteria 
1 istlsolver::printOutput 
  0.5      1.27477e-27      9.93355e-17
1 === rate=9.86754e-33, T=0.224394, TIT=0.448788, IT=0.5
1 finished solveLinearSystemImpl_ 1
1 converged value: 1 7 1 let s go get convergedRemote
1 to  comm_.min(converged) 
0  before v = A * y 
0  before  alpha = rho_new / < rt, v > 
0  before alpha = rho_new / h; 
0  before apply first correction to x 
0  before r = r - alpha*v 
0  before test stop criteria 
0 istlsolver::printOutput 
  0.5      1.27477e-27      9.93355e-17
0 === rate=9.86754e-33, T=0.0299106, TIT=0.0598212, IT=0.5
4  before alpha = rho_new / h; 
4  before apply first correction to x 
4  before r = r - alpha*v 
4  before test stop criteria 
4 istlsolver::printOutput 
  0.5      1.27477e-27      9.93355e-17
4 === rate=9.86754e-33, T=0.2244, TIT=0.448799, IT=0.5
4 finished solveLinearSystemImpl_ 1
4 converged value: 1 7 1 let s go get convergedRemote
4 to  comm_.min(converged) 
3 did  comm_.min(converged) convergedRemote: 1
3 final convergedRemote: 1
3 Update: x^(k+1) = x^k - deltax^k 
3 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
3 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
3 Newton iteration  2 done3 , maximum relative shift = 4.237e-04
3 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
1 did  comm_.min(converged) convergedRemote: 1
1 final convergedRemote: 1
1 Update: x^(k+1) = x^k - deltax^k 
1 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
1 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
1 Newton iteration  2 done1 , maximum relative shift = 4.237e-04
1 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
0 finished solveLinearSystemImpl_ 1
0 converged value: 1 7 1 let s go get convergedRemote
0 to  comm_.min(converged) 
0 did  comm_.min(converged) convergedRemote: 1
0 final convergedRemote: 1
0 Update: x^(k+1) = x^k - deltax^k 
0 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
0 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
0 Newton iteration  2 done0 , maximum relative shift = 4.237e-04
0 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
4 did  comm_.min(converged) convergedRemote: 1
4 final convergedRemote: 1
4 Update: x^(k+1) = x^k - deltax^k 
4 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
4 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
4 Newton iteration  2 done4 , maximum relative shift = 4.237e-04
4 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
5 did  comm_.min(converged) convergedRemote: 1
5 final convergedRemote: 1
5 Update: x^(k+1) = x^k - deltax^k 
5 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
5 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
5 Newton iteration  2 done5 , maximum relative shift = 4.237e-04
5 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
6 did  comm_.min(converged) convergedRemote: 1
6 final convergedRemote: 1
6 Update: x^(k+1) = x^k - deltax^k 
6 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
6 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
6 Newton iteration  2 done6 , maximum relative shift = 4.237e-04
6 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
2 did  comm_.min(converged) convergedRemote: 1
2 final convergedRemote: 1
2 Update: x^(k+1) = x^k - deltax^k 
2 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
2 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
2 Newton iteration  2 done2 , maximum relative shift = 4.237e-04
2 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
5 Solve: M deltax^k = r 
5 Newton::solveLinearSystem : entering the try section 2
5 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
5 solveLinearSystemImpl_, before converged = ls.solve 
5 7 amgbackend::solve isParallel 1
6 Solve: M deltax^k = r 
6 Newton::solveLinearSystem : entering the try section 2
6 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
6 solveLinearSystemImpl_, before converged = ls.solve 
6 7 amgbackend::solve isParallel 1
0 Solve: M deltax^k = r 
0 Newton::solveLinearSystem : entering the try section 2
0 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
0 solveLinearSystemImpl_, before converged = ls.solve 
0 7 amgbackend::solve isParallel 1
3 Solve: M deltax^k = r 
3 Newton::solveLinearSystem : entering the try section 2
3 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
3 solveLinearSystemImpl_, before converged = ls.solve 
3 7 amgbackend::solve isParallel 1
1 Solve: M deltax^k = r 
1 Newton::solveLinearSystem : entering the try section 2
1 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
1 solveLinearSystemImpl_, before converged = ls.solve 
1 7 amgbackend::solve isParallel 1
2 Solve: M deltax^k = r 
2 Newton::solveLinearSystem : entering the try section 2
2 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
2 solveLinearSystemImpl_, before converged = ls.solve 
2 7 amgbackend::solve isParallel 1
4 Solve: M deltax^k = r 
4 Newton::solveLinearSystem : entering the try section 2
4 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
4 solveLinearSystemImpl_, before converged = ls.solve 
4 7 amgbackend::solve isParallel 1
2 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
2 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
1 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
1 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
5 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
5 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
6 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
6 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
0 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
0 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 7
3 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
3 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
4 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
4 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 7
3 7 before Dune::BiCGSTABSolver 
3 Dune::BiCGSTABSolver.apply(x, b, result_) 
3 BiCGSTABSolver::apply 
3 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
2 7 before Dune::BiCGSTABSolver 
2 Dune::BiCGSTABSolver.apply(x, b, result_) 
2 BiCGSTABSolver::apply 
2 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
1 7 before Dune::BiCGSTABSolver 
1 Dune::BiCGSTABSolver.apply(x, b, result_) 
1 BiCGSTABSolver::apply 
1 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
5 7 before Dune::BiCGSTABSolver 
5 Dune::BiCGSTABSolver.apply(x, b, result_) 
5 BiCGSTABSolver::apply 
5 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
6 7 before Dune::BiCGSTABSolver 
6 Dune::BiCGSTABSolver.apply(x, b, result_) 
6 BiCGSTABSolver::apply 
6 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
4 7 before Dune::BiCGSTABSolver 
4 Dune::BiCGSTABSolver.apply(x, b, result_) 
4 BiCGSTABSolver::apply 
4 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
Using a direct coarse solver (SuperLU)
Building hierarchy of 1 levels (inclusive coarse solver) took 0.200542 seconds.
0 7 before Dune::BiCGSTABSolver 
0 Dune::BiCGSTABSolver.apply(x, b, result_) 
4 === BiCGSTABSolver
 Iter          Defect            Rate
    0      3.58652e-13
4  (it = 0.5; it < _maxit; it+=.5) 0.5 250
4  before p = r 
4  before  y = W^-1 * p 
4  before _prec->apply(y,p); 0.5 250
4 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
1 === BiCGSTABSolver
 Iter          Defect            Rate
    0      3.58652e-13
1  (it = 0.5; it < _maxit; it+=.5) 0.5 250
1  before p = r 
1  before  y = W^-1 * p 
1  before _prec->apply(y,p); 0.5 250
1 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
3 === BiCGSTABSolver
 Iter          Defect            Rate
    0      3.58652e-13
3  (it = 0.5; it < _maxit; it+=.5) 0.5 250
3  before p = r 
3  before  y = W^-1 * p 
3  before _prec->apply(y,p); 0.5 250
3 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
2 === BiCGSTABSolver
 Iter          Defect            Rate
    0      3.58652e-13
2  (it = 0.5; it < _maxit; it+=.5) 0.5 250
2  before p = r 
2  before  y = W^-1 * p 
2  before _prec->apply(y,p); 0.5 250
2 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
5 === BiCGSTABSolver
 Iter          Defect            Rate
    0      3.58652e-13
5  (it = 0.5; it < _maxit; it+=.5) 0.5 250
5  before p = r 
5  before  y = W^-1 * p 
5  before _prec->apply(y,p); 0.5 250
5 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
6 === BiCGSTABSolver
 Iter          Defect            Rate
    0      3.58652e-13
6  (it = 0.5; it < _maxit; it+=.5) 0.5 250
6  before p = r 
6  before  y = W^-1 * p 
6  before _prec->apply(y,p); 0.5 250
6 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
0 BiCGSTABSolver::apply 
0 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
0 === BiCGSTABSolver
 Iter          Defect            Rate
    0      3.58652e-13
0  (it = 0.5; it < _maxit; it+=.5) 0.5 250
0  before p = r 
0  before  y = W^-1 * p 
0  before _prec->apply(y,p); 0.5 250
0 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 7
0 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
2 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
2 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
5 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
5 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
6 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
6 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
4 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
4 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
1 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
3 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
1 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
3 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
0 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
0 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
6  before v = A * y 
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
5  before v = A * y 
5  before  alpha = rho_new / < rt, v > 
5  before alpha = rho_new / h; 
5  before apply first correction to x 
5  before r = r - alpha*v 
5  before test stop criteria 
5 istlsolver::printOutput 
  0.5      1.61158e-30      4.49343e-18
5 === rate=2.01909e-35, T=0.229682, TIT=0.459363, IT=0.5
5 finished solveLinearSystemImpl_ 1
5 converged value: 1 7 1 let s go get convergedRemote
5 to  comm_.min(converged) 
6  before  alpha = rho_new / < rt, v > 
6  before alpha = rho_new / h; 
6  before apply first correction to x 
6  before r = r - alpha*v 
6  before test stop criteria 
6 istlsolver::printOutput 
  0.5      1.61158e-30      4.49343e-18
6 === rate=2.01909e-35, T=0.229626, TIT=0.459252, IT=0.5
6 finished solveLinearSystemImpl_ 1
6 converged value: 1 7 1 let s go get convergedRemote
6 to  comm_.min(converged) 
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
3  before v = A * y 
3  before  alpha = rho_new / < rt, v > 
3  before alpha = rho_new / h; 
3  before apply first correction to x 
3  before r = r - alpha*v 
3  before test stop criteria 
3 istlsolver::printOutput 
  0.5      1.61158e-30      4.49343e-18
3 === rate=2.01909e-35, T=0.229682, TIT=0.459365, IT=0.5
3 finished solveLinearSystemImpl_ 1
3 converged value: 1 7 1 let s go get convergedRemote
3 to  comm_.min(converged) 
0  before v = A * y 
0  before  alpha = rho_new / < rt, v > 
0  before alpha = rho_new / h; 
0  before apply first correction to x 
0  before r = r - alpha*v 
0  before test stop criteria 
0 istlsolver::printOutput 
  0.5      1.61158e-30      4.49343e-18
0 === rate=2.01909e-35, T=0.0349676, TIT=0.0699352, IT=0.5
4  before v = A * y 
4  before  alpha = rho_new / < rt, v > 
4  before alpha = rho_new / h; 
4  before apply first correction to x 
4  before r = r - alpha*v 
4  before test stop criteria 
4 istlsolver::printOutput 
  0.5      1.61158e-30      4.49343e-18
4 === rate=2.01909e-35, T=0.229623, TIT=0.459246, IT=0.5
4 finished solveLinearSystemImpl_ 1
4 converged value: 1 7 1 let s go get convergedRemote
4 to  comm_.min(converged) 
2  before v = A * y 
2  before  alpha = rho_new / < rt, v > 
2  before alpha = rho_new / h; 
2  before apply first correction to x 
2  before r = r - alpha*v 
2  before test stop criteria 
2 istlsolver::printOutput 
  0.5      1.61158e-30      4.49343e-18
2 === rate=2.01909e-35, T=0.229619, TIT=0.459238, IT=0.5
2 finished solveLinearSystemImpl_ 1
2 converged value: 1 7 1 let s go get convergedRemote
2 to  comm_.min(converged) 
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
1  before v = A * y 
1  before  alpha = rho_new / < rt, v > 
1  before alpha = rho_new / h; 
1  before apply first correction to x 
1  before r = r - alpha*v 
1  before test stop criteria 
1 istlsolver::printOutput 
  0.5      1.61158e-30      4.49343e-18
1 === rate=2.01909e-35, T=0.229683, TIT=0.459365, IT=0.5
1 finished solveLinearSystemImpl_ 1
1 converged value: 1 7 1 let s go get convergedRemote
1 to  comm_.min(converged) 
0 finished solveLinearSystemImpl_ 1
0 converged value: 1 7 1 let s go get convergedRemote
0 to  comm_.min(converged) 
0 did  comm_.min(converged) convergedRemote: 1
0 final convergedRemote: 1
0 Update: x^(k+1) = x^k - deltax^k 
0 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
0 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
0 Newton iteration  3 done0 , maximum relative shift = 1.213e-05
0 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
4 did  comm_.min(converged) convergedRemote: 1
4 final convergedRemote: 1
4 Update: x^(k+1) = x^k - deltax^k 
4 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
4 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
4 Newton iteration  3 done4 , maximum relative shift = 1.213e-05
4 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
2 did  comm_.min(converged) convergedRemote: 1
2 final convergedRemote: 1
2 Update: x^(k+1) = x^k - deltax^k 
2 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
2 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
2 Newton iteration  3 done2 , maximum relative shift = 1.213e-05
2 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
5 did  comm_.min(converged) convergedRemote: 1
5 final convergedRemote: 1
5 Update: x^(k+1) = x^k - deltax^k 
5 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
5 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
5 Newton iteration  3 done5 , maximum relative shift = 1.213e-05
5 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
1 did  comm_.min(converged) convergedRemote: 1
1 final convergedRemote: 1
1 Update: x^(k+1) = x^k - deltax^k 
1 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
1 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
1 Newton iteration  3 done1 , maximum relative shift = 1.213e-05
1 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
6 did  comm_.min(converged) convergedRemote: 1
6 final convergedRemote: 1
6 Update: x^(k+1) = x^k - deltax^k 
6 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
6 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
6 Newton iteration  3 done6 , maximum relative shift = 1.213e-05
6 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
3 did  comm_.min(converged) convergedRemote: 1
3 final convergedRemote: 1
3 Update: x^(k+1) = x^k - deltax^k 
3 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
3 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
3 Newton iteration  3 done3 , maximum relative shift = 1.213e-05
3 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
4 Solve: M deltax^k = r 
4 Newton::solveLinearSystem : entering the try section 3
4 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
4 solveLinearSystemImpl_, before converged = ls.solve 
4 7 amgbackend::solve isParallel 1
2 Solve: M deltax^k = r 
2 Newton::solveLinearSystem : entering the try section 3
2 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
2 solveLinearSystemImpl_, before converged = ls.solve 
2 7 amgbackend::solve isParallel 1
3 Solve: M deltax^k = r 
3 Newton::solveLinearSystem : entering the try section 3
3 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
3 solveLinearSystemImpl_, before converged = ls.solve 
3 7 amgbackend::solve isParallel 1
1 Solve: M deltax^k = r 
1 Newton::solveLinearSystem : entering the try section 3
1 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
1 solveLinearSystemImpl_, before converged = ls.solve 
1 7 amgbackend::solve isParallel 1
6 Solve: M deltax^k = r 
6 Newton::solveLinearSystem : entering the try section 3
6 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
6 solveLinearSystemImpl_, before converged = ls.solve 
6 7 amgbackend::solve isParallel 1
5 Solve: M deltax^k = r 
5 Newton::solveLinearSystem : entering the try section 3
5 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
5 solveLinearSystemImpl_, before converged = ls.solve 
5 7 amgbackend::solve isParallel 1
0 Solve: M deltax^k = r 
0 Newton::solveLinearSystem : entering the try section 3
0 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
0 solveLinearSystemImpl_, before converged = ls.solve 
0 7 amgbackend::solve isParallel 1
0 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
0 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 7
3 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
3 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
1 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
1 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
6 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
6 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
5 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
5 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
2 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
2 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
4 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
4 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 7
5 7 before Dune::BiCGSTABSolver 
5 Dune::BiCGSTABSolver.apply(x, b, result_) 
5 BiCGSTABSolver::apply 
5 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
2 7 before Dune::BiCGSTABSolver 
2 Dune::BiCGSTABSolver.apply(x, b, result_) 
2 BiCGSTABSolver::apply 
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 7
4 7 before Dune::BiCGSTABSolver 
4 Dune::BiCGSTABSolver.apply(x, b, result_) 
4 BiCGSTABSolver::apply 
3 7 before Dune::BiCGSTABSolver 
3 Dune::BiCGSTABSolver.apply(x, b, result_) 
3 BiCGSTABSolver::apply 
3 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
1 7 before Dune::BiCGSTABSolver 
1 Dune::BiCGSTABSolver.apply(x, b, result_) 
1 BiCGSTABSolver::apply 
1 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
6 7 before Dune::BiCGSTABSolver 
6 Dune::BiCGSTABSolver.apply(x, b, result_) 
6 BiCGSTABSolver::apply 
6 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
4 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
2 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
Using a direct coarse solver (SuperLU)
Building hierarchy of 1 levels (inclusive coarse solver) took 0.201244 seconds.
0 7 before Dune::BiCGSTABSolver 
0 Dune::BiCGSTABSolver.apply(x, b, result_) 
0 BiCGSTABSolver::apply 
4 === BiCGSTABSolver
 Iter          Defect            Rate
    0      1.01146e-14
4  (it = 0.5; it < _maxit; it+=.5) 0.5 250
4  before p = r 
4  before  y = W^-1 * p 
4  before _prec->apply(y,p); 0.5 250
4 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
3 === BiCGSTABSolver
 Iter          Defect            Rate
    0      1.01146e-14
3  (it = 0.5; it < _maxit; it+=.5) 0.5 250
3  before p = r 
3  before  y = W^-1 * p 
3  before _prec->apply(y,p); 0.5 250
3 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
1 === BiCGSTABSolver
 Iter          Defect            Rate
    0      1.01146e-14
1  (it = 0.5; it < _maxit; it+=.5) 0.5 250
1  before p = r 
1  before  y = W^-1 * p 
1  before _prec->apply(y,p); 0.5 250
1 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
5 === BiCGSTABSolver
 Iter          Defect            Rate
    0      1.01146e-14
5  (it = 0.5; it < _maxit; it+=.5) 0.5 250
5  before p = r 
5  before  y = W^-1 * p 
5  before _prec->apply(y,p); 0.5 250
5 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
0 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
0 === BiCGSTABSolver
 Iter          Defect            Rate
    0      1.01146e-14
0  (it = 0.5; it < _maxit; it+=.5) 0.5 250
0  before p = r 
0  before  y = W^-1 * p 
0  before _prec->apply(y,p); 0.5 250
0 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 7
2 === BiCGSTABSolver
 Iter          Defect            Rate
    0      1.01146e-14
2  (it = 0.5; it < _maxit; it+=.5) 0.5 250
2  before p = r 
2  before  y = W^-1 * p 
2  before _prec->apply(y,p); 0.5 250
2 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
6 === BiCGSTABSolver
 Iter          Defect            Rate
    0      1.01146e-14
6  (it = 0.5; it < _maxit; it+=.5) 0.5 250
6  before p = r 
6  before  y = W^-1 * p 
6  before _prec->apply(y,p); 0.5 250
6 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 7
0 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
1 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
6 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
6 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
2 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
2 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
5 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
4 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
4 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
3 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
3 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
0 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
0 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
0  before v = A * y 
6  before v = A * y 
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
2  before v = A * y 
5 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
4  before v = A * y 
1 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
1  before v = A * y 
1  before  alpha = rho_new / < rt, v > 
1  before alpha = rho_new / h; 
1  before apply first correction to x 
1  before r = r - alpha*v 
1  before test stop criteria 
1 istlsolver::printOutput 
  0.5       6.4481e-33      6.37504e-19
1 === rate=4.06411e-37, T=0.224971, TIT=0.449941, IT=0.5
1 finished solveLinearSystemImpl_ 1
1 converged value: 1 7 1 let s go get convergedRemote
1 to  comm_.min(converged) 
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
3  before v = A * y 
3  before  alpha = rho_new / < rt, v > 
3  before alpha = rho_new / h; 
3  before apply first correction to x 
3  before r = r - alpha*v 
3  before test stop criteria 
3 istlsolver::printOutput 
  0.5       6.4481e-33      6.37504e-19
3 === rate=4.06411e-37, T=0.224968, TIT=0.449936, IT=0.5
3 finished solveLinearSystemImpl_ 1
3 converged value: 1 7 1 let s go get convergedRemote
3 to  comm_.min(converged) 
0  before  alpha = rho_new / < rt, v > 
0  before alpha = rho_new / h; 
0  before apply first correction to x 
0  before r = r - alpha*v 
0  before test stop criteria 
0 istlsolver::printOutput 
  0.5       6.4481e-33      6.37504e-19
0 === rate=4.06411e-37, T=0.0300223, TIT=0.0600445, IT=0.5
6  before  alpha = rho_new / < rt, v > 
6  before alpha = rho_new / h; 
6  before apply first correction to x 
6  before r = r - alpha*v 
6  before test stop criteria 
6 istlsolver::printOutput 
  0.5       6.4481e-33      6.37504e-19
6 === rate=4.06411e-37, T=0.224878, TIT=0.449756, IT=0.5
6 finished solveLinearSystemImpl_ 1
6 converged value: 1 7 1 let s go get convergedRemote
6 to  comm_.min(converged) 
2  before  alpha = rho_new / < rt, v > 
2  before alpha = rho_new / h; 
2  before apply first correction to x 
2  before r = r - alpha*v 
2  before test stop criteria 
2 istlsolver::printOutput 
  0.5       6.4481e-33      6.37504e-19
2 === rate=4.06411e-37, T=0.224869, TIT=0.449739, IT=0.5
2 finished solveLinearSystemImpl_ 1
2 converged value: 1 7 1 let s go get convergedRemote
2 to  comm_.min(converged) 
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
5  before v = A * y 
5  before  alpha = rho_new / < rt, v > 
5  before alpha = rho_new / h; 
5  before apply first correction to x 
5  before r = r - alpha*v 
5  before test stop criteria 
5 istlsolver::printOutput 
  0.5       6.4481e-33      6.37504e-19
5 === rate=4.06411e-37, T=0.224968, TIT=0.449935, IT=0.5
5 finished solveLinearSystemImpl_ 1
5 converged value: 1 7 1 let s go get convergedRemote
5 to  comm_.min(converged) 
4  before  alpha = rho_new / < rt, v > 
4  before alpha = rho_new / h; 
4  before apply first correction to x 
4  before r = r - alpha*v 
4  before test stop criteria 
4 istlsolver::printOutput 
  0.5       6.4481e-33      6.37504e-19
4 === rate=4.06411e-37, T=0.224876, TIT=0.449751, IT=0.5
4 finished solveLinearSystemImpl_ 1
4 converged value: 1 7 1 let s go get convergedRemote
4 to  comm_.min(converged) 
3 did  comm_.min(converged) convergedRemote: 1
3 final convergedRemote: 1
3 Update: x^(k+1) = x^k - deltax^k 
3 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
3 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
3 Newton iteration  4 done3 , maximum relative shift = 3.524e-07
3 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
0 finished solveLinearSystemImpl_ 1
0 converged value: 1 7 1 let s go get convergedRemote
0 to  comm_.min(converged) 
0 did  comm_.min(converged) convergedRemote: 1
0 final convergedRemote: 1
0 Update: x^(k+1) = x^k - deltax^k 
0 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
0 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
0 Newton iteration  4 done0 , maximum relative shift = 3.524e-07
0 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
6 did  comm_.min(converged) convergedRemote: 1
6 final convergedRemote: 1
6 Update: x^(k+1) = x^k - deltax^k 
6 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
6 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
6 Newton iteration  4 done6 , maximum relative shift = 3.524e-07
6 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
2 did  comm_.min(converged) convergedRemote: 1
2 final convergedRemote: 1
2 Update: x^(k+1) = x^k - deltax^k 
2 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
2 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
2 Newton iteration  4 done2 , maximum relative shift = 3.524e-07
2 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
5 did  comm_.min(converged) convergedRemote: 1
5 final convergedRemote: 1
5 Update: x^(k+1) = x^k - deltax^k 
5 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
5 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
5 Newton iteration  4 done5 , maximum relative shift = 3.524e-07
5 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
4 did  comm_.min(converged) convergedRemote: 1
4 final convergedRemote: 1
4 Update: x^(k+1) = x^k - deltax^k 
4 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
4 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
4 Newton iteration  4 done4 , maximum relative shift = 3.524e-07
4 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
1 did  comm_.min(converged) convergedRemote: 1
1 final convergedRemote: 1
1 Update: x^(k+1) = x^k - deltax^k 
1 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
1 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
1 Newton iteration  4 done1 , maximum relative shift = 3.524e-07
1 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
1 Solve: M deltax^k = r 
1 Newton::solveLinearSystem : entering the try section 4
1 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
1 solveLinearSystemImpl_, before converged = ls.solve 
1 7 amgbackend::solve isParallel 1
4 Solve: M deltax^k = r 
4 Newton::solveLinearSystem : entering the try section 4
4 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
4 solveLinearSystemImpl_, before converged = ls.solve 
4 7 amgbackend::solve isParallel 1
5 Solve: M deltax^k = r 
5 Newton::solveLinearSystem : entering the try section 4
5 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
5 solveLinearSystemImpl_, before converged = ls.solve 
5 7 amgbackend::solve isParallel 1
3 Solve: M deltax^k = r 
3 Newton::solveLinearSystem : entering the try section 4
3 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
3 solveLinearSystemImpl_, before converged = ls.solve 
3 7 amgbackend::solve isParallel 1
0 Solve: M deltax^k = r 
0 Newton::solveLinearSystem : entering the try section 4
0 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
0 solveLinearSystemImpl_, before converged = ls.solve 
0 7 amgbackend::solve isParallel 1
6 Solve: M deltax^k = r 
6 Newton::solveLinearSystem : entering the try section 4
6 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
6 solveLinearSystemImpl_, before converged = ls.solve 
6 7 amgbackend::solve isParallel 1
2 Solve: M deltax^k = r 
2 Newton::solveLinearSystem : entering the try section 4
2 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
2 solveLinearSystemImpl_, before converged = ls.solve 
2 7 amgbackend::solve isParallel 1
0 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
0 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 7
6 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
6 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
2 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
2 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
1 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
1 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
4 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
4 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
5 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
5 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
3 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
3 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 7
2 7 before Dune::BiCGSTABSolver 
2 Dune::BiCGSTABSolver.apply(x, b, result_) 
2 BiCGSTABSolver::apply 
2 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
1 7 before Dune::BiCGSTABSolver 
1 Dune::BiCGSTABSolver.apply(x, b, result_) 
1 BiCGSTABSolver::apply 
1 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
4 7 before Dune::BiCGSTABSolver 
4 Dune::BiCGSTABSolver.apply(x, b, result_) 
4 BiCGSTABSolver::apply 
4 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
5 7 before Dune::BiCGSTABSolver 
5 Dune::BiCGSTABSolver.apply(x, b, result_) 
5 BiCGSTABSolver::apply 
5 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
6 7 before Dune::BiCGSTABSolver 
6 Dune::BiCGSTABSolver.apply(x, b, result_) 
6 BiCGSTABSolver::apply 
6 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
3 7 before Dune::BiCGSTABSolver 
3 Dune::BiCGSTABSolver.apply(x, b, result_) 
3 BiCGSTABSolver::apply 
3 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 7
Using a direct coarse solver (SuperLU)
Building hierarchy of 1 levels (inclusive coarse solver) took 0.201306 seconds.
0 7 before Dune::BiCGSTABSolver 
0 Dune::BiCGSTABSolver.apply(x, b, result_) 
3 === BiCGSTABSolver
 Iter          Defect            Rate
    0      1.22169e-17
3  (it = 0.5; it < _maxit; it+=.5) 0.5 250
3  before p = r 
3  before  y = W^-1 * p 
3  before _prec->apply(y,p); 0.5 250
3 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
2 === BiCGSTABSolver
 Iter          Defect            Rate
    0      1.22169e-17
2  (it = 0.5; it < _maxit; it+=.5) 0.5 250
2  before p = r 
2  before  y = W^-1 * p 
2  before _prec->apply(y,p); 0.5 250
2 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
1 === BiCGSTABSolver
 Iter          Defect            Rate
    0      1.22169e-17
1  (it = 0.5; it < _maxit; it+=.5) 0.5 250
1  before p = r 
1  before  y = W^-1 * p 
1  before _prec->apply(y,p); 0.5 250
1 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
4 === BiCGSTABSolver
 Iter          Defect            Rate
    0      1.22169e-17
4  (it = 0.5; it < _maxit; it+=.5) 0.5 250
4  before p = r 
4  before  y = W^-1 * p 
4  before _prec->apply(y,p); 0.5 250
4 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
5 === BiCGSTABSolver
 Iter          Defect            Rate
    0      1.22169e-17
5  (it = 0.5; it < _maxit; it+=.5) 0.5 250
5  before p = r 
5  before  y = W^-1 * p 
5  before _prec->apply(y,p); 0.5 250
5 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
6 === BiCGSTABSolver
 Iter          Defect            Rate
    0      1.22169e-17
6  (it = 0.5; it < _maxit; it+=.5) 0.5 250
6  before p = r 
6  before  y = W^-1 * p 
6  before _prec->apply(y,p); 0.5 250
6 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
0 BiCGSTABSolver::apply 
0 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
0 === BiCGSTABSolver
 Iter          Defect            Rate
    0      1.22169e-17
0  (it = 0.5; it < _maxit; it+=.5) 0.5 250
0  before p = r 
0  before  y = W^-1 * p 
0  before _prec->apply(y,p); 0.5 250
0 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 7
0 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
3 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
2 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
2 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
1 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
4 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
4 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
5 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
6 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
6 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
5 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
5  before v = A * y 
3 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
3  before v = A * y 
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
2  before v = A * y 
1 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
1  before v = A * y 
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
4  before v = A * y 
6  before v = A * y 
0 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
0 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
0  before v = A * y 
4  before  alpha = rho_new / < rt, v > 
4  before alpha = rho_new / h; 
4  before apply first correction to x 
4  before r = r - alpha*v 
4  before test stop criteria 
4 istlsolver::printOutput 
  0.5      1.54906e-33      1.26796e-16
4 === rate=1.60773e-32, T=0.224824, TIT=0.449647, IT=0.5
4 finished solveLinearSystemImpl_ 1
4 converged value: 1 7 1 let s go get convergedRemote
4 to  comm_.min(converged) 
4 did  comm_.min(converged) convergedRemote: 1
4 final convergedRemote: 1
4 Update: x^(k+1) = x^k - deltax^k 
4 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
5  before  alpha = rho_new / < rt, v > 
5  before alpha = rho_new / h; 
5  before apply first correction to x 
5  before r = r - alpha*v 
5  before test stop criteria 
5 istlsolver::printOutput 
  0.5      1.54906e-33      1.26796e-16
5 === rate=1.60773e-32, T=0.224746, TIT=0.449492, IT=0.5
5 finished solveLinearSystemImpl_ 1
5 converged value: 1 7 1 let s go get convergedRemote
5 to  comm_.min(converged) 
5 did  comm_.min(converged) convergedRemote: 1
5 final convergedRemote: 1
5 Update: x^(k+1) = x^k - deltax^k 
5 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
5 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
5 Newton iteration  5 done5 , maximum relative shift = 4.238e-10
5 Assemble/solve/update time: 0.370252(24.0312%)/1.16965(75.9159%)/0.00081599(0.0529618%)
5 gridVariables->advanceTimeStep
5 before, nonLinearSolver->suggestTimeStepSize, current time: 213.703 simTime 10800
5 before assembler->setPreviousSolution, ddt: 28.3333
5 nonLinearSolver->solve
3  before  alpha = rho_new / < rt, v > 
3  before alpha = rho_new / h; 
3  before apply first correction to x 
3  before r = r - alpha*v 
3  before test stop criteria 
3 istlsolver::printOutput 
  0.5      1.54906e-33      1.26796e-16
3 === rate=1.60773e-32, T=0.224749, TIT=0.449498, IT=0.5
3 finished solveLinearSystemImpl_ 1
3 converged value: 1 7 1 let s go get convergedRemote
3 to  comm_.min(converged) 
3 did  comm_.min(converged) convergedRemote: 1
3 final convergedRemote: 1
3 Update: x^(k+1) = x^k - deltax^k 
3 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
3 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
3 Newton iteration  5 done3 , maximum relative shift = 4.238e-10
3 Assemble/solve/update time: 0.370221(24.0295%)/1.16966(75.9177%)/0.00081283(0.0527574%)
3 gridVariables->advanceTimeStep
3 before, nonLinearSolver->suggestTimeStepSize, current time: 213.703 simTime 10800
3 before assembler->setPreviousSolution, ddt: 28.3333
3 nonLinearSolver->solve
2  before  alpha = rho_new / < rt, v > 
2  before alpha = rho_new / h; 
2  before apply first correction to x 
2  before r = r - alpha*v 
2  before test stop criteria 
2 istlsolver::printOutput 
  0.5      1.54906e-33      1.26796e-16
2 === rate=1.60773e-32, T=0.224817, TIT=0.449635, IT=0.5
2 finished solveLinearSystemImpl_ 1
2 converged value: 1 7 1 let s go get convergedRemote
2 to  comm_.min(converged) 
2 did  comm_.min(converged) convergedRemote: 1
2 final convergedRemote: 1
2 Update: x^(k+1) = x^k - deltax^k 
2 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
2 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
2 Newton iteration  5 done2 , maximum relative shift = 4.238e-10
1  before  alpha = rho_new / < rt, v > 
1  before alpha = rho_new / h; 
1  before apply first correction to x 
1  before r = r - alpha*v 
1  before test stop criteria 
1 istlsolver::printOutput 
  0.5      1.54906e-33      1.26796e-16
1 === rate=1.60773e-32, T=0.224747, TIT=0.449493, IT=0.5
1 finished solveLinearSystemImpl_ 1
1 converged value: 1 7 1 let s go get convergedRemote
1 to  comm_.min(converged) 
1 did  comm_.min(converged) convergedRemote: 1
1 final convergedRemote: 1
1 Update: x^(k+1) = x^k - deltax^k 
1 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
1 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
1 Newton iteration  5 done1 , maximum relative shift = 4.238e-10
1 Assemble/solve/update time: 0.370233(24.0302%)/1.16965(75.9167%)/0.00081885(0.0531479%)
1 gridVariables->advanceTimeStep
1 before, nonLinearSolver->suggestTimeStepSize, current time: 213.703 simTime 10800
1 before assembler->setPreviousSolution, ddt: 28.3333
1 nonLinearSolver->solve
0  before  alpha = rho_new / < rt, v > 
0  before alpha = rho_new / h; 
0  before apply first correction to x 
0  before r = r - alpha*v 
0  before test stop criteria 
0 istlsolver::printOutput 
  0.5      1.54906e-33      1.26796e-16
0 === rate=1.60773e-32, T=0.0298112, TIT=0.0596224, IT=0.5
0 finished solveLinearSystemImpl_ 1
0 converged value: 1 7 1 let s go get convergedRemote
0 to  comm_.min(converged) 
0 did  comm_.min(converged) convergedRemote: 1
0 final convergedRemote: 1
0 Update: x^(k+1) = x^k - deltax^k 
0 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
0 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
6  before  alpha = rho_new / < rt, v > 
6  before alpha = rho_new / h; 
6  before apply first correction to x 
6  before r = r - alpha*v 
6  before test stop criteria 
6 istlsolver::printOutput 
  0.5      1.54906e-33      1.26796e-16
6 === rate=1.60773e-32, T=0.224827, TIT=0.449653, IT=0.5
6 finished solveLinearSystemImpl_ 1
6 converged value: 1 7 1 let s go get convergedRemote
6 to  comm_.min(converged) 
6 did  comm_.min(converged) convergedRemote: 1
6 final convergedRemote: 1
6 Update: x^(k+1) = x^k - deltax^k 
6 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
6 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
6 Newton iteration  5 done6 , maximum relative shift = 4.238e-10
6 Assemble/solve/update time: 0.369815(24.0048%)/1.17012(75.953%)/0.00064977(0.0421767%)
6 gridVariables->advanceTimeStep
2 Assemble/solve/update time: 0.369664(23.9974%)/1.17012(75.9606%)/0.00064755(0.0420368%)
2 gridVariables->advanceTimeStep
2 before, nonLinearSolver->suggestTimeStepSize, current time: 213.703 simTime 10800
2 before assembler->setPreviousSolution, ddt: 28.3333
2 nonLinearSolver->solve
2 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
6 before, nonLinearSolver->suggestTimeStepSize, current time: 213.703 simTime 10800
6 before assembler->setPreviousSolution, ddt: 28.3333
6 nonLinearSolver->solve
6 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
0 Newton iteration  5 done0 , maximum relative shift = 4.238e-10
0 Assemble/solve/update time: 0.36954(23.9919%)/1.17009(75.9663%)/0.000643569(0.0417829%)
0 gridVariables->advanceTimeStep
0 before, nonLinearSolver->suggestTimeStepSize, current time: 213.703 simTime 10800
0 before assembler->setPreviousSolution, ddt: 28.3333
0 nonLinearSolver->solve
0 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
1 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
4 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
4 Newton iteration  5 done4 , maximum relative shift = 4.238e-10
4 Assemble/solve/update time: 0.369836(24.0062%)/1.1701(75.9516%)/0.000650429(0.0422196%)
4 gridVariables->advanceTimeStep
4 before, nonLinearSolver->suggestTimeStepSize, current time: 213.703 simTime 10800
4 before assembler->setPreviousSolution, ddt: 28.3333
4 nonLinearSolver->solve
4 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
5 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
3 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
6 Solve: M deltax^k = r 
6 Newton::solveLinearSystem : entering the try section 0
6to  comm_.sum(norm2) 
0 Solve: M deltax^k = r 
0 Newton::solveLinearSystem : entering the try section 0
0to  comm_.sum(norm2) 
1 Solve: M deltax^k = r 
1 Newton::solveLinearSystem : entering the try section 0
1to  comm_.sum(norm2) 
4 Solve: M deltax^k = r 
4 Newton::solveLinearSystem : entering the try section 0
4to  comm_.sum(norm2) 
5 Solve: M deltax^k = r 
5 Newton::solveLinearSystem : entering the try section 0
5to  comm_.sum(norm2) 
2 Solve: M deltax^k = r 
2 Newton::solveLinearSystem : entering the try section 0
2to  comm_.sum(norm2) 
3 Solve: M deltax^k = r 
3 Newton::solveLinearSystem : entering the try section 0
3to  comm_.sum(norm2) 
3did  comm_.sum(norm2) 
3 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
3 solveLinearSystemImpl_, before converged = ls.solve 
3 7 amgbackend::solve isParallel 1
2did  comm_.sum(norm2) 
2 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
2 solveLinearSystemImpl_, before converged = ls.solve 
2 7 amgbackend::solve isParallel 1
6did  comm_.sum(norm2) 
6 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
6 solveLinearSystemImpl_, before converged = ls.solve 
6 7 amgbackend::solve isParallel 1
0did  comm_.sum(norm2) 
0 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
0 solveLinearSystemImpl_, before converged = ls.solve 
0 7 amgbackend::solve isParallel 1
1did  comm_.sum(norm2) 
1 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
1 solveLinearSystemImpl_, before converged = ls.solve 
1 7 amgbackend::solve isParallel 1
4did  comm_.sum(norm2) 
4 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
4 solveLinearSystemImpl_, before converged = ls.solve 
4 7 amgbackend::solve isParallel 1
5did  comm_.sum(norm2) 
5 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
5 solveLinearSystemImpl_, before converged = ls.solve 
5 7 amgbackend::solve isParallel 1
0 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
0 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 7
1 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
1 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
4 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
4 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
5 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
5 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
3 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
3 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
2 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
2 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
6 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
6 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 7
1 7 before Dune::BiCGSTABSolver 
1 Dune::BiCGSTABSolver.apply(x, b, result_) 
1 BiCGSTABSolver::apply 
1 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 7
5 7 before Dune::BiCGSTABSolver 
5 Dune::BiCGSTABSolver.apply(x, b, result_) 
5 BiCGSTABSolver::apply 
5 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
3 7 before Dune::BiCGSTABSolver 
3 Dune::BiCGSTABSolver.apply(x, b, result_) 
3 BiCGSTABSolver::apply 
3 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
2 7 before Dune::BiCGSTABSolver 
2 Dune::BiCGSTABSolver.apply(x, b, result_) 
2 BiCGSTABSolver::apply 
2 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
4 7 before Dune::BiCGSTABSolver 
4 Dune::BiCGSTABSolver.apply(x, b, result_) 
4 BiCGSTABSolver::apply 
4 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
6 7 before Dune::BiCGSTABSolver 
6 Dune::BiCGSTABSolver.apply(x, b, result_) 
6 BiCGSTABSolver::apply 
6 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
Using a direct coarse solver (SuperLU)
Building hierarchy of 1 levels (inclusive coarse solver) took 0.202256 seconds.
0 7 before Dune::BiCGSTABSolver 
0 Dune::BiCGSTABSolver.apply(x, b, result_) 
1 === BiCGSTABSolver
 Iter          Defect            Rate
    0      4.48256e-09
1  (it = 0.5; it < _maxit; it+=.5) 0.5 250
1  before p = r 
1  before  y = W^-1 * p 
1  before _prec->apply(y,p); 0.5 250
1 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
0 BiCGSTABSolver::apply 
0 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
0 === BiCGSTABSolver
 Iter          Defect            Rate
    0      4.48256e-09
0  (it = 0.5; it < _maxit; it+=.5) 0.5 250
0  before p = r 
0  before  y = W^-1 * p 
0  before _prec->apply(y,p); 0.5 250
0 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 7
0 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
6 === BiCGSTABSolver
 Iter          Defect            Rate
    0      4.48256e-09
6  (it = 0.5; it < _maxit; it+=.5) 0.5 250
6  before p = r 
6  before  y = W^-1 * p 
6  before _prec->apply(y,p); 0.5 250
6 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
5 === BiCGSTABSolver
 Iter          Defect            Rate
    0      4.48256e-09
5  (it = 0.5; it < _maxit; it+=.5) 0.5 250
5  before p = r 
5  before  y = W^-1 * p 
5  before _prec->apply(y,p); 0.5 250
5 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
3 === BiCGSTABSolver
 Iter          Defect            Rate
    0      4.48256e-09
3  (it = 0.5; it < _maxit; it+=.5) 0.5 250
3  before p = r 
3  before  y = W^-1 * p 
3  before _prec->apply(y,p); 0.5 250
3 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
2 === BiCGSTABSolver
 Iter          Defect            Rate
    0      4.48256e-09
2  (it = 0.5; it < _maxit; it+=.5) 0.5 250
2  before p = r 
2  before  y = W^-1 * p 
2  before _prec->apply(y,p); 0.5 250
2 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
4 === BiCGSTABSolver
 Iter          Defect            Rate
    0      4.48256e-09
4  (it = 0.5; it < _maxit; it+=.5) 0.5 250
4  before p = r 
4  before  y = W^-1 * p 
4  before _prec->apply(y,p); 0.5 250
4 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
6 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
6 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
5 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
3 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
2 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
2 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
1 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
4 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
4 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
3 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
0 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
0 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
5 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
5  before v = A * y 
5  before  alpha = rho_new / < rt, v > 
0  before v = A * y 
0  before  alpha = rho_new / < rt, v > 
0  before alpha = rho_new / h; 
0  before apply first correction to x 
0  before r = r - alpha*v 
0  before test stop criteria 
0 istlsolver::printOutput 
  0.5      6.81807e-25      1.52102e-16
0 === rate=2.3135e-32, T=0.0297285, TIT=0.059457, IT=0.5
0 finished solveLinearSystemImpl_ 1
0 converged value: 1 7 1 let s go get convergedRemote
0 to  comm_.min(converged) 
0 did  comm_.min(converged) convergedRemote: 1
0 final convergedRemote: 1
0 Update: x^(k+1) = x^k - deltax^k 
0 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
0 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
2  before v = A * y 
2  before  alpha = rho_new / < rt, v > 
2  before alpha = rho_new / h; 
2  before apply first correction to x 
2  before r = r - alpha*v 
2  before test stop criteria 
2 istlsolver::printOutput 
  0.5      6.81807e-25      1.52102e-16
2 === rate=2.3135e-32, T=0.22577, TIT=0.451541, IT=0.5
2 finished solveLinearSystemImpl_ 1
2 converged value: 1 7 1 let s go get convergedRemote
2 to  comm_.min(converged) 
2 did  comm_.min(converged) convergedRemote: 1
2 final convergedRemote: 1
2 Update: x^(k+1) = x^k - deltax^k 
2 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
2 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
2 Newton iteration  1 done2 , maximum relative shift = 2.096e-02
2 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
3  before v = A * y 
3  before  alpha = rho_new / < rt, v > 
3  before alpha = rho_new / h; 
3  before apply first correction to x 
3  before r = r - alpha*v 
3  before test stop criteria 
3 istlsolver::printOutput 
  0.5      6.81807e-25      1.52102e-16
3 === rate=2.3135e-32, T=0.225786, TIT=0.451572, IT=0.5
3 finished solveLinearSystemImpl_ 1
3 converged value: 1 7 1 let s go get convergedRemote
3 to  comm_.min(converged) 
3 did  comm_.min(converged) convergedRemote: 1
3 final convergedRemote: 1
3 Update: x^(k+1) = x^k - deltax^k 
3 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
3 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
3 Newton iteration  1 done3 , maximum relative shift = 2.096e-02
3 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
5  before alpha = rho_new / h; 
5  before apply first correction to x 
5  before r = r - alpha*v 
5  before test stop criteria 
5 istlsolver::printOutput 
  0.5      6.81807e-25      1.52102e-16
5 === rate=2.3135e-32, T=0.225785, TIT=0.451569, IT=0.5
5 finished solveLinearSystemImpl_ 1
5 converged value: 1 7 1 let s go get convergedRemote
5 to  comm_.min(converged) 
5 did  comm_.min(converged) convergedRemote: 1
5 final convergedRemote: 1
5 Update: x^(k+1) = x^k - deltax^k 
5 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
5 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
5 Newton iteration  1 done5 , maximum relative shift = 2.096e-02
5 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
1 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
1  before v = A * y 
1  before  alpha = rho_new / < rt, v > 
1  before alpha = rho_new / h; 
1  before apply first correction to x 
1  before r = r - alpha*v 
1  before test stop criteria 
1 istlsolver::printOutput 
  0.5      6.81807e-25      1.52102e-16
1 === rate=2.3135e-32, T=0.225787, TIT=0.451574, IT=0.5
1 finished solveLinearSystemImpl_ 1
1 converged value: 1 7 1 let s go get convergedRemote
1 to  comm_.min(converged) 
1 did  comm_.min(converged) convergedRemote: 1
1 final convergedRemote: 1
1 Update: x^(k+1) = x^k - deltax^k 
1 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
1 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
1 Newton iteration  1 done1 , maximum relative shift = 2.096e-02
1 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
4  before v = A * y 
4  before  alpha = rho_new / < rt, v > 
4  before alpha = rho_new / h; 
4  before apply first correction to x 
4  before r = r - alpha*v 
4  before test stop criteria 
4 istlsolver::printOutput 
  0.5      6.81807e-25      1.52102e-16
4 === rate=2.3135e-32, T=0.22577, TIT=0.451539, IT=0.5
4 finished solveLinearSystemImpl_ 1
4 converged value: 1 7 1 let s go get convergedRemote
4 to  comm_.min(converged) 
4 did  comm_.min(converged) convergedRemote: 1
4 final convergedRemote: 1
4 Update: x^(k+1) = x^k - deltax^k 
4 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
4 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
4 Newton iteration  1 done4 , maximum relative shift = 2.096e-02
4 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
6  before v = A * y 
6  before  alpha = rho_new / < rt, v > 
6  before alpha = rho_new / h; 
6  before apply first correction to x 
6  before r = r - alpha*v 
6  before test stop criteria 
6 istlsolver::printOutput 
  0.5      6.81807e-25      1.52102e-16
6 === rate=2.3135e-32, T=0.225776, TIT=0.451553, IT=0.5
6 finished solveLinearSystemImpl_ 1
6 converged value: 1 7 1 let s go get convergedRemote
6 to  comm_.min(converged) 
6 did  comm_.min(converged) convergedRemote: 1
6 final convergedRemote: 1
6 Update: x^(k+1) = x^k - deltax^k 
6 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
6 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
6 Newton iteration  1 done6 , maximum relative shift = 2.096e-02
6 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
0 Newton iteration  1 done0 , maximum relative shift = 2.096e-02
0 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
1 Solve: M deltax^k = r 
1 Newton::solveLinearSystem : entering the try section 1
1 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
1 solveLinearSystemImpl_, before converged = ls.solve 
1 7 amgbackend::solve isParallel 1
4 Solve: M deltax^k = r 
4 Newton::solveLinearSystem : entering the try section 1
4 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
4 solveLinearSystemImpl_, before converged = ls.solve 
4 7 amgbackend::solve isParallel 1
3 Solve: M deltax^k = r 
3 Newton::solveLinearSystem : entering the try section 1
3 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
3 solveLinearSystemImpl_, before converged = ls.solve 
3 7 amgbackend::solve isParallel 1
6 Solve: M deltax^k = r 
6 Newton::solveLinearSystem : entering the try section 1
6 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
6 solveLinearSystemImpl_, before converged = ls.solve 
6 7 amgbackend::solve isParallel 1
2 Solve: M deltax^k = r 
2 Newton::solveLinearSystem : entering the try section 1
2 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
2 solveLinearSystemImpl_, before converged = ls.solve 
2 7 amgbackend::solve isParallel 1
0 Solve: M deltax^k = r 
0 Newton::solveLinearSystem : entering the try section 1
0 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
0 solveLinearSystemImpl_, before converged = ls.solve 
0 7 amgbackend::solve isParallel 1
5 Solve: M deltax^k = r 
5 Newton::solveLinearSystem : entering the try section 1
5 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
5 solveLinearSystemImpl_, before converged = ls.solve 
5 7 amgbackend::solve isParallel 1
5 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
5 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
0 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
0 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 7
2 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
2 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
1 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
1 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
4 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
4 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
3 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
3 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
6 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
6 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 7
2 7 before Dune::BiCGSTABSolver 
2 Dune::BiCGSTABSolver.apply(x, b, result_) 
2 BiCGSTABSolver::apply 
2 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
1 7 before Dune::BiCGSTABSolver 
1 Dune::BiCGSTABSolver.apply(x, b, result_) 
1 BiCGSTABSolver::apply 
1 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
4 7 before Dune::BiCGSTABSolver 
4 Dune::BiCGSTABSolver.apply(x, b, result_) 
4 BiCGSTABSolver::apply 
4 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
3 7 before Dune::BiCGSTABSolver 
3 Dune::BiCGSTABSolver.apply(x, b, result_) 
3 BiCGSTABSolver::apply 
3 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 7
5 7 before Dune::BiCGSTABSolver 
5 Dune::BiCGSTABSolver.apply(x, b, result_) 
5 BiCGSTABSolver::apply 
5 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
6 7 before Dune::BiCGSTABSolver 
6 Dune::BiCGSTABSolver.apply(x, b, result_) 
6 BiCGSTABSolver::apply 
6 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
Using a direct coarse solver (SuperLU)
Building hierarchy of 1 levels (inclusive coarse solver) took 0.200758 seconds.
0 7 before Dune::BiCGSTABSolver 
0 Dune::BiCGSTABSolver.apply(x, b, result_) 
0 BiCGSTABSolver::apply 
0 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
0 === BiCGSTABSolver
 Iter          Defect            Rate
    0      2.30384e-12
0  (it = 0.5; it < _maxit; it+=.5) 0.5 250
0  before p = r 
0  before  y = W^-1 * p 
0  before _prec->apply(y,p); 0.5 250
0 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 7
0 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
6 === BiCGSTABSolver
 Iter          Defect            Rate
    0      2.30384e-12
6  (it = 0.5; it < _maxit; it+=.5) 0.5 250
6  before p = r 
6  before  y = W^-1 * p 
6  before _prec->apply(y,p); 0.5 250
6 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
5 === BiCGSTABSolver
 Iter          Defect            Rate
    0      2.30384e-12
5  (it = 0.5; it < _maxit; it+=.5) 0.5 250
5  before p = r 
5  before  y = W^-1 * p 
5  before _prec->apply(y,p); 0.5 250
5 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
3 === BiCGSTABSolver
 Iter          Defect            Rate
    0      2.30384e-12
3  (it = 0.5; it < _maxit; it+=.5) 0.5 250
3  before p = r 
3  before  y = W^-1 * p 
3  before _prec->apply(y,p); 0.5 250
3 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
2 === BiCGSTABSolver
 Iter          Defect            Rate
    0      2.30384e-12
2  (it = 0.5; it < _maxit; it+=.5) 0.5 250
2  before p = r 
2  before  y = W^-1 * p 
2  before _prec->apply(y,p); 0.5 250
2 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
1 === BiCGSTABSolver
 Iter          Defect            Rate
    0      2.30384e-12
1  (it = 0.5; it < _maxit; it+=.5) 0.5 250
1  before p = r 
1  before  y = W^-1 * p 
1  before _prec->apply(y,p); 0.5 250
1 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
4 === BiCGSTABSolver
 Iter          Defect            Rate
    0      2.30384e-12
4  (it = 0.5; it < _maxit; it+=.5) 0.5 250
4  before p = r 
4  before  y = W^-1 * p 
4  before _prec->apply(y,p); 0.5 250
4 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
6 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
6 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
5 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
3 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
2 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
2 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
1 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
4 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
4 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
0 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
0 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
5 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
3 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
4  before v = A * y 
4  before  alpha = rho_new / < rt, v > 
4  before alpha = rho_new / h; 
4  before apply first correction to x 
4  before r = r - alpha*v 
4  before test stop criteria 
4 istlsolver::printOutput 
  0.5      3.21504e-28      1.39551e-16
4 === rate=1.94746e-32, T=0.224707, TIT=0.449414, IT=0.5
4 finished solveLinearSystemImpl_ 1
4 converged value: 1 7 1 let s go get convergedRemote
4 to  comm_.min(converged) 
4 did  comm_.min(converged) convergedRemote: 1
4 final convergedRemote: 1
4 Update: x^(k+1) = x^k - deltax^k 
4 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
4 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
4 Newton iteration  2 done4 , maximum relative shift = 2.574e-05
4 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
6  before v = A * y 
6  before  alpha = rho_new / < rt, v > 
6  before alpha = rho_new / h; 
6  before apply first correction to x 
6  before r = r - alpha*v 
6  before test stop criteria 
6 istlsolver::printOutput 
  0.5      3.21504e-28      1.39551e-16
6 === rate=1.94746e-32, T=0.224705, TIT=0.449411, IT=0.5
6 finished solveLinearSystemImpl_ 1
6 converged value: 1 7 1 let s go get convergedRemote
6 to  comm_.min(converged) 
6 did  comm_.min(converged) convergedRemote: 1
6 final convergedRemote: 1
6 Update: x^(k+1) = x^k - deltax^k 
6 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
6 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
6 Newton iteration  2 done6 , maximum relative shift = 2.574e-05
6 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
0  before v = A * y 
0  before  alpha = rho_new / < rt, v > 
0  before alpha = rho_new / h; 
0  before apply first correction to x 
0  before r = r - alpha*v 
0  before test stop criteria 
0 istlsolver::printOutput 
  0.5      3.21504e-28      1.39551e-16
0 === rate=1.94746e-32, T=0.0298686, TIT=0.0597372, IT=0.5
0 finished solveLinearSystemImpl_ 1
0 converged value: 1 7 1 let s go get convergedRemote
0 to  comm_.min(converged) 
0 did  comm_.min(converged) convergedRemote: 1
0 final convergedRemote: 1
0 Update: x^(k+1) = x^k - deltax^k 
0 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
0 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
0 Newton iteration  2 done0 , maximum relative shift = 2.574e-05
0 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
5  before v = A * y 
5  before  alpha = rho_new / < rt, v > 
5  before alpha = rho_new / h; 
5  before apply first correction to x 
5  before r = r - alpha*v 
5  before test stop criteria 
5 istlsolver::printOutput 
  0.5      3.21504e-28      1.39551e-16
5 === rate=1.94746e-32, T=0.224611, TIT=0.449222, IT=0.5
5 finished solveLinearSystemImpl_ 1
5 converged value: 1 7 1 let s go get convergedRemote
5 to  comm_.min(converged) 
5 did  comm_.min(converged) convergedRemote: 1
5 final convergedRemote: 1
5 Update: x^(k+1) = x^k - deltax^k 
5 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
5 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
5 Newton iteration  2 done5 , maximum relative shift = 2.574e-05
5 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
2  before v = A * y 
2  before  alpha = rho_new / < rt, v > 
2  before alpha = rho_new / h; 
2  before apply first correction to x 
2  before r = r - alpha*v 
2  before test stop criteria 
2 istlsolver::printOutput 
  0.5      3.21504e-28      1.39551e-16
2 === rate=1.94746e-32, T=0.224575, TIT=0.44915, IT=0.5
2 finished solveLinearSystemImpl_ 1
2 converged value: 1 7 1 let s go get convergedRemote
2 to  comm_.min(converged) 
2 did  comm_.min(converged) convergedRemote: 1
2 final convergedRemote: 1
2 Update: x^(k+1) = x^k - deltax^k 
2 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
2 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
2 Newton iteration  2 done2 , maximum relative shift = 2.574e-05
2 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
3  before v = A * y 
3  before  alpha = rho_new / < rt, v > 
3  before alpha = rho_new / h; 
3  before apply first correction to x 
3  before r = r - alpha*v 
3  before test stop criteria 
3 istlsolver::printOutput 
  0.5      3.21504e-28      1.39551e-16
3 === rate=1.94746e-32, T=0.224611, TIT=0.449223, IT=0.5
3 finished solveLinearSystemImpl_ 1
3 converged value: 1 7 1 let s go get convergedRemote
3 to  comm_.min(converged) 
3 did  comm_.min(converged) convergedRemote: 1
3 final convergedRemote: 1
3 Update: x^(k+1) = x^k - deltax^k 
3 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
3 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
3 Newton iteration  2 done3 , maximum relative shift = 2.574e-05
3 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
1 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
1  before v = A * y 
1  before  alpha = rho_new / < rt, v > 
1  before alpha = rho_new / h; 
1  before apply first correction to x 
1  before r = r - alpha*v 
1  before test stop criteria 
1 istlsolver::printOutput 
  0.5      3.21504e-28      1.39551e-16
1 === rate=1.94746e-32, T=0.22461, TIT=0.449221, IT=0.5
1 finished solveLinearSystemImpl_ 1
1 converged value: 1 7 1 let s go get convergedRemote
1 to  comm_.min(converged) 
1 did  comm_.min(converged) convergedRemote: 1
1 final convergedRemote: 1
1 Update: x^(k+1) = x^k - deltax^k 
1 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
1 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
1 Newton iteration  2 done1 , maximum relative shift = 2.574e-05
1 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
2 Solve: M deltax^k = r 
2 Newton::solveLinearSystem : entering the try section 2
2 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
2 solveLinearSystemImpl_, before converged = ls.solve 
2 7 amgbackend::solve isParallel 1
4 Solve: M deltax^k = r 
4 Newton::solveLinearSystem : entering the try section 2
4 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
4 solveLinearSystemImpl_, before converged = ls.solve 
4 7 amgbackend::solve isParallel 1
6 Solve: M deltax^k = r 
6 Newton::solveLinearSystem : entering the try section 2
6 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
6 solveLinearSystemImpl_, before converged = ls.solve 
6 7 amgbackend::solve isParallel 1
0 Solve: M deltax^k = r 
0 Newton::solveLinearSystem : entering the try section 2
0 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
0 solveLinearSystemImpl_, before converged = ls.solve 
0 7 amgbackend::solve isParallel 1
5 Solve: M deltax^k = r 
5 Newton::solveLinearSystem : entering the try section 2
5 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
5 solveLinearSystemImpl_, before converged = ls.solve 
5 7 amgbackend::solve isParallel 1
1 Solve: M deltax^k = r 
1 Newton::solveLinearSystem : entering the try section 2
1 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
1 solveLinearSystemImpl_, before converged = ls.solve 
1 7 amgbackend::solve isParallel 1
3 Solve: M deltax^k = r 
3 Newton::solveLinearSystem : entering the try section 2
3 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
3 solveLinearSystemImpl_, before converged = ls.solve 
3 7 amgbackend::solve isParallel 1
2 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
2 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
4 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
4 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
6 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
6 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
0 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
0 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 7
3 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
3 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
1 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
1 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
5 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
5 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 7
3 7 before Dune::BiCGSTABSolver 
3 Dune::BiCGSTABSolver.apply(x, b, result_) 
3 BiCGSTABSolver::apply 
3 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
1 7 before Dune::BiCGSTABSolver 
1 Dune::BiCGSTABSolver.apply(x, b, result_) 
1 BiCGSTABSolver::apply 
1 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
5 7 before Dune::BiCGSTABSolver 
5 Dune::BiCGSTABSolver.apply(x, b, result_) 
5 BiCGSTABSolver::apply 
5 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
2 7 before Dune::BiCGSTABSolver 
2 Dune::BiCGSTABSolver.apply(x, b, result_) 
2 BiCGSTABSolver::apply 
2 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
4 7 before Dune::BiCGSTABSolver 
4 Dune::BiCGSTABSolver.apply(x, b, result_) 
4 BiCGSTABSolver::apply 
4 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
6 7 before Dune::BiCGSTABSolver 
6 Dune::BiCGSTABSolver.apply(x, b, result_) 
6 BiCGSTABSolver::apply 
6 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 7
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
Using a direct coarse solver (SuperLU)
Building hierarchy of 1 levels (inclusive coarse solver) took 0.202871 seconds.
0 7 before Dune::BiCGSTABSolver 
0 Dune::BiCGSTABSolver.apply(x, b, result_) 
0 BiCGSTABSolver::apply 
6 === BiCGSTABSolver
 Iter          Defect            Rate
    0      9.85152e-15
6  (it = 0.5; it < _maxit; it+=.5) 0.5 250
6  before p = r 
6  before  y = W^-1 * p 
6  before _prec->apply(y,p); 0.5 250
6 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
4 === BiCGSTABSolver
 Iter          Defect            Rate
    0      9.85152e-15
4  (it = 0.5; it < _maxit; it+=.5) 0.5 250
4  before p = r 
4  before  y = W^-1 * p 
4  before _prec->apply(y,p); 0.5 250
4 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
3 === BiCGSTABSolver
 Iter          Defect            Rate
    0      9.85152e-15
3  (it = 0.5; it < _maxit; it+=.5) 0.5 250
3  before p = r 
3  before  y = W^-1 * p 
3  before _prec->apply(y,p); 0.5 250
3 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
5 === BiCGSTABSolver
 Iter          Defect            Rate
    0      9.85152e-15
5  (it = 0.5; it < _maxit; it+=.5) 0.5 250
5  before p = r 
5  before  y = W^-1 * p 
5  before _prec->apply(y,p); 0.5 250
5 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
1 === BiCGSTABSolver
 Iter          Defect            Rate
    0      9.85152e-15
1  (it = 0.5; it < _maxit; it+=.5) 0.5 250
1  before p = r 
1  before  y = W^-1 * p 
1  before _prec->apply(y,p); 0.5 250
1 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
2 === BiCGSTABSolver
 Iter          Defect            Rate
    0      9.85152e-15
2  (it = 0.5; it < _maxit; it+=.5) 0.5 250
2  before p = r 
2  before  y = W^-1 * p 
2  before _prec->apply(y,p); 0.5 250
2 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
0 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
0 === BiCGSTABSolver
 Iter          Defect            Rate
    0      9.85152e-15
0  (it = 0.5; it < _maxit; it+=.5) 0.5 250
0  before p = r 
0  before  y = W^-1 * p 
0  before _prec->apply(y,p); 0.5 250
0 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 7
0 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
6 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
4 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
3 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
5 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
1 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
2 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
2 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
5 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
5  before v = A * y 
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
0 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
0 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
1 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
6 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
6  before v = A * y 
4 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
4  before v = A * y 
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
3 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
3  before v = A * y 
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
0  before v = A * y 
0  before  alpha = rho_new / < rt, v > 
0  before alpha = rho_new / h; 
0  before apply first correction to x 
0  before r = r - alpha*v 
0  before test stop criteria 
0 istlsolver::printOutput 
  0.5      9.76327e-31      9.91042e-17
0 === rate=9.82165e-33, T=0.0353841, TIT=0.0707682, IT=0.5
0 finished solveLinearSystemImpl_ 1
0 converged value: 1 7 1 let s go get convergedRemote
0 to  comm_.min(converged) 
0 did  comm_.min(converged) convergedRemote: 1
0 final convergedRemote: 1
0 Update: x^(k+1) = x^k - deltax^k 
0 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
1  before v = A * y 
1  before  alpha = rho_new / < rt, v > 
1  before alpha = rho_new / h; 
1  before apply first correction to x 
1  before r = r - alpha*v 
1  before test stop criteria 
1 istlsolver::printOutput 
  0.5      9.76327e-31      9.91042e-17
1 === rate=9.82165e-33, T=0.232375, TIT=0.46475, IT=0.5
1 finished solveLinearSystemImpl_ 1
1 converged value: 1 7 1 let s go get convergedRemote
1 to  comm_.min(converged) 
1 did  comm_.min(converged) convergedRemote: 1
1 final convergedRemote: 1
1 Update: x^(k+1) = x^k - deltax^k 
1 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
1 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
1 Newton iteration  3 done1 , maximum relative shift = 3.098e-07
1 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
6  before  alpha = rho_new / < rt, v > 
6  before alpha = rho_new / h; 
6  before apply first correction to x 
6  before r = r - alpha*v 
6  before test stop criteria 
6 istlsolver::printOutput 
  0.5      9.76327e-31      9.91042e-17
6 === rate=9.82165e-33, T=0.232322, TIT=0.464643, IT=0.5
6 finished solveLinearSystemImpl_ 1
6 converged value: 1 7 1 let s go get convergedRemote
6 to  comm_.min(converged) 
6 did  comm_.min(converged) convergedRemote: 1
6 final convergedRemote: 1
6 Update: x^(k+1) = x^k - deltax^k 
6 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
4  before  alpha = rho_new / < rt, v > 
4  before alpha = rho_new / h; 
4  before apply first correction to x 
4  before r = r - alpha*v 
4  before test stop criteria 
4 istlsolver::printOutput 
  0.5      9.76327e-31      9.91042e-17
4 === rate=9.82165e-33, T=0.232318, TIT=0.464635, IT=0.5
4 finished solveLinearSystemImpl_ 1
4 converged value: 1 7 1 let s go get convergedRemote
4 to  comm_.min(converged) 
4 did  comm_.min(converged) convergedRemote: 1
4 final convergedRemote: 1
4 Update: x^(k+1) = x^k - deltax^k 
4 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
5  before  alpha = rho_new / < rt, v > 
5  before alpha = rho_new / h; 
5  before apply first correction to x 
5  before r = r - alpha*v 
5  before test stop criteria 
5 istlsolver::printOutput 
  0.5      9.76327e-31      9.91042e-17
5 === rate=9.82165e-33, T=0.232374, TIT=0.464748, IT=0.5
5 finished solveLinearSystemImpl_ 1
5 converged value: 1 7 1 let s go get convergedRemote
5 to  comm_.min(converged) 
5 did  comm_.min(converged) convergedRemote: 1
5 final convergedRemote: 1
5 Update: x^(k+1) = x^k - deltax^k 
5 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
5 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
5 Newton iteration  3 done5 , maximum relative shift = 3.098e-07
5 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
3  before  alpha = rho_new / < rt, v > 
3  before alpha = rho_new / h; 
3  before apply first correction to x 
3  before r = r - alpha*v 
3  before test stop criteria 
3 istlsolver::printOutput 
  0.5      9.76327e-31      9.91042e-17
3 === rate=9.82165e-33, T=0.232373, TIT=0.464747, IT=0.5
3 finished solveLinearSystemImpl_ 1
3 converged value: 1 7 1 let s go get convergedRemote
3 to  comm_.min(converged) 
3 did  comm_.min(converged) convergedRemote: 1
3 final convergedRemote: 1
3 Update: x^(k+1) = x^k - deltax^k 
3 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
3 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
3 Newton iteration  3 done3 , maximum relative shift = 3.098e-07
3 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
2  before v = A * y 
2  before  alpha = rho_new / < rt, v > 
2  before alpha = rho_new / h; 
2  before apply first correction to x 
2  before r = r - alpha*v 
2  before test stop criteria 
2 istlsolver::printOutput 
  0.5      9.76327e-31      9.91042e-17
2 === rate=9.82165e-33, T=0.232313, TIT=0.464625, IT=0.5
2 finished solveLinearSystemImpl_ 1
2 converged value: 1 7 1 let s go get convergedRemote
2 to  comm_.min(converged) 
2 did  comm_.min(converged) convergedRemote: 1
2 final convergedRemote: 1
2 Update: x^(k+1) = x^k - deltax^k 
2 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
0 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
0 Newton iteration  3 done0 , maximum relative shift = 3.098e-07
0 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
6 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
6 Newton iteration  3 done6 , maximum relative shift = 3.098e-07
6 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
4 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
4 Newton iteration  3 done4 , maximum relative shift = 3.098e-07
4 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
2 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
2 Newton iteration  3 done2 , maximum relative shift = 3.098e-07
2 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
3 Solve: M deltax^k = r 
3 Newton::solveLinearSystem : entering the try section 3
3 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
3 solveLinearSystemImpl_, before converged = ls.solve 
3 7 amgbackend::solve isParallel 1
5 Solve: M deltax^k = r 
5 Newton::solveLinearSystem : entering the try section 3
5 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
5 solveLinearSystemImpl_, before converged = ls.solve 
5 7 amgbackend::solve isParallel 1
4 Solve: M deltax^k = r 
4 Newton::solveLinearSystem : entering the try section 3
4 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
4 solveLinearSystemImpl_, before converged = ls.solve 
4 7 amgbackend::solve isParallel 1
1 Solve: M deltax^k = r 
1 Newton::solveLinearSystem : entering the try section 3
1 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
1 solveLinearSystemImpl_, before converged = ls.solve 
1 7 amgbackend::solve isParallel 1
0 Solve: M deltax^k = r 
0 Newton::solveLinearSystem : entering the try section 3
0 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
0 solveLinearSystemImpl_, before converged = ls.solve 
0 7 amgbackend::solve isParallel 1
6 Solve: M deltax^k = r 
6 Newton::solveLinearSystem : entering the try section 3
6 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
6 solveLinearSystemImpl_, before converged = ls.solve 
6 7 amgbackend::solve isParallel 1
2 Solve: M deltax^k = r 
2 Newton::solveLinearSystem : entering the try section 3
2 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
2 solveLinearSystemImpl_, before converged = ls.solve 
2 7 amgbackend::solve isParallel 1
4 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
4 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
1 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
1 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
0 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
0 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 7
2 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
2 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
6 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
6 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
3 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
3 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
5 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
5 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 7
1 7 before Dune::BiCGSTABSolver 
1 Dune::BiCGSTABSolver.apply(x, b, result_) 
1 BiCGSTABSolver::apply 
1 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 7
2 7 before Dune::BiCGSTABSolver 
2 Dune::BiCGSTABSolver.apply(x, b, result_) 
2 BiCGSTABSolver::apply 
6 7 before Dune::BiCGSTABSolver 
6 Dune::BiCGSTABSolver.apply(x, b, result_) 
6 BiCGSTABSolver::apply 
6 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
3 7 before Dune::BiCGSTABSolver 
3 Dune::BiCGSTABSolver.apply(x, b, result_) 
3 BiCGSTABSolver::apply 
3 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
5 7 before Dune::BiCGSTABSolver 
5 Dune::BiCGSTABSolver.apply(x, b, result_) 
5 BiCGSTABSolver::apply 
5 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
4 7 before Dune::BiCGSTABSolver 
4 Dune::BiCGSTABSolver.apply(x, b, result_) 
4 BiCGSTABSolver::apply 
4 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
2 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
Using a direct coarse solver (SuperLU)
Building hierarchy of 1 levels (inclusive coarse solver) took 0.200628 seconds.
0 7 before Dune::BiCGSTABSolver 
0 Dune::BiCGSTABSolver.apply(x, b, result_) 
1 === BiCGSTABSolver
 Iter          Defect            Rate
    0      1.06771e-16
1  (it = 0.5; it < _maxit; it+=.5) 0.5 250
1  before p = r 
1  before  y = W^-1 * p 
1  before _prec->apply(y,p); 0.5 250
1 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
4 === BiCGSTABSolver
 Iter          Defect            Rate
    0      1.06771e-16
4  (it = 0.5; it < _maxit; it+=.5) 0.5 250
4  before p = r 
4  before  y = W^-1 * p 
4  before _prec->apply(y,p); 0.5 250
4 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
6 === BiCGSTABSolver
 Iter          Defect            Rate
    0      1.06771e-16
6  (it = 0.5; it < _maxit; it+=.5) 0.5 250
6  before p = r 
6  before  y = W^-1 * p 
6  before _prec->apply(y,p); 0.5 250
6 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
5 === BiCGSTABSolver
 Iter          Defect            Rate
    0      1.06771e-16
5  (it = 0.5; it < _maxit; it+=.5) 0.5 250
5  before p = r 
5  before  y = W^-1 * p 
5  before _prec->apply(y,p); 0.5 250
5 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
3 === BiCGSTABSolver
 Iter          Defect            Rate
    0      1.06771e-16
3  (it = 0.5; it < _maxit; it+=.5) 0.5 250
3  before p = r 
3  before  y = W^-1 * p 
3  before _prec->apply(y,p); 0.5 250
3 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
0 BiCGSTABSolver::apply 
0 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
0 === BiCGSTABSolver
 Iter          Defect            Rate
    0      1.06771e-16
0  (it = 0.5; it < _maxit; it+=.5) 0.5 250
0  before p = r 
0  before  y = W^-1 * p 
0  before _prec->apply(y,p); 0.5 250
0 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 7
0 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
2 === BiCGSTABSolver
 Iter          Defect            Rate
    0      1.06771e-16
2  (it = 0.5; it < _maxit; it+=.5) 0.5 250
2  before p = r 
2  before  y = W^-1 * p 
2  before _prec->apply(y,p); 0.5 250
2 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
0 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
3 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
3 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
1 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
4 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
4 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
6 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
6 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
5 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
5 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
2 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
2 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
3  before v = A * y 
3  before  alpha = rho_new / < rt, v > 
1 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
1  before v = A * y 
4  before v = A * y 
4  before  alpha = rho_new / < rt, v > 
6  before v = A * y 
6  before  alpha = rho_new / < rt, v > 
2  before v = A * y 
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
0 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
0  before v = A * y 
1  before  alpha = rho_new / < rt, v > 
1  before alpha = rho_new / h; 
1  before apply first correction to x 
1  before r = r - alpha*v 
1  before test stop criteria 
1 istlsolver::printOutput 
  0.5      5.26927e-33       4.9351e-17
1 === rate=2.43552e-33, T=0.224094, TIT=0.448188, IT=0.5
1 finished solveLinearSystemImpl_ 1
1 converged value: 1 7 1 let s go get convergedRemote
1 to  comm_.min(converged) 
4  before alpha = rho_new / h; 
4  before apply first correction to x 
4  before r = r - alpha*v 
4  before test stop criteria 
4 istlsolver::printOutput 
  0.5      5.26927e-33       4.9351e-17
4 === rate=2.43552e-33, T=0.224137, TIT=0.448274, IT=0.5
4 finished solveLinearSystemImpl_ 1
4 converged value: 1 7 1 let s go get convergedRemote
4 to  comm_.min(converged) 
0  before  alpha = rho_new / < rt, v > 
0  before alpha = rho_new / h; 
0  before apply first correction to x 
0  before r = r - alpha*v 
0  before test stop criteria 
0 istlsolver::printOutput 
  0.5      5.26927e-33       4.9351e-17
0 === rate=2.43552e-33, T=0.0293151, TIT=0.0586303, IT=0.5
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
5  before v = A * y 
5  before  alpha = rho_new / < rt, v > 
5  before alpha = rho_new / h; 
5  before apply first correction to x 
5  before r = r - alpha*v 
5  before test stop criteria 
5 istlsolver::printOutput 
  0.5      5.26927e-33       4.9351e-17
5 === rate=2.43552e-33, T=0.224094, TIT=0.448189, IT=0.5
5 finished solveLinearSystemImpl_ 1
5 converged value: 1 7 1 let s go get convergedRemote
5 to  comm_.min(converged) 
2  before  alpha = rho_new / < rt, v > 
2  before alpha = rho_new / h; 
2  before apply first correction to x 
2  before r = r - alpha*v 
2  before test stop criteria 
2 istlsolver::printOutput 
  0.5      5.26927e-33       4.9351e-17
2 === rate=2.43552e-33, T=0.22414, TIT=0.448279, IT=0.5
2 finished solveLinearSystemImpl_ 1
2 converged value: 1 7 1 let s go get convergedRemote
2 to  comm_.min(converged) 
6  before alpha = rho_new / h; 
6  before apply first correction to x 
6  before r = r - alpha*v 
6  before test stop criteria 
6 istlsolver::printOutput 
  0.5      5.26927e-33       4.9351e-17
6 === rate=2.43552e-33, T=0.224143, TIT=0.448286, IT=0.5
6 finished solveLinearSystemImpl_ 1
6 converged value: 1 7 1 let s go get convergedRemote
6 to  comm_.min(converged) 
3  before alpha = rho_new / h; 
3  before apply first correction to x 
3  before r = r - alpha*v 
3  before test stop criteria 
3 istlsolver::printOutput 
  0.5      5.26927e-33       4.9351e-17
3 === rate=2.43552e-33, T=0.224096, TIT=0.448192, IT=0.5
3 finished solveLinearSystemImpl_ 1
3 converged value: 1 7 1 let s go get convergedRemote
3 to  comm_.min(converged) 
1 did  comm_.min(converged) convergedRemote: 1
1 final convergedRemote: 1
1 Update: x^(k+1) = x^k - deltax^k 
1 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
1 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
1 Newton iteration  4 done1 , maximum relative shift = 3.619e-09
3 did  comm_.min(converged) convergedRemote: 1
3 final convergedRemote: 1
3 Update: x^(k+1) = x^k - deltax^k 
3 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
3 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
3 Newton iteration  4 done3 , maximum relative shift = 3.619e-09
0 finished solveLinearSystemImpl_ 1
0 converged value: 1 7 1 let s go get convergedRemote
0 to  comm_.min(converged) 
0 did  comm_.min(converged) convergedRemote: 1
0 final convergedRemote: 1
0 Update: x^(k+1) = x^k - deltax^k 
0 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
5 did  comm_.min(converged) convergedRemote: 1
5 final convergedRemote: 1
5 Update: x^(k+1) = x^k - deltax^k 
5 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
5 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
5 Newton iteration  4 done5 , maximum relative shift = 3.619e-09
2 did  comm_.min(converged) convergedRemote: 1
2 final convergedRemote: 1
2 Update: x^(k+1) = x^k - deltax^k 
2 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
6 did  comm_.min(converged) convergedRemote: 1
6 final convergedRemote: 1
6 Update: x^(k+1) = x^k - deltax^k 
6 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
4 did  comm_.min(converged) convergedRemote: 1
4 final convergedRemote: 1
4 Update: x^(k+1) = x^k - deltax^k 
4 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
2 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
2 Newton iteration  4 done2 , maximum relative shift = 3.619e-09
2 Assemble/solve/update time: 0.296763(24.0025%)/0.93912(75.9571%)/0.000498836(0.0403464%)
2 gridVariables->advanceTimeStep
2 before, nonLinearSolver->suggestTimeStepSize, current time: 233.703 simTime 10800
2 before assembler->setPreviousSolution, ddt: 30
2 nonLinearSolver->solve
2 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
6 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
6 Newton iteration  4 done6 , maximum relative shift = 3.619e-09
6 Assemble/solve/update time: 0.296781(24.0035%)/0.939123(75.956%)/0.000500555(0.0404847%)
6 gridVariables->advanceTimeStep
6 before, nonLinearSolver->suggestTimeStepSize, current time: 233.703 simTime 10800
6 before assembler->setPreviousSolution, ddt: 30
6 nonLinearSolver->solve
6 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
1 Assemble/solve/update time: 0.296918(24.019%)/0.938662(75.9324%)/0.000600789(0.0486004%)
1 gridVariables->advanceTimeStep
1 before, nonLinearSolver->suggestTimeStepSize, current time: 233.703 simTime 10800
1 before assembler->setPreviousSolution, ddt: 30
1 nonLinearSolver->solve
1 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
4 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
4 Newton iteration  4 done4 , maximum relative shift = 3.619e-09
4 Assemble/solve/update time: 0.29643(23.9823%)/0.939098(75.9767%)/0.000506676(0.040992%)
4 gridVariables->advanceTimeStep
4 before, nonLinearSolver->suggestTimeStepSize, current time: 233.703 simTime 10800
4 before assembler->setPreviousSolution, ddt: 30
4 nonLinearSolver->solve
4 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
3 Assemble/solve/update time: 0.296902(24.0178%)/0.938675(75.9339%)/0.000597269(0.0483159%)
3 gridVariables->advanceTimeStep
3 before, nonLinearSolver->suggestTimeStepSize, current time: 233.703 simTime 10800
3 before assembler->setPreviousSolution, ddt: 30
3 nonLinearSolver->solve
3 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
0 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
0 Newton iteration  4 done0 , maximum relative shift = 3.619e-09
0 Assemble/solve/update time: 0.296224(23.9697%)/0.939109(75.9903%)/0.000494176(0.0399874%)
0 gridVariables->advanceTimeStep
0 before, nonLinearSolver->suggestTimeStepSize, current time: 233.703 simTime 10800
0 before assembler->setPreviousSolution, ddt: 30
0 nonLinearSolver->solve
0 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
5 Assemble/solve/update time: 0.29693(24.0198%)/0.938659(75.9318%)/0.000598529(0.0484173%)
5 gridVariables->advanceTimeStep
5 before, nonLinearSolver->suggestTimeStepSize, current time: 233.703 simTime 10800
5 before assembler->setPreviousSolution, ddt: 30
5 nonLinearSolver->solve
5 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
6 Solve: M deltax^k = r 
6 Newton::solveLinearSystem : entering the try section 0
6to  comm_.sum(norm2) 
6did  comm_.sum(norm2) 
6 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
6 solveLinearSystemImpl_, before converged = ls.solve 
6 7 amgbackend::solve isParallel 1
5 Solve: M deltax^k = r 
5 Newton::solveLinearSystem : entering the try section 0
5to  comm_.sum(norm2) 
5did  comm_.sum(norm2) 
5 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
5 solveLinearSystemImpl_, before converged = ls.solve 
4 Solve: M deltax^k = r 
4 Newton::solveLinearSystem : entering the try section 0
4to  comm_.sum(norm2) 
4did  comm_.sum(norm2) 
4 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
4 solveLinearSystemImpl_, before converged = ls.solve 
4 7 amgbackend::solve isParallel 1
3 Solve: M deltax^k = r 
3 Newton::solveLinearSystem : entering the try section 0
3to  comm_.sum(norm2) 
3did  comm_.sum(norm2) 
3 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
0 Solve: M deltax^k = r 
0 Newton::solveLinearSystem : entering the try section 0
0to  comm_.sum(norm2) 
0did  comm_.sum(norm2) 
0 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
1 Solve: M deltax^k = r 
1 Newton::solveLinearSystem : entering the try section 0
1to  comm_.sum(norm2) 
1did  comm_.sum(norm2) 
1 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
1 solveLinearSystemImpl_, before converged = ls.solve 
1 7 amgbackend::solve isParallel 1
2 Solve: M deltax^k = r 
2 Newton::solveLinearSystem : entering the try section 0
2to  comm_.sum(norm2) 
2did  comm_.sum(norm2) 
2 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
5 7 amgbackend::solve isParallel 1
3 solveLinearSystemImpl_, before converged = ls.solve 
3 7 amgbackend::solve isParallel 1
0 solveLinearSystemImpl_, before converged = ls.solve 
0 7 amgbackend::solve isParallel 1
2 solveLinearSystemImpl_, before converged = ls.solve 
2 7 amgbackend::solve isParallel 1
4 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
4 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
5 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
5 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
3 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
3 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
1 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
1 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
6 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
6 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
2 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
2 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
0 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
0 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 7
1 7 before Dune::BiCGSTABSolver 
1 Dune::BiCGSTABSolver.apply(x, b, result_) 
1 BiCGSTABSolver::apply 
1 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 7
2 7 before Dune::BiCGSTABSolver 
2 Dune::BiCGSTABSolver.apply(x, b, result_) 
2 BiCGSTABSolver::apply 
2 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
6 7 before Dune::BiCGSTABSolver 
6 Dune::BiCGSTABSolver.apply(x, b, result_) 
6 BiCGSTABSolver::apply 
6 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
4 7 before Dune::BiCGSTABSolver 
4 Dune::BiCGSTABSolver.apply(x, b, result_) 
4 BiCGSTABSolver::apply 
4 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
5 7 before Dune::BiCGSTABSolver 
5 Dune::BiCGSTABSolver.apply(x, b, result_) 
5 BiCGSTABSolver::apply 
5 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
3 7 before Dune::BiCGSTABSolver 
3 Dune::BiCGSTABSolver.apply(x, b, result_) 
3 BiCGSTABSolver::apply 
3 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
Using a direct coarse solver (SuperLU)
Building hierarchy of 1 levels (inclusive coarse solver) took 0.201327 seconds.
0 7 before Dune::BiCGSTABSolver 
0 Dune::BiCGSTABSolver.apply(x, b, result_) 
0 BiCGSTABSolver::apply 
4 === BiCGSTABSolver
 Iter          Defect            Rate
    0      4.48037e-09
4  (it = 0.5; it < _maxit; it+=.5) 0.5 250
4  before p = r 
4  before  y = W^-1 * p 
4  before _prec->apply(y,p); 0.5 250
4 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
3 === BiCGSTABSolver
 Iter          Defect            Rate
    0      4.48037e-09
3  (it = 0.5; it < _maxit; it+=.5) 0.5 250
3  before p = r 
3  before  y = W^-1 * p 
3  before _prec->apply(y,p); 0.5 250
3 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
6 === BiCGSTABSolver
 Iter          Defect            Rate
    0      4.48037e-09
6  (it = 0.5; it < _maxit; it+=.5) 0.5 250
6  before p = r 
6  before  y = W^-1 * p 
6  before _prec->apply(y,p); 0.5 250
6 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
2 === BiCGSTABSolver
 Iter          Defect            Rate
    0      4.48037e-09
2  (it = 0.5; it < _maxit; it+=.5) 0.5 250
2  before p = r 
2  before  y = W^-1 * p 
2  before _prec->apply(y,p); 0.5 250
2 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
5 === BiCGSTABSolver
 Iter          Defect            Rate
    0      4.48037e-09
5  (it = 0.5; it < _maxit; it+=.5) 0.5 250
5  before p = r 
5  before  y = W^-1 * p 
5  before _prec->apply(y,p); 0.5 250
5 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
1 === BiCGSTABSolver
 Iter          Defect            Rate
    0      4.48037e-09
1  (it = 0.5; it < _maxit; it+=.5) 0.5 250
1  before p = r 
1  before  y = W^-1 * p 
1  before _prec->apply(y,p); 0.5 250
1 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
0 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
0 === BiCGSTABSolver
 Iter          Defect            Rate
    0      4.48037e-09
0  (it = 0.5; it < _maxit; it+=.5) 0.5 250
0  before p = r 
0  before  y = W^-1 * p 
0  before _prec->apply(y,p); 0.5 250
0 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 7
0 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
6 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
6 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
2 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
2 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
5 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
1 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
4 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
4 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
3 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
0 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
0 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
0  before v = A * y 
1 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
1  before v = A * y 
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
4  before v = A * y 
4  before  alpha = rho_new / < rt, v > 
5 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
5  before v = A * y 
6  before v = A * y 
6  before  alpha = rho_new / < rt, v > 
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
2  before v = A * y 
3 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
3  before v = A * y 
1  before  alpha = rho_new / < rt, v > 
1  before alpha = rho_new / h; 
1  before apply first correction to x 
1  before r = r - alpha*v 
1  before test stop criteria 
1 istlsolver::printOutput 
  0.5       1.3491e-24      3.01114e-16
1 === rate=9.06697e-32, T=0.225223, TIT=0.450446, IT=0.5
1 finished solveLinearSystemImpl_ 1
1 converged value: 1 7 1 let s go get convergedRemote
1 to  comm_.min(converged) 
4  before alpha = rho_new / h; 
4  before apply first correction to x 
4  before r = r - alpha*v 
4  before test stop criteria 
4 istlsolver::printOutput 
  0.5       1.3491e-24      3.01114e-16
4 === rate=9.06697e-32, T=0.225333, TIT=0.450667, IT=0.5
4 finished solveLinearSystemImpl_ 1
4 converged value: 1 7 1 let s go get convergedRemote
4 to  comm_.min(converged) 
5  before  alpha = rho_new / < rt, v > 
5  before alpha = rho_new / h; 
5  before apply first correction to x 
5  before r = r - alpha*v 
5  before test stop criteria 
5 istlsolver::printOutput 
  0.5       1.3491e-24      3.01114e-16
5 === rate=9.06697e-32, T=0.225223, TIT=0.450446, IT=0.5
5 finished solveLinearSystemImpl_ 1
5 converged value: 1 7 1 let s go get convergedRemote
5 to  comm_.min(converged) 
6  before alpha = rho_new / h; 
6  before apply first correction to x 
6  before r = r - alpha*v 
6  before test stop criteria 
6 istlsolver::printOutput 
  0.5       1.3491e-24      3.01114e-16
6 === rate=9.06697e-32, T=0.225336, TIT=0.450672, IT=0.5
6 finished solveLinearSystemImpl_ 1
6 converged value: 1 7 1 let s go get convergedRemote
6 to  comm_.min(converged) 
3  before  alpha = rho_new / < rt, v > 
3  before alpha = rho_new / h; 
3  before apply first correction to x 
3  before r = r - alpha*v 
3  before test stop criteria 
3 istlsolver::printOutput 
  0.5       1.3491e-24      3.01114e-16
3 === rate=9.06697e-32, T=0.225224, TIT=0.450448, IT=0.5
3 finished solveLinearSystemImpl_ 1
3 converged value: 1 7 1 let s go get convergedRemote
3 to  comm_.min(converged) 
2  before  alpha = rho_new / < rt, v > 
2  before alpha = rho_new / h; 
2  before apply first correction to x 
2  before r = r - alpha*v 
2  before test stop criteria 
2 istlsolver::printOutput 
  0.5       1.3491e-24      3.01114e-16
2 === rate=9.06697e-32, T=0.225332, TIT=0.450664, IT=0.5
2 finished solveLinearSystemImpl_ 1
2 converged value: 1 7 1 let s go get convergedRemote
2 to  comm_.min(converged) 
0  before  alpha = rho_new / < rt, v > 
0  before alpha = rho_new / h; 
0  before apply first correction to x 
0  before r = r - alpha*v 
0  before test stop criteria 
0 istlsolver::printOutput 
  0.5       1.3491e-24      3.01114e-16
0 === rate=9.06697e-32, T=0.0301973, TIT=0.0603946, IT=0.5
3 did  comm_.min(converged) convergedRemote: 1
3 final convergedRemote: 1
3 Update: x^(k+1) = x^k - deltax^k 
3 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
3 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
3 Newton iteration  1 done3 , maximum relative shift = 2.231e-02
3 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
2 did  comm_.min(converged) convergedRemote: 1
2 final convergedRemote: 1
2 Update: x^(k+1) = x^k - deltax^k 
2 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
2 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
2 Newton iteration  1 done2 , maximum relative shift = 2.231e-02
2 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
6 did  comm_.min(converged) convergedRemote: 1
6 final convergedRemote: 1
6 Update: x^(k+1) = x^k - deltax^k 
6 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
6 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
6 Newton iteration  1 done6 , maximum relative shift = 2.231e-02
6 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
1 did  comm_.min(converged) convergedRemote: 1
1 final convergedRemote: 1
1 Update: x^(k+1) = x^k - deltax^k 
1 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
1 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
1 Newton iteration  1 done1 , maximum relative shift = 2.231e-02
1 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
4 did  comm_.min(converged) convergedRemote: 1
4 final convergedRemote: 1
4 Update: x^(k+1) = x^k - deltax^k 
4 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
4 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
4 Newton iteration  1 done4 , maximum relative shift = 2.231e-02
4 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
5 did  comm_.min(converged) convergedRemote: 1
5 final convergedRemote: 1
5 Update: x^(k+1) = x^k - deltax^k 
5 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
5 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
5 Newton iteration  1 done5 , maximum relative shift = 2.231e-02
5 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
0 finished solveLinearSystemImpl_ 1
0 converged value: 1 7 1 let s go get convergedRemote
0 to  comm_.min(converged) 
0 did  comm_.min(converged) convergedRemote: 1
0 final convergedRemote: 1
0 Update: x^(k+1) = x^k - deltax^k 
0 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
0 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
0 Newton iteration  1 done0 , maximum relative shift = 2.231e-02
0 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
6 Solve: M deltax^k = r 
6 Newton::solveLinearSystem : entering the try section 1
6 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
6 solveLinearSystemImpl_, before converged = ls.solve 
6 7 amgbackend::solve isParallel 1
1 Solve: M deltax^k = r 
1 Newton::solveLinearSystem : entering the try section 1
1 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
1 solveLinearSystemImpl_, before converged = ls.solve 
1 7 amgbackend::solve isParallel 1
4 Solve: M deltax^k = r 
4 Newton::solveLinearSystem : entering the try section 1
4 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
4 solveLinearSystemImpl_, before converged = ls.solve 
4 7 amgbackend::solve isParallel 1
0 Solve: M deltax^k = r 
0 Newton::solveLinearSystem : entering the try section 1
0 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
0 solveLinearSystemImpl_, before converged = ls.solve 
0 7 amgbackend::solve isParallel 1
5 Solve: M deltax^k = r 
5 Newton::solveLinearSystem : entering the try section 1
5 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
5 solveLinearSystemImpl_, before converged = ls.solve 
5 7 amgbackend::solve isParallel 1
3 Solve: M deltax^k = r 
3 Newton::solveLinearSystem : entering the try section 1
3 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
3 solveLinearSystemImpl_, before converged = ls.solve 
3 7 amgbackend::solve isParallel 1
2 Solve: M deltax^k = r 
2 Newton::solveLinearSystem : entering the try section 1
2 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
2 solveLinearSystemImpl_, before converged = ls.solve 
2 7 amgbackend::solve isParallel 1
3 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
3 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
4 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
4 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
6 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
6 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
1 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
1 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
2 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
2 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
0 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
0 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 7
5 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
5 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 7
3 7 before Dune::BiCGSTABSolver 
3 Dune::BiCGSTABSolver.apply(x, b, result_) 
3 BiCGSTABSolver::apply 
3 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
4 7 before Dune::BiCGSTABSolver 
4 Dune::BiCGSTABSolver.apply(x, b, result_) 
4 BiCGSTABSolver::apply 
6 7 before Dune::BiCGSTABSolver 
6 Dune::BiCGSTABSolver.apply(x, b, result_) 
6 BiCGSTABSolver::apply 
6 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 7
2 7 before Dune::BiCGSTABSolver 
2 Dune::BiCGSTABSolver.apply(x, b, result_) 
2 BiCGSTABSolver::apply 
5 7 before Dune::BiCGSTABSolver 
5 Dune::BiCGSTABSolver.apply(x, b, result_) 
5 BiCGSTABSolver::apply 
5 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
1 7 before Dune::BiCGSTABSolver 
1 Dune::BiCGSTABSolver.apply(x, b, result_) 
1 BiCGSTABSolver::apply 
1 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
2 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
4 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
Using a direct coarse solver (SuperLU)
Building hierarchy of 1 levels (inclusive coarse solver) took 0.200984 seconds.
0 7 before Dune::BiCGSTABSolver 
0 Dune::BiCGSTABSolver.apply(x, b, result_) 
0 BiCGSTABSolver::apply 
4 === BiCGSTABSolver
 Iter          Defect            Rate
    0      2.67443e-11
4  (it = 0.5; it < _maxit; it+=.5) 0.5 250
4  before p = r 
4  before  y = W^-1 * p 
4  before _prec->apply(y,p); 0.5 250
4 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
5 === BiCGSTABSolver
 Iter          Defect            Rate
    0      2.67443e-11
5  (it = 0.5; it < _maxit; it+=.5) 0.5 250
5  before p = r 
5  before  y = W^-1 * p 
5  before _prec->apply(y,p); 0.5 250
5 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
6 === BiCGSTABSolver
 Iter          Defect            Rate
    0      2.67443e-11
6  (it = 0.5; it < _maxit; it+=.5) 0.5 250
6  before p = r 
6  before  y = W^-1 * p 
6  before _prec->apply(y,p); 0.5 250
6 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
2 === BiCGSTABSolver
 Iter          Defect            Rate
    0      2.67443e-11
2  (it = 0.5; it < _maxit; it+=.5) 0.5 250
2  before p = r 
2  before  y = W^-1 * p 
2  before _prec->apply(y,p); 0.5 250
2 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
3 === BiCGSTABSolver
 Iter          Defect            Rate
    0      2.67443e-11
3  (it = 0.5; it < _maxit; it+=.5) 0.5 250
3  before p = r 
3  before  y = W^-1 * p 
3  before _prec->apply(y,p); 0.5 250
3 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
0 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
0 === BiCGSTABSolver
 Iter          Defect            Rate
    0      2.67443e-11
0  (it = 0.5; it < _maxit; it+=.5) 0.5 250
0  before p = r 
0  before  y = W^-1 * p 
0  before _prec->apply(y,p); 0.5 250
0 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 7
0 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
1 === BiCGSTABSolver
 Iter          Defect            Rate
    0      2.67443e-11
1  (it = 0.5; it < _maxit; it+=.5) 0.5 250
1  before p = r 
1  before  y = W^-1 * p 
1  before _prec->apply(y,p); 0.5 250
1 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
1 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
3 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
4 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
4 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
5 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
6 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
6 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
2 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
2 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
0 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
0 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
3 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
3  before v = A * y 
3  before  alpha = rho_new / < rt, v > 
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
4  before v = A * y 
4  before  alpha = rho_new / < rt, v > 
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
5 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
5  before v = A * y 
6  before v = A * y 
6  before  alpha = rho_new / < rt, v > 
0  before v = A * y 
0  before  alpha = rho_new / < rt, v > 
0  before alpha = rho_new / h; 
0  before apply first correction to x 
0  before r = r - alpha*v 
0  before test stop criteria 
0 istlsolver::printOutput 
  0.5      2.27208e-28      8.49557e-18
0 === rate=7.21747e-35, T=0.0354869, TIT=0.0709738, IT=0.5
0 finished solveLinearSystemImpl_ 1
0 converged value: 1 7 1 let s go get convergedRemote
0 to  comm_.min(converged) 
0 did  comm_.min(converged) convergedRemote: 1
0 final convergedRemote: 1
0 Update: x^(k+1) = x^k - deltax^k 
0 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
0 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
0 Newton iteration  2 done0 , maximum relative shift = 9.554e-04
0 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
3  before alpha = rho_new / h; 
3  before apply first correction to x 
3  before r = r - alpha*v 
3  before test stop criteria 
3 istlsolver::printOutput 
  0.5      2.27208e-28      8.49557e-18
3 === rate=7.21747e-35, T=0.230365, TIT=0.460729, IT=0.5
3 finished solveLinearSystemImpl_ 1
3 converged value: 1 7 1 let s go get convergedRemote
3 to  comm_.min(converged) 
3 did  comm_.min(converged) convergedRemote: 1
3 final convergedRemote: 1
3 Update: x^(k+1) = x^k - deltax^k 
3 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
3 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
3 Newton iteration  2 done3 , maximum relative shift = 9.554e-04
3 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
4  before alpha = rho_new / h; 
4  before apply first correction to x 
4  before r = r - alpha*v 
4  before test stop criteria 
4 istlsolver::printOutput 
  0.5      2.27208e-28      8.49557e-18
4 === rate=7.21747e-35, T=0.230263, TIT=0.460526, IT=0.5
4 finished solveLinearSystemImpl_ 1
4 converged value: 1 7 1 let s go get convergedRemote
4 to  comm_.min(converged) 
4 did  comm_.min(converged) convergedRemote: 1
4 final convergedRemote: 1
4 Update: x^(k+1) = x^k - deltax^k 
4 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
4 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
4 Newton iteration  2 done4 , maximum relative shift = 9.554e-04
4 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
5  before  alpha = rho_new / < rt, v > 
5  before alpha = rho_new / h; 
5  before apply first correction to x 
5  before r = r - alpha*v 
5  before test stop criteria 
5 istlsolver::printOutput 
  0.5      2.27208e-28      8.49557e-18
5 === rate=7.21747e-35, T=0.230367, TIT=0.460734, IT=0.5
5 finished solveLinearSystemImpl_ 1
5 converged value: 1 7 1 let s go get convergedRemote
5 to  comm_.min(converged) 
5 did  comm_.min(converged) convergedRemote: 1
5 final convergedRemote: 1
5 Update: x^(k+1) = x^k - deltax^k 
5 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
5 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
5 Newton iteration  2 done5 , maximum relative shift = 9.554e-04
5 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
6  before alpha = rho_new / h; 
6  before apply first correction to x 
6  before r = r - alpha*v 
6  before test stop criteria 
6 istlsolver::printOutput 
  0.5      2.27208e-28      8.49557e-18
6 === rate=7.21747e-35, T=0.23026, TIT=0.46052, IT=0.5
6 finished solveLinearSystemImpl_ 1
6 converged value: 1 7 1 let s go get convergedRemote
6 to  comm_.min(converged) 
6 did  comm_.min(converged) convergedRemote: 1
6 final convergedRemote: 1
6 Update: x^(k+1) = x^k - deltax^k 
6 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
6 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
6 Newton iteration  2 done6 , maximum relative shift = 9.554e-04
6 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
1 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
1  before v = A * y 
1  before  alpha = rho_new / < rt, v > 
1  before alpha = rho_new / h; 
1  before apply first correction to x 
1  before r = r - alpha*v 
1  before test stop criteria 
1 istlsolver::printOutput 
  0.5      2.27208e-28      8.49557e-18
1 === rate=7.21747e-35, T=0.230367, TIT=0.460733, IT=0.5
1 finished solveLinearSystemImpl_ 1
1 converged value: 1 7 1 let s go get convergedRemote
1 to  comm_.min(converged) 
1 did  comm_.min(converged) convergedRemote: 1
1 final convergedRemote: 1
1 Update: x^(k+1) = x^k - deltax^k 
1 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
1 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
1 Newton iteration  2 done1 , maximum relative shift = 9.554e-04
1 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
2  before v = A * y 
2  before  alpha = rho_new / < rt, v > 
2  before alpha = rho_new / h; 
2  before apply first correction to x 
2  before r = r - alpha*v 
2  before test stop criteria 
2 istlsolver::printOutput 
  0.5      2.27208e-28      8.49557e-18
2 === rate=7.21747e-35, T=0.230259, TIT=0.460517, IT=0.5
2 finished solveLinearSystemImpl_ 1
2 converged value: 1 7 1 let s go get convergedRemote
2 to  comm_.min(converged) 
2 did  comm_.min(converged) convergedRemote: 1
2 final convergedRemote: 1
2 Update: x^(k+1) = x^k - deltax^k 
2 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
2 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
2 Newton iteration  2 done2 , maximum relative shift = 9.554e-04
2 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
5 Solve: M deltax^k = r 
5 Newton::solveLinearSystem : entering the try section 2
5 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
5 solveLinearSystemImpl_, before converged = ls.solve 
5 7 amgbackend::solve isParallel 1
0 Solve: M deltax^k = r 
0 Newton::solveLinearSystem : entering the try section 2
0 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
0 solveLinearSystemImpl_, before converged = ls.solve 
0 7 amgbackend::solve isParallel 1
3 Solve: M deltax^k = r 
3 Newton::solveLinearSystem : entering the try section 2
3 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
3 solveLinearSystemImpl_, before converged = ls.solve 
3 7 amgbackend::solve isParallel 1
4 Solve: M deltax^k = r 
4 Newton::solveLinearSystem : entering the try section 2
4 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
4 solveLinearSystemImpl_, before converged = ls.solve 
4 7 amgbackend::solve isParallel 1
2 Solve: M deltax^k = r 
2 Newton::solveLinearSystem : entering the try section 2
2 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
2 solveLinearSystemImpl_, before converged = ls.solve 
2 7 amgbackend::solve isParallel 1
6 Solve: M deltax^k = r 
6 Newton::solveLinearSystem : entering the try section 2
6 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
6 solveLinearSystemImpl_, before converged = ls.solve 
6 7 amgbackend::solve isParallel 1
1 Solve: M deltax^k = r 
1 Newton::solveLinearSystem : entering the try section 2
1 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
1 solveLinearSystemImpl_, before converged = ls.solve 
1 7 amgbackend::solve isParallel 1
1 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
1 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
2 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
2 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
6 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
6 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
4 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
4 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
5 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
5 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
0 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
0 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 7
3 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
3 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 7
2 7 before Dune::BiCGSTABSolver 
2 Dune::BiCGSTABSolver.apply(x, b, result_) 
2 BiCGSTABSolver::apply 
2 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
6 7 before Dune::BiCGSTABSolver 
6 Dune::BiCGSTABSolver.apply(x, b, result_) 
6 BiCGSTABSolver::apply 
6 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
4 7 before Dune::BiCGSTABSolver 
4 Dune::BiCGSTABSolver.apply(x, b, result_) 
4 BiCGSTABSolver::apply 
4 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
5 7 before Dune::BiCGSTABSolver 
5 Dune::BiCGSTABSolver.apply(x, b, result_) 
5 BiCGSTABSolver::apply 
5 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
3 7 before Dune::BiCGSTABSolver 
3 Dune::BiCGSTABSolver.apply(x, b, result_) 
3 BiCGSTABSolver::apply 
3 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 7
1 7 before Dune::BiCGSTABSolver 
1 Dune::BiCGSTABSolver.apply(x, b, result_) 
1 BiCGSTABSolver::apply 
1 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
Using a direct coarse solver (SuperLU)
Building hierarchy of 1 levels (inclusive coarse solver) took 0.202643 seconds.
0 7 before Dune::BiCGSTABSolver 
0 Dune::BiCGSTABSolver.apply(x, b, result_) 
1 === BiCGSTABSolver
 Iter          Defect            Rate
    0      2.44917e-13
1  (it = 0.5; it < _maxit; it+=.5) 0.5 250
1  before p = r 
1  before  y = W^-1 * p 
1  before _prec->apply(y,p); 0.5 250
1 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
3 === BiCGSTABSolver
 Iter          Defect            Rate
    0      2.44917e-13
3  (it = 0.5; it < _maxit; it+=.5) 0.5 250
3  before p = r 
3  before  y = W^-1 * p 
3  before _prec->apply(y,p); 0.5 250
3 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
2 === BiCGSTABSolver
 Iter          Defect            Rate
    0      2.44917e-13
2  (it = 0.5; it < _maxit; it+=.5) 0.5 250
2  before p = r 
2  before  y = W^-1 * p 
2  before _prec->apply(y,p); 0.5 250
2 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
6 === BiCGSTABSolver
 Iter          Defect            Rate
    0      2.44917e-13
6  (it = 0.5; it < _maxit; it+=.5) 0.5 250
6  before p = r 
6  before  y = W^-1 * p 
6  before _prec->apply(y,p); 0.5 250
6 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
4 === BiCGSTABSolver
 Iter          Defect            Rate
    0      2.44917e-13
4  (it = 0.5; it < _maxit; it+=.5) 0.5 250
4  before p = r 
4  before  y = W^-1 * p 
4  before _prec->apply(y,p); 0.5 250
4 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
5 === BiCGSTABSolver
 Iter          Defect            Rate
    0      2.44917e-13
5  (it = 0.5; it < _maxit; it+=.5) 0.5 250
5  before p = r 
5  before  y = W^-1 * p 
5  before _prec->apply(y,p); 0.5 250
5 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
0 BiCGSTABSolver::apply 
0 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
0 === BiCGSTABSolver
 Iter          Defect            Rate
    0      2.44917e-13
0  (it = 0.5; it < _maxit; it+=.5) 0.5 250
0  before p = r 
0  before  y = W^-1 * p 
0  before _prec->apply(y,p); 0.5 250
0 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 7
0 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
5 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
1 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
3 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
2 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
2 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
6 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
6 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
4 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
4 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
6  before v = A * y 
6  before  alpha = rho_new / < rt, v > 
0 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
0 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
5 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
5  before v = A * y 
1 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
3 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
3  before v = A * y 
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
4  before v = A * y 
4  before  alpha = rho_new / < rt, v > 
4  before alpha = rho_new / h; 
4  before apply first correction to x 
4  before r = r - alpha*v 
4  before test stop criteria 
4 istlsolver::printOutput 
  0.5      7.99663e-31      3.26503e-18
4 === rate=1.06604e-35, T=0.226704, TIT=0.453408, IT=0.5
4 finished solveLinearSystemImpl_ 1
4 converged value: 1 7 1 let s go get convergedRemote
4 to  comm_.min(converged) 
0  before v = A * y 
0  before  alpha = rho_new / < rt, v > 
0  before alpha = rho_new / h; 
0  before apply first correction to x 
0  before r = r - alpha*v 
0  before test stop criteria 
0 istlsolver::printOutput 
  0.5      7.99663e-31      3.26503e-18
0 === rate=1.06604e-35, T=0.0300844, TIT=0.0601689, IT=0.5
5  before  alpha = rho_new / < rt, v > 
5  before alpha = rho_new / h; 
5  before apply first correction to x 
5  before r = r - alpha*v 
5  before test stop criteria 
5 istlsolver::printOutput 
  0.5      7.99663e-31      3.26503e-18
5 === rate=1.06604e-35, T=0.226774, TIT=0.453547, IT=0.5
5 finished solveLinearSystemImpl_ 1
5 converged value: 1 7 1 let s go get convergedRemote
5 to  comm_.min(converged) 
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
1  before v = A * y 
1  before  alpha = rho_new / < rt, v > 
1  before alpha = rho_new / h; 
1  before apply first correction to x 
1  before r = r - alpha*v 
1  before test stop criteria 
1 istlsolver::printOutput 
  0.5      7.99663e-31      3.26503e-18
1 === rate=1.06604e-35, T=0.226775, TIT=0.45355, IT=0.5
1 finished solveLinearSystemImpl_ 1
1 converged value: 1 7 1 let s go get convergedRemote
1 to  comm_.min(converged) 
3  before  alpha = rho_new / < rt, v > 
3  before alpha = rho_new / h; 
3  before apply first correction to x 
3  before r = r - alpha*v 
3  before test stop criteria 
3 istlsolver::printOutput 
  0.5      7.99663e-31      3.26503e-18
3 === rate=1.06604e-35, T=0.226774, TIT=0.453549, IT=0.5
3 finished solveLinearSystemImpl_ 1
3 converged value: 1 7 1 let s go get convergedRemote
3 to  comm_.min(converged) 
6  before alpha = rho_new / h; 
6  before apply first correction to x 
6  before r = r - alpha*v 
6  before test stop criteria 
6 istlsolver::printOutput 
  0.5      7.99663e-31      3.26503e-18
6 === rate=1.06604e-35, T=0.226706, TIT=0.453413, IT=0.5
6 finished solveLinearSystemImpl_ 1
6 converged value: 1 7 1 let s go get convergedRemote
6 to  comm_.min(converged) 
2  before v = A * y 
2  before  alpha = rho_new / < rt, v > 
2  before alpha = rho_new / h; 
2  before apply first correction to x 
2  before r = r - alpha*v 
2  before test stop criteria 
2 istlsolver::printOutput 
  0.5      7.99663e-31      3.26503e-18
2 === rate=1.06604e-35, T=0.226697, TIT=0.453394, IT=0.5
2 finished solveLinearSystemImpl_ 1
2 converged value: 1 7 1 let s go get convergedRemote
2 to  comm_.min(converged) 
2 did  comm_.min(converged) convergedRemote: 1
2 final convergedRemote: 1
2 Update: x^(k+1) = x^k - deltax^k 
2 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
2 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
2 Newton iteration  3 done2 , maximum relative shift = 8.815e-06
2 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
6 did  comm_.min(converged) convergedRemote: 1
6 final convergedRemote: 1
6 Update: x^(k+1) = x^k - deltax^k 
6 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
6 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
6 Newton iteration  3 done6 , maximum relative shift = 8.815e-06
6 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
4 did  comm_.min(converged) convergedRemote: 1
4 final convergedRemote: 1
4 Update: x^(k+1) = x^k - deltax^k 
4 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
4 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
4 Newton iteration  3 done4 , maximum relative shift = 8.815e-06
4 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
0 finished solveLinearSystemImpl_ 1
0 converged value: 1 7 1 let s go get convergedRemote
0 to  comm_.min(converged) 
0 did  comm_.min(converged) convergedRemote: 1
0 final convergedRemote: 1
0 Update: x^(k+1) = x^k - deltax^k 
0 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
0 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
0 Newton iteration  3 done0 , maximum relative shift = 8.815e-06
0 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
5 did  comm_.min(converged) convergedRemote: 1
5 final convergedRemote: 1
5 Update: x^(k+1) = x^k - deltax^k 
5 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
5 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
5 Newton iteration  3 done5 , maximum relative shift = 8.815e-06
5 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
1 did  comm_.min(converged) convergedRemote: 1
1 final convergedRemote: 1
1 Update: x^(k+1) = x^k - deltax^k 
1 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
1 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
1 Newton iteration  3 done1 , maximum relative shift = 8.815e-06
1 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
3 did  comm_.min(converged) convergedRemote: 1
3 final convergedRemote: 1
3 Update: x^(k+1) = x^k - deltax^k 
3 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
3 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
3 Newton iteration  3 done3 , maximum relative shift = 8.815e-06
3 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
3 Solve: M deltax^k = r 
3 Newton::solveLinearSystem : entering the try section 3
3 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
3 solveLinearSystemImpl_, before converged = ls.solve 
3 7 amgbackend::solve isParallel 1
2 Solve: M deltax^k = r 
2 Newton::solveLinearSystem : entering the try section 3
2 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
2 solveLinearSystemImpl_, before converged = ls.solve 
2 7 amgbackend::solve isParallel 1
6 Solve: M deltax^k = r 
6 Newton::solveLinearSystem : entering the try section 3
6 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
6 solveLinearSystemImpl_, before converged = ls.solve 
6 7 amgbackend::solve isParallel 1
4 Solve: M deltax^k = r 
4 Newton::solveLinearSystem : entering the try section 3
4 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
4 solveLinearSystemImpl_, before converged = ls.solve 
4 7 amgbackend::solve isParallel 1
0 Solve: M deltax^k = r 
0 Newton::solveLinearSystem : entering the try section 3
0 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
0 solveLinearSystemImpl_, before converged = ls.solve 
0 7 amgbackend::solve isParallel 1
5 Solve: M deltax^k = r 
5 Newton::solveLinearSystem : entering the try section 3
5 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
5 solveLinearSystemImpl_, before converged = ls.solve 
5 7 amgbackend::solve isParallel 1
1 Solve: M deltax^k = r 
1 Newton::solveLinearSystem : entering the try section 3
1 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
1 solveLinearSystemImpl_, before converged = ls.solve 
1 7 amgbackend::solve isParallel 1
4 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
4 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
0 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
0 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 7
5 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
5 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
1 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
1 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
3 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
3 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
2 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
2 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
6 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
6 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 7
1 7 before Dune::BiCGSTABSolver 
1 Dune::BiCGSTABSolver.apply(x, b, result_) 
1 BiCGSTABSolver::apply 
1 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
3 7 before Dune::BiCGSTABSolver 
3 Dune::BiCGSTABSolver.apply(x, b, result_) 
3 BiCGSTABSolver::apply 
3 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
2 7 before Dune::BiCGSTABSolver 
2 Dune::BiCGSTABSolver.apply(x, b, result_) 
2 BiCGSTABSolver::apply 
2 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
5 7 before Dune::BiCGSTABSolver 
5 Dune::BiCGSTABSolver.apply(x, b, result_) 
5 BiCGSTABSolver::apply 
5 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
4 7 before Dune::BiCGSTABSolver 
4 Dune::BiCGSTABSolver.apply(x, b, result_) 
4 BiCGSTABSolver::apply 
4 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
6 7 before Dune::BiCGSTABSolver 
6 Dune::BiCGSTABSolver.apply(x, b, result_) 
6 BiCGSTABSolver::apply 
6 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 7
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
Using a direct coarse solver (SuperLU)
Building hierarchy of 1 levels (inclusive coarse solver) took 0.205513 seconds.
0 7 before Dune::BiCGSTABSolver 
0 Dune::BiCGSTABSolver.apply(x, b, result_) 
0 BiCGSTABSolver::apply 
0 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
0 === BiCGSTABSolver
 Iter          Defect            Rate
    0      2.03474e-15
0  (it = 0.5; it < _maxit; it+=.5) 0.5 250
0  before p = r 
0  before  y = W^-1 * p 
0  before _prec->apply(y,p); 0.5 250
0 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 7
6 === BiCGSTABSolver
 Iter          Defect            Rate
    0      2.03474e-15
6  (it = 0.5; it < _maxit; it+=.5) 0.5 250
6  before p = r 
6  before  y = W^-1 * p 
6  before _prec->apply(y,p); 0.5 250
6 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
1 === BiCGSTABSolver
 Iter          Defect            Rate
    0      2.03474e-15
1  (it = 0.5; it < _maxit; it+=.5) 0.5 250
1  before p = r 
1  before  y = W^-1 * p 
1  before _prec->apply(y,p); 0.5 250
1 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
3 === BiCGSTABSolver
 Iter          Defect            Rate
    0      2.03474e-15
3  (it = 0.5; it < _maxit; it+=.5) 0.5 250
3  before p = r 
3  before  y = W^-1 * p 
3  before _prec->apply(y,p); 0.5 250
3 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
2 === BiCGSTABSolver
 Iter          Defect            Rate
    0      2.03474e-15
2  (it = 0.5; it < _maxit; it+=.5) 0.5 250
2  before p = r 
2  before  y = W^-1 * p 
2  before _prec->apply(y,p); 0.5 250
2 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
5 === BiCGSTABSolver
 Iter          Defect            Rate
    0      2.03474e-15
5  (it = 0.5; it < _maxit; it+=.5) 0.5 250
5  before p = r 
5  before  y = W^-1 * p 
5  before _prec->apply(y,p); 0.5 250
5 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
4 === BiCGSTABSolver
 Iter          Defect            Rate
    0      2.03474e-15
4  (it = 0.5; it < _maxit; it+=.5) 0.5 250
4  before p = r 
4  before  y = W^-1 * p 
4  before _prec->apply(y,p); 0.5 250
4 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 7
0 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
6 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
6 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
1 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
3 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
3 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
2 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
2 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
5 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
5 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
0 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
4 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
4 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
0 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
0  before v = A * y 
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
5  before v = A * y 
6  before v = A * y 
6  before  alpha = rho_new / < rt, v > 
1 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
1  before v = A * y 
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
3  before v = A * y 
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
2  before v = A * y 
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
4  before v = A * y 
4  before  alpha = rho_new / < rt, v > 
6  before alpha = rho_new / h; 
6  before apply first correction to x 
6  before r = r - alpha*v 
6  before test stop criteria 
6 istlsolver::printOutput 
  0.5      8.26614e-34      4.06249e-19
6 === rate=1.65039e-37, T=0.229278, TIT=0.458555, IT=0.5
6 finished solveLinearSystemImpl_ 1
6 converged value: 1 7 1 let s go get convergedRemote
6 to  comm_.min(converged) 
1  before  alpha = rho_new / < rt, v > 
1  before alpha = rho_new / h; 
1  before apply first correction to x 
1  before r = r - alpha*v 
1  before test stop criteria 
1 istlsolver::printOutput 
  0.5      8.26614e-34      4.06249e-19
1 === rate=1.65039e-37, T=0.22922, TIT=0.45844, IT=0.5
1 finished solveLinearSystemImpl_ 1
1 converged value: 1 7 1 let s go get convergedRemote
1 to  comm_.min(converged) 
3  before  alpha = rho_new / < rt, v > 
3  before alpha = rho_new / h; 
3  before apply first correction to x 
3  before r = r - alpha*v 
3  before test stop criteria 
3 istlsolver::printOutput 
  0.5      8.26614e-34      4.06249e-19
3 === rate=1.65039e-37, T=0.229219, TIT=0.458438, IT=0.5
3 finished solveLinearSystemImpl_ 1
3 converged value: 1 7 1 let s go get convergedRemote
3 to  comm_.min(converged) 
4  before alpha = rho_new / h; 
4  before apply first correction to x 
4  before r = r - alpha*v 
4  before test stop criteria 
4 istlsolver::printOutput 
  0.5      8.26614e-34      4.06249e-19
4 === rate=1.65039e-37, T=0.229274, TIT=0.458547, IT=0.5
4 finished solveLinearSystemImpl_ 1
4 converged value: 1 7 1 let s go get convergedRemote
4 to  comm_.min(converged) 
2  before  alpha = rho_new / < rt, v > 
2  before alpha = rho_new / h; 
2  before apply first correction to x 
2  before r = r - alpha*v 
2  before test stop criteria 
2 istlsolver::printOutput 
  0.5      8.26614e-34      4.06249e-19
2 === rate=1.65039e-37, T=0.229269, TIT=0.458538, IT=0.5
2 finished solveLinearSystemImpl_ 1
2 converged value: 1 7 1 let s go get convergedRemote
2 to  comm_.min(converged) 
0  before  alpha = rho_new / < rt, v > 
0  before alpha = rho_new / h; 
0  before apply first correction to x 
0  before r = r - alpha*v 
0  before test stop criteria 
0 istlsolver::printOutput 
  0.5      8.26614e-34      4.06249e-19
0 === rate=1.65039e-37, T=0.0298263, TIT=0.0596527, IT=0.5
5  before  alpha = rho_new / < rt, v > 
5  before alpha = rho_new / h; 
5  before apply first correction to x 
5  before r = r - alpha*v 
5  before test stop criteria 
5 istlsolver::printOutput 
  0.5      8.26614e-34      4.06249e-19
5 === rate=1.65039e-37, T=0.229219, TIT=0.458438, IT=0.5
5 finished solveLinearSystemImpl_ 1
5 converged value: 1 7 1 let s go get convergedRemote
5 to  comm_.min(converged) 
2 did  comm_.min(converged) convergedRemote: 1
2 final convergedRemote: 1
2 Update: x^(k+1) = x^k - deltax^k 
2 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
2 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
2 Newton iteration  4 done2 , maximum relative shift = 7.561e-08
0 finished solveLinearSystemImpl_ 1
0 converged value: 1 7 1 let s go get convergedRemote
0 to  comm_.min(converged) 
0 did  comm_.min(converged) convergedRemote: 1
0 final convergedRemote: 1
0 Update: x^(k+1) = x^k - deltax^k 
0 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
0 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
3 did  comm_.min(converged) convergedRemote: 1
3 final convergedRemote: 1
3 Update: x^(k+1) = x^k - deltax^k 
3 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
3 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
3 Newton iteration  4 done3 , maximum relative shift = 7.561e-08
3 Assemble/solve/update time: 0.297501(23.9605%)/0.943605(75.9972%)/0.000524677(0.0422571%)
3 gridVariables->advanceTimeStep
3 before, nonLinearSolver->suggestTimeStepSize, current time: 253.703 simTime 10800
3 before assembler->setPreviousSolution, ddt: 30
3 nonLinearSolver->solve
6 did  comm_.min(converged) convergedRemote: 1
6 final convergedRemote: 1
6 Update: x^(k+1) = x^k - deltax^k 
6 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
6 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
6 Newton iteration  4 done6 , maximum relative shift = 7.561e-08
1 did  comm_.min(converged) convergedRemote: 1
1 final convergedRemote: 1
1 Update: x^(k+1) = x^k - deltax^k 
1 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
1 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
1 Newton iteration  4 done1 , maximum relative shift = 7.561e-08
1 Assemble/solve/update time: 0.297628(23.9685%)/0.943593(75.9891%)/0.000526627(0.0424102%)
1 gridVariables->advanceTimeStep
1 before, nonLinearSolver->suggestTimeStepSize, current time: 253.703 simTime 10800
1 before assembler->setPreviousSolution, ddt: 30
1 nonLinearSolver->solve
5 did  comm_.min(converged) convergedRemote: 1
5 final convergedRemote: 1
5 Update: x^(k+1) = x^k - deltax^k 
5 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
5 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
5 Newton iteration  4 done5 , maximum relative shift = 7.561e-08
5 Assemble/solve/update time: 0.297642(23.9694%)/0.943588(75.9882%)/0.000526487(0.0423986%)
5 gridVariables->advanceTimeStep
5 before, nonLinearSolver->suggestTimeStepSize, current time: 253.703 simTime 10800
5 before assembler->setPreviousSolution, ddt: 30
5 nonLinearSolver->solve
4 did  comm_.min(converged) convergedRemote: 1
4 final convergedRemote: 1
4 Update: x^(k+1) = x^k - deltax^k 
4 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
4 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
4 Newton iteration  4 done4 , maximum relative shift = 7.561e-08
6 Assemble/solve/update time: 0.297328(23.9447%)/0.943924(76.0168%)/0.000478405(0.0385273%)
6 gridVariables->advanceTimeStep
6 before, nonLinearSolver->suggestTimeStepSize, current time: 253.703 simTime 10800
6 before assembler->setPreviousSolution, ddt: 30
6 nonLinearSolver->solve
6 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
4 Assemble/solve/update time: 0.297178(23.9358%)/0.943903(76.0254%)/0.000481365(0.0387709%)
4 gridVariables->advanceTimeStep
4 before, nonLinearSolver->suggestTimeStepSize, current time: 253.703 simTime 10800
4 before assembler->setPreviousSolution, ddt: 30
4 nonLinearSolver->solve
4 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
5 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
1 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
2 Assemble/solve/update time: 0.297145(23.9335%)/0.943921(76.028%)/0.000478085(0.0385073%)
2 gridVariables->advanceTimeStep
2 before, nonLinearSolver->suggestTimeStepSize, current time: 253.703 simTime 10800
2 before assembler->setPreviousSolution, ddt: 30
2 nonLinearSolver->solve
2 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
0 Newton iteration  4 done0 , maximum relative shift = 7.561e-08
0 Assemble/solve/update time: 0.297121(23.9323%)/0.943907(76.0293%)/0.000476505(0.0383813%)
0 gridVariables->advanceTimeStep
0 before, nonLinearSolver->suggestTimeStepSize, current time: 253.703 simTime 10800
0 before assembler->setPreviousSolution, ddt: 30
0 nonLinearSolver->solve
0 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
3 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
2 Solve: M deltax^k = r 
2 Newton::solveLinearSystem : entering the try section 0
2to  comm_.sum(norm2) 
0 Solve: M deltax^k = r 
0 Newton::solveLinearSystem : entering the try section 0
0to  comm_.sum(norm2) 
6 Solve: M deltax^k = r 
6 Newton::solveLinearSystem : entering the try section 0
6to  comm_.sum(norm2) 
3 Solve: M deltax^k = r 
3 Newton::solveLinearSystem : entering the try section 0
3to  comm_.sum(norm2) 
4 Solve: M deltax^k = r 
4 Newton::solveLinearSystem : entering the try section 0
4to  comm_.sum(norm2) 
5 Solve: M deltax^k = r 
5 Newton::solveLinearSystem : entering the try section 0
5to  comm_.sum(norm2) 
1 Solve: M deltax^k = r 
1 Newton::solveLinearSystem : entering the try section 0
1to  comm_.sum(norm2) 
0did  comm_.sum(norm2) 
0 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
0 solveLinearSystemImpl_, before converged = ls.solve 
0 7 amgbackend::solve isParallel 1
1did  comm_.sum(norm2) 
1 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
1 solveLinearSystemImpl_, before converged = ls.solve 
1 7 amgbackend::solve isParallel 1
3did  comm_.sum(norm2) 
3 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
3 solveLinearSystemImpl_, before converged = ls.solve 
3 7 amgbackend::solve isParallel 1
4did  comm_.sum(norm2) 
4 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
4 solveLinearSystemImpl_, before converged = ls.solve 
4 7 amgbackend::solve isParallel 1
5did  comm_.sum(norm2) 
5 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
5 solveLinearSystemImpl_, before converged = ls.solve 
5 7 amgbackend::solve isParallel 1
6did  comm_.sum(norm2) 
6 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
6 solveLinearSystemImpl_, before converged = ls.solve 
6 7 amgbackend::solve isParallel 1
2did  comm_.sum(norm2) 
2 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
2 solveLinearSystemImpl_, before converged = ls.solve 
2 7 amgbackend::solve isParallel 1
2 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
2 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
1 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
1 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
3 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
3 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
4 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
4 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
5 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
5 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
6 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
6 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
0 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
0 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 7
2 7 before Dune::BiCGSTABSolver 
2 Dune::BiCGSTABSolver.apply(x, b, result_) 
2 BiCGSTABSolver::apply 
2 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
1 7 before Dune::BiCGSTABSolver 
1 Dune::BiCGSTABSolver.apply(x, b, result_) 
1 BiCGSTABSolver::apply 
1 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
3 7 before Dune::BiCGSTABSolver 
3 Dune::BiCGSTABSolver.apply(x, b, result_) 
3 BiCGSTABSolver::apply 
3 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
4 7 before Dune::BiCGSTABSolver 
4 Dune::BiCGSTABSolver.apply(x, b, result_) 
4 BiCGSTABSolver::apply 
4 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
5 7 before Dune::BiCGSTABSolver 
5 Dune::BiCGSTABSolver.apply(x, b, result_) 
5 BiCGSTABSolver::apply 
5 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
6 7 before Dune::BiCGSTABSolver 
6 Dune::BiCGSTABSolver.apply(x, b, result_) 
6 BiCGSTABSolver::apply 
6 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
Using a direct coarse solver (SuperLU)
Building hierarchy of 1 levels (inclusive coarse solver) took 0.201213 seconds.
0 7 before Dune::BiCGSTABSolver 
0 Dune::BiCGSTABSolver.apply(x, b, result_) 
6 === BiCGSTABSolver
 Iter          Defect            Rate
    0      4.47819e-09
6  (it = 0.5; it < _maxit; it+=.5) 0.5 250
6  before p = r 
6  before  y = W^-1 * p 
6  before _prec->apply(y,p); 0.5 250
6 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
2 === BiCGSTABSolver
 Iter          Defect            Rate
    0      4.47819e-09
2  (it = 0.5; it < _maxit; it+=.5) 0.5 250
2  before p = r 
2  before  y = W^-1 * p 
2  before _prec->apply(y,p); 0.5 250
2 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
1 === BiCGSTABSolver
 Iter          Defect            Rate
    0      4.47819e-09
1  (it = 0.5; it < _maxit; it+=.5) 0.5 250
1  before p = r 
1  before  y = W^-1 * p 
1  before _prec->apply(y,p); 0.5 250
1 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
3 === BiCGSTABSolver
 Iter          Defect            Rate
    0      4.47819e-09
3  (it = 0.5; it < _maxit; it+=.5) 0.5 250
3  before p = r 
3  before  y = W^-1 * p 
3  before _prec->apply(y,p); 0.5 250
3 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
4 === BiCGSTABSolver
 Iter          Defect            Rate
    0      4.47819e-09
4  (it = 0.5; it < _maxit; it+=.5) 0.5 250
4  before p = r 
4  before  y = W^-1 * p 
4  before _prec->apply(y,p); 0.5 250
4 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
5 === BiCGSTABSolver
 Iter          Defect            Rate
    0      4.47819e-09
5  (it = 0.5; it < _maxit; it+=.5) 0.5 250
5  before p = r 
5  before  y = W^-1 * p 
5  before _prec->apply(y,p); 0.5 250
5 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
0 BiCGSTABSolver::apply 
0 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
0 === BiCGSTABSolver
 Iter          Defect            Rate
    0      4.47819e-09
0  (it = 0.5; it < _maxit; it+=.5) 0.5 250
0  before p = r 
0  before  y = W^-1 * p 
0  before _prec->apply(y,p); 0.5 250
0 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 7
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
0 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
3 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
4 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
4 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
5 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
6 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
2 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
2 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
1 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
6 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
6  before v = A * y 
6  before  alpha = rho_new / < rt, v > 
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
2  before v = A * y 
2  before  alpha = rho_new / < rt, v > 
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
4  before v = A * y 
4  before  alpha = rho_new / < rt, v > 
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
3 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
3  before v = A * y 
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
1 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
1  before v = A * y 
1  before  alpha = rho_new / < rt, v > 
0 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
0 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
0  before v = A * y 
0  before  alpha = rho_new / < rt, v > 
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
5 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
5  before v = A * y 
5  before  alpha = rho_new / < rt, v > 
1  before alpha = rho_new / h; 
1  before apply first correction to x 
1  before r = r - alpha*v 
1  before test stop criteria 
1 istlsolver::printOutput 
  0.5      7.62532e-25      1.70277e-16
1 === rate=2.89942e-32, T=0.225459, TIT=0.450917, IT=0.5
1 finished solveLinearSystemImpl_ 1
1 converged value: 1 7 1 let s go get convergedRemote
1 to  comm_.min(converged) 
1 did  comm_.min(converged) convergedRemote: 1
1 final convergedRemote: 1
1 Update: x^(k+1) = x^k - deltax^k 
1 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
1 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
1 Newton iteration  1 done1 , maximum relative shift = 2.093e-02
1 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
0  before alpha = rho_new / h; 
0  before apply first correction to x 
0  before r = r - alpha*v 
0  before test stop criteria 
0 istlsolver::printOutput 
  0.5      7.62532e-25      1.70277e-16
0 === rate=2.89942e-32, T=0.0300749, TIT=0.0601497, IT=0.5
0 finished solveLinearSystemImpl_ 1
0 converged value: 1 7 1 let s go get convergedRemote
0 to  comm_.min(converged) 
0 did  comm_.min(converged) convergedRemote: 1
0 final convergedRemote: 1
0 Update: x^(k+1) = x^k - deltax^k 
0 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
0 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
0 Newton iteration  1 done0 , maximum relative shift = 2.093e-02
0 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
3  before  alpha = rho_new / < rt, v > 
3  before alpha = rho_new / h; 
3  before apply first correction to x 
3  before r = r - alpha*v 
3  before test stop criteria 
3 istlsolver::printOutput 
  0.5      7.62532e-25      1.70277e-16
3 === rate=2.89942e-32, T=0.225458, TIT=0.450915, IT=0.5
3 finished solveLinearSystemImpl_ 1
3 converged value: 1 7 1 let s go get convergedRemote
3 to  comm_.min(converged) 
3 did  comm_.min(converged) convergedRemote: 1
3 final convergedRemote: 1
3 Update: x^(k+1) = x^k - deltax^k 
3 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
3 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
3 Newton iteration  1 done3 , maximum relative shift = 2.093e-02
3 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
6  before alpha = rho_new / h; 
6  before apply first correction to x 
6  before r = r - alpha*v 
6  before test stop criteria 
6 istlsolver::printOutput 
  0.5      7.62532e-25      1.70277e-16
6 === rate=2.89942e-32, T=0.225542, TIT=0.451084, IT=0.5
6 finished solveLinearSystemImpl_ 1
6 converged value: 1 7 1 let s go get convergedRemote
6 to  comm_.min(converged) 
6 did  comm_.min(converged) convergedRemote: 1
6 final convergedRemote: 1
6 Update: x^(k+1) = x^k - deltax^k 
6 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
6 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
6 Newton iteration  1 done6 , maximum relative shift = 2.093e-02
6 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
2  before alpha = rho_new / h; 
2  before apply first correction to x 
2  before r = r - alpha*v 
2  before test stop criteria 
2 istlsolver::printOutput 
  0.5      7.62532e-25      1.70277e-16
2 === rate=2.89942e-32, T=0.225535, TIT=0.45107, IT=0.5
2 finished solveLinearSystemImpl_ 1
2 converged value: 1 7 1 let s go get convergedRemote
2 to  comm_.min(converged) 
2 did  comm_.min(converged) convergedRemote: 1
2 final convergedRemote: 1
2 Update: x^(k+1) = x^k - deltax^k 
2 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
2 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
2 Newton iteration  1 done2 , maximum relative shift = 2.093e-02
2 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
4  before alpha = rho_new / h; 
4  before apply first correction to x 
4  before r = r - alpha*v 
4  before test stop criteria 
4 istlsolver::printOutput 
  0.5      7.62532e-25      1.70277e-16
4 === rate=2.89942e-32, T=0.225539, TIT=0.451078, IT=0.5
4 finished solveLinearSystemImpl_ 1
4 converged value: 1 7 1 let s go get convergedRemote
4 to  comm_.min(converged) 
4 did  comm_.min(converged) convergedRemote: 1
4 final convergedRemote: 1
4 Update: x^(k+1) = x^k - deltax^k 
4 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
4 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
4 Newton iteration  1 done4 , maximum relative shift = 2.093e-02
4 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
5  before alpha = rho_new / h; 
5  before apply first correction to x 
5  before r = r - alpha*v 
5  before test stop criteria 
5 istlsolver::printOutput 
  0.5      7.62532e-25      1.70277e-16
5 === rate=2.89942e-32, T=0.225457, TIT=0.450914, IT=0.5
5 finished solveLinearSystemImpl_ 1
5 converged value: 1 7 1 let s go get convergedRemote
5 to  comm_.min(converged) 
5 did  comm_.min(converged) convergedRemote: 1
5 final convergedRemote: 1
5 Update: x^(k+1) = x^k - deltax^k 
5 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
5 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
5 Newton iteration  1 done5 , maximum relative shift = 2.093e-02
5 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
4 Solve: M deltax^k = r 
4 Newton::solveLinearSystem : entering the try section 1
4 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
4 solveLinearSystemImpl_, before converged = ls.solve 
4 7 amgbackend::solve isParallel 1
1 Solve: M deltax^k = r 
1 Newton::solveLinearSystem : entering the try section 1
1 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
1 solveLinearSystemImpl_, before converged = ls.solve 
1 7 amgbackend::solve isParallel 1
0 Solve: M deltax^k = r 
0 Newton::solveLinearSystem : entering the try section 1
0 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
0 solveLinearSystemImpl_, before converged = ls.solve 
0 7 amgbackend::solve isParallel 1
3 Solve: M deltax^k = r 
3 Newton::solveLinearSystem : entering the try section 1
3 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
3 solveLinearSystemImpl_, before converged = ls.solve 
3 7 amgbackend::solve isParallel 1
6 Solve: M deltax^k = r 
6 Newton::solveLinearSystem : entering the try section 1
6 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
6 solveLinearSystemImpl_, before converged = ls.solve 
6 7 amgbackend::solve isParallel 1
2 Solve: M deltax^k = r 
2 Newton::solveLinearSystem : entering the try section 1
2 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
2 solveLinearSystemImpl_, before converged = ls.solve 
2 7 amgbackend::solve isParallel 1
5 Solve: M deltax^k = r 
5 Newton::solveLinearSystem : entering the try section 1
5 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
5 solveLinearSystemImpl_, before converged = ls.solve 
5 7 amgbackend::solve isParallel 1
1 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
1 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
0 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
0 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 7
3 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
3 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
6 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
6 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
5 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
5 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
2 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
2 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
4 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
4 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 7
5 7 before Dune::BiCGSTABSolver 
5 Dune::BiCGSTABSolver.apply(x, b, result_) 
5 BiCGSTABSolver::apply 
5 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
2 7 before Dune::BiCGSTABSolver 
2 Dune::BiCGSTABSolver.apply(x, b, result_) 
2 BiCGSTABSolver::apply 
2 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
6 7 before Dune::BiCGSTABSolver 
6 Dune::BiCGSTABSolver.apply(x, b, result_) 
6 BiCGSTABSolver::apply 
6 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
1 7 before Dune::BiCGSTABSolver 
1 Dune::BiCGSTABSolver.apply(x, b, result_) 
1 BiCGSTABSolver::apply 
1 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
4 7 before Dune::BiCGSTABSolver 
4 Dune::BiCGSTABSolver.apply(x, b, result_) 
4 BiCGSTABSolver::apply 
4 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
3 7 before Dune::BiCGSTABSolver 
3 Dune::BiCGSTABSolver.apply(x, b, result_) 
3 BiCGSTABSolver::apply 
3 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 7
Using a direct coarse solver (SuperLU)
Building hierarchy of 1 levels (inclusive coarse solver) took 0.202454 seconds.
0 7 before Dune::BiCGSTABSolver 
0 Dune::BiCGSTABSolver.apply(x, b, result_) 
5 === BiCGSTABSolver
 Iter          Defect            Rate
    0      2.28692e-11
5  (it = 0.5; it < _maxit; it+=.5) 0.5 250
5  before p = r 
5  before  y = W^-1 * p 
5  before _prec->apply(y,p); 0.5 250
5 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
2 === BiCGSTABSolver
 Iter          Defect            Rate
    0      2.28692e-11
2  (it = 0.5; it < _maxit; it+=.5) 0.5 250
2  before p = r 
2  before  y = W^-1 * p 
2  before _prec->apply(y,p); 0.5 250
2 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
6 === BiCGSTABSolver
 Iter          Defect            Rate
    0      2.28692e-11
6  (it = 0.5; it < _maxit; it+=.5) 0.5 250
6  before p = r 
6  before  y = W^-1 * p 
6  before _prec->apply(y,p); 0.5 250
6 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
1 === BiCGSTABSolver
 Iter          Defect            Rate
    0      2.28692e-11
1  (it = 0.5; it < _maxit; it+=.5) 0.5 250
1  before p = r 
1  before  y = W^-1 * p 
1  before _prec->apply(y,p); 0.5 250
1 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
4 === BiCGSTABSolver
 Iter          Defect            Rate
    0      2.28692e-11
4  (it = 0.5; it < _maxit; it+=.5) 0.5 250
4  before p = r 
4  before  y = W^-1 * p 
4  before _prec->apply(y,p); 0.5 250
4 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
0 BiCGSTABSolver::apply 
0 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
0 === BiCGSTABSolver
 Iter          Defect            Rate
    0      2.28692e-11
0  (it = 0.5; it < _maxit; it+=.5) 0.5 250
0  before p = r 
0  before  y = W^-1 * p 
0  before _prec->apply(y,p); 0.5 250
0 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 7
3 === BiCGSTABSolver
 Iter          Defect            Rate
    0      2.28692e-11
3  (it = 0.5; it < _maxit; it+=.5) 0.5 250
3  before p = r 
3  before  y = W^-1 * p 
3  before _prec->apply(y,p); 0.5 250
3 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 7
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 7
0 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
1 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
3 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
6 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
6 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
5 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
4 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
4 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
2 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
2 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
3 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
3  before v = A * y 
6  before v = A * y 
6  before  alpha = rho_new / < rt, v > 
5 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
5  before v = A * y 
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
4  before v = A * y 
4  before  alpha = rho_new / < rt, v > 
1 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
1  before v = A * y 
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
2  before v = A * y 
0 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
0 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
0  before v = A * y 
0  before  alpha = rho_new / < rt, v > 
0  before alpha = rho_new / h; 
0  before apply first correction to x 
0  before r = r - alpha*v 
0  before test stop criteria 
0 istlsolver::printOutput 
  0.5      1.84902e-28      8.08522e-18
0 === rate=6.53707e-35, T=0.0352512, TIT=0.0705023, IT=0.5
0 finished solveLinearSystemImpl_ 1
0 converged value: 1 7 1 let s go get convergedRemote
0 to  comm_.min(converged) 
2  before  alpha = rho_new / < rt, v > 
2  before alpha = rho_new / h; 
2  before apply first correction to x 
2  before r = r - alpha*v 
2  before test stop criteria 
2 istlsolver::printOutput 
  0.5      1.84902e-28      8.08522e-18
2 === rate=6.53707e-35, T=0.231697, TIT=0.463394, IT=0.5
2 finished solveLinearSystemImpl_ 1
2 converged value: 1 7 1 let s go get convergedRemote
2 to  comm_.min(converged) 
1  before  alpha = rho_new / < rt, v > 
1  before alpha = rho_new / h; 
1  before apply first correction to x 
1  before r = r - alpha*v 
1  before test stop criteria 
1 istlsolver::printOutput 
  0.5      1.84902e-28      8.08522e-18
1 === rate=6.53707e-35, T=0.23175, TIT=0.463501, IT=0.5
1 finished solveLinearSystemImpl_ 1
1 converged value: 1 7 1 let s go get convergedRemote
1 to  comm_.min(converged) 
1 did  comm_.min(converged) convergedRemote: 1
1 final convergedRemote: 1
1 Update: x^(k+1) = x^k - deltax^k 
1 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
3  before  alpha = rho_new / < rt, v > 
3  before alpha = rho_new / h; 
3  before apply first correction to x 
3  before r = r - alpha*v 
3  before test stop criteria 
3 istlsolver::printOutput 
  0.5      1.84902e-28      8.08522e-18
3 === rate=6.53707e-35, T=0.231753, TIT=0.463506, IT=0.5
3 finished solveLinearSystemImpl_ 1
3 converged value: 1 7 1 let s go get convergedRemote
3 to  comm_.min(converged) 
3 did  comm_.min(converged) convergedRemote: 1
3 final convergedRemote: 1
3 Update: x^(k+1) = x^k - deltax^k 
3 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
6  before alpha = rho_new / h; 
6  before apply first correction to x 
6  before r = r - alpha*v 
6  before test stop criteria 
6 istlsolver::printOutput 
  0.5      1.84902e-28      8.08522e-18
6 === rate=6.53707e-35, T=0.231705, TIT=0.46341, IT=0.5
6 finished solveLinearSystemImpl_ 1
6 converged value: 1 7 1 let s go get convergedRemote
6 to  comm_.min(converged) 
5  before  alpha = rho_new / < rt, v > 
5  before alpha = rho_new / h; 
5  before apply first correction to x 
5  before r = r - alpha*v 
5  before test stop criteria 
5 istlsolver::printOutput 
  0.5      1.84902e-28      8.08522e-18
5 === rate=6.53707e-35, T=0.23175, TIT=0.4635, IT=0.5
5 finished solveLinearSystemImpl_ 1
5 converged value: 1 7 1 let s go get convergedRemote
5 to  comm_.min(converged) 
5 did  comm_.min(converged) convergedRemote: 1
5 final convergedRemote: 1
5 Update: x^(k+1) = x^k - deltax^k 
5 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
4  before alpha = rho_new / h; 
4  before apply first correction to x 
4  before r = r - alpha*v 
4  before test stop criteria 
4 istlsolver::printOutput 
  0.5      1.84902e-28      8.08522e-18
4 === rate=6.53707e-35, T=0.231698, TIT=0.463396, IT=0.5
4 finished solveLinearSystemImpl_ 1
4 converged value: 1 7 1 let s go get convergedRemote
4 to  comm_.min(converged) 
0 did  comm_.min(converged) convergedRemote: 1
0 final convergedRemote: 1
0 Update: x^(k+1) = x^k - deltax^k 
0 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
0 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
0 Newton iteration  2 done0 , maximum relative shift = 8.463e-04
0 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
2 did  comm_.min(converged) convergedRemote: 1
2 final convergedRemote: 1
2 Update: x^(k+1) = x^k - deltax^k 
2 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
2 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
2 Newton iteration  2 done2 , maximum relative shift = 8.463e-04
2 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
1 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
1 Newton iteration  2 done1 , maximum relative shift = 8.463e-04
1 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
3 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
3 Newton iteration  2 done3 , maximum relative shift = 8.463e-04
3 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
6 did  comm_.min(converged) convergedRemote: 1
6 final convergedRemote: 1
6 Update: x^(k+1) = x^k - deltax^k 
6 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
6 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
6 Newton iteration  2 done6 , maximum relative shift = 8.463e-04
6 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
5 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
5 Newton iteration  2 done5 , maximum relative shift = 8.463e-04
5 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
4 did  comm_.min(converged) convergedRemote: 1
4 final convergedRemote: 1
4 Update: x^(k+1) = x^k - deltax^k 
4 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
4 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
4 Newton iteration  2 done4 , maximum relative shift = 8.463e-04
4 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
3 Solve: M deltax^k = r 
3 Newton::solveLinearSystem : entering the try section 2
3 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
3 solveLinearSystemImpl_, before converged = ls.solve 
3 7 amgbackend::solve isParallel 1
6 Solve: M deltax^k = r 
6 Newton::solveLinearSystem : entering the try section 2
6 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
6 solveLinearSystemImpl_, before converged = ls.solve 
6 7 amgbackend::solve isParallel 1
4 Solve: M deltax^k = r 
4 Newton::solveLinearSystem : entering the try section 2
4 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
4 solveLinearSystemImpl_, before converged = ls.solve 
4 7 amgbackend::solve isParallel 1
5 Solve: M deltax^k = r 
5 Newton::solveLinearSystem : entering the try section 2
5 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
5 solveLinearSystemImpl_, before converged = ls.solve 
5 7 amgbackend::solve isParallel 1
0 Solve: M deltax^k = r 
0 Newton::solveLinearSystem : entering the try section 2
0 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
0 solveLinearSystemImpl_, before converged = ls.solve 
0 7 amgbackend::solve isParallel 1
2 Solve: M deltax^k = r 
2 Newton::solveLinearSystem : entering the try section 2
2 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
2 solveLinearSystemImpl_, before converged = ls.solve 
2 7 amgbackend::solve isParallel 1
1 Solve: M deltax^k = r 
1 Newton::solveLinearSystem : entering the try section 2
1 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
1 solveLinearSystemImpl_, before converged = ls.solve 
1 7 amgbackend::solve isParallel 1
2 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
2 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
6 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
6 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
3 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
3 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
1 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
1 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
4 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
4 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
5 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
5 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
0 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
0 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 7
1 7 before Dune::BiCGSTABSolver 
1 Dune::BiCGSTABSolver.apply(x, b, result_) 
1 BiCGSTABSolver::apply 
1 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
4 7 before Dune::BiCGSTABSolver 
4 Dune::BiCGSTABSolver.apply(x, b, result_) 
4 BiCGSTABSolver::apply 
4 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
5 7 before Dune::BiCGSTABSolver 
5 Dune::BiCGSTABSolver.apply(x, b, result_) 
5 BiCGSTABSolver::apply 
5 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
3 7 before Dune::BiCGSTABSolver 
3 Dune::BiCGSTABSolver.apply(x, b, result_) 
3 BiCGSTABSolver::apply 
3 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
2 7 before Dune::BiCGSTABSolver 
2 Dune::BiCGSTABSolver.apply(x, b, result_) 
2 BiCGSTABSolver::apply 
2 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
6 7 before Dune::BiCGSTABSolver 
6 Dune::BiCGSTABSolver.apply(x, b, result_) 
6 BiCGSTABSolver::apply 
6 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
Using a direct coarse solver (SuperLU)
Building hierarchy of 1 levels (inclusive coarse solver) took 0.200581 seconds.
0 7 before Dune::BiCGSTABSolver 
0 Dune::BiCGSTABSolver.apply(x, b, result_) 
0 BiCGSTABSolver::apply 
3 === BiCGSTABSolver
 Iter          Defect            Rate
    0       7.8595e-14
3  (it = 0.5; it < _maxit; it+=.5) 0.5 250
3  before p = r 
3  before  y = W^-1 * p 
3  before _prec->apply(y,p); 0.5 250
3 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
2 === BiCGSTABSolver
 Iter          Defect            Rate
    0       7.8595e-14
2  (it = 0.5; it < _maxit; it+=.5) 0.5 250
2  before p = r 
2  before  y = W^-1 * p 
2  before _prec->apply(y,p); 0.5 250
2 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
0 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
0 === BiCGSTABSolver
 Iter          Defect            Rate
    0       7.8595e-14
0  (it = 0.5; it < _maxit; it+=.5) 0.5 250
0  before p = r 
0  before  y = W^-1 * p 
0  before _prec->apply(y,p); 0.5 250
0 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 7
0 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
6 === BiCGSTABSolver
 Iter          Defect            Rate
    0       7.8595e-14
6  (it = 0.5; it < _maxit; it+=.5) 0.5 250
6  before p = r 
6  before  y = W^-1 * p 
6  before _prec->apply(y,p); 0.5 250
6 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
1 === BiCGSTABSolver
 Iter          Defect            Rate
    0       7.8595e-14
1  (it = 0.5; it < _maxit; it+=.5) 0.5 250
1  before p = r 
1  before  y = W^-1 * p 
1  before _prec->apply(y,p); 0.5 250
1 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
4 === BiCGSTABSolver
 Iter          Defect            Rate
    0       7.8595e-14
4  (it = 0.5; it < _maxit; it+=.5) 0.5 250
4  before p = r 
4  before  y = W^-1 * p 
4  before _prec->apply(y,p); 0.5 250
4 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
5 === BiCGSTABSolver
 Iter          Defect            Rate
    0       7.8595e-14
5  (it = 0.5; it < _maxit; it+=.5) 0.5 250
5  before p = r 
5  before  y = W^-1 * p 
5  before _prec->apply(y,p); 0.5 250
5 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
5 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
6 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
6 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
1 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
4 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
4 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
2 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
2 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
3 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
5 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
5  before v = A * y 
5  before  alpha = rho_new / < rt, v > 
0 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
6  before v = A * y 
6  before  alpha = rho_new / < rt, v > 
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
4  before v = A * y 
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
3 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
3  before v = A * y 
2  before v = A * y 
2  before  alpha = rho_new / < rt, v > 
2  before alpha = rho_new / h; 
2  before apply first correction to x 
2  before r = r - alpha*v 
2  before test stop criteria 
2 istlsolver::printOutput 
  0.5      1.17554e-31       1.4957e-18
2 === rate=2.23711e-36, T=0.229974, TIT=0.459947, IT=0.5
2 finished solveLinearSystemImpl_ 1
2 converged value: 1 7 1 let s go get convergedRemote
2 to  comm_.min(converged) 
2 did  comm_.min(converged) convergedRemote: 1
2 final convergedRemote: 1
2 Update: x^(k+1) = x^k - deltax^k 
2 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
5  before alpha = rho_new / h; 
5  before apply first correction to x 
5  before r = r - alpha*v 
5  before test stop criteria 
5 istlsolver::printOutput 
  0.5      1.17554e-31       1.4957e-18
5 === rate=2.23711e-36, T=0.229864, TIT=0.459728, IT=0.5
5 finished solveLinearSystemImpl_ 1
5 converged value: 1 7 1 let s go get convergedRemote
5 to  comm_.min(converged) 
5 did  comm_.min(converged) convergedRemote: 1
5 final convergedRemote: 1
5 Update: x^(k+1) = x^k - deltax^k 
5 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
5 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
5 Newton iteration  3 done5 , maximum relative shift = 2.831e-06
5 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
0 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
0  before v = A * y 
0  before  alpha = rho_new / < rt, v > 
0  before alpha = rho_new / h; 
0  before apply first correction to x 
0  before r = r - alpha*v 
0  before test stop criteria 
0 istlsolver::printOutput 
  0.5      1.17554e-31       1.4957e-18
0 === rate=2.23711e-36, T=0.0354206, TIT=0.0708412, IT=0.5
0 finished solveLinearSystemImpl_ 1
0 converged value: 1 7 1 let s go get convergedRemote
0 to  comm_.min(converged) 
0 did  comm_.min(converged) convergedRemote: 1
0 final convergedRemote: 1
0 Update: x^(k+1) = x^k - deltax^k 
0 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
1 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
1  before v = A * y 
1  before  alpha = rho_new / < rt, v > 
1  before alpha = rho_new / h; 
1  before apply first correction to x 
1  before r = r - alpha*v 
1  before test stop criteria 
1 istlsolver::printOutput 
  0.5      1.17554e-31       1.4957e-18
1 === rate=2.23711e-36, T=0.229865, TIT=0.45973, IT=0.5
1 finished solveLinearSystemImpl_ 1
1 converged value: 1 7 1 let s go get convergedRemote
1 to  comm_.min(converged) 
1 did  comm_.min(converged) convergedRemote: 1
1 final convergedRemote: 1
1 Update: x^(k+1) = x^k - deltax^k 
1 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
1 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
1 Newton iteration  3 done1 , maximum relative shift = 2.831e-06
1 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
6  before alpha = rho_new / h; 
6  before apply first correction to x 
6  before r = r - alpha*v 
6  before test stop criteria 
6 istlsolver::printOutput 
  0.5      1.17554e-31       1.4957e-18
6 === rate=2.23711e-36, T=0.229988, TIT=0.459977, IT=0.5
6 finished solveLinearSystemImpl_ 1
6 converged value: 1 7 1 let s go get convergedRemote
6 to  comm_.min(converged) 
6 did  comm_.min(converged) convergedRemote: 1
6 final convergedRemote: 1
6 Update: x^(k+1) = x^k - deltax^k 
6 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
4  before  alpha = rho_new / < rt, v > 
4  before alpha = rho_new / h; 
4  before apply first correction to x 
4  before r = r - alpha*v 
4  before test stop criteria 
4 istlsolver::printOutput 
  0.5      1.17554e-31       1.4957e-18
4 === rate=2.23711e-36, T=0.229979, TIT=0.459959, IT=0.5
4 finished solveLinearSystemImpl_ 1
4 converged value: 1 7 1 let s go get convergedRemote
4 to  comm_.min(converged) 
4 did  comm_.min(converged) convergedRemote: 1
4 final convergedRemote: 1
4 Update: x^(k+1) = x^k - deltax^k 
4 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
3  before  alpha = rho_new / < rt, v > 
3  before alpha = rho_new / h; 
3  before apply first correction to x 
3  before r = r - alpha*v 
3  before test stop criteria 
3 istlsolver::printOutput 
  0.5      1.17554e-31       1.4957e-18
3 === rate=2.23711e-36, T=0.229866, TIT=0.459732, IT=0.5
3 finished solveLinearSystemImpl_ 1
3 converged value: 1 7 1 let s go get convergedRemote
3 to  comm_.min(converged) 
3 did  comm_.min(converged) convergedRemote: 1
3 final convergedRemote: 1
3 Update: x^(k+1) = x^k - deltax^k 
3 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
3 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
3 Newton iteration  3 done3 , maximum relative shift = 2.831e-06
3 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
4 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
4 Newton iteration  3 done4 , maximum relative shift = 2.831e-06
4 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
2 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
2 Newton iteration  3 done2 , maximum relative shift = 2.831e-06
2 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
0 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
0 Newton iteration  3 done0 , maximum relative shift = 2.831e-06
0 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
6 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
6 Newton iteration  3 done6 , maximum relative shift = 2.831e-06
6 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
5 Solve: M deltax^k = r 
5 Newton::solveLinearSystem : entering the try section 3
5 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
5 solveLinearSystemImpl_, before converged = ls.solve 
5 7 amgbackend::solve isParallel 1
2 Solve: M deltax^k = r 
2 Newton::solveLinearSystem : entering the try section 3
2 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
2 solveLinearSystemImpl_, before converged = ls.solve 
2 7 amgbackend::solve isParallel 1
1 Solve: M deltax^k = r 
1 Newton::solveLinearSystem : entering the try section 3
1 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
1 solveLinearSystemImpl_, before converged = ls.solve 
1 7 amgbackend::solve isParallel 1
0 Solve: M deltax^k = r 
0 Newton::solveLinearSystem : entering the try section 3
0 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
0 solveLinearSystemImpl_, before converged = ls.solve 
0 7 amgbackend::solve isParallel 1
6 Solve: M deltax^k = r 
6 Newton::solveLinearSystem : entering the try section 3
6 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
6 solveLinearSystemImpl_, before converged = ls.solve 
6 7 amgbackend::solve isParallel 1
3 Solve: M deltax^k = r 
3 Newton::solveLinearSystem : entering the try section 3
3 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
3 solveLinearSystemImpl_, before converged = ls.solve 
3 7 amgbackend::solve isParallel 1
4 Solve: M deltax^k = r 
4 Newton::solveLinearSystem : entering the try section 3
4 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
4 solveLinearSystemImpl_, before converged = ls.solve 
4 7 amgbackend::solve isParallel 1
6 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
6 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
3 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
3 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
0 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
0 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 7
5 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
5 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
2 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
2 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
1 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
1 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
4 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
4 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 7
2 7 before Dune::BiCGSTABSolver 
2 Dune::BiCGSTABSolver.apply(x, b, result_) 
2 BiCGSTABSolver::apply 
2 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 7
1 7 before Dune::BiCGSTABSolver 
1 Dune::BiCGSTABSolver.apply(x, b, result_) 
1 BiCGSTABSolver::apply 
1 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
6 7 before Dune::BiCGSTABSolver 
6 Dune::BiCGSTABSolver.apply(x, b, result_) 
6 BiCGSTABSolver::apply 
6 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
3 7 before Dune::BiCGSTABSolver 
3 Dune::BiCGSTABSolver.apply(x, b, result_) 
3 BiCGSTABSolver::apply 
3 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
4 7 before Dune::BiCGSTABSolver 
4 Dune::BiCGSTABSolver.apply(x, b, result_) 
4 BiCGSTABSolver::apply 
4 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
5 7 before Dune::BiCGSTABSolver 
5 Dune::BiCGSTABSolver.apply(x, b, result_) 
5 BiCGSTABSolver::apply 
5 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
Using a direct coarse solver (SuperLU)
Building hierarchy of 1 levels (inclusive coarse solver) took 0.201665 seconds.
0 7 before Dune::BiCGSTABSolver 
0 Dune::BiCGSTABSolver.apply(x, b, result_) 
0 BiCGSTABSolver::apply 
5 === BiCGSTABSolver
 Iter          Defect            Rate
    0      2.24929e-15
5  (it = 0.5; it < _maxit; it+=.5) 0.5 250
5  before p = r 
5  before  y = W^-1 * p 
5  before _prec->apply(y,p); 0.5 250
5 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
4 === BiCGSTABSolver
 Iter          Defect            Rate
    0      2.24929e-15
4  (it = 0.5; it < _maxit; it+=.5) 0.5 250
4  before p = r 
4  before  y = W^-1 * p 
4  before _prec->apply(y,p); 0.5 250
4 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
3 === BiCGSTABSolver
 Iter          Defect            Rate
    0      2.24929e-15
3  (it = 0.5; it < _maxit; it+=.5) 0.5 250
3  before p = r 
3  before  y = W^-1 * p 
3  before _prec->apply(y,p); 0.5 250
3 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
0 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
0 === BiCGSTABSolver
 Iter          Defect            Rate
    0      2.24929e-15
0  (it = 0.5; it < _maxit; it+=.5) 0.5 250
0  before p = r 
0  before  y = W^-1 * p 
0  before _prec->apply(y,p); 0.5 250
0 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 7
0 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
1 === BiCGSTABSolver
 Iter          Defect            Rate
    0      2.24929e-15
1  (it = 0.5; it < _maxit; it+=.5) 0.5 250
1  before p = r 
1  before  y = W^-1 * p 
1  before _prec->apply(y,p); 0.5 250
1 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
2 === BiCGSTABSolver
 Iter          Defect            Rate
    0      2.24929e-15
2  (it = 0.5; it < _maxit; it+=.5) 0.5 250
2  before p = r 
2  before  y = W^-1 * p 
2  before _prec->apply(y,p); 0.5 250
2 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
6 === BiCGSTABSolver
 Iter          Defect            Rate
    0      2.24929e-15
6  (it = 0.5; it < _maxit; it+=.5) 0.5 250
6  before p = r 
6  before  y = W^-1 * p 
6  before _prec->apply(y,p); 0.5 250
6 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
3 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
5 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
4 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
4 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
6 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
6 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
1 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
2 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
2 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
3 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
5 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
0 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
1 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
1  before v = A * y 
1  before  alpha = rho_new / < rt, v > 
1  before alpha = rho_new / h; 
1  before apply first correction to x 
1  before r = r - alpha*v 
1  before test stop criteria 
1 istlsolver::printOutput 
  0.5      1.73761e-33      7.72515e-19
1 === rate=5.96779e-37, T=0.225462, TIT=0.450925, IT=0.5
1 finished solveLinearSystemImpl_ 1
1 converged value: 1 7 1 let s go get convergedRemote
1 to  comm_.min(converged) 
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
5  before v = A * y 
5  before  alpha = rho_new / < rt, v > 
5  before alpha = rho_new / h; 
5  before apply first correction to x 
5  before r = r - alpha*v 
5  before test stop criteria 
5 istlsolver::printOutput 
  0.5      1.73761e-33      7.72515e-19
5 === rate=5.96779e-37, T=0.225462, TIT=0.450924, IT=0.5
5 finished solveLinearSystemImpl_ 1
5 converged value: 1 7 1 let s go get convergedRemote
5 to  comm_.min(converged) 
4  before v = A * y 
4  before  alpha = rho_new / < rt, v > 
4  before alpha = rho_new / h; 
4  before apply first correction to x 
4  before r = r - alpha*v 
4  before test stop criteria 
4 istlsolver::printOutput 
  0.5      1.73761e-33      7.72515e-19
4 === rate=5.96779e-37, T=0.225476, TIT=0.450952, IT=0.5
4 finished solveLinearSystemImpl_ 1
4 converged value: 1 7 1 let s go get convergedRemote
4 to  comm_.min(converged) 
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
3  before v = A * y 
3  before  alpha = rho_new / < rt, v > 
3  before alpha = rho_new / h; 
3  before apply first correction to x 
3  before r = r - alpha*v 
3  before test stop criteria 
3 istlsolver::printOutput 
  0.5      1.73761e-33      7.72515e-19
3 === rate=5.96779e-37, T=0.225461, TIT=0.450921, IT=0.5
3 finished solveLinearSystemImpl_ 1
3 converged value: 1 7 1 let s go get convergedRemote
3 to  comm_.min(converged) 
0 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
0  before v = A * y 
0  before  alpha = rho_new / < rt, v > 
0  before alpha = rho_new / h; 
0  before apply first correction to x 
0  before r = r - alpha*v 
0  before test stop criteria 
0 istlsolver::printOutput 
  0.5      1.73761e-33      7.72515e-19
0 === rate=5.96779e-37, T=0.029712, TIT=0.059424, IT=0.5
6  before v = A * y 
6  before  alpha = rho_new / < rt, v > 
6  before alpha = rho_new / h; 
6  before apply first correction to x 
6  before r = r - alpha*v 
6  before test stop criteria 
6 istlsolver::printOutput 
  0.5      1.73761e-33      7.72515e-19
6 === rate=5.96779e-37, T=0.225484, TIT=0.450967, IT=0.5
6 finished solveLinearSystemImpl_ 1
6 converged value: 1 7 1 let s go get convergedRemote
6 to  comm_.min(converged) 
2  before v = A * y 
2  before  alpha = rho_new / < rt, v > 
2  before alpha = rho_new / h; 
2  before apply first correction to x 
2  before r = r - alpha*v 
2  before test stop criteria 
2 istlsolver::printOutput 
  0.5      1.73761e-33      7.72515e-19
2 === rate=5.96779e-37, T=0.225472, TIT=0.450945, IT=0.5
2 finished solveLinearSystemImpl_ 1
2 converged value: 1 7 1 let s go get convergedRemote
2 to  comm_.min(converged) 
0 finished solveLinearSystemImpl_ 1
0 converged value: 1 7 1 let s go get convergedRemote
0 to  comm_.min(converged) 
0 did  comm_.min(converged) convergedRemote: 1
0 final convergedRemote: 1
0 Update: x^(k+1) = x^k - deltax^k 
0 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
0 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
0 Newton iteration  4 done0 , maximum relative shift = 8.368e-08
0 Assemble/solve/update time: 0.295561(23.8272%)/0.944444(76.1381%)/0.000430035(0.0346681%)
0 gridVariables->advanceTimeStep
0 before, nonLinearSolver->suggestTimeStepSize, current time: 273.703 simTime 10800
0 before assembler->setPreviousSolution, ddt: 30
0 nonLinearSolver->solve
0 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
6 did  comm_.min(converged) convergedRemote: 1
6 final convergedRemote: 1
6 Update: x^(k+1) = x^k - deltax^k 
6 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
6 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
6 Newton iteration  4 done6 , maximum relative shift = 8.368e-08
6 Assemble/solve/update time: 0.296573(23.8888%)/0.944461(76.076%)/0.000436405(0.0351523%)
6 gridVariables->advanceTimeStep
6 before, nonLinearSolver->suggestTimeStepSize, current time: 273.703 simTime 10800
6 before assembler->setPreviousSolution, ddt: 30
6 nonLinearSolver->solve
6 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
3 did  comm_.min(converged) convergedRemote: 1
3 final convergedRemote: 1
3 Update: x^(k+1) = x^k - deltax^k 
3 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
3 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
3 Newton iteration  4 done3 , maximum relative shift = 8.368e-08
3 Assemble/solve/update time: 0.296993(23.9209%)/0.94402(76.0347%)/0.000551295(0.0444033%)
3 gridVariables->advanceTimeStep
3 before, nonLinearSolver->suggestTimeStepSize, current time: 273.703 simTime 10800
3 before assembler->setPreviousSolution, ddt: 30
3 nonLinearSolver->solve
3 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
1 did  comm_.min(converged) convergedRemote: 1
1 final convergedRemote: 1
1 Update: x^(k+1) = x^k - deltax^k 
1 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
1 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
1 Newton iteration  4 done1 , maximum relative shift = 8.368e-08
1 Assemble/solve/update time: 0.296913(23.9162%)/0.944009(76.0395%)/0.000550295(0.044326%)
1 gridVariables->advanceTimeStep
1 before, nonLinearSolver->suggestTimeStepSize, current time: 273.703 simTime 10800
1 before assembler->setPreviousSolution, ddt: 30
1 nonLinearSolver->solve
1 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
5 did  comm_.min(converged) convergedRemote: 1
5 final convergedRemote: 1
5 Update: x^(k+1) = x^k - deltax^k 
5 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
5 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
5 Newton iteration  4 done5 , maximum relative shift = 8.368e-08
5 Assemble/solve/update time: 0.297031(23.9236%)/0.944003(76.0322%)/0.000549055(0.0442222%)
5 gridVariables->advanceTimeStep
5 before, nonLinearSolver->suggestTimeStepSize, current time: 273.703 simTime 10800
5 before assembler->setPreviousSolution, ddt: 30
5 nonLinearSolver->solve
5 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
4 did  comm_.min(converged) convergedRemote: 1
4 final convergedRemote: 1
4 Update: x^(k+1) = x^k - deltax^k 
4 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
4 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
4 Newton iteration  4 done4 , maximum relative shift = 8.368e-08
4 Assemble/solve/update time: 0.296384(23.8776%)/0.944444(76.0874%)/0.000434385(0.0349954%)
4 gridVariables->advanceTimeStep
4 before, nonLinearSolver->suggestTimeStepSize, current time: 273.703 simTime 10800
4 before assembler->setPreviousSolution, ddt: 30
4 nonLinearSolver->solve
4 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
2 did  comm_.min(converged) convergedRemote: 1
2 final convergedRemote: 1
2 Update: x^(k+1) = x^k - deltax^k 
2 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
2 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
2 Newton iteration  4 done2 , maximum relative shift = 8.368e-08
2 Assemble/solve/update time: 0.29633(23.8741%)/0.944458(76.0909%)/0.000434965(0.0350432%)
2 gridVariables->advanceTimeStep
2 before, nonLinearSolver->suggestTimeStepSize, current time: 273.703 simTime 10800
2 before assembler->setPreviousSolution, ddt: 30
2 nonLinearSolver->solve
2 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
6 Solve: M deltax^k = r 
6 Newton::solveLinearSystem : entering the try section 0
6to  comm_.sum(norm2) 
3 Solve: M deltax^k = r 
3 Newton::solveLinearSystem : entering the try section 0
3to  comm_.sum(norm2) 
1 Solve: M deltax^k = r 
1 Newton::solveLinearSystem : entering the try section 0
1to  comm_.sum(norm2) 
5 Solve: M deltax^k = r 
5 Newton::solveLinearSystem : entering the try section 0
5to  comm_.sum(norm2) 
2 Solve: M deltax^k = r 
2 Newton::solveLinearSystem : entering the try section 0
2to  comm_.sum(norm2) 
4 Solve: M deltax^k = r 
4 Newton::solveLinearSystem : entering the try section 0
4to  comm_.sum(norm2) 
0 Solve: M deltax^k = r 
0 Newton::solveLinearSystem : entering the try section 0
0to  comm_.sum(norm2) 
0did  comm_.sum(norm2) 
0 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
0 solveLinearSystemImpl_, before converged = ls.solve 
0 7 amgbackend::solve isParallel 1
2did  comm_.sum(norm2) 
2 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
2 solveLinearSystemImpl_, before converged = ls.solve 
2 7 amgbackend::solve isParallel 1
4did  comm_.sum(norm2) 
4 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
4 solveLinearSystemImpl_, before converged = ls.solve 
4 7 amgbackend::solve isParallel 1
5did  comm_.sum(norm2) 
5 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
5 solveLinearSystemImpl_, before converged = ls.solve 
5 7 amgbackend::solve isParallel 1
6did  comm_.sum(norm2) 
6 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
6 solveLinearSystemImpl_, before converged = ls.solve 
6 7 amgbackend::solve isParallel 1
3did  comm_.sum(norm2) 
3 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
3 solveLinearSystemImpl_, before converged = ls.solve 
3 7 amgbackend::solve isParallel 1
1did  comm_.sum(norm2) 
1 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
1 solveLinearSystemImpl_, before converged = ls.solve 
1 7 amgbackend::solve isParallel 1
0 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
0 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 7
2 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
2 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
4 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
4 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
5 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
5 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
6 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
6 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
3 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
3 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
1 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
1 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
2 7 before Dune::BiCGSTABSolver 
2 Dune::BiCGSTABSolver.apply(x, b, result_) 
2 BiCGSTABSolver::apply 
2 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
4 7 before Dune::BiCGSTABSolver 
4 Dune::BiCGSTABSolver.apply(x, b, result_) 
4 BiCGSTABSolver::apply 
4 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
5 7 before Dune::BiCGSTABSolver 
5 Dune::BiCGSTABSolver.apply(x, b, result_) 
5 BiCGSTABSolver::apply 
5 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
6 7 before Dune::BiCGSTABSolver 
6 Dune::BiCGSTABSolver.apply(x, b, result_) 
6 BiCGSTABSolver::apply 
6 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
1 7 before Dune::BiCGSTABSolver 
1 Dune::BiCGSTABSolver.apply(x, b, result_) 
1 BiCGSTABSolver::apply 
1 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
3 7 before Dune::BiCGSTABSolver 
3 Dune::BiCGSTABSolver.apply(x, b, result_) 
3 BiCGSTABSolver::apply 
3 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 7
Using a direct coarse solver (SuperLU)
Building hierarchy of 1 levels (inclusive coarse solver) took 0.201655 seconds.
0 7 before Dune::BiCGSTABSolver 
0 Dune::BiCGSTABSolver.apply(x, b, result_) 
0 BiCGSTABSolver::apply 
0 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
5 === BiCGSTABSolver
 Iter          Defect            Rate
    0      4.47601e-09
5  (it = 0.5; it < _maxit; it+=.5) 0.5 250
5  before p = r 
5  before  y = W^-1 * p 
5  before _prec->apply(y,p); 0.5 250
5 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
0 === BiCGSTABSolver
 Iter          Defect            Rate
    0      4.47601e-09
0  (it = 0.5; it < _maxit; it+=.5) 0.5 250
0  before p = r 
0  before  y = W^-1 * p 
0  before _prec->apply(y,p); 0.5 250
0 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 7
0 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
1 === BiCGSTABSolver
 Iter          Defect            Rate
    0      4.47601e-09
1  (it = 0.5; it < _maxit; it+=.5) 0.5 250
1  before p = r 
1  before  y = W^-1 * p 
1  before _prec->apply(y,p); 0.5 250
1 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
3 === BiCGSTABSolver
 Iter          Defect            Rate
    0      4.47601e-09
3  (it = 0.5; it < _maxit; it+=.5) 0.5 250
3  before p = r 
3  before  y = W^-1 * p 
3  before _prec->apply(y,p); 0.5 250
3 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
6 === BiCGSTABSolver
 Iter          Defect            Rate
    0      4.47601e-09
6  (it = 0.5; it < _maxit; it+=.5) 0.5 250
6  before p = r 
6  before  y = W^-1 * p 
6  before _prec->apply(y,p); 0.5 250
6 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
2 === BiCGSTABSolver
 Iter          Defect            Rate
    0      4.47601e-09
2  (it = 0.5; it < _maxit; it+=.5) 0.5 250
2  before p = r 
2  before  y = W^-1 * p 
2  before _prec->apply(y,p); 0.5 250
2 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
4 === BiCGSTABSolver
 Iter          Defect            Rate
    0      4.47601e-09
4  (it = 0.5; it < _maxit; it+=.5) 0.5 250
4  before p = r 
4  before  y = W^-1 * p 
4  before _prec->apply(y,p); 0.5 250
4 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
3 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
3 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
6 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
6 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
2 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
2 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
5 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
5 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
4 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
4 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
0 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
0 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
1 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
1 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
2  before v = A * y 
1  before v = A * y 
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
4  before v = A * y 
4  before  alpha = rho_new / < rt, v > 
0  before v = A * y 
0  before  alpha = rho_new / < rt, v > 
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
5  before v = A * y 
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
6  before v = A * y 
6  before  alpha = rho_new / < rt, v > 
0  before alpha = rho_new / h; 
0  before apply first correction to x 
0  before r = r - alpha*v 
0  before test stop criteria 
0 istlsolver::printOutput 
  0.5      7.33991e-25      1.63983e-16
0 === rate=2.68906e-32, T=0.0293078, TIT=0.0586157, IT=0.5
5  before  alpha = rho_new / < rt, v > 
5  before alpha = rho_new / h; 
5  before apply first correction to x 
5  before r = r - alpha*v 
5  before test stop criteria 
5 istlsolver::printOutput 
  0.5      7.33991e-25      1.63983e-16
5 === rate=2.68906e-32, T=0.224403, TIT=0.448806, IT=0.5
5 finished solveLinearSystemImpl_ 1
5 converged value: 1 7 1 let s go get convergedRemote
5 to  comm_.min(converged) 
3  before v = A * y 
3  before  alpha = rho_new / < rt, v > 
3  before alpha = rho_new / h; 
3  before apply first correction to x 
3  before r = r - alpha*v 
3  before test stop criteria 
3 istlsolver::printOutput 
  0.5      7.33991e-25      1.63983e-16
3 === rate=2.68906e-32, T=0.224526, TIT=0.449051, IT=0.5
3 finished solveLinearSystemImpl_ 1
3 converged value: 1 7 1 let s go get convergedRemote
3 to  comm_.min(converged) 
2  before  alpha = rho_new / < rt, v > 
2  before alpha = rho_new / h; 
2  before apply first correction to x 
2  before r = r - alpha*v 
2  before test stop criteria 
2 istlsolver::printOutput 
  0.5      7.33991e-25      1.63983e-16
2 === rate=2.68906e-32, T=0.224498, TIT=0.448995, IT=0.5
2 finished solveLinearSystemImpl_ 1
2 converged value: 1 7 1 let s go get convergedRemote
2 to  comm_.min(converged) 
6  before alpha = rho_new / h; 
6  before apply first correction to x 
6  before r = r - alpha*v 
6  before test stop criteria 
6 istlsolver::printOutput 
  0.5      7.33991e-25      1.63983e-16
6 === rate=2.68906e-32, T=0.224507, TIT=0.449014, IT=0.5
6 finished solveLinearSystemImpl_ 1
6 converged value: 1 7 1 let s go get convergedRemote
6 to  comm_.min(converged) 
1  before  alpha = rho_new / < rt, v > 
1  before alpha = rho_new / h; 
1  before apply first correction to x 
1  before r = r - alpha*v 
1  before test stop criteria 
1 istlsolver::printOutput 
  0.5      7.33991e-25      1.63983e-16
1 === rate=2.68906e-32, T=0.224406, TIT=0.448812, IT=0.5
1 finished solveLinearSystemImpl_ 1
1 converged value: 1 7 1 let s go get convergedRemote
1 to  comm_.min(converged) 
4  before alpha = rho_new / h; 
4  before apply first correction to x 
4  before r = r - alpha*v 
4  before test stop criteria 
4 istlsolver::printOutput 
  0.5      7.33991e-25      1.63983e-16
4 === rate=2.68906e-32, T=0.224503, TIT=0.449006, IT=0.5
4 finished solveLinearSystemImpl_ 1
4 converged value: 1 7 1 let s go get convergedRemote
4 to  comm_.min(converged) 
6 did  comm_.min(converged) convergedRemote: 1
6 final convergedRemote: 1
6 Update: x^(k+1) = x^k - deltax^k 
6 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
6 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
6 Newton iteration  1 done6 , maximum relative shift = 2.304e-02
6 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
1 did  comm_.min(converged) convergedRemote: 1
1 final convergedRemote: 1
1 Update: x^(k+1) = x^k - deltax^k 
1 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
1 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
1 Newton iteration  1 done1 , maximum relative shift = 2.304e-02
1 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
2 did  comm_.min(converged) convergedRemote: 1
2 final convergedRemote: 1
2 Update: x^(k+1) = x^k - deltax^k 
2 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
2 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
2 Newton iteration  1 done2 , maximum relative shift = 2.304e-02
2 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
0 finished solveLinearSystemImpl_ 1
0 converged value: 1 7 1 let s go get convergedRemote
0 to  comm_.min(converged) 
0 did  comm_.min(converged) convergedRemote: 1
0 final convergedRemote: 1
0 Update: x^(k+1) = x^k - deltax^k 
0 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
0 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
0 Newton iteration  1 done0 , maximum relative shift = 2.304e-02
0 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
5 did  comm_.min(converged) convergedRemote: 1
5 final convergedRemote: 1
5 Update: x^(k+1) = x^k - deltax^k 
5 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
5 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
5 Newton iteration  1 done5 , maximum relative shift = 2.304e-02
5 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
3 did  comm_.min(converged) convergedRemote: 1
3 final convergedRemote: 1
3 Update: x^(k+1) = x^k - deltax^k 
3 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
3 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
3 Newton iteration  1 done3 , maximum relative shift = 2.304e-02
3 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
4 did  comm_.min(converged) convergedRemote: 1
4 final convergedRemote: 1
4 Update: x^(k+1) = x^k - deltax^k 
4 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
4 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
4 Newton iteration  1 done4 , maximum relative shift = 2.304e-02
4 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
6 Solve: M deltax^k = r 
6 Newton::solveLinearSystem : entering the try section 1
6 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
6 solveLinearSystemImpl_, before converged = ls.solve 
6 7 amgbackend::solve isParallel 1
1 Solve: M deltax^k = r 
1 Newton::solveLinearSystem : entering the try section 1
1 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
1 solveLinearSystemImpl_, before converged = ls.solve 
1 7 amgbackend::solve isParallel 1
2 Solve: M deltax^k = r 
2 Newton::solveLinearSystem : entering the try section 1
2 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
2 solveLinearSystemImpl_, before converged = ls.solve 
2 7 amgbackend::solve isParallel 1
0 Solve: M deltax^k = r 
0 Newton::solveLinearSystem : entering the try section 1
0 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
0 solveLinearSystemImpl_, before converged = ls.solve 
0 7 amgbackend::solve isParallel 1
5 Solve: M deltax^k = r 
5 Newton::solveLinearSystem : entering the try section 1
5 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
5 solveLinearSystemImpl_, before converged = ls.solve 
5 7 amgbackend::solve isParallel 1
4 Solve: M deltax^k = r 
4 Newton::solveLinearSystem : entering the try section 1
4 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
4 solveLinearSystemImpl_, before converged = ls.solve 
4 7 amgbackend::solve isParallel 1
3 Solve: M deltax^k = r 
3 Newton::solveLinearSystem : entering the try section 1
3 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
3 solveLinearSystemImpl_, before converged = ls.solve 
3 7 amgbackend::solve isParallel 1
5 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
5 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
6 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
6 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
1 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
1 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
2 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
2 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
0 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
0 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
3 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
3 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
4 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
4 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 7
3 7 before Dune::BiCGSTABSolver 
3 Dune::BiCGSTABSolver.apply(x, b, result_) 
3 BiCGSTABSolver::apply 
3 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
4 7 before Dune::BiCGSTABSolver 
4 Dune::BiCGSTABSolver.apply(x, b, result_) 
4 BiCGSTABSolver::apply 
4 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
5 7 before Dune::BiCGSTABSolver 
5 Dune::BiCGSTABSolver.apply(x, b, result_) 
5 BiCGSTABSolver::apply 
5 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
6 7 before Dune::BiCGSTABSolver 
6 Dune::BiCGSTABSolver.apply(x, b, result_) 
6 BiCGSTABSolver::apply 
6 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
1 7 before Dune::BiCGSTABSolver 
1 Dune::BiCGSTABSolver.apply(x, b, result_) 
1 BiCGSTABSolver::apply 
1 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
2 7 before Dune::BiCGSTABSolver 
2 Dune::BiCGSTABSolver.apply(x, b, result_) 
2 BiCGSTABSolver::apply 
2 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
Using a direct coarse solver (SuperLU)
Building hierarchy of 1 levels (inclusive coarse solver) took 0.204449 seconds.
0 7 before Dune::BiCGSTABSolver 
0 Dune::BiCGSTABSolver.apply(x, b, result_) 
0 BiCGSTABSolver::apply 
0 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
4 === BiCGSTABSolver
 Iter          Defect            Rate
    0      2.16052e-11
4  (it = 0.5; it < _maxit; it+=.5) 0.5 250
4  before p = r 
4  before  y = W^-1 * p 
4  before _prec->apply(y,p); 0.5 250
4 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
5 === BiCGSTABSolver
 Iter          Defect            Rate
    0      2.16052e-11
5  (it = 0.5; it < _maxit; it+=.5) 0.5 250
5  before p = r 
5  before  y = W^-1 * p 
5  before _prec->apply(y,p); 0.5 250
5 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
6 === BiCGSTABSolver
 Iter          Defect            Rate
    0      2.16052e-11
6  (it = 0.5; it < _maxit; it+=.5) 0.5 250
6  before p = r 
6  before  y = W^-1 * p 
6  before _prec->apply(y,p); 0.5 250
6 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
1 === BiCGSTABSolver
 Iter          Defect            Rate
    0      2.16052e-11
1  (it = 0.5; it < _maxit; it+=.5) 0.5 250
1  before p = r 
1  before  y = W^-1 * p 
1  before _prec->apply(y,p); 0.5 250
1 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
0 === BiCGSTABSolver
 Iter          Defect            Rate
    0      2.16052e-11
0  (it = 0.5; it < _maxit; it+=.5) 0.5 250
0  before p = r 
0  before  y = W^-1 * p 
0  before _prec->apply(y,p); 0.5 250
0 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 7
0 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
2 === BiCGSTABSolver
 Iter          Defect            Rate
    0      2.16052e-11
2  (it = 0.5; it < _maxit; it+=.5) 0.5 250
2  before p = r 
2  before  y = W^-1 * p 
2  before _prec->apply(y,p); 0.5 250
2 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
3 === BiCGSTABSolver
 Iter          Defect            Rate
    0      2.16052e-11
3  (it = 0.5; it < _maxit; it+=.5) 0.5 250
3  before p = r 
3  before  y = W^-1 * p 
3  before _prec->apply(y,p); 0.5 250
3 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
3 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
2 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
1 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
4 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
5 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
6 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
0 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
0 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
2 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
1 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
4 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
5 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
5  before v = A * y 
6 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
3 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
3  before v = A * y 
6  before v = A * y 
6  before  alpha = rho_new / < rt, v > 
6  before alpha = rho_new / h; 
6  before apply first correction to x 
6  before r = r - alpha*v 
6  before test stop criteria 
6 istlsolver::printOutput 
  0.5      3.26647e-27      1.51189e-16
6 === rate=2.28582e-32, T=0.223819, TIT=0.447639, IT=0.5
6 finished solveLinearSystemImpl_ 1
6 converged value: 1 7 1 let s go get convergedRemote
6 to  comm_.min(converged) 
5  before  alpha = rho_new / < rt, v > 
5  before alpha = rho_new / h; 
5  before apply first correction to x 
5  before r = r - alpha*v 
5  before test stop criteria 
5 istlsolver::printOutput 
  0.5      3.26647e-27      1.51189e-16
5 === rate=2.28582e-32, T=0.22376, TIT=0.44752, IT=0.5
5 finished solveLinearSystemImpl_ 1
5 converged value: 1 7 1 let s go get convergedRemote
5 to  comm_.min(converged) 
5 did  comm_.min(converged) convergedRemote: 1
5 final convergedRemote: 1
5 Update: x^(k+1) = x^k - deltax^k 
5 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
0  before v = A * y 
0  before  alpha = rho_new / < rt, v > 
0  before alpha = rho_new / h; 
0  before apply first correction to x 
0  before r = r - alpha*v 
0  before test stop criteria 
0 istlsolver::printOutput 
  0.5      3.26647e-27      1.51189e-16
0 === rate=2.28582e-32, T=0.0298158, TIT=0.0596316, IT=0.5
0 finished solveLinearSystemImpl_ 1
0 converged value: 1 7 1 let s go get convergedRemote
0 to  comm_.min(converged) 
2  before v = A * y 
2  before  alpha = rho_new / < rt, v > 
2  before alpha = rho_new / h; 
2  before apply first correction to x 
2  before r = r - alpha*v 
2  before test stop criteria 
2 istlsolver::printOutput 
  0.5      3.26647e-27      1.51189e-16
2 === rate=2.28582e-32, T=0.22381, TIT=0.44762, IT=0.5
2 finished solveLinearSystemImpl_ 1
2 converged value: 1 7 1 let s go get convergedRemote
2 to  comm_.min(converged) 
1  before v = A * y 
1  before  alpha = rho_new / < rt, v > 
1  before alpha = rho_new / h; 
1  before apply first correction to x 
1  before r = r - alpha*v 
1  before test stop criteria 
1 istlsolver::printOutput 
  0.5      3.26647e-27      1.51189e-16
1 === rate=2.28582e-32, T=0.223761, TIT=0.447522, IT=0.5
1 finished solveLinearSystemImpl_ 1
1 converged value: 1 7 1 let s go get convergedRemote
1 to  comm_.min(converged) 
1 did  comm_.min(converged) convergedRemote: 1
1 final convergedRemote: 1
1 Update: x^(k+1) = x^k - deltax^k 
1 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
4  before v = A * y 
4  before  alpha = rho_new / < rt, v > 
4  before alpha = rho_new / h; 
4  before apply first correction to x 
4  before r = r - alpha*v 
4  before test stop criteria 
4 istlsolver::printOutput 
  0.5      3.26647e-27      1.51189e-16
4 === rate=2.28582e-32, T=0.223814, TIT=0.447628, IT=0.5
4 finished solveLinearSystemImpl_ 1
4 converged value: 1 7 1 let s go get convergedRemote
4 to  comm_.min(converged) 
3  before  alpha = rho_new / < rt, v > 
3  before alpha = rho_new / h; 
3  before apply first correction to x 
3  before r = r - alpha*v 
3  before test stop criteria 
3 istlsolver::printOutput 
  0.5      3.26647e-27      1.51189e-16
3 === rate=2.28582e-32, T=0.223758, TIT=0.447517, IT=0.5
3 finished solveLinearSystemImpl_ 1
3 converged value: 1 7 1 let s go get convergedRemote
3 to  comm_.min(converged) 
3 did  comm_.min(converged) convergedRemote: 1
3 final convergedRemote: 1
3 Update: x^(k+1) = x^k - deltax^k 
3 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
1 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
1 Newton iteration  2 done1 , maximum relative shift = 7.947e-04
1 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
3 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
3 Newton iteration  2 done3 , maximum relative shift = 7.947e-04
3 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
4 did  comm_.min(converged) convergedRemote: 1
4 final convergedRemote: 1
4 Update: x^(k+1) = x^k - deltax^k 
4 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
4 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
4 Newton iteration  2 done4 , maximum relative shift = 7.947e-04
4 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
6 did  comm_.min(converged) convergedRemote: 1
6 final convergedRemote: 1
6 Update: x^(k+1) = x^k - deltax^k 
6 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
6 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
6 Newton iteration  2 done6 , maximum relative shift = 7.947e-04
6 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
5 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
5 Newton iteration  2 done5 , maximum relative shift = 7.947e-04
5 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
0 did  comm_.min(converged) convergedRemote: 1
0 final convergedRemote: 1
0 Update: x^(k+1) = x^k - deltax^k 
0 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
0 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
0 Newton iteration  2 done0 , maximum relative shift = 7.947e-04
0 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
2 did  comm_.min(converged) convergedRemote: 1
2 final convergedRemote: 1
2 Update: x^(k+1) = x^k - deltax^k 
2 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
2 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
2 Newton iteration  2 done2 , maximum relative shift = 7.947e-04
2 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
2 Solve: M deltax^k = r 
2 Newton::solveLinearSystem : entering the try section 2
2 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
2 solveLinearSystemImpl_, before converged = ls.solve 
2 7 amgbackend::solve isParallel 1
3 Solve: M deltax^k = r 
3 Newton::solveLinearSystem : entering the try section 2
3 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
3 solveLinearSystemImpl_, before converged = ls.solve 
3 7 amgbackend::solve isParallel 1
4 Solve: M deltax^k = r 
4 Newton::solveLinearSystem : entering the try section 2
4 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
4 solveLinearSystemImpl_, before converged = ls.solve 
4 7 amgbackend::solve isParallel 1
6 Solve: M deltax^k = r 
6 Newton::solveLinearSystem : entering the try section 2
6 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
6 solveLinearSystemImpl_, before converged = ls.solve 
6 7 amgbackend::solve isParallel 1
5 Solve: M deltax^k = r 
5 Newton::solveLinearSystem : entering the try section 2
5 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
5 solveLinearSystemImpl_, before converged = ls.solve 
5 7 amgbackend::solve isParallel 1
0 Solve: M deltax^k = r 
0 Newton::solveLinearSystem : entering the try section 2
0 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
0 solveLinearSystemImpl_, before converged = ls.solve 
0 7 amgbackend::solve isParallel 1
1 Solve: M deltax^k = r 
1 Newton::solveLinearSystem : entering the try section 2
1 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
1 solveLinearSystemImpl_, before converged = ls.solve 
1 7 amgbackend::solve isParallel 1
2 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
2 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
3 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
3 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
4 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
4 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
6 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
6 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
5 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
5 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
0 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
0 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 7
1 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
1 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 7
2 7 before Dune::BiCGSTABSolver 
2 Dune::BiCGSTABSolver.apply(x, b, result_) 
2 BiCGSTABSolver::apply 
2 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
3 7 before Dune::BiCGSTABSolver 
3 Dune::BiCGSTABSolver.apply(x, b, result_) 
3 BiCGSTABSolver::apply 
3 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
4 7 before Dune::BiCGSTABSolver 
4 Dune::BiCGSTABSolver.apply(x, b, result_) 
4 BiCGSTABSolver::apply 
4 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
6 7 before Dune::BiCGSTABSolver 
6 Dune::BiCGSTABSolver.apply(x, b, result_) 
6 BiCGSTABSolver::apply 
6 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
5 7 before Dune::BiCGSTABSolver 
5 Dune::BiCGSTABSolver.apply(x, b, result_) 
5 BiCGSTABSolver::apply 
5 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 7
1 7 before Dune::BiCGSTABSolver 
1 Dune::BiCGSTABSolver.apply(x, b, result_) 
1 BiCGSTABSolver::apply 
1 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
Using a direct coarse solver (SuperLU)
Building hierarchy of 1 levels (inclusive coarse solver) took 0.202349 seconds.
0 7 before Dune::BiCGSTABSolver 
0 Dune::BiCGSTABSolver.apply(x, b, result_) 
0 BiCGSTABSolver::apply 
0 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
2 === BiCGSTABSolver
 Iter          Defect            Rate
    0       4.0674e-13
2  (it = 0.5; it < _maxit; it+=.5) 0.5 250
2  before p = r 
2  before  y = W^-1 * p 
2  before _prec->apply(y,p); 0.5 250
2 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
6 === BiCGSTABSolver
 Iter          Defect            Rate
    0       4.0674e-13
6  (it = 0.5; it < _maxit; it+=.5) 0.5 250
6  before p = r 
6  before  y = W^-1 * p 
6  before _prec->apply(y,p); 0.5 250
6 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
0 === BiCGSTABSolver
 Iter          Defect            Rate
    0       4.0674e-13
0  (it = 0.5; it < _maxit; it+=.5) 0.5 250
0  before p = r 
0  before  y = W^-1 * p 
0  before _prec->apply(y,p); 0.5 250
0 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 7
0 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
1 === BiCGSTABSolver
 Iter          Defect            Rate
    0       4.0674e-13
1  (it = 0.5; it < _maxit; it+=.5) 0.5 250
1  before p = r 
1  before  y = W^-1 * p 
1  before _prec->apply(y,p); 0.5 250
1 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
5 === BiCGSTABSolver
 Iter          Defect            Rate
    0       4.0674e-13
5  (it = 0.5; it < _maxit; it+=.5) 0.5 250
5  before p = r 
5  before  y = W^-1 * p 
5  before _prec->apply(y,p); 0.5 250
5 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
4 === BiCGSTABSolver
 Iter          Defect            Rate
    0       4.0674e-13
4  (it = 0.5; it < _maxit; it+=.5) 0.5 250
4  before p = r 
4  before  y = W^-1 * p 
4  before _prec->apply(y,p); 0.5 250
4 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
3 === BiCGSTABSolver
 Iter          Defect            Rate
    0       4.0674e-13
3  (it = 0.5; it < _maxit; it+=.5) 0.5 250
3  before p = r 
3  before  y = W^-1 * p 
3  before _prec->apply(y,p); 0.5 250
3 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
4 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
4 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
6 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
6 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
2 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
2 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
3 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
1 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
5 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
5 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
5  before v = A * y 
5  before  alpha = rho_new / < rt, v > 
3 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
3  before v = A * y 
3  before  alpha = rho_new / < rt, v > 
0 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
0 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
0  before v = A * y 
1 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
1  before v = A * y 
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
2  before v = A * y 
2  before  alpha = rho_new / < rt, v > 
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
4  before v = A * y 
4  before  alpha = rho_new / < rt, v > 
6  before v = A * y 
6  before  alpha = rho_new / < rt, v > 
2  before alpha = rho_new / h; 
2  before apply first correction to x 
2  before r = r - alpha*v 
2  before test stop criteria 
2 istlsolver::printOutput 
  0.5      4.00419e-29      9.84458e-17
2 === rate=9.69158e-33, T=0.231299, TIT=0.462598, IT=0.5
2 finished solveLinearSystemImpl_ 1
2 converged value: 1 7 1 let s go get convergedRemote
2 to  comm_.min(converged) 
4  before alpha = rho_new / h; 
4  before apply first correction to x 
4  before r = r - alpha*v 
4  before test stop criteria 
4 istlsolver::printOutput 
  0.5      4.00419e-29      9.84458e-17
4 === rate=9.69158e-33, T=0.2313, TIT=0.4626, IT=0.5
4 finished solveLinearSystemImpl_ 1
4 converged value: 1 7 1 let s go get convergedRemote
4 to  comm_.min(converged) 
6  before alpha = rho_new / h; 
6  before apply first correction to x 
6  before r = r - alpha*v 
6  before test stop criteria 
6 istlsolver::printOutput 
  0.5      4.00419e-29      9.84458e-17
6 === rate=9.69158e-33, T=0.231154, TIT=0.462307, IT=0.5
6 finished solveLinearSystemImpl_ 1
6 converged value: 1 7 1 let s go get convergedRemote
6 to  comm_.min(converged) 
5  before alpha = rho_new / h; 
5  before apply first correction to x 
5  before r = r - alpha*v 
5  before test stop criteria 
5 istlsolver::printOutput 
  0.5      4.00419e-29      9.84458e-17
5 === rate=9.69158e-33, T=0.231266, TIT=0.462532, IT=0.5
5 finished solveLinearSystemImpl_ 1
5 converged value: 1 7 1 let s go get convergedRemote
5 to  comm_.min(converged) 
3  before alpha = rho_new / h; 
3  before apply first correction to x 
3  before r = r - alpha*v 
3  before test stop criteria 
3 istlsolver::printOutput 
  0.5      4.00419e-29      9.84458e-17
3 === rate=9.69158e-33, T=0.231268, TIT=0.462536, IT=0.5
3 finished solveLinearSystemImpl_ 1
3 converged value: 1 7 1 let s go get convergedRemote
3 to  comm_.min(converged) 
0  before  alpha = rho_new / < rt, v > 
0  before alpha = rho_new / h; 
0  before apply first correction to x 
0  before r = r - alpha*v 
0  before test stop criteria 
0 istlsolver::printOutput 
  0.5      4.00419e-29      9.84458e-17
0 === rate=9.69158e-33, T=0.035122, TIT=0.0702439, IT=0.5
1  before  alpha = rho_new / < rt, v > 
1  before alpha = rho_new / h; 
1  before apply first correction to x 
1  before r = r - alpha*v 
1  before test stop criteria 
1 istlsolver::printOutput 
  0.5      4.00419e-29      9.84458e-17
1 === rate=9.69158e-33, T=0.231267, TIT=0.462534, IT=0.5
1 finished solveLinearSystemImpl_ 1
1 converged value: 1 7 1 let s go get convergedRemote
1 to  comm_.min(converged) 
6 did  comm_.min(converged) convergedRemote: 1
6 final convergedRemote: 1
6 Update: x^(k+1) = x^k - deltax^k 
6 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
6 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
6 Newton iteration  3 done6 , maximum relative shift = 1.564e-05
6 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
5 did  comm_.min(converged) convergedRemote: 1
5 final convergedRemote: 1
5 Update: x^(k+1) = x^k - deltax^k 
5 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
5 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
5 Newton iteration  3 done5 , maximum relative shift = 1.564e-05
5 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
3 did  comm_.min(converged) convergedRemote: 1
3 final convergedRemote: 1
3 Update: x^(k+1) = x^k - deltax^k 
3 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
3 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
3 Newton iteration  3 done3 , maximum relative shift = 1.564e-05
3 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
0 finished solveLinearSystemImpl_ 1
0 converged value: 1 7 1 let s go get convergedRemote
0 to  comm_.min(converged) 
0 did  comm_.min(converged) convergedRemote: 1
0 final convergedRemote: 1
0 Update: x^(k+1) = x^k - deltax^k 
0 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
0 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
0 Newton iteration  3 done0 , maximum relative shift = 1.564e-05
0 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
4 did  comm_.min(converged) convergedRemote: 1
4 final convergedRemote: 1
4 Update: x^(k+1) = x^k - deltax^k 
4 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
4 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
4 Newton iteration  3 done4 , maximum relative shift = 1.564e-05
4 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
2 did  comm_.min(converged) convergedRemote: 1
2 final convergedRemote: 1
2 Update: x^(k+1) = x^k - deltax^k 
2 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
2 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
2 Newton iteration  3 done2 , maximum relative shift = 1.564e-05
2 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
1 did  comm_.min(converged) convergedRemote: 1
1 final convergedRemote: 1
1 Update: x^(k+1) = x^k - deltax^k 
1 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
1 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
1 Newton iteration  3 done1 , maximum relative shift = 1.564e-05
1 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
3 Solve: M deltax^k = r 
3 Newton::solveLinearSystem : entering the try section 3
3 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
3 solveLinearSystemImpl_, before converged = ls.solve 
3 7 amgbackend::solve isParallel 1
0 Solve: M deltax^k = r 
0 Newton::solveLinearSystem : entering the try section 3
0 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
0 solveLinearSystemImpl_, before converged = ls.solve 
0 7 amgbackend::solve isParallel 1
4 Solve: M deltax^k = r 
4 Newton::solveLinearSystem : entering the try section 3
4 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
4 solveLinearSystemImpl_, before converged = ls.solve 
4 7 amgbackend::solve isParallel 1
1 Solve: M deltax^k = r 
1 Newton::solveLinearSystem : entering the try section 3
1 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
1 solveLinearSystemImpl_, before converged = ls.solve 
1 7 amgbackend::solve isParallel 1
2 Solve: M deltax^k = r 
2 Newton::solveLinearSystem : entering the try section 3
2 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
2 solveLinearSystemImpl_, before converged = ls.solve 
2 7 amgbackend::solve isParallel 1
6 Solve: M deltax^k = r 
6 Newton::solveLinearSystem : entering the try section 3
6 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
6 solveLinearSystemImpl_, before converged = ls.solve 
6 7 amgbackend::solve isParallel 1
5 Solve: M deltax^k = r 
5 Newton::solveLinearSystem : entering the try section 3
5 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
5 solveLinearSystemImpl_, before converged = ls.solve 
5 7 amgbackend::solve isParallel 1
1 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
1 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
2 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
2 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
6 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
6 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
4 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
4 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
3 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
3 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
0 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
0 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 7
5 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
5 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 7
3 7 before Dune::BiCGSTABSolver 
3 Dune::BiCGSTABSolver.apply(x, b, result_) 
3 BiCGSTABSolver::apply 
3 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 7
5 7 before Dune::BiCGSTABSolver 
5 Dune::BiCGSTABSolver.apply(x, b, result_) 
5 BiCGSTABSolver::apply 
5 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
1 7 before Dune::BiCGSTABSolver 
1 Dune::BiCGSTABSolver.apply(x, b, result_) 
1 BiCGSTABSolver::apply 
1 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
2 7 before Dune::BiCGSTABSolver 
2 Dune::BiCGSTABSolver.apply(x, b, result_) 
2 BiCGSTABSolver::apply 
2 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
6 7 before Dune::BiCGSTABSolver 
6 Dune::BiCGSTABSolver.apply(x, b, result_) 
6 BiCGSTABSolver::apply 
6 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
4 7 before Dune::BiCGSTABSolver 
4 Dune::BiCGSTABSolver.apply(x, b, result_) 
4 BiCGSTABSolver::apply 
4 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
Using a direct coarse solver (SuperLU)
Building hierarchy of 1 levels (inclusive coarse solver) took 0.2007 seconds.
0 7 before Dune::BiCGSTABSolver 
0 Dune::BiCGSTABSolver.apply(x, b, result_) 
0 BiCGSTABSolver::apply 
0 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
2 === BiCGSTABSolver
 Iter          Defect            Rate
    0      5.84369e-15
2  (it = 0.5; it < _maxit; it+=.5) 0.5 250
2  before p = r 
2  before  y = W^-1 * p 
2  before _prec->apply(y,p); 0.5 250
2 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
6 === BiCGSTABSolver
 Iter          Defect            Rate
    0      5.84369e-15
6  (it = 0.5; it < _maxit; it+=.5) 0.5 250
6  before p = r 
6  before  y = W^-1 * p 
6  before _prec->apply(y,p); 0.5 250
6 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
3 === BiCGSTABSolver
 Iter          Defect            Rate
    0      5.84369e-15
3  (it = 0.5; it < _maxit; it+=.5) 0.5 250
3  before p = r 
3  before  y = W^-1 * p 
3  before _prec->apply(y,p); 0.5 250
3 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
0 === BiCGSTABSolver
 Iter          Defect            Rate
    0      5.84369e-15
0  (it = 0.5; it < _maxit; it+=.5) 0.5 250
0  before p = r 
0  before  y = W^-1 * p 
0  before _prec->apply(y,p); 0.5 250
0 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 7
0 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
4 === BiCGSTABSolver
 Iter          Defect            Rate
    0      5.84369e-15
4  (it = 0.5; it < _maxit; it+=.5) 0.5 250
4  before p = r 
4  before  y = W^-1 * p 
4  before _prec->apply(y,p); 0.5 250
4 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
5 === BiCGSTABSolver
 Iter          Defect            Rate
    0      5.84369e-15
5  (it = 0.5; it < _maxit; it+=.5) 0.5 250
5  before p = r 
5  before  y = W^-1 * p 
5  before _prec->apply(y,p); 0.5 250
5 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
1 === BiCGSTABSolver
 Iter          Defect            Rate
    0      5.84369e-15
1  (it = 0.5; it < _maxit; it+=.5) 0.5 250
1  before p = r 
1  before  y = W^-1 * p 
1  before _prec->apply(y,p); 0.5 250
1 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
6 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
6 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
1 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
4 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
4 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
5 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
3 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
2 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
2 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
6  before v = A * y 
6  before  alpha = rho_new / < rt, v > 
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
2  before v = A * y 
2  before  alpha = rho_new / < rt, v > 
0 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
0 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
0  before v = A * y 
0  before  alpha = rho_new / < rt, v > 
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
4  before v = A * y 
4  before  alpha = rho_new / < rt, v > 
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
5 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
5  before v = A * y 
3 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
3  before v = A * y 
3  before  alpha = rho_new / < rt, v > 
0  before alpha = rho_new / h; 
0  before apply first correction to x 
0  before r = r - alpha*v 
0  before test stop criteria 
0 istlsolver::printOutput 
  0.5      5.08729e-31      8.70561e-17
0 === rate=7.57877e-33, T=0.0298886, TIT=0.0597772, IT=0.5
0 finished solveLinearSystemImpl_ 1
0 converged value: 1 7 1 let s go get convergedRemote
0 to  comm_.min(converged) 
0 did  comm_.min(converged) convergedRemote: 1
0 final convergedRemote: 1
0 Update: x^(k+1) = x^k - deltax^k 
0 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
0 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
4  before alpha = rho_new / h; 
4  before apply first correction to x 
4  before r = r - alpha*v 
4  before test stop criteria 
4 istlsolver::printOutput 
  0.5      5.08729e-31      8.70561e-17
4 === rate=7.57877e-33, T=0.224314, TIT=0.448627, IT=0.5
4 finished solveLinearSystemImpl_ 1
4 converged value: 1 7 1 let s go get convergedRemote
4 to  comm_.min(converged) 
4 did  comm_.min(converged) convergedRemote: 1
4 final convergedRemote: 1
4 Update: x^(k+1) = x^k - deltax^k 
4 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
4 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
4 Newton iteration  4 done4 , maximum relative shift = 2.176e-07
5  before  alpha = rho_new / < rt, v > 
5  before alpha = rho_new / h; 
5  before apply first correction to x 
5  before r = r - alpha*v 
5  before test stop criteria 
5 istlsolver::printOutput 
  0.5      5.08729e-31      8.70561e-17
5 === rate=7.57877e-33, T=0.224355, TIT=0.448709, IT=0.5
5 finished solveLinearSystemImpl_ 1
5 converged value: 1 7 1 let s go get convergedRemote
5 to  comm_.min(converged) 
5 did  comm_.min(converged) convergedRemote: 1
5 final convergedRemote: 1
5 Update: x^(k+1) = x^k - deltax^k 
5 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
5 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
5 Newton iteration  4 done5 , maximum relative shift = 2.176e-07
5 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
6  before alpha = rho_new / h; 
6  before apply first correction to x 
6  before r = r - alpha*v 
6  before test stop criteria 
6 istlsolver::printOutput 
  0.5      5.08729e-31      8.70561e-17
6 === rate=7.57877e-33, T=0.224322, TIT=0.448644, IT=0.5
6 finished solveLinearSystemImpl_ 1
6 converged value: 1 7 1 let s go get convergedRemote
6 to  comm_.min(converged) 
6 did  comm_.min(converged) convergedRemote: 1
6 final convergedRemote: 1
6 Update: x^(k+1) = x^k - deltax^k 
6 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
6 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
6 Newton iteration  4 done6 , maximum relative shift = 2.176e-07
6 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
3  before alpha = rho_new / h; 
3  before apply first correction to x 
3  before r = r - alpha*v 
3  before test stop criteria 
3 istlsolver::printOutput 
  0.5      5.08729e-31      8.70561e-17
3 === rate=7.57877e-33, T=0.224351, TIT=0.448703, IT=0.5
3 finished solveLinearSystemImpl_ 1
3 converged value: 1 7 1 let s go get convergedRemote
3 to  comm_.min(converged) 
3 did  comm_.min(converged) convergedRemote: 1
3 final convergedRemote: 1
3 Update: x^(k+1) = x^k - deltax^k 
3 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
3 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
3 Newton iteration  4 done3 , maximum relative shift = 2.176e-07
3 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
1 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
1  before v = A * y 
1  before  alpha = rho_new / < rt, v > 
1  before alpha = rho_new / h; 
1  before apply first correction to x 
1  before r = r - alpha*v 
1  before test stop criteria 
1 istlsolver::printOutput 
  0.5      5.08729e-31      8.70561e-17
1 === rate=7.57877e-33, T=0.224353, TIT=0.448706, IT=0.5
1 finished solveLinearSystemImpl_ 1
1 converged value: 1 7 1 let s go get convergedRemote
1 to  comm_.min(converged) 
1 did  comm_.min(converged) convergedRemote: 1
1 final convergedRemote: 1
1 Update: x^(k+1) = x^k - deltax^k 
1 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
1 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
1 Newton iteration  4 done1 , maximum relative shift = 2.176e-07
1 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
2  before alpha = rho_new / h; 
2  before apply first correction to x 
2  before r = r - alpha*v 
2  before test stop criteria 
2 istlsolver::printOutput 
  0.5      5.08729e-31      8.70561e-17
2 === rate=7.57877e-33, T=0.224308, TIT=0.448617, IT=0.5
2 finished solveLinearSystemImpl_ 1
2 converged value: 1 7 1 let s go get convergedRemote
2 to  comm_.min(converged) 
2 did  comm_.min(converged) convergedRemote: 1
2 final convergedRemote: 1
2 Update: x^(k+1) = x^k - deltax^k 
2 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
2 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
2 Newton iteration  4 done2 , maximum relative shift = 2.176e-07
2 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
0 Newton iteration  4 done0 , maximum relative shift = 2.176e-07
0 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
4 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
0 Solve: M deltax^k = r 
0 Newton::solveLinearSystem : entering the try section 4
0 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
0 solveLinearSystemImpl_, before converged = ls.solve 
0 7 amgbackend::solve isParallel 1
5 Solve: M deltax^k = r 
5 Newton::solveLinearSystem : entering the try section 4
5 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
5 solveLinearSystemImpl_, before converged = ls.solve 
5 7 amgbackend::solve isParallel 1
4 Solve: M deltax^k = r 
4 Newton::solveLinearSystem : entering the try section 4
4 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
4 solveLinearSystemImpl_, before converged = ls.solve 
4 7 amgbackend::solve isParallel 1
3 Solve: M deltax^k = r 
3 Newton::solveLinearSystem : entering the try section 4
3 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
3 solveLinearSystemImpl_, before converged = ls.solve 
3 7 amgbackend::solve isParallel 1
1 Solve: M deltax^k = r 
1 Newton::solveLinearSystem : entering the try section 4
1 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
1 solveLinearSystemImpl_, before converged = ls.solve 
1 7 amgbackend::solve isParallel 1
6 Solve: M deltax^k = r 
6 Newton::solveLinearSystem : entering the try section 4
6 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
6 solveLinearSystemImpl_, before converged = ls.solve 
6 7 amgbackend::solve isParallel 1
2 Solve: M deltax^k = r 
2 Newton::solveLinearSystem : entering the try section 4
2 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
2 solveLinearSystemImpl_, before converged = ls.solve 
2 7 amgbackend::solve isParallel 1
3 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
3 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
1 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
1 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
6 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
6 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
5 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
5 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
0 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
0 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 7
2 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
2 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
4 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
4 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 7
2 7 before Dune::BiCGSTABSolver 
2 Dune::BiCGSTABSolver.apply(x, b, result_) 
2 BiCGSTABSolver::apply 
2 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
4 7 before Dune::BiCGSTABSolver 
4 Dune::BiCGSTABSolver.apply(x, b, result_) 
4 BiCGSTABSolver::apply 
4 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
3 7 before Dune::BiCGSTABSolver 
3 Dune::BiCGSTABSolver.apply(x, b, result_) 
3 BiCGSTABSolver::apply 
3 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
1 7 before Dune::BiCGSTABSolver 
1 Dune::BiCGSTABSolver.apply(x, b, result_) 
1 BiCGSTABSolver::apply 
1 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
6 7 before Dune::BiCGSTABSolver 
6 Dune::BiCGSTABSolver.apply(x, b, result_) 
6 BiCGSTABSolver::apply 
6 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
5 7 before Dune::BiCGSTABSolver 
5 Dune::BiCGSTABSolver.apply(x, b, result_) 
5 BiCGSTABSolver::apply 
5 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 7
