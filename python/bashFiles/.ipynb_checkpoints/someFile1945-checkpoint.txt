67: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 5 1 -32766 0 0
67: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 13
67: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 5 1 -32766 0 0
67: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 13
67: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 5 1 -32766 0 0
67: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 13
67: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 5 1 -32766 0 0
67: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 13
67: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 5 1 -32766 0 0
67: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 13
67: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 5 1 -32766 0 0
67: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 13
67: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 5 1 -32766 0 0
67: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 13
67: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 5 1 -32766 0 0
67: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 13
67: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 5 1 -32766 0 0
67: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 10 13
67: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 5 1 -32766 0 0
67: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 11 13
67: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 5 1 -32766 0 0
67: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 12 13
67: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 5 1 -32766 0 0
Newton::solveLinearSystem : Caught exception from the linear solver when getting converged: "BCRSMatrixError [operator++:/home/m.giraud/DUMUXexud/dune-istl/dune/istl/bcrsmatrix.hh:940]: matrix already built up"
67 converged value: 0 121 1 let s go get convergedRemote
67 to  comm_.min(converged) 
67 did  comm_.min(converged) convergedRemote: 0
67 final convergedRemote: 0
67 about to throw Linear solver did not converge
67 Newton::solveLinearSystem : Caught exception: "NumericalProblem [solveLinearSystem:/home/m.giraud/DUMUXexud/dumux/dumux/nonlinear/newtonsolver.hh:381]: Linear solver did not converge when getting convergedRemote "
67 Newton: Caught exception: "NumericalProblem [solveLinearSystem:/home/m.giraud/DUMUXexud/dumux/dumux/nonlinear/newtonsolver.hh:381]: Linear solver did not converge"
67 Newton solver did not converge with dt = 0.0495 seconds. Retrying with time step of 0.02475 seconds
67 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
112 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
112: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 5
112: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 9.7e-07
112: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 7.3e-06
112: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 1.774e-05
112: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 2.159e-05
112: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 2.488e-05
112: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 2.81e-05
112: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 3.936e-05
112: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 4.277e-05
112: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 4.544e-05
112: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 4.833e-05
112: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 5.093e-05
112: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 5.438e-05
112: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 6.8901e-05
112: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 7.2351e-05
112: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 7.5361e-05
112: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 9.6541e-05
112: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000113681
112: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000117891
112: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000139042
112: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000390775
112: communicator.hh::sendRecv,MPI_Testany 3 1 -32766 0 0 0.000454305
112: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 3 1 -32766 0 0
112: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 5
112: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 3 1 -32766 0 0
112: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 5
112: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 3 1 -32766 0 0
112: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 5
112: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 3 1 -32766 0 0
112: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 5
112: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 3 1 -32766 0 0
Newton::solveLinearSystem : Caught exception from the linear solver when getting converged: "BCRSMatrixError [operator++:/home/m.giraud/DUMUXexud/dune-istl/dune/istl/bcrsmatrix.hh:940]: matrix already built up"
112 converged value: 0 121 1 let s go get convergedRemote
112 to  comm_.min(converged) 
112 did  comm_.min(converged) convergedRemote: 0
112 final convergedRemote: 0
112 about to throw Linear solver did not converge
112 Newton::solveLinearSystem : Caught exception: "NumericalProblem [solveLinearSystem:/home/m.giraud/DUMUXexud/dumux/dumux/nonlinear/newtonsolver.hh:381]: Linear solver did not converge when getting convergedRemote "
112 Newton: Caught exception: "NumericalProblem [solveLinearSystem:/home/m.giraud/DUMUXexud/dumux/dumux/nonlinear/newtonsolver.hh:381]: Linear solver did not converge"
112 Newton solver did not converge with dt = 0.0495 seconds. Retrying with time step of 0.02475 seconds
112 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
48 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
48: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 15
48: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 1.04e-06
48: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 7.26e-06
48: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 1.061e-05
48: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 2.6991e-05
48: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 3.0301e-05
48: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 3.3161e-05
48: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 5.0511e-05
48: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 5.3791e-05
48: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 5.6621e-05
48: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 7.5531e-05
48: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 9.6141e-05
48: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000230623
48: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000369335
48: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000372395
48: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000375035
48: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000377655
48: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000380245
48: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000383165
48: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000386055
48: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000388775
48: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000391485
48: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000394035
48: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000396595
48: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000399215
48: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000401875
48: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000404435
48: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000407115
48: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000409675
48: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000412465
48: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000439096
48: communicator.hh::sendRecv,MPI_Testany 9 1 -32766 0 0 0.000442556
48: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 9 1 -32766 0 0
48: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 15
48: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 9 1 -32766 0 0
48: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 15
48: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 9 1 -32766 0 0
48: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 15
48: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 9 1 -32766 0 0
48: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 15
48: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 9 1 -32766 0 0
48: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 15
48: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 9 1 -32766 0 0
48: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 15
48: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 9 1 -32766 0 0
48: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 15
48: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 9 1 -32766 0 0
48: communicator.hh::sendRecv, be5 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 11
5: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 2.121e-06
5: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 9.261e-06
5: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 1.2951e-05
5: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 1.6571e-05
5: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 2.0201e-05
5: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 2.3881e-05
5: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 2.7811e-05
5: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 3.2171e-05
5: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 3.6721e-05
5: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 4.1301e-05
5: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 4.7611e-05
5: communicator.hh::sendRecv,MPI_Testany 0 1 -32766 0 0 8.6582e-05
5: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 0 1 -32766 0 0
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 11
5: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 0 1 -32766 0 0
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 11
5: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 0 1 -32766 0 0
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 11
5: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 0 1 -32766 0 0
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 11
5: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 0 1 -32766 0 0
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 11
5: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 0 1 -32766 0 0
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 11
5: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 0 1 -32766 0 0
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 11
5: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 0 1 -32766 0 0
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 11
5: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 0 1 -32766 0 0
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 11
5: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 0 1 -32766 0 0
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 10 11
5: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 0 1 -32766 0 0
Newton::solveLinearSystem : Caught exception from the linear solver when getting converged: "BCRSMatrixError [operator++:/home/m.giraud/DUMUXexud/dune-istl/dune/istl/bcrsmatrix.hh:940]: matrix already built up"
5 converged value: 0 121 1 let s go get convergedRemote
5 to  comm_.min(converged) 
5 did  comm_.min(converged) convergedRemote: 0
5 final convergedRemote: 0
5 about to throw Linear solver did not converge
5 Newton::solveLinearSystem : Caught exception: "NumericalProblem [solveLinearSystem:/home/m.giraud/DUMUXexud/dumux/dumux/nonlinear/newtonsolver.hh:381]: Linear solver did not converge when getting convergedRemote "
5 Newton: Caught exception: "NumericalProblem [solveLinearSystem:/home/m.giraud/DUMUXexud/dumux/dumux/nonlinear/newtonsolver.hh:381]: Linear solver did not converge"
5 Newton solver did not converge with dt = 0.0495 seco36 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
36: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 14
36: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 5.22e-06
36: communicator.hh::sendRecv,MPI_Testany 2 1 -32766 0 0 1.16e-05
36: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 2 1 -32766 0 0
36: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 14
36: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 2 1 -32766 0 0
36: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 14
36: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 2 1 -32766 0 0
36: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 14
36: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 2 1 -32766 0 0
36: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 14
36: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 2 1 -32766 0 0
36: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 14
36: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 2 1 -32766 0 0
36: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 14
36: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 2 1 -32766 0 0
36: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 14
36: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 2 1 -32766 0 0
36: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 14
36: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 2 1 -32766 0 0
36: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 14
36: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 2 1 -32766 0 0
36: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 10 14
36: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 2 1 -32766 0 0
36: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 11 14
36: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 2 1 -32766 0 0
36: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 12 14
36: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 2 1 -32766 0 0
36: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 13 14
36: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 2 1 -32766 0 0
Newton::solveLinearSystem : Caught exception from the linear solver when getting converged: "BCRSMatrixError [operator++:/home/m.giraud/DUMUXexud/dune-istl/dune/istl/bcrsmatrix.hh:940]: matrix already built up"
36 converged value: 0 121 1 let s go get convergedRemote
36 to  comm_.min(converged) 
36 did  comm_.min(converged) convergedRemote: 0
36 final convergedRemote: 0
36 about to throw Linear solver did not converge
36 Newton::solveLinearSystem : Caught exception: "NumericalProblem [solveLinearSystem:/home/m.giraud/DUMUXexud/dumux/dumux/nonlinear/newtonsolver.hh:381]: Linear solver did not converge when getting convergedRemote "
36 Newton: Caught exception: "NumericalProblem [solveLinearSystem:/home/m.giraud/DUMUXexud/dumux/dumux/nonlinear/newtonsolver.hh:381]: Linear solver did not converge"
36 Newton solver did not converge with dt = 0.0495 seconds. Retrying with time step of 0.02475 seconds
36 Assemble: r(x^k) 52 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
52: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 26
52: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 5.96e-06
52: communicator.hh::sendRecv,MPI_Testany 4 1 -32766 0 0 1.167e-05
52: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 4 1 -32766 0 0
52: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 26
52: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 4 1 -32766 0 0
52: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 26
52: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 4 1 -32766 0 0
52: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 26
52: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 4 1 -32766 0 0
52: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 26
52: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 4 1 -32766 0 0
52: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 26
52: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 4 1 -32766 0 0
52: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 26
52: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 4 1 -32766 0 0
52: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 26
52: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 4 1 -32766 0 0
52: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 26
52: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 4 1 -32766 0 0
52: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 26
52: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 4 1 -32766 0 0
52: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 10 26
52: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 4 1 -32766 0 0
52: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 11 26
52: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 4 1 -32766 0 0
52: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 12 26
52: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 4 1 -32766 0 0
52: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 13 26
52: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 4 1 -32766 0 0
52: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 14 26
52: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 4 1 -32766 0 0
52: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 15 26
52: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 4 1 -32766 0 0
52: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 16 26
52: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 4 1 -32766 0 0
52: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 17 26
52: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 4 1 -32766 0 0
52: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finishe3 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 9
3: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 1.3e-06
3: communicator.hh::sendRecv,MPI_Testany 1 1 -32766 0 0 6.69e-06
3: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 1 1 -32766 0 0
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 9
3: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 1 1 -32766 0 0
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 9
3: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 1 1 -32766 0 0
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 9
3: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 1 1 -32766 0 0
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 9
3: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 1 1 -32766 0 0
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 9
3: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 1 1 -32766 0 0
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 9
3: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 1 1 -32766 0 0
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 9
3: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 1 1 -32766 0 0
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 9
3: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 1 1 -32766 0 0
Newton::solveLinearSystem : Caught exception from the linear solver when getting converged: "BCRSMatrixError [operator++:/home/m.giraud/DUMUXexud/dune-istl/dune/istl/bcrsmatrix.hh:940]: matrix already built up"
3 converged value: 0 121 1 let s go get convergedRemote
3 to  comm_.min(converged) 
3 did  comm_.min(converged) convergedRemote: 0
3 final convergedRemote: 0
3 about to throw Linear solver did not converge
3 Newton::solveLinearSystem : Caught exception: "NumericalProblem [solveLinearSystem:/home/m.giraud/DUMUXexud/dumux/dumux/nonlinear/newtonsolver.hh:381]: Linear solver did not converge when getting convergedRemote "
3 Newton: Caught exception: "NumericalProblem [solveLinearSystem:/home/m.giraud/DUMUXexud/dumux/dumux/nonlinear/newtonsolver.hh:381]: Linear solver did not converge"
3 Newton solver did not converge with dt = 0.0495 seconds. Retrying with time step of 0.02475 seconds
3 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
76 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
76: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 12
76: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 2.08e-06
76: communicator.hh::sendRecv,MPI_Testany 7 1 -32766 0 0 7.8e-06
76: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 7 1 -32766 0 0
76: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 12
76: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 7 1 -32766 0 0
76: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 12
76: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 7 1 -32766 0 0
76: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 12
76: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 7 1 -32766 0 0
76: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 12
76: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 7 1 -32766 0 0
76: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 12
76: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 7 1 -32766 0 0
76: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 12
76: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 7 1 -32766 0 0
76: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 12
76: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 7 1 -32766 0 0
76: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 12
76: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 7 1 -32766 0 0
76: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 12
76: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 7 1 -32766 0 0
76: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 10 12
76: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 7 1 -32766 0 0
76: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 11 12
76: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 7 1 -32766 0 0
Newton::solveLinearSystem : Caught exception from the linear solver when getting converged: "BCRSMatrixError [operator++:/home/m.giraud/DUMUXexud/dune-istl/dune/istl/bcrsmatrix.hh:940]: matrix already built up"
76 converged value: 0 121 1 let s go get convergedRemote
76 to  comm_.min(converged) 
76 did  comm_.min(converged) convergedRemote: 0
76 final convergedRemote: 0
76 about to throw Linear solver did not converge
76 Newton::solveLinearSystem : Caught exception: "NumericalProblem [solveLinearSystem:/home/m.giraud/DUMUXexud/dumux/dumux/nonlinear/newtonsolver.hh:381]: Linear solver did not converge when getting convergedRemote "
76 Newton: Caught exception: "NumericalProblem [solveLinearSystem:/home/m.giraud/DUMUXexud/dumux/dumux/nonlinear/newtonsolver.hh:381]: Linear solver did not converge"
76 Newton solver did not converge with dt = 0.0495 seconds. Retrying with time step of 0.02475 seconds
76 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
66 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
66: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 15
66: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 4.14e-06
66: communicator.hh::sendRecv,MPI_Testany 10 1 -32766 0 0 9.84e-06
66: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 10 1 -32766 0 0
66: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 15
66: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 10 1 -32766 0 0
66: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 15
66: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 10 1 -32766 0 0
66: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 15
66: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 10 1 -32766 0 0
66: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 15
66: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 10 1 -32766 0 0
66: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 15
66: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 10 1 -32766 0 0
66: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 15
66: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 10 1 -32766 0 0
66: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 15
66: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 10 1 -32766 0 0
66: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 15
66: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 10 1 -32766 0 0
66: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 15
66: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 10 1 -32766 0 0
66: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 10 15
66: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 10 1 -32766 0 0
66: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 11 15
66: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 10 1 -32766 0 0
66: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 12 15
66: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 10 1 -32766 0 0
66: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 13 15
66: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 10 1 -32766 0 0
66: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 14 15
66: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 10 1 -32766 0 0
Newton::solveLinearSystem : Caught exception from the linear solver when getting converged: "BCRSMatrixError [operator++:/home/m.giraud/DUMUXexud/dune-istl/dune/istl/bcrsmatrix.hh:940]: matrix already built up"
66 converged value: 0 121 1 let s go get convergedRemote
66 to  comm_.min(converged) 
66 did  comm_.min(converged) convergedRemote: 0
66 final convergedRemote: 0
66 about to throw Linear solver did not converge
66 Newton::solveLinearSystem : Caught exception: "NumericalProblem [solveLinearSystem:/home/m.giraud/DUMUXexud/dumux/dumux/nonlinear/newtonsolver.hh:381]: Linear solver did not converge when getting convergedRemote "
66 Newton: Caught exception: "NumericalProblem [solveLinearSystem:/55 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
55: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 11
55: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 1.72e-06
55: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 1.0221e-05
55: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 2.9521e-05
55: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 3.3121e-05
55: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 3.6771e-05
55: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 4.0121e-05
55: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 4.3441e-05
55: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 6.6631e-05
55: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 7.0401e-05
55: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 7.3601e-05
55: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 7.7531e-05
55: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 8.1351e-05
55: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 8.5181e-05
55: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 8.8242e-05
55: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 9.1562e-05
55: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 9.4902e-05
55: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000117882
55: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000121852
55: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000125302
55: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000128752
55: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000131892
55: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000134992
55: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000138292
55: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000164602
55: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000168013
55: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000171603
55: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000174683
55: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000178113
55: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000181193
55: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000184413
55: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000214523
55: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000218693
55: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000222013
55: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000225323
55: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000228903
55: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000232233
55: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000235993
55: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000268374
55: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000272394
55: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000275684
55: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000278994
55: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000282804
55: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000286654
55: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000290814
55: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000294874
55: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000298934
55: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000303014
55: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000354555
55: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000359335
55: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000363485
55: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000367615
55: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000372055
55: co30 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
30: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 6
30: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 6.6e-07
30: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000255934
30: communicator.hh::sendRecv,MPI_Testany 3 1 -32766 0 0 0.000320934
30: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 3 1 -32766 0 0
30: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 6
30: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 3 1 -32766 0 0
30: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 6
30: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 3 1 -32766 0 0
30: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 6
30: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 3 1 -32766 0 0
30: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 6
30: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 3 1 -32766 0 0
30: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 6
30: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 3 1 -32766 0 0
Newton::solveLinearSystem : Caught exception from the linear solver when getting converged: "BCRSMatrixError [operator++:/home/m.giraud/DUMUXexud/dune-istl/dune/istl/bcrsmatrix.hh:940]: matrix already built up"
30 converged value: 0 121 1 let s go get convergedRemote
30 to  comm_.min(converged) 
30 did  comm_.min(converged) convergedRemote: 0
30 final convergedRemote: 0
30 about to throw Linear solver did not converge
30 Newton::solveLinearSystem : Caught exception: "NumericalProblem [solveLinearSystem:/home/m.giraud/DUMUXexud/dumux/dumux/nonlinear/newtonsolver.hh:381]: Linear solver did not converge when getting convergedRemote "
30 Newton: Caught exception: "NumericalProblem [solveLinearSystem:/home/m.giraud/DUMUXexud/dumux/dumux/nonlinear/newtonsolver.hh:381]: Linear solver did not converge"
30 Newton solver did not converge with dt = 0.0495 seconds. Retrying with time step of 0.02475 seconds
30 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
69 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
69: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 7
69: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 5.5e-07
69: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 7.81e-06
69: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 1.144e-05
69: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 1.482e-05
69: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 6.1831e-05
69: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 6.5651e-05
69: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 6.9101e-05
69: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 7.2521e-05
69: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 7.5851e-05
69: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 7.9281e-05
69: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 8.3521e-05
69: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 8.8001e-05
69: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 9.1971e-05
69: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 9.5421e-05
69: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 9.9361e-05
69: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000102601
69: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000106011
69: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000110311
69: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000113881
69: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000117141
69: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000120471
69: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000124261
69: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000160482
69: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000165162
69: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000169032
69: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000172322
69: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000175862
69: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000179172
69: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000184192
69: communicator.hh::sendRecv,MPI_Testany 2 1 -32766 0 0 0.000187932
69: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 2 1 -32766 0 0
69: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 7
69: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 2 1 -32766 0 0
69: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 7
69: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 2 1 -32766 0 0
69: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 7
69: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 2 1 -32766 0 0
69: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 7
69: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 2 1 -32766 0 0
69: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 7
69: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 2 1 -32766 0 0
69: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 7
69: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 2 1 -32766 0 0
Newton::solveLinearSystem : Caught exception from the linear solver when getting converged: "BCRSMatrixError [operator++:/home/m.giraud/DUMUXexud/dune-istl/dune/istl/bcrsmatrix.hh:940]: matrix already built up"
69 converged value: 0 121 1 let s go get convergedRemote
69 to  comm_.min(converged) 
69 did  comm_.min(converged) 40 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
40: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 15
40: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 6.17e-06
40: communicator.hh::sendRecv,MPI_Testany 9 1 -32766 0 0 1.367e-05
40: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 9 1 -32766 0 0
40: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 15
40: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 9 1 -32766 0 0
40: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 15
40: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 9 1 -32766 0 0
40: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 15
40: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 9 1 -32766 0 0
40: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 15
40: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 9 1 -32766 0 0
40: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 15
40: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 9 1 -32766 0 0
40: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 15
40: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 9 1 -32766 0 0
40: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 15
40: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 9 1 -32766 0 0
40: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 15
40: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 9 1 -32766 0 0
40: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 15
40: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 9 1 -32766 0 0
40: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 10 15
40: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 9 1 -32766 0 0
40: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 11 15
40: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 9 1 -32766 0 0
40: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 12 15
40: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 9 1 -32766 0 0
40: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 13 15
40: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 9 1 -32766 0 0
40: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 14 15
40: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 9 1 -32766 0 0
Newton::solveLinearSystem : Caught exception from the linear solver when getting converged: "BCRSMatrixError [operator++:/home/m.giraud/DUMUXexud/dune-istl/dune/istl/bcrsmatrix.hh:940]: matrix already built up"
40 converged value: 0 121 1 let s go get convergedRemote
40 to  comm_.min(converged) 
40 did  comm_.min(converged) convergedRemote: 0
40 final convergedRemote: 0
40 about to throw Linear solver did not converge
40 Newton::solveLinearSystem : Caught exception: "NumericalProblem [solveLinearSystem:/home/m.giraud/DUMUXexud/dumux/dumux/nonlinear/newtonsolver.hh:381]: Linear solver did not converge when getting convergedRemote "
40 Newton: Caught exception: "NumericalProblem [solveLinearSystem:/home/m.giraud/D16 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
16: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 6
16: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 1.03e-06
16: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 1.277e-05
16: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000124741
16: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000205282
16: communicator.hh::sendRecv,MPI_Testany 1 1 -32766 0 0 0.000247353
16: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 1 1 -32766 0 0
16: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 6
16: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 1 1 -32766 0 0
16: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 6
16: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 1 1 -32766 0 0
16: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 6
16: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 1 1 -32766 0 0
16: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 6
16: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 1 1 -32766 0 0
16: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 6
16: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 1 1 -32766 0 0
Newton::solveLinearSystem : Caught exception from the linear solver when getting converged: "BCRSMatrixError [operator++:/home/m.giraud/DUMUXexud/dune-istl/dune/istl/bcrsmatrix.hh:940]: matrix already built up"
16 converged value: 0 121 1 let s go get convergedRemote
16 to  comm_.min(converged) 
16 did  comm_.min(converged) convergedRemote: 0
16 final convergedRemote: 0
16 about to throw Linear solver did not converge
16 Newton::solveLinearSystem : Caught exception: "NumericalProblem [solveLinearSystem:/home/m.giraud/DUMUXexud/dumux/dumux/nonlinear/newtonsolver.hh:381]: Linear solver did not converge when getting convergedRemote "
16 Newton: Caught exception: "NumericalProblem [solveLinearSystem:/home/m.giraud/DUMUXexud/dumux/dumux/nonlinear/newtonsolver.hh:381]: Linear solver did not converge"
16 Newton solver did not converge with dt = 0.0495 seconds. Retrying with time step of 0.02475 seconds
16 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
105 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
105: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 9
105: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 2.7e-06
105: communicator.hh::sendRecv,MPI_Testany 7 1 -32766 0 0 8.95e-06
105: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 7 1 -32766 0 0
105: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 9
105: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 7 1 -32766 0 0
105: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 9
105: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 7 1 -32766 0 0
105: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 9
105: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 7 1 -32766 0 0
105: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 9
105: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 7 1 -32766 0 0
105: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 9
105: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 7 1 -32766 0 0
105: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 9
105: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 7 1 -32766 0 0
105: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 9
105: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 7 1 -32766 0 0
105: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 9
105: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 7 1 -32766 0 0
Newton::solveLinearSystem : Caught exception from the linear solver when getting converged: "BCRSMatrixError [operator++:/home/m.giraud/DUMUXexud/dune-istl/dune/istl/bcrsmatrix.hh:940]: matrix already built up"
105 converged value: 0 121 1 let s go get convergedRemote
105 to  comm_.min(converged) 
105 did  comm_.min(converged) convergedRemote: 0
105 final convergedRemote: 0
105 about to throw Linear solver did not converge
105 Newton::solveLinearSystem : Caught exception: "NumericalProblem [solveLinearSystem:/home/m.giraud/DUMUXexud/dumux/dumux/nonlinear/newtonsolver.hh:381]: Linear solver did not converge when getting convergedRemote "
105 Newton: Caught exception: "NumericalProblem [solveLinearSystem:/home/m.giraud/DUMUXexud/dumux/dumux/nonlinear/newtonsolver.hh:381]: Linear solver did not converge"
105 Newton solver did not converge with dt = 0.0495 seconds. Retrying with time step of 0.02475 seconds
105 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
37 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
37: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 15
37: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 2.36e-06
37: communicator.hh::sendRecv,MPI_Testany 7 1 -32766 0 0 1.121e-05
37: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 7 1 -32766 0 0
37: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 15
37: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 7 1 -32766 0 0
37: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 15
37: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 7 1 -32766 0 0
37: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 15
37: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 7 1 -32766 0 0
37: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 15
37: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 7 1 -32766 0 0
37: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 15
37: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 7 1 -32766 0 0
37: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 15
37: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 7 1 -32766 0 0
37: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 15
37: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 7 1 -32766 0 0
37: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 15
37: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 7 1 -32766 0 0
37: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 15
37: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 7 1 -32766 0 0
37: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 10 15
37: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 7 1 -32766 0 0
37: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 11 15
37: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 7 1 -32766 0 0
37: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 12 15
37: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 7 1 -32766 0 0
37: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 13 15
37: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 7 1 -32766 0 0
37: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 14 15
37: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 7 1 -32766 0 0
Newton::solveLinearSystem : Caught exception from the linear solver when getting converged: "BCRSMatrixError [operator++:/home/m.giraud/DUMUXexud/dune-istl/dune/istl/bcrsmatrix.hh:940]: matrix already built up"
37 converged value: 0 121 1 let s go get convergedRemote
37 to  comm_.min(converged) 
37 did  comm_.min(converged) convergedRemote: 0
37 final convergedRemote: 0
37 about to throw Linear solver did not converge
37 Newton::solveLinearSystem : Caught exception: "NumericalProblem [solveLinearSystem:/home/m.giraud/DUMUXexud/dumux/dumux/nonlinear/newtonsolver.hh:381]: Linear solver did not converge when getting convergedRemote "
37 Newton: Caught exception: "NumericalProblem [solveLinearSystem:/home/m.giraud/D70 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
70: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 13
70: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 4.29e-06
70: communicator.hh::sendRecv,MPI_Testany 8 1 -32766 0 0 9.9e-06
70: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 8 1 -32766 0 0
70: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 13
70: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 8 1 -32766 0 0
70: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 13
70: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 8 1 -32766 0 0
70: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 13
70: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 8 1 -32766 0 0
70: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 13
70: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 8 1 -32766 0 0
70: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 13
70: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 8 1 -32766 0 0
70: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 13
70: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 8 1 -32766 0 0
70: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 13
70: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 8 1 -32766 0 0
70: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 13
70: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 8 1 -32766 0 0
70: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 13
70: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 8 1 -32766 0 0
70: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 10 13
70: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 8 1 -32766 0 0
70: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 11 13
70: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 8 1 -32766 0 0
70: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 12 13
70: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 8 1 -32766 0 0
Newton::solveLinearSystem : Caught exception from the linear solver when getting converged: "BCRSMatrixError [operator++:/home/m.giraud/DUMUXexud/dune-istl/dune/istl/bcrsmatrix.hh:940]: matrix already built up"
70 converged value: 0 121 1 let s go get convergedRemote
70 to  comm_.min(converged) 
70 did  comm_.min(converged) convergedRemote: 0
70 final convergedRemote: 0
70 about to throw Linear solver did not converge
70 Newton::solveLinearSystem : Caught exception: "NumericalProblem [solveLinearSystem:/home/m.giraud/DUMUXexud/dumux/dumux/nonlinear/newtonsolver.hh:381]: Linear solver did not converge when getting convergedRemote "
70 Newton: Caught exception: "NumericalProblem [solveLinearSystem:/home/m.giraud/DUMUXexud/dumux/dumux/nonlinear/newtonsolver.hh:381]: Linear solver did not converge"
70 Newton solver did not converge with dt = 0.0495 seconds. Retrying with time step of 0.02475 seconds
70 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
26 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
26: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 26
26: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 7.33e-06
26: communicator.hh::sendRecv,MPI_Testany 9 1 -32766 0 0 1.281e-05
26: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 9 1 -32766 0 0
26: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 26
26: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 9 1 -32766 0 0
26: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 26
26: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 9 1 -32766 0 0
26: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 26
26: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 9 1 -32766 0 0
26: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 26
26: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 9 1 -32766 0 0
26: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 26
26: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 9 1 -32766 0 0
26: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 26
26: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 9 1 -32766 0 0
26: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 26
26: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 9 1 -32766 0 0
26: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 26
26: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 9 1 -32766 0 0
26: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 26
26: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 9 1 -32766 0 0
26: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 10 26
26: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 9 1 -32766 0 0
26: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 11 26
26: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 9 1 -32766 0 0
26: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 12 26
26: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 9 1 -32766 0 0
26: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 13 26
26: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 9 1 -32766 0 0
26: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 14 26
26: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 9 1 -32766 0 0
26: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 15 26
26: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 9 1 -32766 0 0
26: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 16 26
26: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 9 1 -32766 0 0
26: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 17 26
26: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 9 1 -32766 0 0
26: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finishe35 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
35: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 9
35: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 1.41e-06
35: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 6.79e-06
35: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 1.073e-05
35: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 1.424e-05
35: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 1.823e-05
35: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 2.158e-05
35: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 2.525e-05
35: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 7.5031e-05
35: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 7.9091e-05
35: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 8.2431e-05
35: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 8.6031e-05
35: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 8.9621e-05
35: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 9.3111e-05
35: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 9.6511e-05
35: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 9.9861e-05
35: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000103191
35: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000107031
35: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000111031
35: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000116061
35: communicator.hh::sendRecv,MPI_Testany 3 1 -32766 0 0 0.000119451
35: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 3 1 -32766 0 0
35: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 9
35: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 3 1 -32766 0 0
35: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 9
35: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 3 1 -32766 0 0
35: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 9
35: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 3 1 -32766 0 0
35: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 9
35: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 3 1 -32766 0 0
35: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 9
35: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 3 1 -32766 0 0
35: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 9
35: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 3 1 -32766 0 0
35: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 9
35: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 3 1 -32766 0 0
35: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 9
35: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 3 1 -32766 0 0
Newton::solveLinearSystem : Caught exception from the linear solver when getting converged: "BCRSMatrixError [operator++:/home/m.giraud/DUMUXexud/dune-istl/dune/istl/bcrsmatrix.hh:940]: matrix already built up"
35 converged value: 0 121 1 let s go get convergedRemote
35 to  comm_.min(converged) 
35 did  comm_.min(converged) convergedRemote: 0
35 final convergedRemote: 0
35 about to throw Linear solver did not converge
35 Newton::solveLinearSystem : Caught exception: "NumericalProblem [solveLinearSystem:/home/m.giraud/DUMUXexud/dumux/dumux/nonlinear/newtonsolver.hh:381]: Linear solver did not converge when getting convergedRemote "
35 Newton: Caught e93 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
93: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 13
93: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 5.2e-07
93: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 6.35e-06
93: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 9.3e-06
93: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 1.213e-05
93: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 1.523e-05
93: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 3.705e-05
93: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 3.979e-05
93: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 4.239e-05
93: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 4.509e-05
93: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 4.78e-05
93: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 5.058e-05
93: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 5.336e-05
93: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 5.605e-05
93: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 5.862e-05
93: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 6.133e-05
93: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 8.5421e-05
93: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 8.8461e-05
93: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 9.1071e-05
93: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 9.3971e-05
93: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 9.6671e-05
93: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 9.9251e-05
93: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000101841
93: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000104621
93: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000107571
93: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000128561
93: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000131651
93: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000134291
93: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000137021
93: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000139601
93: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000142211
93: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000144982
93: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000147712
93: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000150432
93: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000153152
93: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000155912
93: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000183412
93: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000186232
93: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000188962
93: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000191972
93: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000194702
93: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000197282
93: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000199872
93: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000202752
93: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000233103
93: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000236353
93: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000239173
93: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000241763
93: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000244593
93: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000247353
93: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000250073
93: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000253363
93: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000256353
93: communicator.hh::sendRecv,114 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
114: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 13
114: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 1.78e-06
114: communicator.hh::sendRecv,MPI_Testany 7 1 -32766 0 0 2.315e-05
114: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 7 1 -32766 0 0
114: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 13
114: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 7 1 -32766 0 0
114: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 13
114: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 7 1 -32766 0 0
114: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 13
114: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 7 1 -32766 0 0
114: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 13
114: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 7 1 -32766 0 0
114: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 13
114: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 7 1 -32766 0 0
114: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 13
114: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 7 1 -32766 0 0
114: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 13
114: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 7 1 -32766 0 0
114: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 13
114: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 7 1 -32766 0 0
114: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 13
114: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 7 1 -32766 0 0
114: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 10 13
114: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 7 1 -32766 0 0
114: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 11 13
114: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 7 1 -32766 0 0
114: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 12 13
114: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 7 1 -32766 0 0
Newton::solveLinearSystem : Caught exception from the linear solver when getting converged: "BCRSMatrixError [operator++:/home/m.giraud/DUMUXexud/dune-istl/dune/istl/bcrsmatrix.hh:940]: matrix already built up"
114 converged value: 0 121 1 let s go get convergedRemote
114 to  comm_.min(converged) 
114 did  comm_.min(converged) convergedRemote: 0
114 final convergedRemote: 0
114 about to throw Linear solver did not converge
114 Newton::solveLinearSystem : Caught exception: "NumericalProblem [solveLinearSystem:/home/m.giraud/DUMUXexud/dumux/dumux/nonlinear/newtonsolver.hh:381]: Linear solver did not converge when getting convergedRemote "
114 Newton: Caught exception: "NumericalProblem [solveLinearSystem:/home/m.giraud/DUMUXexud/dumux/dumux/nonlinear/newtonsolver.hh:381]: Linear solver did not converge"
114 Newton solver did not converge with dt = 0.0495 seconds. Retrying with time step of 0.02475 seconds
114 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
101 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
101: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 24
101: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 8.29e-06
101: communicator.hh::sendRecv,MPI_Testany 6 1 -32766 0 0 2.262e-05
101: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 6 1 -32766 0 0
101: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 24
101: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 6 1 -32766 0 0
101: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 24
101: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 6 1 -32766 0 0
101: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 24
101: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 6 1 -32766 0 0
101: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 24
101: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 6 1 -32766 0 0
101: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 24
101: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 6 1 -32766 0 0
101: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 24
101: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 6 1 -32766 0 0
101: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 24
101: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 6 1 -32766 0 0
101: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 24
101: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 6 1 -32766 0 0
101: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 24
101: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 6 1 -32766 0 0
101: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 10 24
101: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 6 1 -32766 0 0
101: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 11 24
101: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 6 1 -32766 0 0
101: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 12 24
101: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 6 1 -32766 0 0
101: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 13 24
101: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 6 1 -32766 0 0
101: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 14 24
101: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 6 1 -32766 0 0
101: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 15 24
101: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 6 1 -32766 0 0
101: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 16 24
101: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 6 1 -32766 0 0
101: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 17 24
101: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 6 1 -32766 0 0
101: communicator.hh::sendRecv, before MPI_Waitany(messageInf13 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
13: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 11
13: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 2.78e-06
13: communicator.hh::sendRecv,MPI_Testany 2 1 -32766 0 0 1.214e-05
13: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 2 1 -32766 0 0
13: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 11
13: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 2 1 -32766 0 0
13: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 11
13: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 2 1 -32766 0 0
13: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 11
13: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 2 1 -32766 0 0
13: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 11
13: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 2 1 -32766 0 0
13: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 11
13: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 2 1 -32766 0 0
13: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 11
13: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 2 1 -32766 0 0
13: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 11
13: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 2 1 -32766 0 0
13: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 11
13: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 2 1 -32766 0 0
13: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 11
13: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 2 1 -32766 0 0
13: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 10 11
13: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 2 1 -32766 0 0
Newton::solveLinearSystem : Caught exception from the linear solver when getting converged: "BCRSMatrixError [operator++:/home/m.giraud/DUMUXexud/dune-istl/dune/istl/bcrsmatrix.hh:940]: matrix already built up"
13 converged value: 0 121 1 let s go get convergedRemote
13 to  comm_.min(converged) 
13 did  comm_.min(converged) convergedRemote: 0
13 final convergedRemote: 0
13 about to throw Linear solver did not converge
13 Newton::solveLinearSystem : Caught exception: "NumericalProblem [solveLinearSystem:/home/m.giraud/DUMUXexud/dumux/dumux/nonlinear/newtonsolver.hh:381]: Linear solver did not converge when getting convergedRemote "
13 Newton: Caught exception: "NumericalProblem [solveLinearSystem:/home/m.giraud/DUMUXexud/dumux/dumux/nonlinear/newtonsolver.hh:381]: Linear solver did not converge"
13 Newton solver did not converge with dt = 0.0495 seconds. Retrying with time step of 0.02475 seconds
13 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
39 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
39: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 9
39: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 9.9e-07
39: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 7.49e-06
39: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 1.102e-05
39: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 1.451e-05
39: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 1.79e-05
39: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 2.134e-05
39: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 2.462e-05
39: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 2.826e-05
39: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 7.6241e-05
39: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 8.0171e-05
39: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 8.3611e-05
39: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 8.7121e-05
39: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 9.0511e-05
39: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 9.3761e-05
39: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 9.7381e-05
39: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000100661
39: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000104241
39: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000108031
39: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000111611
39: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000115111
39: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000118612
39: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000122012
39: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000125262
39: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000128722
39: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000132812
39: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000136112
39: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000139372
39: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000142692
39: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000146372
39: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000150352
39: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000153792
39: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000188792
39: communicator.hh::sendRecv,MPI_Testany 7 1 -32766 0 0 0.000192842
39: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 7 1 -32766 0 0
39: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 9
39: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 7 1 -32766 0 0
39: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 9
39: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 7 1 -32766 0 0
39: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 9
39: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 7 1 -32766 0 0
39: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 9
39: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 7 1 -32766 0 0
39: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 9
39: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 7 1 -32766 0 0
39: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 9
39: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 7 1 -32766 0 0
39: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &statu79 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
79: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 16
79: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 2.41e-06
79: communicator.hh::sendRecv,MPI_Testany 11 1 -32766 0 0 9.23e-06
79: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 11 1 -32766 0 0
79: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 16
79: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 11 1 -32766 0 0
79: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 16
79: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 11 1 -32766 0 0
79: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 16
79: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 11 1 -32766 0 0
79: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 16
79: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 11 1 -32766 0 0
79: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 16
79: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 11 1 -32766 0 0
79: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 16
79: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 11 1 -32766 0 0
79: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 16
79: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 11 1 -32766 0 0
79: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 16
79: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 11 1 -32766 0 0
79: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 16
79: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 11 1 -32766 0 0
79: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 10 16
79: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 11 1 -32766 0 0
79: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 11 16
79: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 11 1 -32766 0 0
79: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 12 16
79: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 11 1 -32766 0 0
79: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 13 16
79: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 11 1 -32766 0 0
79: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 14 16
79: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 11 1 -32766 0 0
79: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 15 16
79: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 11 1 -32766 0 0
Newton::solveLinearSystem : Caught exception from the linear solver when getting converged: "BCRSMatrixError [operator++:/home/m.giraud/DUMUXexud/dune-istl/dune/istl/bcrsmatrix.hh:940]: matrix already built up"
79 converged value: 0 121 1 let s go get convergedRemote
79 to  comm_.min(converged) 
79 did  comm_.min(converged) convergedRemote: 0
79 final convergedRemote: 0
79 about to throw Linear solver did not converge
79 Newton::solveLinearSystem : Caught exception: "NumericalProblem [solveLin117 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
117: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 22
117: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 1.94e-06
117: communicator.hh::sendRecv,MPI_Testany 17 1 -32766 0 0 7.93e-06
117: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 17 1 -32766 0 0
117: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 22
117: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 17 1 -32766 0 0
117: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 22
117: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 17 1 -32766 0 0
117: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 22
117: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 17 1 -32766 0 0
117: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 22
117: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 17 1 -32766 0 0
117: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 22
117: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 17 1 -32766 0 0
117: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 22
117: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 17 1 -32766 0 0
117: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 22
117: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 17 1 -32766 0 0
117: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 22
117: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 17 1 -32766 0 0
117: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 22
117: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 17 1 -32766 0 0
117: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 10 22
117: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 17 1 -32766 0 0
117: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 11 22
117: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 17 1 -32766 0 0
117: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 12 22
117: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 17 1 -32766 0 0
117: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 13 22
117: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 17 1 -32766 0 0
117: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 14 22
117: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 17 1 -32766 0 0
117: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 15 22
117: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 17 1 -32766 0 0
117: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 16 22
117: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 17 1 -32766 0 0
117: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 17 22
117: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 17 1 -32766 0 0
117: communicator.hh::sendRecv, before MPI_43 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
43: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 11
43: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 2.08e-06
43: communicator.hh::sendRecv,MPI_Testany 8 1 -32766 0 0 9.06e-06
43: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 8 1 -32766 0 0
43: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 11
43: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 8 1 -32766 0 0
43: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 11
43: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 8 1 -32766 0 0
43: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 11
43: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 8 1 -32766 0 0
43: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 11
43: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 8 1 -32766 0 0
43: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 11
43: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 8 1 -32766 0 0
43: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 11
43: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 8 1 -32766 0 0
43: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 11
43: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 8 1 -32766 0 0
43: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 11
43: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 8 1 -32766 0 0
43: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 11
43: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 8 1 -32766 0 0
43: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 10 11
43: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 8 1 -32766 0 0
Newton::solveLinearSystem : Caught exception from the linear solver when getting converged: "BCRSMatrixError [operator++:/home/m.giraud/DUMUXexud/dune-istl/dune/istl/bcrsmatrix.hh:940]: matrix already built up"
43 converged value: 0 121 1 let s go get convergedRemote
43 to  comm_.min(converged) 
43 did  comm_.min(converged) convergedRemote: 0
43 final convergedRemote: 0
43 about to throw Linear solver did not converge
43 Newton::solveLinearSystem : Caught exception: "NumericalProblem [solveLinearSystem:/home/m.giraud/DUMUXexud/dumux/dumux/nonlinear/newtonsolver.hh:381]: Linear solver did not converge when getting convergedRemote "
43 Newton: Caught exception: "NumericalProblem [solveLinearSystem:/home/m.giraud/DUMUXexud/dumux/dumux/nonlinear/newtonsolver.hh:381]: Linear solver did not converge"
43 Newton solver did not converge with dt = 0.0495 seconds. Retrying with time step of 0.02475 seconds
43 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
46 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
46: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 12
46: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 3.071e-06
46: communicator.hh::sendRecv,MPI_Testany 1 1 -32766 0 0 1.0851e-05
46: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 1 1 -32766 0 0
46: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 12
46: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 1 1 -32766 0 0
46: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 12
46: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 1 1 -32766 0 0
46: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 12
46: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 1 1 -32766 0 0
46: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 12
46: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 1 1 -32766 0 0
46: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 12
46: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 1 1 -32766 0 0
46: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 12
46: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 1 1 -32766 0 0
46: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 12
46: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 1 1 -32766 0 0
46: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 12
46: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 1 1 -32766 0 0
46: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 12
46: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 1 1 -32766 0 0
46: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 10 12
46: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 1 1 -32766 0 0
46: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 11 12
46: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 1 1 -32766 0 0
Newton::solveLinearSystem : Caught exception from the linear solver when getting converged: "BCRSMatrixError [operator++:/home/m.giraud/DUMUXexud/dune-istl/dune/istl/bcrsmatrix.hh:940]: matrix already built up"
46 converged value: 0 121 1 let s go get convergedRemote
46 to  comm_.min(converged) 
46 did  comm_.min(converged) convergedRemote: 0
46 final convergedRemote: 0
46 about to throw Linear solver did not converge
46 Newton::solveLinearSystem : Caught exception: "NumericalProblem [solveLinearSystem:/home/m.giraud/DUMUXexud/dumux/dumux/nonlinear/newtonsolver.hh:381]: Linear solver did not converge when getting convergedRemote "
46 Newton: Caught exception: "NumericalProblem [solveLinearSystem:/home/m.giraud/DUMUXexud/dumux/dumux/nonlinear/newtonsolver.hh:381]: Linear solver did not converge"
46 Newton solver did not converge with dt = 0.0495 seconds. Retrying with time step of 0.02475 seconds
46 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
12 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
12: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 26
12: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 1.24e-05
12: communicator.hh::sendRecv,MPI_Testany 0 1 -32766 0 0 2.0291e-05
12: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 0 1 -32766 0 0
12: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 26
12: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 0 1 -32766 0 0
12: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 26
12: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 0 1 -32766 0 0
12: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 26
12: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 0 1 -32766 0 0
12: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 26
12: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 0 1 -32766 0 0
12: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 26
12: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 0 1 -32766 0 0
12: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 26
12: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 0 1 -32766 0 0
12: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 26
12: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 0 1 -32766 0 0
12: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 26
12: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 0 1 -32766 0 0
12: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 26
12: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 0 1 -32766 0 0
12: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 10 26
12: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 0 1 -32766 0 0
12: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 11 26
12: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 0 1 -32766 0 0
12: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 12 26
12: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 0 1 -32766 0 0
12: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 13 26
12: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 0 1 -32766 0 0
12: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 14 26
12: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 0 1 -32766 0 0
12: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 15 26
12: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 0 1 -32766 0 0
12: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 16 26
12: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 0 1 -32766 0 0
12: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 17 26
12: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 0 1 -32766 0 0
12: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finish27 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
27: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 17
27: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 9.74e-06
27: communicator.hh::sendRecv,MPI_Testany 5 1 -32766 0 0 1.867e-05
27: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 5 1 -32766 0 0
27: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 17
27: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 5 1 -32766 0 0
27: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 17
27: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 5 1 -32766 0 0
27: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 17
27: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 5 1 -32766 0 0
27: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 17
27: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 5 1 -32766 0 0
27: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 17
27: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 5 1 -32766 0 0
27: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 17
27: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 5 1 -32766 0 0
27: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 17
27: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 5 1 -32766 0 0
27: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 17
27: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 5 1 -32766 0 0
27: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 17
27: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 5 1 -32766 0 0
27: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 10 17
27: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 5 1 -32766 0 0
27: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 11 17
27: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 5 1 -32766 0 0
27: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 12 17
27: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 5 1 -32766 0 0
27: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 13 17
27: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 5 1 -32766 0 0
27: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 14 17
27: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 5 1 -32766 0 0
27: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 15 17
27: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 5 1 -32766 0 0
27: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 16 17
27: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 5 1 -32766 0 0
Newton::solveLinearSystem : Caught exception from the linear solver when getting converged: "BCRSMatrixError [operator++:/home/m.giraud/DUMUXexud/dune-istl/dune/istl/bcrsmatrix.hh:940]: matrix already built up"
27 converged value: 0 121 1 let s go get convergedRemote
27 to  comm_.min(converged) 
27 did  co75 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
75: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 17
75: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 2.77e-06
75: communicator.hh::sendRecv,MPI_Testany 12 1 -32766 0 0 9.99e-06
75: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 12 1 -32766 0 0
75: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 17
75: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 12 1 -32766 0 0
75: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 17
75: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 12 1 -32766 0 0
75: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 17
75: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 12 1 -32766 0 0
75: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 17
75: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 12 1 -32766 0 0
75: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 17
75: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 12 1 -32766 0 0
75: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 17
75: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 12 1 -32766 0 0
75: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 17
75: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 12 1 -32766 0 0
75: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 17
75: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 12 1 -32766 0 0
75: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 17
75: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 12 1 -32766 0 0
75: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 10 17
75: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 12 1 -32766 0 0
75: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 11 17
75: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 12 1 -32766 0 0
75: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 12 17
75: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 12 1 -32766 0 0
75: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 13 17
75: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 12 1 -32766 0 0
75: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 14 17
75: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 12 1 -32766 0 0
75: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 15 17
75: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 12 1 -32766 0 0
75: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 16 17
75: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 12 1 -32766 0 0
Newton::solveLinearSystem : Caught exception from the linear solver when getting converged: "BCRSMatrixError [operator++:/home/m.giraud/DUMUXexud/dune-istl/dune/istl/bcrsmatrix.hh:940]: matrix already built up"
75 converged value: 0 121 1 let s go get convergedRemote
75 to  comm_.min(conve120 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
120: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 7
120: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 2.53e-06
120: communicator.hh::sendRecv,MPI_Testany 3 1 -32766 0 0 9.77e-06
120: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 3 1 -32766 0 0
120: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 7
120: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 3 1 -32766 0 0
120: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 7
120: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 3 1 -32766 0 0
120: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 7
120: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 3 1 -32766 0 0
120: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 7
120: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 3 1 -32766 0 0
120: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 7
120: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 3 1 -32766 0 0
120: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 7
120: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 3 1 -32766 0 0
Newton::solveLinearSystem : Caught exception from the linear solver when getting converged: "BCRSMatrixError [operator++:/home/m.giraud/DUMUXexud/dune-istl/dune/istl/bcrsmatrix.hh:940]: matrix already built up"
120 converged value: 0 121 1 let s go get convergedRemote
120 to  comm_.min(converged) 
120 did  comm_.min(converged) convergedRemote: 0
120 final convergedRemote: 0
120 about to throw Linear solver did not converge
120 Newton::solveLinearSystem : Caught exception: "NumericalProblem [solveLinearSystem:/home/m.giraud/DUMUXexud/dumux/dumux/nonlinear/newtonsolver.hh:381]: Linear solver did not converge when getting convergedRemote "
120 Newton: Caught exception: "NumericalProblem [solveLinearSystem:/home/m.giraud/DUMUXexud/dumux/dumux/nonlinear/newtonsolver.hh:381]: Linear solver did not converge"
120 Newton solver did not converge with dt = 0.0495 seconds. Retrying with time step of 0.02475 seconds
120 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
54 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
54: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 18
54: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 9.98e-06
54: communicator.hh::sendRecv,MPI_Testany 1 1 -32766 0 0 4.3261e-05
54: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 1 1 -32766 0 0
54: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 18
54: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 1 1 -32766 0 0
54: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 18
54: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 1 1 -32766 0 0
54: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 18
54: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 1 1 -32766 0 0
54: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 18
54: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 1 1 -32766 0 0
54: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 18
54: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 1 1 -32766 0 0
54: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 18
54: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 1 1 -32766 0 0
54: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 18
54: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 1 1 -32766 0 0
54: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 18
54: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 1 1 -32766 0 0
54: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 18
54: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 1 1 -32766 0 0
54: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 10 18
54: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 1 1 -32766 0 0
54: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 11 18
54: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 1 1 -32766 0 0
54: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 12 18
54: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 1 1 -32766 0 0
54: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 13 18
54: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 1 1 -32766 0 0
54: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 14 18
54: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 1 1 -32766 0 0
54: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 15 18
54: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 1 1 -32766 0 0
54: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 16 18
54: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 1 1 -32766 0 0
54: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 17 18
54: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 1 1 -32766 0 0
Newton::solveLinearSystem : Caught exception from the linear solver when getting converged: "BCRSMa81 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
81: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 11
81: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 1.77e-06
81: communicator.hh::sendRecv,MPI_Testany 8 1 -32766 0 0 8.62e-06
81: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 8 1 -32766 0 0
81: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 11
81: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 8 1 -32766 0 0
81: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 11
81: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 8 1 -32766 0 0
81: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 11
81: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 8 1 -32766 0 0
81: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 11
81: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 8 1 -32766 0 0
81: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 11
81: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 8 1 -32766 0 0
81: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 11
81: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 8 1 -32766 0 0
81: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 11
81: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 8 1 -32766 0 0
81: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 11
81: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 8 1 -32766 0 0
81: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 11
81: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 8 1 -32766 0 0
81: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 10 11
81: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 8 1 -32766 0 0
Newton::solveLinearSystem : Caught exception from the linear solver when getting converged: "BCRSMatrixError [operator++:/home/m.giraud/DUMUXexud/dune-istl/dune/istl/bcrsmatrix.hh:940]: matrix already built up"
81 converged value: 0 121 1 let s go get convergedRemote
81 to  comm_.min(converged) 
81 did  comm_.min(converged) convergedRemote: 0
81 final convergedRemote: 0
81 about to throw Linear solver did not converge
81 Newton::solveLinearSystem : Caught exception: "NumericalProblem [solveLinearSystem:/home/m.giraud/DUMUXexud/dumux/dumux/nonlinear/newtonsolver.hh:381]: Linear solver did not converge when getting convergedRemote "
81 Newton: Caught exception: "NumericalProblem [solveLinearSystem:/home/m.giraud/DUMUXexud/dumux/dumux/nonlinear/newtonsolver.hh:381]: Linear solver did not converge"
81 Newton solver did not converge with dt = 0.0495 seconds. Retrying with time step of 0.02475 seconds
81 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
6 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 17
6: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 4.39e-06
6: communicator.hh::sendRecv,MPI_Testany 0 1 -32766 0 0 1.016e-05
6: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 0 1 -32766 0 0
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 17
6: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 0 1 -32766 0 0
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 17
6: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 0 1 -32766 0 0
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 17
6: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 0 1 -32766 0 0
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 17
6: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 0 1 -32766 0 0
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 17
6: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 0 1 -32766 0 0
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 17
6: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 0 1 -32766 0 0
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 17
6: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 0 1 -32766 0 0
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 17
6: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 0 1 -32766 0 0
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 17
6: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 0 1 -32766 0 0
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 10 17
6: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 0 1 -32766 0 0
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 11 17
6: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 0 1 -32766 0 0
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 12 17
6: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 0 1 -32766 0 0
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 13 17
6: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 0 1 -32766 0 0
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 14 17
6: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 0 1 -32766 0 0
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 15 17
6: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 0 1 -32766 0 0
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 16 17
6: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 0 1 -32766 0 0
Newton::solveLinearSystem : Caught exception from the linear solver when getting converged: "BCRSMatrixError [operator++:/home/m.giraud/DUMUXexud/dune-istl/dune/istl/bcrsmatrix.hh:940]: matrix already built up"
6 converged value: 0 121 1 let s go get convergedRemote
6 to  comm_.min(converged) 
6 did  comm_.min(converged) convergedRemote: 0
6 99 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
99: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 13
99: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 3.81e-06
99: communicator.hh::sendRecv,MPI_Testany 9 1 -32766 0 0 1.079e-05
99: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 9 1 -32766 0 0
99: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 13
99: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 9 1 -32766 0 0
99: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 13
99: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 9 1 -32766 0 0
99: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 13
99: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 9 1 -32766 0 0
99: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 13
99: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 9 1 -32766 0 0
99: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 13
99: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 9 1 -32766 0 0
99: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 13
99: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 9 1 -32766 0 0
99: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 13
99: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 9 1 -32766 0 0
99: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 13
99: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 9 1 -32766 0 0
99: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 13
99: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 9 1 -32766 0 0
99: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 10 13
99: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 9 1 -32766 0 0
99: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 11 13
99: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 9 1 -32766 0 0
99: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 12 13
99: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 9 1 -32766 0 0
Newton::solveLinearSystem : Caught exception from the linear solver when getting converged: "BCRSMatrixError [operator++:/home/m.giraud/DUMUXexud/dune-istl/dune/istl/bcrsmatrix.hh:940]: matrix already built up"
99 converged value: 0 121 1 let s go get convergedRemote
99 to  comm_.min(converged) 
99 did  comm_.min(converged) convergedRemote: 0
99 final convergedRemote: 0
99 about to throw Linear solver did not converge
99 Newton::solveLinearSystem : Caught exception: "NumericalProblem [solveLinearSystem:/home/m.giraud/DUMUXexud/dumux/dumux/nonlinear/newtonsolver.hh:381]: Linear solver did not converge when getting convergedRemote "
99 Newton: Caught exception: "NumericalProblem [solveLinearSystem:/home/m.giraud/DUMUXexud/dumux/dumux/nonlinear/newtonsolver.hh:381]: Linear solver did not converge"
99 Newton solver did not converge with dt = 0.0495 seconds. Retrying with time step of 0.02475 seconds
99 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
119 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
119: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 13
119: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 1.13e-06
119: communicator.hh::sendRecv,MPI_Testany 8 1 -32766 0 0 2.114e-05
119: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 8 1 -32766 0 0
119: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 13
119: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 8 1 -32766 0 0
119: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 13
119: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 8 1 -32766 0 0
119: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 13
119: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 8 1 -32766 0 0
119: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 13
119: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 8 1 -32766 0 0
119: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 13
119: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 8 1 -32766 0 0
119: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 13
119: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 8 1 -32766 0 0
119: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 13
119: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 8 1 -32766 0 0
119: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 13
119: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 8 1 -32766 0 0
119: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 13
119: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 8 1 -32766 0 0
119: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 10 13
119: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 8 1 -32766 0 0
119: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 11 13
119: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 8 1 -32766 0 0
119: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 12 13
119: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 8 1 -32766 0 0
Newton::solveLinearSystem : Caught exception from the linear solver when getting converged: "BCRSMatrixError [operator++:/home/m.giraud/DUMUXexud/dune-istl/dune/istl/bcrsmatrix.hh:940]: matrix already built up"
119 converged value: 0 121 1 let s go get convergedRemote
119 to  comm_.min(converged) 
119 did  comm_.min(converged) convergedRemote: 0
119 final convergedRemote: 0
119 about to throw Linear solver did not converge
119 Newton::solveLinearSystem : Caught exception: "NumericalProblem [solveLinearSystem:/home/m.giraud/DUMUXexud/dumux/dumux/nonlinear/newtonsolver.hh:381]: Linear solver did not converge when getting convergedRemote "
119 Newton: Caught exception: "NumericalProblem [solveLinearSystem:/home/m.giraud/DUMUXexud/dumux/dumux/nonlinear/newtonsolver.hh:381]: Linear solver did not converge"
119 Newton solver did not converge with dt = 0.0495 seconds. Retrying with time step of 0.02475 seconds
119 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
32 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
32: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 16
32: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 3.51e-06
32: communicator.hh::sendRecv,MPI_Testany 13 1 -32766 0 0 1.025e-05
32: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 13 1 -32766 0 0
32: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 16
32: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 13 1 -32766 0 0
32: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 16
32: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 13 1 -32766 0 0
32: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 16
32: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 13 1 -32766 0 0
32: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 16
32: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 13 1 -32766 0 0
32: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 16
32: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 13 1 -32766 0 0
32: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 16
32: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 13 1 -32766 0 0
32: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 16
32: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 13 1 -32766 0 0
32: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 16
32: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 13 1 -32766 0 0
32: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 16
32: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 13 1 -32766 0 0
32: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 10 16
32: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 13 1 -32766 0 0
32: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 11 16
32: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 13 1 -32766 0 0
32: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 12 16
32: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 13 1 -32766 0 0
32: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 13 16
32: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 13 1 -32766 0 0
32: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 14 16
32: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 13 1 -32766 0 0
32: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 15 16
32: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 13 1 -32766 0 0
Newton::solveLinearSystem : Caught exception from the linear solver when getting converged: "BCRSMatrixError [operator++:/home/m.giraud/DUMUXexud/dune-istl/dune/istl/bcrsmatrix.hh:940]: matrix already built up"
32 converged value: 0 121 1 let s go get convergedRemote
32 to  comm_.min(converged) 
32 did  comm_.min(converged) convergedRemote: 0
32 final convergedRemote: 0
32 about to throw Linear solver did not converge
32 Newton::solveLinearSystem : Caught exception: "NumericalProblem [solveLi57 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
57: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 19
57: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 2.26e-06
57: communicator.hh::sendRecv,MPI_Testany 18 1 -32766 0 0 9.77e-06
57: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 18 1 -32766 0 0
57: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 19
57: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 18 1 -32766 0 0
57: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 19
57: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 18 1 -32766 0 0
57: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 19
57: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 18 1 -32766 0 0
57: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 19
57: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 18 1 -32766 0 0
57: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 19
57: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 18 1 -32766 0 0
57: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 19
57: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 18 1 -32766 0 0
57: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 19
57: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 18 1 -32766 0 0
57: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 19
57: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 18 1 -32766 0 0
57: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 19
57: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 18 1 -32766 0 0
57: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 10 19
57: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 18 1 -32766 0 0
57: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 11 19
57: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 18 1 -32766 0 0
57: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 12 19
57: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 18 1 -32766 0 0
57: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 13 19
57: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 18 1 -32766 0 0
57: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 14 19
57: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 18 1 -32766 0 0
57: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 15 19
57: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 18 1 -32766 0 0
57: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 16 19
57: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 18 1 -32766 0 0
57: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 17 19
57: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 18 1 -32766 0 0
57: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recv56 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
56: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 31
56: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 7.15e-06
56: communicator.hh::sendRecv,MPI_Testany 11 1 -32766 0 0 1.32e-05
56: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 11 1 -32766 0 0
56: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 31
56: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 11 1 -32766 0 0
56: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 31
56: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 11 1 -32766 0 0
56: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 31
56: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 11 1 -32766 0 0
56: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 31
56: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 11 1 -32766 0 0
56: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 31
56: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 11 1 -32766 0 0
56: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 31
56: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 11 1 -32766 0 0
56: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 31
56: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 11 1 -32766 0 0
56: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 31
56: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 11 1 -32766 0 0
56: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 31
56: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 11 1 -32766 0 0
56: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 10 31
56: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 11 1 -32766 0 0
56: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 11 31
56: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 11 1 -32766 0 0
56: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 12 31
56: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 11 1 -32766 0 0
56: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 13 31
56: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 11 1 -32766 0 0
56: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 14 31
56: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 11 1 -32766 0 0
56: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 15 31
56: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 11 1 -32766 0 0
56: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 16 31
56: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 11 1 -32766 0 0
56: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 17 31
56: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 11 1 -32766 0 0
56: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recv116 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
116: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 7
116: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 4.4e-07
116: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 9.5341e-05
116: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000102922
116: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000106942
116: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000109732
116: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000112432
116: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000115132
116: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000118312
116: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000121182
116: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000123802
116: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000126422
116: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000129032
116: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000131632
116: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000134782
116: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000137892
116: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000140502
116: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000143102
116: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000145702
116: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000148312
116: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000151182
116: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000154022
116: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000156632
116: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000159252
116: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000161862
116: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000164462
116: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000167352
116: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000170182
116: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000172782
116: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000175393
116: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000178003
116: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000180623
116: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000183493
116: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000186323
116: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000188933
116: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000191553
116: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000194153
116: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000196773
116: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000199653
116: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000202483
116: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000205093
116: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000207673
116: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000210273
116: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000212883
116: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000216003
116: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000218613
116: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000221243
116: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000223873
116: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000310164
116: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000313524
116: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000387935
116: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000469006
116: communicat60 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
60: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 6
60: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 5e-07
60: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000111811
60: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000115181
60: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000119651
60: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000214742
60: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000217872
60: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000307903
60: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000311784
60: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000438285
60: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000523186
60: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000597707
60: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000679908
60: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.00080759
60: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.00081514
60: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.00081854
60: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.00082776
60: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000939441
60: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.00101866
60: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.0010594
60: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.00118038
60: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.00119516
60: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.00119793
60: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.00120082
60: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.00120378
60: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.00120649
60: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.00120937
60: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.00121221
60: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.00122422
60: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.00122772
60: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.00123043
60: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.0012334
60: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.00124909
60: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.00125223
60: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.00125507
60: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.00125766
60: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.00126051
60: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.00127686
60: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.00127983
60: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.0012994
60: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.00130289
60: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.00132145
60: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.00134061
60: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.00159119
60: communicator.hh::sendRecv,MPI_Testany 3 1 -32766 0 0 0.00159433
60: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 3 1 -32766 0 0
60: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 6
60: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 3 1 -32766 0 0
60: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 6
60: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 3 1 -32766 0 0
60: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 6
60: communi53 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
53: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 17
53: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 1.32e-06
53: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 7.07e-06
53: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 9.99e-06
53: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 1.28e-05
53: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 1.581e-05
53: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 1.883e-05
53: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 2.156e-05
53: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 2.432e-05
53: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 4.8701e-05
53: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 5.2071e-05
53: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 5.5001e-05
53: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 5.8161e-05
53: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 6.1021e-05
53: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 6.3671e-05
53: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 6.6311e-05
53: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 6.8941e-05
53: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 8.8571e-05
53: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 9.1831e-05
53: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 9.4991e-05
53: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 9.7891e-05
53: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000101101
53: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000104181
53: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000106891
53: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000109522
53: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000112782
53: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000135912
53: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000139192
53: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000142062
53: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000145242
53: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000147902
53: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000150812
53: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000153442
53: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000156072
53: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000158692
53: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000161442
53: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000193573
53: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000197283
53: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000199923
53: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000202553
53: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000205183
53: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000207803
53: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000210433
53: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000213753
53: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000256073
53: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000259803
53: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000263083
53: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000266343
53: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000269614
53: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000273204
53: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000276504
53: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000279784
53: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000283064
53: communicator.hh:108 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
108: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 9
108: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 1.04e-06
108: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 1.809e-05
108: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 2.126e-05
108: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 2.407e-05
108: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 2.717e-05
108: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 4.496e-05
108: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 4.928e-05
108: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 5.277e-05
108: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 6.7911e-05
108: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 7.2411e-05
108: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 9.3851e-05
108: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000111211
108: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000115901
108: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000136622
108: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000390785
108: communicator.hh::sendRecv,MPI_Testany 4 1 -32766 0 0 0.000451186
108: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 4 1 -32766 0 0
108: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 9
108: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 4 1 -32766 0 0
108: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 9
108: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 4 1 -32766 0 0
108: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 9
108: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 4 1 -32766 0 0
108: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 9
108: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 4 1 -32766 0 0
108: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 9
108: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 4 1 -32766 0 0
108: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 9
108: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 4 1 -32766 0 0
108: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 9
108: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 4 1 -32766 0 0
108: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 9
108: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 4 1 -32766 0 0
Newton::solveLinearSystem : Caught exception from the linear solver when getting converged: "BCRSMatrixError [operator++:/home/m.giraud/DUMUXexud/dune-istl/dune/istl/bcrsmatrix.hh:940]: matrix already built up"
108 converged value: 0 121 1 let s go get convergedRemote
108 to  comm_.min(converged) 
108 did  comm_.min(converged) convergedRemote: 0
108 final convergedRemote: 0
108 about to throw Linear solver did not converge
108 Newton::solveLinearSystem : Caught exception: "NumericalProblem [solveLinearSystem:/home/m.giraud/DUMUXexud/dumux/dumux/nonlinear/newtonsolver.hh:381]: Linear solver did not converge when getting convergedRemote "
108 Newton: Caught exception: "NumericalProblem [solveLinearSystem:/home/m.giraud/DUMUXexud/dumux/dumux/nonlinear/newtonsolver.hh:381]: Linear solver did not converge"
108 Newton solver did not converge with dt = 0.0495 seconds. Retrying with time step of 0.02475 second4 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 14
4: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 3.9e-06
4: communicator.hh::sendRecv,MPI_Testany 0 1 -32766 0 0 1.112e-05
4: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 0 1 -32766 0 0
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 14
4: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 0 1 -32766 0 0
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 14
4: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 0 1 -32766 0 0
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 14
4: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 0 1 -32766 0 0
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 14
4: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 0 1 -32766 0 0
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 14
4: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 0 1 -32766 0 0
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 14
4: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 0 1 -32766 0 0
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 14
4: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 0 1 -32766 0 0
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 14
4: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 0 1 -32766 0 0
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 14
4: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 0 1 -32766 0 0
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 10 14
4: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 0 1 -32766 0 0
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 11 14
4: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 0 1 -32766 0 0
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 12 14
4: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 0 1 -32766 0 0
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 13 14
4: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 0 1 -32766 0 0
Newton::solveLinearSystem : Caught exception from the linear solver when getting converged: "BCRSMatrixError [operator++:/home/m.giraud/DUMUXexud/dune-istl/dune/istl/bcrsmatrix.hh:940]: matrix already built up"
4 converged value: 0 121 1 let s go get convergedRemote
4 to  comm_.min(converged) 
4 did  comm_.min(converged) convergedRemote: 0
4 final convergedRemote: 0
4 about to throw Linear solver did not converge
4 Newton::solveLinearSystem : Caught exception: "NumericalProblem [solveLinearSystem:/home/m.giraud/DUMUXexud/dumux/dumux/nonlinear/newtonsolver.hh:381]: Linear solver did not converge when getting convergedRemote "
4 Newton: Caught exception: "NumericalProblem [solveLinearSystem:/home/m.giraud/DUMUXexud/dumux/dumux/nonlinear/newtonsolver.hh:381]: Linear solver did not converge"
4 Newton solver did not converge with dt = 0.0495 seconds. Retrying with time step of 0.02475 seconds
4 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
84 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
84: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 18
84: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 6.46e-06
84: communicator.hh::sendRecv,MPI_Testany 8 1 -32766 0 0 1.403e-05
84: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 8 1 -32766 0 0
84: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 18
84: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 8 1 -32766 0 0
84: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 18
84: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 8 1 -32766 0 0
84: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 18
84: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 8 1 -32766 0 0
84: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 18
84: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 8 1 -32766 0 0
84: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 18
84: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 8 1 -32766 0 0
84: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 18
84: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 8 1 -32766 0 0
84: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 18
84: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 8 1 -32766 0 0
84: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 18
84: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 8 1 -32766 0 0
84: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 18
84: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 8 1 -32766 0 0
84: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 10 18
84: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 8 1 -32766 0 0
84: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 11 18
84: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 8 1 -32766 0 0
84: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 12 18
84: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 8 1 -32766 0 0
84: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 13 18
84: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 8 1 -32766 0 0
84: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 14 18
84: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 8 1 -32766 0 0
84: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 15 18
84: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 8 1 -32766 0 0
84: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 16 18
84: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 8 1 -32766 0 0
84: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 17 18
84: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 8 1 -32766 0 0
Newton::solveLinearSystem : Caught exception from the linear solver when getting converged: "BCRSMat50 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
50: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 10
50: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 8.5e-07
50: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 2.145e-05
50: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 2.469e-05
50: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 2.736e-05
50: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 3.013e-05
50: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 4.5331e-05
50: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 4.8491e-05
50: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 5.1651e-05
50: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 7.0951e-05
50: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 8.9561e-05
50: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 9.2431e-05
50: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 9.5681e-05
50: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000116372
50: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000367325
50: communicator.hh::sendRecv,MPI_Testany 2 1 -32766 0 0 0.000370895
50: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 2 1 -32766 0 0
50: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 10
50: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 2 1 -32766 0 0
50: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 10
50: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 2 1 -32766 0 0
50: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 10
50: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 2 1 -32766 0 0
50: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 10
50: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 2 1 -32766 0 0
50: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 10
50: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 2 1 -32766 0 0
50: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 10
50: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 2 1 -32766 0 0
50: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 10
50: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 2 1 -32766 0 0
50: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 10
50: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 2 1 -32766 0 0
50: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 10
50: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 2 1 -32766 0 0
Newton::solveLinearSystem : Caught exception from the linear solver when getting converged: "BCRSMatrixError [operator++:/home/m.giraud/DUMUXexud/dune-istl/dune/istl/bcrsmatrix.hh:940]: matrix already built up"
50 converged value: 0 121 1 let s go get convergedRemote
50 to  comm_.min(converged) 
50 did  comm_.min(converged) convergedRemote: 0
50 final convergedRemote: 0
50 about to throw Linear solver did not converge
50 Newton::solveLinearSystem : Caught exception: "NumericalProblem [solveLinearSystem:/home/m.giraud/DUMUXexud/dumux/dumux/nonlinear/newtonsolver.hh:381]: Linear solver did not converge when getting convergedRemote "
50 Newton: Caught exception: "NumericalProblem [solveLinearSystem:/home/m.giraud/DUMUXexud/dumux/dumux/nonlinear/newtonsolver.hh:381]: Linear solver did not converge"
5045 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
45: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 17
45: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 4.43e-06
45: communicator.hh::sendRecv,MPI_Testany 3 1 -32766 0 0 1.074e-05
45: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 3 1 -32766 0 0
45: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 17
45: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 3 1 -32766 0 0
45: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 17
45: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 3 1 -32766 0 0
45: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 17
45: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 3 1 -32766 0 0
45: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 17
45: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 3 1 -32766 0 0
45: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 17
45: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 3 1 -32766 0 0
45: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 17
45: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 3 1 -32766 0 0
45: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 17
45: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 3 1 -32766 0 0
45: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 17
45: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 3 1 -32766 0 0
45: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 17
45: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 3 1 -32766 0 0
45: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 10 17
45: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 3 1 -32766 0 0
45: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 11 17
45: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 3 1 -32766 0 0
45: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 12 17
45: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 3 1 -32766 0 0
45: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 13 17
45: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 3 1 -32766 0 0
45: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 14 17
45: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 3 1 -32766 0 0
45: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 15 17
45: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 3 1 -32766 0 0
45: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 16 17
45: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 3 1 -32766 0 0
Newton::solveLinearSystem : Caught exception from the linear solver when getting converged: "BCRSMatrixError [operator++:/home/m.giraud/DUMUXexud/dune-istl/dune/istl/bcrsmatrix.hh:940]: matrix already built up"
45 converged value: 0 121 1 let s go get convergedRemote
45 to  comm_.min(converged) 
45 did  co109 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
109: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 20
109: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 5.13e-06
109: communicator.hh::sendRecv,MPI_Testany 5 1 -32766 0 0 1.351e-05
109: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 5 1 -32766 0 0
109: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 20
109: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 5 1 -32766 0 0
109: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 20
109: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 5 1 -32766 0 0
109: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 20
109: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 5 1 -32766 0 0
109: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 20
109: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 5 1 -32766 0 0
109: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 20
109: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 5 1 -32766 0 0
109: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 20
109: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 5 1 -32766 0 0
109: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 20
109: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 5 1 -32766 0 0
109: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 20
109: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 5 1 -32766 0 0
109: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 20
109: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 5 1 -32766 0 0
109: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 10 20
109: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 5 1 -32766 0 0
109: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 11 20
109: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 5 1 -32766 0 0
109: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 12 20
109: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 5 1 -32766 0 0
109: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 13 20
109: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 5 1 -32766 0 0
109: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 14 20
109: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 5 1 -32766 0 0
109: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 15 20
109: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 5 1 -32766 0 0
109: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 16 20
109: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 5 1 -32766 0 0
109: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 17 20
109: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 5 1 -32766 0 0
109: communicator.hh::sendRecv, before MPI_Waitany(messageInf71 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
71: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 20
71: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 2.76e-06
71: communicator.hh::sendRecv,MPI_Testany 3 1 -32766 0 0 1.039e-05
71: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 3 1 -32766 0 0
71: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 20
71: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 3 1 -32766 0 0
71: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 20
71: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 3 1 -32766 0 0
71: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 20
71: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 3 1 -32766 0 0
71: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 20
71: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 3 1 -32766 0 0
71: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 20
71: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 3 1 -32766 0 0
71: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 20
71: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 3 1 -32766 0 0
71: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 20
71: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 3 1 -32766 0 0
71: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 20
71: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 3 1 -32766 0 0
71: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 20
71: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 3 1 -32766 0 0
71: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 10 20
71: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 3 1 -32766 0 0
71: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 11 20
71: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 3 1 -32766 0 0
71: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 12 20
71: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 3 1 -32766 0 0
71: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 13 20
71: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 3 1 -32766 0 0
71: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 14 20
71: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 3 1 -32766 0 0
71: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 15 20
71: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 3 1 -32766 0 0
71: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 16 20
71: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 3 1 -32766 0 0
71: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 17 20
71: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 3 1 -32766 0 0
71: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finishe62 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
62: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 15
62: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 2.36e-06
62: communicator.hh::sendRecv,MPI_Testany 0 1 -32766 0 0 8.231e-06
62: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 0 1 -32766 0 0
62: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 15
62: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 0 1 -32766 0 0
62: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 15
62: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 0 1 -32766 0 0
62: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 15
62: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 0 1 -32766 0 0
62: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 15
62: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 0 1 -32766 0 0
62: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 15
62: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 0 1 -32766 0 0
62: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 15
62: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 0 1 -32766 0 0
62: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 15
62: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 0 1 -32766 0 0
62: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 15
62: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 0 1 -32766 0 0
62: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 15
62: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 0 1 -32766 0 0
62: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 10 15
62: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 0 1 -32766 0 0
62: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 11 15
62: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 0 1 -32766 0 0
62: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 12 15
62: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 0 1 -32766 0 0
62: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 13 15
62: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 0 1 -32766 0 0
62: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 14 15
62: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 0 1 -32766 0 0
Newton::solveLinearSystem : Caught exception from the linear solver when getting converged: "BCRSMatrixError [operator++:/home/m.giraud/DUMUXexud/dune-istl/dune/istl/bcrsmatrix.hh:940]: matrix already built up"
62 converged value: 0 121 1 let s go get convergedRemote
62 to  comm_.min(converged) 
62 did  comm_.min(converged) convergedRemote: 0
62 final convergedRemote: 0
62 about to throw Linear solver did not converge
62 Newton::solveLinearSystem : Caught exception: "NumericalProblem [solveLinearSystem:/home/m.giraud/DUMUXexud/dumux/dumux/nonlinear/newtonsolver.hh:381]: Linear solver did not converge when getting convergedRemote "
62 Newton: Caught exception: "NumericalProblem [solveLinearSystem:/home/m.giraud/D61 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
61: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 9
61: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 4.9e-07
61: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 6.09e-06
61: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 9.03e-06
61: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 1.193e-05
61: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 3.7421e-05
61: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 4.0501e-05
61: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 4.3141e-05
61: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 4.5751e-05
61: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 4.8381e-05
61: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 5.0981e-05
61: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 5.3601e-05
61: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 5.6391e-05
61: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 5.9261e-05
61: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 8.6711e-05
61: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 8.9671e-05
61: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 9.2291e-05
61: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 9.4891e-05
61: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 9.7641e-05
61: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000100481
61: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000103311
61: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000106181
61: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000142082
61: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000145262
61: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000148022
61: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000150962
61: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000153712
61: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000156732
61: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000159552
61: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000162532
61: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000165782
61: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000169182
61: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000172412
61: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000175662
61: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000179042
61: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000226873
61: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000230743
61: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000234263
61: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000237593
61: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000240923
61: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000244213
61: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000247523
61: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000250843
61: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000254573
61: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000257913
61: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000261233
61: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000264743
61: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000268083
61: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000271414
61: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000274814
61: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000278114
61: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000281474
61: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000286474
61: communicato89 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
89: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 11
89: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 8.4e-07
89: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 2.327e-05
89: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 2.638e-05
89: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 2.92e-05
89: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 3.223e-05
89: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 3.495e-05
89: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 3.791e-05
89: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 4.058e-05
89: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 4.325e-05
89: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 4.593e-05
89: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 4.924e-05
89: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 7.1861e-05
89: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 7.4791e-05
89: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 7.7511e-05
89: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 8.0191e-05
89: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 8.2851e-05
89: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 8.5531e-05
89: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 8.8221e-05
89: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 9.1071e-05
89: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 9.3891e-05
89: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000115331
89: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000118901
89: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000121831
89: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000124691
89: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000127651
89: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000130341
89: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000133041
89: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000135841
89: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000161152
89: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000164332
89: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000167212
89: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000169872
89: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000172542
89: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000175372
89: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000178192
89: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000181262
89: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000184532
89: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000212552
89: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000215882
89: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000218812
89: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000221732
89: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000224412
89: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000227282
89: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000229943
89: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000266493
89: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000269583
89: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000272403
89: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000275373
89: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000278863
89: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000281653
89: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000284703
89: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000287853
89: communicator.hh::s111 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
111: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 11
111: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 9.4e-07
111: communicator.hh::sendRecv,MPI_Testany 8 1 -32766 0 0 2.3e-05
111: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 8 1 -32766 0 0
111: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 11
111: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 8 1 -32766 0 0
111: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 11
111: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 8 1 -32766 0 0
111: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 11
111: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 8 1 -32766 0 0
111: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 11
111: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 8 1 -32766 0 0
111: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 11
111: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 8 1 -32766 0 0
111: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 11
111: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 8 1 -32766 0 0
111: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 11
111: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 8 1 -32766 0 0
111: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 11
111: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 8 1 -32766 0 0
111: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 11
111: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 8 1 -32766 0 0
111: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 10 11
111: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 8 1 -32766 0 0
Newton::solveLinearSystem : Caught exception from the linear solver when getting converged: "BCRSMatrixError [operator++:/home/m.giraud/DUMUXexud/dune-istl/dune/istl/bcrsmatrix.hh:940]: matrix already built up"
111 converged value: 0 121 1 let s go get convergedRemote
111 to  comm_.min(converged) 
111 did  comm_.min(converged) convergedRemote: 0
111 final convergedRemote: 0
111 about to throw Linear solver did not converge
111 Newton::solveLinearSystem : Caught exception: "NumericalProblem [solveLinearSystem:/home/m.giraud/DUMUXexud/dumux/dumux/nonlinear/newtonsolver.hh:381]: Linear solver did not converge when getting convergedRemote "
111 Newton: Caught exception: "NumericalProblem [solveLinearSystem:/home/m.giraud/DUMUXexud/dumux/dumux/nonlinear/newtonsolver.hh:381]: Linear solver did not converge"
111 Newton solver did not converge with dt = 0.0495 seconds. Retrying with time step of 0.02475 seconds
111 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
100 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
100: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 6
100: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 9.4e-07
100: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 6.98e-06
100: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 9.91e-06
100: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 1.295e-05
100: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 2.44e-05
100: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 2.75e-05
100: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 3.014e-05
100: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 3.301e-05
100: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 4.902e-05
100: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 5.2081e-05
100: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 5.4761e-05
100: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 5.8261e-05
100: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 7.2551e-05
100: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 7.6161e-05
100: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 9.7051e-05
100: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000117451
100: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000120501
100: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000254983
100: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000396335
100: communicator.hh::sendRecv,MPI_Testany 4 1 -32766 0 0 0.000399255
100: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 4 1 -32766 0 0
100: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 6
100: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 4 1 -32766 0 0
100: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 6
100: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 4 1 -32766 0 0
100: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 6
100: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 4 1 -32766 0 0
100: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 6
100: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 4 1 -32766 0 0
100: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 6
100: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 4 1 -32766 0 0
Newton::solveLinearSystem : Caught exception from the linear solver when getting converged: "BCRSMatrixError [operator++:/home/m.giraud/DUMUXexud/dune-istl/dune/istl/bcrsmatrix.hh:940]: matrix already built up"
100 converged value: 0 121 1 let s go get convergedRemote
100 to  comm_.min(converged) 
100 did  comm_.min(converged) convergedRemote: 0
100 final convergedRemote: 0
100 about to throw Linear solver did not converge
100 Newton::solveLinearSystem : Caught exception: "NumericalProblem [solveLinearSystem:/home/m.giraud/DUMUXexud/dumux/dumux/nonlinear/newtonsolver.hh:381]: Linear solver did not converge when getting convergedRemote "
100 Newton: Caught exception: "NumericalProblem [solveLinearSystem:/home/m.giraud/DUMUXexud/dumux/dumux/nonlinear/newtonsolver.hh:381]: Linear solver did not converge"
100 Newton solver did not converge with dt = 0.0495 seconds. Retrying with time step of 0.02475 seconds
100 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
64 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
64: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 14
64: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 2.18e-06
64: communicator.hh::sendRecv,MPI_Testany 0 1 -32766 0 0 7.75e-06
64: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 0 1 -32766 0 0
64: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 14
64: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 0 1 -32766 0 0
64: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 14
64: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 0 1 -32766 0 0
64: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 14
64: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 0 1 -32766 0 0
64: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 14
64: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 0 1 -32766 0 0
64: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 14
64: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 0 1 -32766 0 0
64: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 14
64: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 0 1 -32766 0 0
64: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 14
64: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 0 1 -32766 0 0
64: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 14
64: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 0 1 -32766 0 0
64: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 14
64: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 0 1 -32766 0 0
64: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 10 14
64: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 0 1 -32766 0 0
64: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 11 14
64: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 0 1 -32766 0 0
64: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 12 14
64: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 0 1 -32766 0 0
64: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 13 14
64: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 0 1 -32766 0 0
Newton::solveLinearSystem : Caught exception from the linear solver when getting converged: "BCRSMatrixError [operator++:/home/m.giraud/DUMUXexud/dune-istl/dune/istl/bcrsmatrix.hh:940]: matrix already built up"
64 converged value: 0 121 1 let s go get convergedRemote
64 to  comm_.min(converged) 
64 did  comm_.min(converged) convergedRemote: 0
64 final convergedRemote: 0
64 about to throw Linear solver did not converge
64 Newton::solveLinearSystem : Caught exception: "NumericalProblem [solveLinearSystem:/home/m.giraud/DUMUXexud/dumux/dumux/nonlinear/newtonsolver.hh:381]: Linear solver did not converge when getting convergedRemote "
64 Newton: Caught exception: "NumericalProblem [solveLinearSystem:/home/m.giraud/DUMUXexud/dumux/dumux/nonlinear/newtonsolver.hh:381]: Linear solver did not converge"
64 Newton solver did not converge with dt = 0.0495 seconds. Retrying with time step of 0.02475 seconds
64 Assemble: r(x^k) 7 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
7: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 13
7: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 2.55e-06
7: communicator.hh::sendRecv,MPI_Testany 1 1 -32766 0 0 9.64e-06
7: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 1 1 -32766 0 0
7: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 13
7: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 1 1 -32766 0 0
7: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 13
7: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 1 1 -32766 0 0
7: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 13
7: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 1 1 -32766 0 0
7: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 13
7: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 1 1 -32766 0 0
7: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 13
7: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 1 1 -32766 0 0
7: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 13
7: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 1 1 -32766 0 0
7: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 13
7: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 1 1 -32766 0 0
7: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 13
7: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 1 1 -32766 0 0
7: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 13
7: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 1 1 -32766 0 0
7: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 10 13
7: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 1 1 -32766 0 0
7: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 11 13
7: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 1 1 -32766 0 0
7: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 12 13
7: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 1 1 -32766 0 0
Newton::solveLinearSystem : Caught exception from the linear solver when getting converged: "BCRSMatrixError [operator++:/home/m.giraud/DUMUXexud/dune-istl/dune/istl/bcrsmatrix.hh:940]: matrix already built up"
7 converged value: 0 121 1 let s go get convergedRemote
7 to  comm_.min(converged) 
7 did  comm_.min(converged) convergedRemote: 0
7 final convergedRemote: 0
7 about to throw Linear solver did not converge
7 Newton::solveLinearSystem : Caught exception: "NumericalProblem [solveLinearSystem:/home/m.giraud/DUMUXexud/dumux/dumux/nonlinear/newtonsolver.hh:381]: Linear solver did not converge when getting convergedRemote "
7 Newton: Caught exception: "NumericalProblem [solveLinearSystem:/home/m.giraud/DUMUXexud/dumux/dumux/nonlinear/newtonsolver.hh:381]: Linear solver did not converge"
7 Newton solver did not converge with dt = 0.0495 seconds. Retrying with time step of 0.02475 seconds
7 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
113 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
113: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 18
113: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 1.041e-06
113: communicator.hh::sendRecv,MPI_Testany 12 1 -32766 0 0 7.041e-06
113: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 12 1 -32766 0 0
113: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 18
113: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 12 1 -32766 0 0
113: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 18
113: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 12 1 -32766 0 0
113: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 18
113: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 12 1 -32766 0 0
113: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 18
113: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 12 1 -32766 0 0
113: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 18
113: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 12 1 -32766 0 0
113: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 18
113: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 12 1 -32766 0 0
113: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 18
113: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 12 1 -32766 0 0
113: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 18
113: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 12 1 -32766 0 0
113: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 18
113: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 12 1 -32766 0 0
113: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 10 18
113: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 12 1 -32766 0 0
113: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 11 18
113: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 12 1 -32766 0 0
113: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 12 18
113: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 12 1 -32766 0 0
113: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 13 18
113: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 12 1 -32766 0 0
113: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 14 18
113: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 12 1 -32766 0 0
113: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 15 18
113: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 12 1 -32766 0 0
113: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 16 18
113: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 12 1 -32766 0 0
113: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 17 18
113: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 12 1 -32766 0 0
Newton::solveLinearSystem : Caught except23 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
23: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 11
23: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 1.05e-06
23: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 1.965e-05
23: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 2.275e-05
23: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 3.997e-05
23: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 4.345e-05
23: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 4.624e-05
23: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 6.956e-05
23: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 7.269e-05
23: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 7.5441e-05
23: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 7.8261e-05
23: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 8.1051e-05
23: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 9.9341e-05
23: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000102521
23: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000105461
23: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000108291
23: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000110981
23: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000113641
23: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000116601
23: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000119351
23: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000121961
23: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000143911
23: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000147031
23: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000150891
23: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000153792
23: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000156562
23: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000159172
23: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000161812
23: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000164432
23: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000167192
23: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000188672
23: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000191922
23: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000194582
23: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000197192
23: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000199822
23: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000202462
23: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000205272
23: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000208102
23: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000210852
23: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000236963
23: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000239883
23: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000242513
23: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000245313
23: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000248063
23: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000250773
23: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000253503
23: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000256253
23: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000259313
23: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000289173
23: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000292413
23: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000295053
23: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000297683
23: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000300333
23: commu20 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
20: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 10
20: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 2.99e-06
20: communicator.hh::sendRecv,MPI_Testany 1 1 -32766 0 0 1.078e-05
20: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 1 1 -32766 0 0
20: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 10
20: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 1 1 -32766 0 0
20: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 10
20: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 1 1 -32766 0 0
20: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 10
20: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 1 1 -32766 0 0
20: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 10
20: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 1 1 -32766 0 0
20: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 10
20: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 1 1 -32766 0 0
20: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 10
20: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 1 1 -32766 0 0
20: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 10
20: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 1 1 -32766 0 0
20: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 10
20: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 1 1 -32766 0 0
20: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 10
20: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 1 1 -32766 0 0
Newton::solveLinearSystem : Caught exception from the linear solver when getting converged: "BCRSMatrixError [operator++:/home/m.giraud/DUMUXexud/dune-istl/dune/istl/bcrsmatrix.hh:940]: matrix already built up"
20 converged value: 0 121 1 let s go get convergedRemote
20 to  comm_.min(converged) 
20 did  comm_.min(converged) convergedRemote: 0
20 final convergedRemote: 0
20 about to throw Linear solver did not converge
20 Newton::solveLinearSystem : Caught exception: "NumericalProblem [solveLinearSystem:/home/m.giraud/DUMUXexud/dumux/dumux/nonlinear/newtonsolver.hh:381]: Linear solver did not converge when getting convergedRemote "
20 Newton: Caught exception: "NumericalProblem [solveLinearSystem:/home/m.giraud/DUMUXexud/dumux/dumux/nonlinear/newtonsolver.hh:381]: Linear solver did not converge"
20 Newton solver did not converge with dt = 0.0495 seconds. Retrying with time step of 0.02475 seconds
20 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
90 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
90: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 6
90: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 3.52e-06
90: communicator.hh::sendRecv,MPI_Testany 3 1 -32766 0 0 1.083e-05
90: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 3 1 -32766 0 0
90: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 6
90: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 3 1 -32766 0 0
90: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 6
90: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 3 1 -32766 0 0
90: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 6
90: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 3 1 -32766 0 0
90: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 6
90: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 3 1 -32766 0 0
90: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 6
90: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 3 1 -32766 0 0
Newton::solveLinearSystem : Caught exception from the linear solver when getting converged: "BCRSMatrixError [operator++:/home/m.giraud/DUMUXexud/dune-istl/dune/istl/bcrsmatrix.hh:940]: matrix already built up"
90 converged value: 0 121 1 let s go get convergedRemote
90 to  comm_.min(converged) 
90 did  comm_.min(converged) convergedRemote: 0
90 final convergedRemote: 0
90 about to throw Linear solver did not converge
90 Newton::solveLinearSystem : Caught exception: "NumericalProblem [solveLinearSystem:/home/m.giraud/DUMUXexud/dumux/dumux/nonlinear/newtonsolver.hh:381]: Linear solver did not converge when getting convergedRemote "
90 Newton: Caught exception: "NumericalProblem [solveLinearSystem:/home/m.giraud/DUMUXexud/dumux/dumux/nonlinear/newtonsolver.hh:381]: Linear solver did not converge"
90 Newton solver did not converge with dt = 0.0495 seconds. Retrying with time step of 0.02475 seconds
90 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
110 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
110: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 11
110: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 6.8e-06
110: communicator.hh::sendRecv,MPI_Testany 0 1 -32766 0 0 1.3371e-05
110: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 0 1 -32766 0 0
110: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 11
110: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 0 1 -32766 0 0
110: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 11
110: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 0 1 -32766 0 0
110: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 11
110: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 0 1 -32766 0 0
110: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 11
110: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 0 1 -32766 0 0
110: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 11
110: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 0 1 -32766 0 0
110: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 11
110: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 0 1 -32766 0 0
110: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 11
110: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 0 1 -32766 0 0
110: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 11
110: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 0 1 -32766 0 0
110: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 11
110: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 0 1 -32766 0 0
110: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 10 11
110: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 0 1 -32766 0 0
Newton::solveLinearSystem : Caught exception from the linear solver when getting converged: "BCRSMatrixError [operator++:/home/m.giraud/DUMUXexud/dune-istl/dune/istl/bcrsmatrix.hh:940]: matrix already built up"
110 converged value: 0 121 1 let s go get convergedRemote
110 to  comm_.min(converged) 
110 did  comm_.min(converged) convergedRemote: 0
110 final convergedRemote: 0
110 about to throw Linear solver did not converge
110 Newton::solveLinearSystem : Caught exception: "NumericalProblem [solveLinearSystem:/home/m.giraud/DUMUXexud/dumux/dumux/nonlinear/newtonsolver.hh:381]: Linear solver did not converge when getting convergedRemote "
110 Newton: Caught exception: "NumericalProblem [solveLinearSystem:/home/m.giraud/DUMUXexud/dumux/dumux/nonlinear/newtonsolver.hh:381]: Linear solver did not converge"
110 Newton solver did not converge with dt = 0.0495 seconds. Retrying with time step of 0.02475 seconds
110 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
73 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
73: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 11
73: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 9e-07
73: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 2.205e-05
73: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 2.528e-05
73: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 2.815e-05
73: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 3.12e-05
73: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 3.412e-05
73: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 3.675e-05
73: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 3.953e-05
73: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 4.23e-05
73: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 4.527e-05
73: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 4.805e-05
73: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 6.6681e-05
73: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 6.9651e-05
73: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 7.2341e-05
73: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 7.4971e-05
73: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 7.7601e-05
73: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 8.0241e-05
73: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 8.2991e-05
73: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 8.5861e-05
73: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 8.8601e-05
73: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 9.1321e-05
73: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000111451
73: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000114661
73: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000117481
73: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000120561
73: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000123371
73: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000126151
73: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000128881
73: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000131901
73: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000134611
73: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000137531
73: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000160512
73: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000163862
73: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000166632
73: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000169242
73: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000171862
73: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000174922
73: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000177732
73: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000180362
73: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000182972
73: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000185742
73: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000215442
73: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000218892
73: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000221653
73: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000224283
73: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000226913
73: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000229533
73: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000232163
73: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000235173
73: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000277673
73: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000281503
73: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000284883
73: communicator.hh::sendR63 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
63: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 9
63: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 7.1e-07
63: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 6.36e-06
63: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 9.26e-06
63: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 1.206e-05
63: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 1.485e-05
63: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 1.7651e-05
63: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 4.0981e-05
63: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 4.3921e-05
63: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 4.6671e-05
63: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 4.9251e-05
63: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 5.2021e-05
63: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 5.4851e-05
63: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 5.7581e-05
63: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 6.0301e-05
63: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 8.2831e-05
63: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 8.5901e-05
63: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 8.8631e-05
63: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 9.1371e-05
63: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 9.4261e-05
63: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 9.7012e-05
63: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 9.9592e-05
63: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000102172
63: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000104912
63: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000107762
63: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000135022
63: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000137932
63: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000140842
63: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000143482
63: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000146072
63: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000148662
63: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000151432
63: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000154452
63: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000157212
63: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000187353
63: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000190373
63: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000192993
63: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000195753
63: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000198463
63: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000201083
63: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000203663
63: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000206533
63: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000209413
63: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000212353
63: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000215613
63: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000261944
63: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000265184
63: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000267934
63: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000270734
63: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000273754
63: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000276454
63: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000279334
63: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000282154
63: communicator.hh18 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
18: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 14
18: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 5.49e-06
18: communicator.hh::sendRecv,MPI_Testany 6 1 -32766 0 0 1.273e-05
18: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 6 1 -32766 0 0
18: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 14
18: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 6 1 -32766 0 0
18: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 14
18: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 6 1 -32766 0 0
18: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 14
18: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 6 1 -32766 0 0
18: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 14
18: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 6 1 -32766 0 0
18: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 14
18: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 6 1 -32766 0 0
18: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 14
18: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 6 1 -32766 0 0
18: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 14
18: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 6 1 -32766 0 0
18: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 14
18: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 6 1 -32766 0 0
18: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 14
18: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 6 1 -32766 0 0
18: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 10 14
18: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 6 1 -32766 0 0
18: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 11 14
18: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 6 1 -32766 0 0
18: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 12 14
18: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 6 1 -32766 0 0
18: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 13 14
18: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 6 1 -32766 0 0
Newton::solveLinearSystem : Caught exception from the linear solver when getting converged: "BCRSMatrixError [operator++:/home/m.giraud/DUMUXexud/dune-istl/dune/istl/bcrsmatrix.hh:940]: matrix already built up"
18 converged value: 0 121 1 let s go get convergedRemote
18 to  comm_.min(converged) 
18 did  comm_.min(converged) convergedRemote: 0
18 final convergedRemote: 0
18 about to throw Linear solver did not converge
18 Newton::solveLinearSystem : Caught exception: "NumericalProblem [solveLinearSystem:/home/m.giraud/DUMUXexud/dumux/dumux/nonlinear/newtonsolver.hh:381]: Linear solver did not converge when getting convergedRemote "
18 Newton: Caught exception: "NumericalProblem [solveLinearSystem:/home/m.giraud/DUMUXexud/dumux/dumux/nonlinear/newtonsolver.hh:381]: Linear solver did not converge"
18 Newton solver did not converge with dt = 0.0495 seconds. Retrying with time step of 0.02475 seconds
18 Assemble: r(x^k)31 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
31: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 10
31: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 8.9e-07
31: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 6.571e-06
31: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 9.481e-06
31: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 1.2331e-05
31: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 3.5011e-05
31: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 3.8411e-05
31: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 4.1031e-05
31: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 4.3641e-05
31: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 4.6281e-05
31: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 4.8941e-05
31: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 5.1551e-05
31: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 5.4381e-05
31: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 8.0832e-05
31: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 8.4192e-05
31: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 8.6932e-05
31: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 8.9552e-05
31: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 9.2152e-05
31: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 9.4912e-05
31: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 9.7712e-05
31: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000100462
31: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000128432
31: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000131842
31: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000134492
31: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000137142
31: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000139972
31: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000142762
31: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000145522
31: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000148412
31: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000181943
31: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000185283
31: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000188203
31: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000191233
31: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000193873
31: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000196503
31: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000199143
31: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000201823
31: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000205233
31: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000208153
31: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000211133
31: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000214403
31: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000217793
31: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000265444
31: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000270084
31: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000273464
31: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000276894
31: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000280304
31: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000283674
31: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000287004
31: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000290674
31: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000294354
31: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000299534
31: communicator.hh::sendRecv,MPI_Testany 6 1 -32766 0 0 0.000303144
31: communicator.96 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
96: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 12
96: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 6.06e-06
96: communicator.hh::sendRecv,MPI_Testany 3 1 -32766 0 0 1.255e-05
96: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 3 1 -32766 0 0
96: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 12
96: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 3 1 -32766 0 0
96: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 12
96: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 3 1 -32766 0 0
96: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 12
96: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 3 1 -32766 0 0
96: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 12
96: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 3 1 -32766 0 0
96: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 12
96: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 3 1 -32766 0 0
96: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 12
96: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 3 1 -32766 0 0
96: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 12
96: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 3 1 -32766 0 0
96: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 12
96: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 3 1 -32766 0 0
96: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 12
96: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 3 1 -32766 0 0
96: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 10 12
96: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 3 1 -32766 0 0
96: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 11 12
96: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 3 1 -32766 0 0
Newton::solveLinearSystem : Caught exception from the linear solver when getting converged: "BCRSMatrixError [operator++:/home/m.giraud/DUMUXexud/dune-istl/dune/istl/bcrsmatrix.hh:940]: matrix already built up"
96 converged value: 0 121 1 let s go get convergedRemote
96 to  comm_.min(converged) 
96 did  comm_.min(converged) convergedRemote: 0
96 final convergedRemote: 0
96 about to throw Linear solver did not converge
96 Newton::solveLinearSystem : Caught exception: "NumericalProblem [solveLinearSystem:/home/m.giraud/DUMUXexud/dumux/dumux/nonlinear/newtonsolver.hh:381]: Linear solver did not converge when getting convergedRemote "
96 Newton: Caught exception: "NumericalProblem [solveLinearSystem:/home/m.giraud/DUMUXexud/dumux/dumux/nonlinear/newtonsolver.hh:381]: Linear solver did not converge"
96 Newton solver did not converge with dt = 0.0495 seconds. Retrying with time step of 0.02475 seconds
96 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
cator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 3 1 -32766 0 0
60: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 6
60: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 3 1 -32766 0 0
60: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 6
60: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 3 1 -32766 0 0
Newton::solveLinearSystem : Caught exception from the linear solver when getting converged: "BCRSMatrixError [operator++:/home/m.giraud/DUMUXexud/dune-istl/dune/istl/bcrsmatrix.hh:940]: matrix already built up"
60 converged value: 0 121 1 let s go get convergedRemote
60 to  comm_.min(converged) 
60 did  comm_.min(converged) convergedRemote: 0
60 final convergedRemote: 0
60 about to throw Linear solver did not converge
60 Newton::solveLinearSystem : Caught exception: "NumericalProblem [solveLinearSystem:/home/m.giraud/DUMUXexud/dumux/dumux/nonlinear/newtonsolver.hh:381]: Linear solver did not converge when getting convergedRemote "
60 Newton: Caught exception: "NumericalProblem [solveLinearSystem:/home/m.giraud/DUMUXexud/dumux/dumux/nonlinear/newtonsolver.hh:381]: Linear solver did not converge"
60 Newton solver did not converge with dt = 0.0495 seconds. Retrying with time step of 0.02475 seconds
60 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
11 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
11: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 17
11: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 3.61e-06
11: communicator.hh::sendRecv,MPI_Testany 6 1 -32766 0 0 1.052e-05
11: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 6 1 -32766 0 0
11: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 17
11: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 6 1 -32766 0 0
11: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 17
11: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 6 1 -32766 0 0
11: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 17
11: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 6 1 -32766 0 0
11: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 17
11: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 6 1 -32766 0 0
11: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 17
11: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 6 1 -32766 0 0
11: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 17
11: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 6 1 -32766 0 0
11: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 17
11: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 6 1 -32766 0 0
11: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 17
11: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 6 1 -32766 0 0
11: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 17
11: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 6 1 -32766 0 0
11: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 10 17
11: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 6 1 -32766 0 0
11: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 11 17
11: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 6 1 -32766 0 0
11: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 12 17
11: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 6 1 -32766 0 0
11: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 13 17
11: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 6 1 -32766 0 0
11: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 14 17
11: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 6 1 -32766 0 0
11: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 15 17
11: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 6 1 -32766 0 0
11: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 16 17
11: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 6 1 -32766 0 0
Newton::solveLinearSystem : Caught exception from the linear solver when getting converged: "BCRSMatrixError [operator++:/home/m.giraud/DUMUXexud/dune-istl/dune/istl/bcrsmatrix.hh:940]: matrix already built up"
11 converged value: 0 121 1 let s go get convergedRemote
11 to  comm_.min(converged) 
11 did  co22 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
22: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 20
22: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 5.48e-06
22: communicator.hh::sendRecv,MPI_Testany 8 1 -32766 0 0 1.2941e-05
22: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 8 1 -32766 0 0
22: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 20
22: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 8 1 -32766 0 0
22: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 20
22: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 8 1 -32766 0 0
22: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 20
22: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 8 1 -32766 0 0
22: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 20
22: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 8 1 -32766 0 0
22: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 20
22: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 8 1 -32766 0 0
22: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 20
22: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 8 1 -32766 0 0
22: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 20
22: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 8 1 -32766 0 0
22: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 20
22: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 8 1 -32766 0 0
22: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 20
22: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 8 1 -32766 0 0
22: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 10 20
22: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 8 1 -32766 0 0
22: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 11 20
22: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 8 1 -32766 0 0
22: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 12 20
22: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 8 1 -32766 0 0
22: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 13 20
22: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 8 1 -32766 0 0
22: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 14 20
22: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 8 1 -32766 0 0
22: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 15 20
22: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 8 1 -32766 0 0
22: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 16 20
22: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 8 1 -32766 0 0
22: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 17 20
22: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 8 1 -32766 0 0
22: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finish:sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000286364
53: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000289644
53: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000293494
53: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000297104
53: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000300424
53: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000305214
53: communicator.hh::sendRecv,MPI_Testany 5 1 -32766 0 0 0.000308884
53: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 5 1 -32766 0 0
53: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 17
53: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 5 1 -32766 0 0
53: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 17
53: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 5 1 -32766 0 0
53: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 17
53: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 5 1 -32766 0 0
53: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 17
53: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 5 1 -32766 0 0
53: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 17
53: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 5 1 -32766 0 0
53: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 17
53: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 5 1 -32766 0 0
53: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 17
53: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 5 1 -32766 0 0
53: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 17
53: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 5 1 -32766 0 0
53: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 17
53: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 5 1 -32766 0 0
53: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 10 17
53: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 5 1 -32766 0 0
53: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 11 17
53: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 5 1 -32766 0 0
53: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 12 17
53: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 5 1 -32766 0 0
53: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 13 17
53: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 5 1 -32766 0 0
53: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 14 17
53: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 5 1 -32766 0 0
53: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 15 17
53: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 5 1 -32766 0 0
53: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 16 17
53: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 5 1 -32766 0 0
Newton::solveLinearSystem : Caught exception from the linear solver when getting converged: "BCRSMatrixError [operator++:/home/m.giraud/DUMUXexud/dune-istl/dune/istl/bcrsmatrix.hh:940]: matrix already built up"
44 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
44: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 11
44: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 2.51e-06
44: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 8.45e-06
44: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 1.156e-05
44: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 1.462e-05
44: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 1.7431e-05
44: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 2.0301e-05
44: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 2.3111e-05
44: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 2.6341e-05
44: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 2.8991e-05
44: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 3.1561e-05
44: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 3.4331e-05
44: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 3.6941e-05
44: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 3.9541e-05
44: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 4.2151e-05
44: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 4.4751e-05
44: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 4.7331e-05
44: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 5.0171e-05
44: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 5.2981e-05
44: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 5.5611e-05
44: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 5.8221e-05
44: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 6.2171e-05
44: communicator.hh::sendRecv,MPI_Testany 0 1 -32766 0 0 6.4841e-05
44: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 0 1 -32766 0 0
44: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 11
44: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 0 1 -32766 0 0
44: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 11
44: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 0 1 -32766 0 0
44: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 11
44: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 0 1 -32766 0 0
44: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 11
44: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 0 1 -32766 0 0
44: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 11
44: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 0 1 -32766 0 0
44: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 11
44: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 0 1 -32766 0 0
44: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 11
44: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 0 1 -32766 0 0
44: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 11
44: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 0 1 -32766 0 0
44: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 11
44: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 0 1 -32766 0 0
44: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 10 11
44: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 0 1 -32766 0 0
Newton::solveLinearSystem : Caught exception from the linear solver when getting converged: s
108 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
rixError [operator++:/home/m.giraud/DUMUXexud/dune-istl/dune/istl/bcrsmatrix.hh:940]: matrix already built up"
84 converged value: 0 121 1 let s go get convergedRemote
84 to  comm_.min(converged) 
84 did  comm_.min(converged) convergedRemote: 0
84 final convergedRemote: 0
84 about to throw Linear solver did not converge
84 Newton::solveLinearSystem : Caught exception: "NumericalProblem [solveLinearSystem:/home/m.giraud/DUMUXexud/dumux/dumux/nonlinear/newtonsolver.hh:381]: Linear solver did not converge when getting convergedRemote "
84 Newton: Caught exception: "NumericalProblem [solveLinearSystem:/home/m.giraud/DUMUXexud/dumux/dumux/nonlinear/newtonsolver.hh:381]: Linear solver did not converge"
84 Newton solver did not converge with dt = 0.0495 seconds. Retrying with time step of 0.02475 seconds
84 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
 Newton solver did not converge with dt = 0.0495 seconds. Retrying with time step of 0.02475 seconds
50 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
mm_.min(converged) convergedRemote: 0
45 final convergedRemote: 0
45 about to throw Linear solver did not converge
45 Newton::solveLinearSystem : Caught exception: "NumericalProblem [solveLinearSystem:/home/m.giraud/DUMUXexud/dumux/dumux/nonlinear/newtonsolver.hh:381]: Linear solver did not converge when getting convergedRemote "
45 Newton: Caught exception: "NumericalProblem [solveLinearSystem:/home/m.giraud/DUMUXexud/dumux/dumux/nonlinear/newtonsolver.hh:381]: Linear solver did not converge"
45 Newton solver did not converge with dt = 0.0495 seconds. Retrying with time step of 0.02475 seconds
45 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
ormation_.size(), recvRequests, &finished, &status); 18 20
109: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 5 1 -32766 0 0
109: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 19 20
109: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 5 1 -32766 0 0
Newton::solveLinearSystem : Caught exception from the linear solver when getting converged: "BCRSMatrixError [operator++:/home/m.giraud/DUMUXexud/dune-istl/dune/istl/bcrsmatrix.hh:940]: matrix already built up"
109 converged value: 0 121 1 let s go get convergedRemote
109 to  comm_.min(converged) 
109 did  comm_.min(converged) convergedRemote: 0
109 final convergedRemote: 0
109 about to throw Linear solver did not converge
109 Newton::solveLinearSystem : Caught exception: "NumericalProblem [solveLinearSystem:/home/m.giraud/DUMUXexud/dumux/dumux/nonlinear/newtonsolver.hh:381]: Linear solver did not converge when getting convergedRemote "
109 Newton: Caught exception: "NumericalProblem [solveLinearSystem:/home/m.giraud/DUMUXexud/dumux/dumux/nonlinear/newtonsolver.hh:381]: Linear solver did not converge"
109 Newton solver did not converge with dt = 0.0495 seconds. Retrying with time step of 0.02475 seconds
109 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
d, &status); 18 20
71: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 3 1 -32766 0 0
71: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 19 20
71: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 3 1 -32766 0 0
Newton::solveLinearSystem : Caught exception from the linear solver when getting converged: "BCRSMatrixError [operator++:/home/m.giraud/DUMUXexud/dune-istl/dune/istl/bcrsmatrix.hh:940]: matrix already built up"
71 converged value: 0 121 1 let s go get convergedRemote
71 to  comm_.min(converged) 
71 did  comm_.min(converged) convergedRemote: 0
71 final convergedRemote: 0
71 about to throw Linear solver did not converge
71 Newton::solveLinearSystem : Caught exception: "NumericalProblem [solveLinearSystem:/home/m.giraud/DUMUXexud/dumux/dumux/nonlinear/newtonsolver.hh:381]: Linear solver did not converge when getting convergedRemote "
71 Newton: Caught exception: "NumericalProblem [solveLinearSystem:/home/m.giraud/DUMUXexud/dumux/dumux/nonlinear/newtonsolver.hh:381]: Linear solver did not converge"
71 Newton solver did not converge with dt = 0.0495 seconds. Retrying with time step of 0.02475 seconds
71 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
UMUXexud/dumux/dumux/nonlinear/newtonsolver.hh:381]: Linear solver did not converge"
62 Newton solver did not converge with dt = 0.0495 seconds. Retrying with time step of 0.02475 seconds
62 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
r.hh::sendRecv,MPI_Testany 0 1 -32766 0 0 0.000289854
61: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 0 1 -32766 0 0
61: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 9
61: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 0 1 -32766 0 0
61: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 9
61: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 0 1 -32766 0 0
61: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 9
61: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 0 1 -32766 0 0
61: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 9
61: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 0 1 -32766 0 0
61: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 9
61: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 0 1 -32766 0 0
61: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 9
61: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 0 1 -32766 0 0
61: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 9
61: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 0 1 -32766 0 0
61: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 9
61: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 0 1 -32766 0 0
Newton::solveLinearSystem : Caught exception from the linear solver when getting converged: "BCRSMatrixError [operator++:/home/m.giraud/DUMUXexud/dune-istl/dune/istl/bcrsmatrix.hh:940]: matrix already built up"
61 converged value: 0 121 1 let s go get convergedRemote
61 to  comm_.min(converged) 
61 did  comm_.min(converged) convergedRemote: 0
61 final convergedRemote: 0
61 about to throw Linear solver did not converge
61 Newton::solveLinearSystem : Caught exception: "NumericalProblem [solveLinearSystem:/home/m.giraud/DUMUXexud/dumux/dumux/nonlinear/newtonsolver.hh:381]: Linear solver did not converge when getting convergedRemote "
61 Newton: Caught exception: "NumericalProblem [solveLinearSystem:/home/m.giraud/DUMUXexud/dumux/dumux/nonlinear/newtonsolver.hh:381]: Linear solver did not converge"
61 Newton solver did not converge with dt = 0.0495 seconds. Retrying with time step of 0.02475 seconds
61 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
Requests, &finished, &status); 18 19
57: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 18 1 -32766 0 0
Newton::solveLinearSystem : Caught exception from the linear solver when getting converged: "BCRSMatrixError [operator++:/home/m.giraud/DUMUXexud/dune-istl/dune/istl/bcrsmatrix.hh:940]: matrix already built up"
57 converged value: 0 121 1 let s go get convergedRemote
57 to  comm_.min(converged) 
57 did  comm_.min(converged) convergedRemote: 0
57 final convergedRemote: 0
57 about to throw Linear solver did not converge
57 Newton::solveLinearSystem : Caught exception: "NumericalProblem [solveLinearSystem:/home/m.giraud/DUMUXexud/dumux/dumux/nonlinear/newtonsolver.hh:381]: Linear solver did not converge when getting convergedRemote "
57 Newton: Caught exception: "NumericalProblem [solveLinearSystem:/home/m.giraud/DUMUXexud/dumux/dumux/nonlinear/newtonsolver.hh:381]: Linear solver did not converge"
57 Newton solver did not converge with dt = 0.0495 seconds. Retrying with time step of 0.02475 seconds
57 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
Newton::solveLinearSystem : Caught exception from the linear solver when getting converged: "BCRSMatrixError [operator++:/home/m.giraud/DUMUXexud/dune-istl/dune/istl/bcrsmatrix.hh:940]: matrix already built up"
17 converged value: 0 121 1 let s go get convergedRemote
17 to  comm_.min(converged) 
17 did  comm_.min(converged) convergedRemote: 0
17 final convergedRemote: 0
17 about to throw Linear solver did not converge
17 Newton::solveLinearSystem : Caught exception: "NumericalProblem [solveLinearSystem:/home/m.giraud/DUMUXexud/dumux/dumux/nonlinear/newtonsolver.hh:381]: Linear solver did not converge when getting convergedRemote "
17 Newton: Caught exception: "NumericalProblem [solveLinearSystem:/home/m.giraud/DUMUXexud/dumux/dumux/nonlinear/newtonsolver.hh:381]: Linear solver did not converge"
17 Newton solver did not converge with dt = 0.0495 seconds. Retrying with time step of 0.02475 seconds
17 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
Newton::solveLinearSystem : Caught exception from the linear solver when getting converged: "BCRSMatrixError [operator++:/home/m.giraud/DUMUXexud/dune-istl/dune/istl/bcrsmatrix.hh:940]: matrix already built up"
102 converged value: 0 121 1 let s go get convergedRemote
102 to  comm_.min(converged) 
102 did  comm_.min(converged) convergedRemote: 0
102 final convergedRemote: 0
102 about to throw Linear solver did not converge
102 Newton::solveLinearSystem : Caught exception: "NumericalProblem [solveLinearSystem:/home/m.giraud/DUMUXexud/dumux/dumux/nonlinear/newtonsolver.hh:381]: Linear solver did not converge when getting convergedRemote "
102 Newton: Caught exception: "NumericalProblem [solveLinearSystem:/home/m.giraud/DUMUXexud/dumux/dumux/nonlinear/newtonsolver.hh:381]: Linear solver did not converge"
102 Newton solver did not converge with dt = 0.0495 seconds. Retrying with time step of 0.02475 seconds
102 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
78: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 15
78: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 10 1 -32766 0 0
78: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 15
78: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 10 1 -32766 0 0
78: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 15
78: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 10 1 -32766 0 0
78: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 15
78: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 10 1 -32766 0 0
78: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 15
78: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 10 1 -32766 0 0
78: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 15
78: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 10 1 -32766 0 0
78: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 15
78: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 10 1 -32766 0 0
78: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 15
78: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 10 1 -32766 0 0
78: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 15
78: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 10 1 -32766 0 0
78: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 10 15
78: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 10 1 -32766 0 0
78: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 11 15
78: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 10 1 -32766 0 0
78: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 12 15
78: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 10 1 -32766 0 0
78: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 13 15
78: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 10 1 -32766 0 0
78: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 14 15
78: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 10 1 -32766 0 0
Newton::solveLinearSystem : Caught exception from the linear solver when getting converged: "BCRSMatrixError [operator++:/home/m.giraud/DUMUXexud/dune-istl/dune/istl/bcrsmatrix.hh:940]: matrix already built up"
78 converged value: 0 121 1 let s go get convergedRemote
78 to  comm_.min(converged) 
78 did  comm_.min(converged) convergedRemote: 0
78 final convergedRemote: 0
78 about to throw Linear solver did not converge
78 Newton::solveLinearSystem : Caught exception: "NumericalProblem [solveLinearSystem:/home/m.giraud/DUMUXexud/dumux/dumux/nonlinear/newtonsolver.hh:381]: Linear solver did not converge when getting convergedRemote "
78 Newton: Caught exception: "NumericalProblem [solveLinearSystem:/home/m.giraud/DUMUXexud/dumux/dumux/nonlinear/newtonsolver.hh:381]: Linear solver did not converge"
78 Newton solver did not converge with dt = 0.0495 seconds. Retrying with time step of 0.02475 seconds
78 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
00880941
9: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000884191
9: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000887211
9: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000890771
9: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000894031
9: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000897031
9: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000900111
9: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000927711
9: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000931191
9: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000935151
9: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000938532
9: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000941682
9: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000944832
9: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000948122
9: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000991902
9: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000996622
9: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.0010005
9: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.00100518
9: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.00100907
9: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.00101284
9: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.00101658
9: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.00102056
9: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.00102433
9: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.00102809
9: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.00103247
9: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.00103648
9: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.00104043
9: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.00107793
9: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.00108243
9: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.00108638
9: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.00109053
9: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.00109487
9: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.00109893
9: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.00110271
9: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.00110943
9: communicator.hh::sendRecv,MPI_Testany 0 1 -32766 0 0 0.00111357
9: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 0 1 -32766 0 0
9: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 7
9: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 0 1 -32766 0 0
9: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 7
9: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 0 1 -32766 0 0
9: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 7
9: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 0 1 -32766 0 0
9: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 7
9: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 0 1 -32766 0 0
9: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 7
9: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 0 1 -32766 0 0
9: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 7
9: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 0 1 -32766 0 0
Newton::solveLinearSystem : Caught exception from the linear solver when getting converged: "BCRSMatrixError [operator++:/home/m.giraud/DUMUXexud/dune-istl/dune/istl/bcrsmatrix.hh:940]: matrix already built up"
9 converged value: 0 121 1 let s go get ched, &status); 18 20
72: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 0 1 -32766 0 0
72: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 19 20
72: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 0 1 -32766 0 0
Newton::solveLinearSystem : Caught exception from the linear solver when getting converged: "BCRSMatrixError [operator++:/home/m.giraud/DUMUXexud/dune-istl/dune/istl/bcrsmatrix.hh:940]: matrix already built up"
72 converged value: 0 121 1 let s go get convergedRemote
72 to  comm_.min(converged) 
72 did  comm_.min(converged) convergedRemote: 0
72 final convergedRemote: 0
72 about to throw Linear solver did not converge
72 Newton::solveLinearSystem : Caught exception: "NumericalProblem [solveLinearSystem:/home/m.giraud/DUMUXexud/dumux/dumux/nonlinear/newtonsolver.hh:381]: Linear solver did not converge when getting convergedRemote "
72 Newton: Caught exception: "NumericalProblem [solveLinearSystem:/home/m.giraud/DUMUXexud/dumux/dumux/nonlinear/newtonsolver.hh:381]: Linear solver did not converge"
72 Newton solver did not converge with dt = 0.0495 seconds. Retrying with time step of 0.02475 seconds
72 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
Newton::solveLinearSystem : Caught exception from the linear solver when getting converged: "BCRSMatrixError [operator++:/home/m.giraud/DUMUXexud/dune-istl/dune/istl/bcrsmatrix.hh:940]: matrix already built up"
65 converged value: 0 121 1 let s go get convergedRemote
65 to  comm_.min(converged) 
65 did  comm_.min(converged) convergedRemote: 0
65 final convergedRemote: 0
65 about to throw Linear solver did not converge
65 Newton::solveLinearSystem : Caught exception: "NumericalProblem [solveLinearSystem:/home/m.giraud/DUMUXexud/dumux/dumux/nonlinear/newtonsolver.hh:381]: Linear solver did not converge when getting convergedRemote "
65 Newton: Caught exception: "NumericalProblem [solveLinearSystem:/home/m.giraud/DUMUXexud/dumux/dumux/nonlinear/newtonsolver.hh:381]: Linear solver did not converge"
65 Newton solver did not converge with dt = 0.0495 seconds. Retrying with time step of 0.02475 seconds
65 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
cv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 18 20
41: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 1 1 -32766 0 0
41: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 19 20
41: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 1 1 -32766 0 0
Newton::solveLinearSystem : Caught exception from the linear solver when getting converged: "BCRSMatrixError [operator++:/home/m.giraud/DUMUXexud/dune-istl/dune/istl/bcrsmatrix.hh:940]: matrix already built up"
41 converged value: 0 121 1 let s go get convergedRemote
41 to  comm_.min(converged) 
41 did  comm_.min(converged) convergedRemote: 0
41 final convergedRemote: 0
41 about to throw Linear solver did not converge
41 Newton::solveLinearSystem : Caught exception: "NumericalProblem [solveLinearSystem:/home/m.giraud/DUMUXexud/dumux/dumux/nonlinear/newtonsolver.hh:381]: Linear solver did not converge when getting convergedRemote "
41 Newton: Caught exception: "NumericalProblem [solveLinearSystem:/home/m.giraud/DUMUXexud/dumux/dumux/nonlinear/newtonsolver.hh:381]: Linear solver did not converge"
41 Newton solver did not converge with dt = 0.0495 seconds. Retrying with time step of 0.02475 seconds
41 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
ed, &status); 18 21
58: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 2 1 -32766 0 0
58: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 19 21
58: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 2 1 -32766 0 0
58: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 20 21
58: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 2 1 -32766 0 0
Newton::solveLinearSystem : Caught exception from the linear solver when getting converged: "BCRSMatrixError [operator++:/home/m.giraud/DUMUXexud/dune-istl/dune/istl/bcrsmatrix.hh:940]: matrix already built up"
58 converged value: 0 121 1 let s go get convergedRemote
58 to  comm_.min(converged) 
58 did  comm_.min(converged) convergedRemote: 0
58 final convergedRemote: 0
58 about to throw Linear solver did not converge
58 Newton::solveLinearSystem : Caught exception: "NumericalProblem [solveLinearSystem:/home/m.giraud/DUMUXexud/dumux/dumux/nonlinear/newtonsolver.hh:381]: Linear solver did not converge when getting convergedRemote "
58 Newton: Caught exception: "NumericalProblem [solveLinearSystem:/home/m.giraud/DUMUXexud/dumux/dumux/nonlinear/newtonsolver.hh:381]: Linear solver did not converge"
58 Newton solver did not converge with dt = 0.0495 seconds. Retrying with time step of 0.02475 seconds
58 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
Newton::solveLinearSystem : Caught exception from the linear solver when getting converged: "BCRSMatrixError [operator++:/home/m.giraud/DUMUXexud/dune-istl/dune/istl/bcrsmatrix.hh:940]: matrix already built up"
106 converged value: 0 121 1 let s go get convergedRemote
106 to  comm_.min(converged) 
106 did  comm_.min(converged) convergedRemote: 0
106 final convergedRemote: 0
106 about to throw Linear solver did not converge
106 Newton::solveLinearSystem : Caught exception: "NumericalProblem [solveLinearSystem:/home/m.giraud/DUMUXexud/dumux/dumux/nonlinear/newtonsolver.hh:381]: Linear solver did not converge when getting convergedRemote "
106 Newton: Caught exception: "NumericalProblem [solveLinearSystem:/home/m.giraud/DUMUXexud/dumux/dumux/nonlinear/newtonsolver.hh:381]: Linear solver did not converge"
106 Newton solver did not converge with dt = 0.0495 seconds. Retrying with time step of 0.02475 seconds
106 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
ommunicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000298514
85: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000301324
85: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000304044
85: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000306624
85: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000334424
85: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000337494
85: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000340234
85: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000343184
85: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000345924
85: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000348644
85: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000351375
85: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000354075
85: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000393245
85: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000396465
85: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000399565
85: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000402525
85: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000405895
85: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000409875
85: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000413125
85: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000416655
85: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000420125
85: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000423375
85: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000426615
85: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000429846
85: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000433076
85: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000436506
85: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000440126
85: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000443516
85: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000447026
85: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000488746
85: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000492086
85: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000495676
85: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000499126
85: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000502566
85: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000505886
85: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000510127
85: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000513457
85: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000516987
85: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000520417
85: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000524627
85: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000528047
85: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000531367
85: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000534667
85: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000538177
85: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000541617
85: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000545067
85: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000548487
85: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000551777
85: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000555077
85: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000558547
85: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000561927
85: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000594858
85: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000598338
85: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000601678
85: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000605498
85: communicator.hh::sendRecv,Newton::solveLinearSystem : Caught exception from the linear solver when getting converged: "BCRSMatrixError [operator++:/home/m.giraud/DUMUXexud/dune-istl/dune/istl/bcrsmatrix.hh:940]: matrix already built up"
34 converged value: 0 121 1 let s go get convergedRemote
34 to  comm_.min(converged) 
34 did  comm_.min(converged) convergedRemote: 0
34 final convergedRemote: 0
34 about to throw Linear solver did not converge
34 Newton::solveLinearSystem : Caught exception: "NumericalProblem [solveLinearSystem:/home/m.giraud/DUMUXexud/dumux/dumux/nonlinear/newtonsolver.hh:381]: Linear solver did not converge when getting convergedRemote "
34 Newton: Caught exception: "NumericalProblem [solveLinearSystem:/home/m.giraud/DUMUXexud/dumux/dumux/nonlinear/newtonsolver.hh:381]: Linear solver did not converge"
34 Newton solver did not converge with dt = 0.0495 seconds. Retrying with time step of 0.02475 seconds
34 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
cvRequests, &finished, &status); 18 26
82: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 14 1 -32766 0 0
82: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 19 26
82: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 14 1 -32766 0 0
82: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 20 26
82: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 14 1 -32766 0 0
82: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 21 26
82: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 14 1 -32766 0 0
82: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 22 26
82: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 14 1 -32766 0 0
82: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 23 26
82: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 14 1 -32766 0 0
82: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 24 26
82: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 14 1 -32766 0 0
82: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 25 26
82: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 14 1 -32766 0 0
Newton::solveLinearSystem : Caught exception from the linear solver when getting converged: "BCRSMatrixError [operator++:/home/m.giraud/DUMUXexud/dune-istl/dune/istl/bcrsmatrix.hh:940]: matrix already built up"
82 converged value: 0 121 1 let s go get convergedRemote
82 to  comm_.min(converged) 
82 did  comm_.min(converged) convergedRemote: 0
82 final convergedRemote: 0
82 about to throw Linear solver did not converge
82 Newton::solveLinearSystem : Caught exception: "NumericalProblem [solveLinearSystem:/home/m.giraud/DUMUXexud/dumux/dumux/nonlinear/newtonsolver.hh:381]: Linear solver did not converge when getting convergedRemote "
82 Newton: Caught exception: "NumericalProblem [solveLinearSystem:/home/m.giraud/DUMUXexud/dumux/dumux/nonlinear/newtonsolver.hh:381]: Linear solver did not converge"
82 Newton solver did not converge with dt = 0.0495 seconds. Retrying with time step of 0.02475 seconds
82 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
115 did  comm_.min(converged) convergedRemote: 0
115 final convergedRemote: 0
115 about to throw Linear solver did not converge
115 Newton::solveLinearSystem : Caught exception: "NumericalProblem [solveLinearSystem:/home/m.giraud/DUMUXexud/dumux/dumux/nonlinear/newtonsolver.hh:381]: Linear solver did not converge when getting convergedRemote "
115 Newton: Caught exception: "NumericalProblem [solveLinearSystem:/home/m.giraud/DUMUXexud/dumux/dumux/nonlinear/newtonsolver.hh:381]: Linear solver did not converge"
115 Newton solver did not converge with dt = 0.0495 seconds. Retrying with time step of 0.02475 seconds
115 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
v,MPI_Testany -32766 0 -32766 0 0 0.000299563
29: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000303354
29: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000306844
29: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000312064
29: communicator.hh::sendRecv,MPI_Testany 2 1 -32766 0 0 0.000315444
29: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 2 1 -32766 0 0
29: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 11
29: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 2 1 -32766 0 0
29: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 11
29: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 2 1 -32766 0 0
29: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 11
29: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 2 1 -32766 0 0
29: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 11
29: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 2 1 -32766 0 0
29: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 11
29: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 2 1 -32766 0 0
29: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 11
29: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 2 1 -32766 0 0
29: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 11
29: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 2 1 -32766 0 0
29: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 11
29: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 2 1 -32766 0 0
29: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 11
29: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 2 1 -32766 0 0
29: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 10 11
29: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 2 1 -32766 0 0
Newton::solveLinearSystem : Caught exception from the linear solver when getting converged: "BCRSMatrixError [operator++:/home/m.giraud/DUMUXexud/dune-istl/dune/istl/bcrsmatrix.hh:940]: matrix already built up"
29 converged value: 0 121 1 let s go get convergedRemote
29 to  comm_.min(converged) 
29 did  comm_.min(converged) convergedRemote: 0
29 final convergedRemote: 0
29 about to throw Linear solver did not converge
29 Newton::solveLinearSystem : Caught exception: "NumericalProblem [solveLinearSystem:/home/m.giraud/DUMUXexud/dumux/dumux/nonlinear/newtonsolver.hh:381]: Linear solver did not converge when getting convergedRemote "
29 Newton: Caught exception: "NumericalProblem [solveLinearSystem:/home/m.giraud/DUMUXexud/dumux/dumux/nonlinear/newtonsolver.hh:381]: Linear solver did not converge"
29 Newton solver did not converge with dt = 0.0495 seconds. Retrying with time step of 0.02475 seconds
29 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
d, &status); 18 22
68: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 0 1 -32766 0 0
68: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 19 22
68: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 0 1 -32766 0 0
68: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 20 22
68: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 0 1 -32766 0 0
68: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 21 22
68: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 0 1 -32766 0 0
Newton::solveLinearSystem : Caught exception from the linear solver when getting converged: "BCRSMatrixError [operator++:/home/m.giraud/DUMUXexud/dune-istl/dune/istl/bcrsmatrix.hh:940]: matrix already built up"
68 converged value: 0 121 1 let s go get convergedRemote
68 to  comm_.min(converged) 
68 did  comm_.min(converged) convergedRemote: 0
68 final convergedRemote: 0
68 about to throw Linear solver did not converge
68 Newton::solveLinearSystem : Caught exception: "NumericalProblem [solveLinearSystem:/home/m.giraud/DUMUXexud/dumux/dumux/nonlinear/newtonsolver.hh:381]: Linear solver did not converge when getting convergedRemote "
68 Newton: Caught exception: "NumericalProblem [solveLinearSystem:/home/m.giraud/DUMUXexud/dumux/dumux/nonlinear/newtonsolver.hh:381]: Linear solver did not converge"
68 Newton solver did not converge with dt = 0.0495 seconds. Retrying with time step of 0.02475 seconds
68 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
or.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000303054
95: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000305824
95: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000308574
95: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000311164
95: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000313764
95: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000316354
95: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000318944
95: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000322054
95: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000324904
95: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000353514
95: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000356234
95: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000358814
95: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000361404
95: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000364305
95: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000366895
95: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000369505
95: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000372505
95: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000416335
95: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000419865
95: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000423705
95: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000427105
95: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000430485
95: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000433845
95: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000437105
95: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000440355
95: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000443866
95: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000447216
95: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000450466
95: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000453686
95: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000456936
95: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000460186
95: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000504766
95: communicator.hh::sendRecv,MPI_Testany 2 1 -32766 0 0 0.000508586
95: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 2 1 -32766 0 0
95: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 7
95: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 2 1 -32766 0 0
95: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 7
95: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 2 1 -32766 0 0
95: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 7
95: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 2 1 -32766 0 0
95: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 7
95: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 2 1 -32766 0 0
95: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 7
95: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 2 1 -32766 0 0
95: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 7
95: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 2 1 -32766 0 0
Newton::solveLinearSystem : Caught exception from the linear solver when getting converged: "BCRSMatrixError [operator++:/home/m.giraud/DUMUXexud/dune-istl/dune/istl/bcrsmatrix.hh:940]: matrix already built up"
95 converged value: 0 121 1 let s go get convergedRemote
95 to  comm_.min(converged) 
95 did  comm_.min(converged) convergedRemote: 0
95 final Newton::solveLinearSystem : Caught exception from the linear solver when getting converged: "BCRSMatrixError [operator++:/home/m.giraud/DUMUXexud/dune-istl/dune/istl/bcrsmatrix.hh:940]: matrix already built up"
10 converged value: 0 121 1 let s go get convergedRemote
10 to  comm_.min(converged) 
10 did  comm_.min(converged) convergedRemote: 0
10 final convergedRemote: 0
10 about to throw Linear solver did not converge
10 Newton::solveLinearSystem : Caught exception: "NumericalProblem [solveLinearSystem:/home/m.giraud/DUMUXexud/dumux/dumux/nonlinear/newtonsolver.hh:381]: Linear solver did not converge when getting convergedRemote "
10 Newton: Caught exception: "NumericalProblem [solveLinearSystem:/home/m.giraud/DUMUXexud/dumux/dumux/nonlinear/newtonsolver.hh:381]: Linear solver did not converge"
10 Newton solver did not converge with dt = 0.0495 seconds. Retrying with time step of 0.02475 seconds
10 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
hed, &status); 18 21
88: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 2 1 -32766 0 0
88: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 19 21
88: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 2 1 -32766 0 0
88: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 20 21
88: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 2 1 -32766 0 0
Newton::solveLinearSystem : Caught exception from the linear solver when getting converged: "BCRSMatrixError [operator++:/home/m.giraud/DUMUXexud/dune-istl/dune/istl/bcrsmatrix.hh:940]: matrix already built up"
88 converged value: 0 121 1 let s go get convergedRemote
88 to  comm_.min(converged) 
88 did  comm_.min(converged) convergedRemote: 0
88 final convergedRemote: 0
88 about to throw Linear solver did not converge
88 Newton::solveLinearSystem : Caught exception: "NumericalProblem [solveLinearSystem:/home/m.giraud/DUMUXexud/dumux/dumux/nonlinear/newtonsolver.hh:381]: Linear solver did not converge when getting convergedRemote "
88 Newton: Caught exception: "NumericalProblem [solveLinearSystem:/home/m.giraud/DUMUXexud/dumux/dumux/nonlinear/newtonsolver.hh:381]: Linear solver did not converge"
88 Newton solver did not converge with dt = 0.0495 seconds. Retrying with time step of 0.02475 seconds
88 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
v,MPI_Testany -32766 0 -32766 0 0 0.00155451
1: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.00155728
1: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.00156002
1: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.00156278
1: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.00158309
1: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.00158625
1: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.00158887
1: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.001592
1: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.00159476
1: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.00159749
1: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.00160024
1: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.00160303
1: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.00160567
1: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.00160828
1: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.00163098
1: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.00163415
1: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.00163676
1: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.00163938
1: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.00164198
1: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.00164461
1: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.00164722
1: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.00165017
1: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.00165292
1: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.00167663
1: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.00167977
1: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.00168238
1: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.001685
1: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.00168766
1: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.00169048
1: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.00169309
1: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.0016957
1: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.00169833
1: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.00170121
1: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.00173109
1: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.00173408
1: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.00173769
1: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.00174047
1: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.0017437
1: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.00174634
1: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.00174894
1: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.00175166
1: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.00179488
1: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.00179842
1: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.00180184
1: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.00180508
1: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.00180835
1: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.0018116
1: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.00181485
1: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.0018181
1: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.00182167
1: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.00182502
1: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.0018283
1: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.00183157
1: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.00183481
1: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.00183808
1: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.00184132
1: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.00184475
1: communicator.hh::sendRecnicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000310904
25: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000337814
25: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000341644
25: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000344494
25: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000347334
25: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000349934
25: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000352524
25: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000355404
25: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000357984
25: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000360824
25: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000393535
25: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000396385
25: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000399225
25: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000402165
25: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000404785
25: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000407665
25: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000410515
25: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000413085
25: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000415685
25: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000419095
25: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000422115
25: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000425415
25: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000428705
25: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000475796
25: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000479246
25: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000482756
25: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000485986
25: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000489446
25: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000492866
25: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000496316
25: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000499746
25: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000503306
25: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000507016
25: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000510406
25: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000513756
25: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000517116
25: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000520346
25: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000523656
25: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000526886
25: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000530116
25: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000533357
25: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000536567
25: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000539817
25: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000543087
25: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000546307
25: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000549567
25: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000552787
25: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000587547
25: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000590957
25: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000594447
25: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000599217
25: communicator.hh::sendRecv,MPI_Testany 6 1 -32766 0 0 0.000602737
25: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 6 1 -32766 0 0
25: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 7
25: communicator.hh::sendRecv, before assert(finished !=14 did  comm_.min(converged) convergedRemote: 0
14 final convergedRemote: 0
14 about to throw Linear solver did not converge
14 Newton::solveLinearSystem : Caught exception: "NumericalProblem [solveLinearSystem:/home/m.giraud/DUMUXexud/dumux/dumux/nonlinear/newtonsolver.hh:381]: Linear solver did not converge when getting convergedRemote "
14 Newton: Caught exception: "NumericalProblem [solveLinearSystem:/home/m.giraud/DUMUXexud/dumux/dumux/nonlinear/newtonsolver.hh:381]: Linear solver did not converge"
14 Newton solver did not converge with dt = 0.0495 seconds. Retrying with time step of 0.02475 seconds
14 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
 converged: "BCRSMatrixError [operator++:/home/m.giraud/DUMUXexud/dune-istl/dune/istl/bcrsmatrix.hh:940]: matrix already built up"
98 converged value: 0 121 1 let s go get convergedRemote
98 to  comm_.min(converged) 
98 did  comm_.min(converged) convergedRemote: 0
98 final convergedRemote: 0
98 about to throw Linear solver did not converge
98 Newton::solveLinearSystem : Caught exception: "NumericalProblem [solveLinearSystem:/home/m.giraud/DUMUXexud/dumux/dumux/nonlinear/newtonsolver.hh:381]: Linear solver did not converge when getting convergedRemote "
98 Newton: Caught exception: "NumericalProblem [solveLinearSystem:/home/m.giraud/DUMUXexud/dumux/dumux/nonlinear/newtonsolver.hh:381]: Linear solver did not converge"
98 Newton solver did not converge with dt = 0.0495 seconds. Retrying with time step of 0.02475 seconds
98 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
h::sendRecv, before assert(finished != MPI_UNDEFINED); 5 1 -32766 0 0
Newton::solveLinearSystem : Caught exception from the linear solver when getting converged: "BCRSMatrixError [operator++:/home/m.giraud/DUMUXexud/dune-istl/dune/istl/bcrsmatrix.hh:940]: matrix already built up"
49 converged value: 0 121 1 let s go get convergedRemote
49 to  comm_.min(converged) 
49 did  comm_.min(converged) convergedRemote: 0
49 final convergedRemote: 0
49 about to throw Linear solver did not converge
49 Newton::solveLinearSystem : Caught exception: "NumericalProblem [solveLinearSystem:/home/m.giraud/DUMUXexud/dumux/dumux/nonlinear/newtonsolver.hh:381]: Linear solver did not converge when getting convergedRemote "
49 Newton: Caught exception: "NumericalProblem [solveLinearSystem:/home/m.giraud/DUMUXexud/dumux/dumux/nonlinear/newtonsolver.hh:381]: Linear solver did not converge"
49 Newton solver did not converge with dt = 0.0495 seconds. Retrying with time step of 0.02475 seconds
49 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
103 did  comm_.min(converged) convergedRemote: 0
103 final convergedRemote: 0
103 about to throw Linear solver did not converge
103 Newton::solveLinearSystem : Caught exception: "NumericalProblem [solveLinearSystem:/home/m.giraud/DUMUXexud/dumux/dumux/nonlinear/newtonsolver.hh:381]: Linear solver did not converge when getting convergedRemote "
103 Newton: Caught exception: "NumericalProblem [solveLinearSystem:/home/m.giraud/DUMUXexud/dumux/dumux/nonlinear/newtonsolver.hh:381]: Linear solver did not converge"
103 Newton solver did not converge with dt = 0.0495 seconds. Retrying with time step of 0.02475 seconds
103 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
104 did  comm_.min(converged) convergedRemote: 0
104 final convergedRemote: 0
104 about to throw Linear solver did not converge
104 Newton::solveLinearSystem : Caught exception: "NumericalProblem [solveLinearSystem:/home/m.giraud/DUMUXexud/dumux/dumux/nonlinear/newtonsolver.hh:381]: Linear solver did not converge when getting convergedRemote "
104 Newton: Caught exception: "NumericalProblem [solveLinearSystem:/home/m.giraud/DUMUXexud/dumux/dumux/nonlinear/newtonsolver.hh:381]: Linear solver did not converge"
104 Newton solver did not converge with dt = 0.0495 seconds. Retrying with time step of 0.02475 seconds
104 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
hed, &status); 18 21
94: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 6 1 -32766 0 0
94: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 19 21
94: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 6 1 -32766 0 0
94: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 20 21
94: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 6 1 -32766 0 0
Newton::solveLinearSystem : Caught exception from the linear solver when getting converged: "BCRSMatrixError [operator++:/home/m.giraud/DUMUXexud/dune-istl/dune/istl/bcrsmatrix.hh:940]: matrix already built up"
94 converged value: 0 121 1 let s go get convergedRemote
94 to  comm_.min(converged) 
94 did  comm_.min(converged) convergedRemote: 0
94 final convergedRemote: 0
94 about to throw Linear solver did not converge
94 Newton::solveLinearSystem : Caught exception: "NumericalProblem [solveLinearSystem:/home/m.giraud/DUMUXexud/dumux/dumux/nonlinear/newtonsolver.hh:381]: Linear solver did not converge when getting convergedRemote "
94 Newton: Caught exception: "NumericalProblem [solveLinearSystem:/home/m.giraud/DUMUXexud/dumux/dumux/nonlinear/newtonsolver.hh:381]: Linear solver did not converge"
94 Newton solver did not converge with dt = 0.0495 seconds. Retrying with time step of 0.02475 seconds
94 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
-32766 0 0 0.000581138
0: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000583698
0: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000586278
0: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000589128
0: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000591858
0: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000594438
0: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000596998
0: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000599558
0: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000602118
0: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000604678
0: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000607688
0: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000610438
0: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000613258
0: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000616048
0: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000618838
0: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000621598
0: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000624368
0: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000627388
0: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000630158
0: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000632918
0: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000635718
0: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000638288
0: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000640858
0: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000643418
0: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000646389
0: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.0007735
0: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000857671
0: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000933052
0: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.00101601
0: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.00114473
0: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.00115194
0: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.00115522
0: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.00116365
0: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.00127488
0: communicator.hh::sendRecv,MPI_Testany 0 1 -32766 0 0 0.00135301
0: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 0 1 -32766 0 0
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 3
0: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 0 1 -32766 0 0
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 3
0: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 0 1 -32766 0 0
Newton::solveLinearSystem : Caught exception from the linear solver when getting converged: "BCRSMatrixError [operator++:/home/m.giraud/DUMUXexud/dune-istl/dune/istl/bcrsmatrix.hh:940]: matrix already built up"
0 converged value: 0 121 1 let s go get convergedRemote
0 to  comm_.min(converged) 
0 did  comm_.min(converged) convergedRemote: 0
0 final convergedRemote: 0
0 about to throw Linear solver did not converge
0 Newton::solveLinearSystem : Caught exception: "NumericalProblem [solveLinearSystem:/home/m.giraud/DUMUXexud/dumux/dumux/nonlinear/newtonsolver.hh:381]: Linear solver did not converge when getting convergedRemote "
0 Newton: Caught exception: "NumericalProblem [solveLinearSystem:/home/m.giraud/DUMUXexud/dumux/dumux/nonlinear/newtonsolver.hh:381]: Linear solver did not converge"
0 Newton solver did not converge with dt = 0.0495 seconds. Retrying with time step of 0.02475 seconds
0 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
24 did  comm_.min(converged) convergedRemote: 0
24 final convergedRemote: 0
24 about to throw Linear solver did not converge
24 Newton::solveLinearSystem : Caught exception: "NumericalProblem [solveLinearSystem:/home/m.giraud/DUMUXexud/dumux/dumux/nonlinear/newtonsolver.hh:381]: Linear solver did not converge when getting convergedRemote "
24 Newton: Caught exception: "NumericalProblem [solveLinearSystem:/home/m.giraud/DUMUXexud/dumux/dumux/nonlinear/newtonsolver.hh:381]: Linear solver did not converge"
24 Newton solver did not converge with dt = 0.0495 seconds. Retrying with time step of 0.02475 seconds
24 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
d, &status); 18 20
42: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 0 1 -32766 0 0
42: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 19 20
42: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 0 1 -32766 0 0
Newton::solveLinearSystem : Caught exception from the linear solver when getting converged: "BCRSMatrixError [operator++:/home/m.giraud/DUMUXexud/dune-istl/dune/istl/bcrsmatrix.hh:940]: matrix already built up"
42 converged value: 0 121 1 let s go get convergedRemote
42 to  comm_.min(converged) 
42 did  comm_.min(converged) convergedRemote: 0
42 final convergedRemote: 0
42 about to throw Linear solver did not converge
42 Newton::solveLinearSystem : Caught exception: "NumericalProblem [solveLinearSystem:/home/m.giraud/DUMUXexud/dumux/dumux/nonlinear/newtonsolver.hh:381]: Linear solver did not converge when getting convergedRemote "
42 Newton: Caught exception: "NumericalProblem [solveLinearSystem:/home/m.giraud/DUMUXexud/dumux/dumux/nonlinear/newtonsolver.hh:381]: Linear solver did not converge"
42 Newton solver did not converge with dt = 0.0495 seconds. Retrying with time step of 0.02475 seconds
42 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
59 did  comm_.min(converged) convergedRemote: 0
59 final convergedRemote: 0
59 about to throw Linear solver did not converge
59 Newton::solveLinearSystem : Caught exception: "NumericalProblem [solveLinearSystem:/home/m.giraud/DUMUXexud/dumux/dumux/nonlinear/newtonsolver.hh:381]: Linear solver did not converge when getting convergedRemote "
59 Newton: Caught exception: "NumericalProblem [solveLinearSystem:/home/m.giraud/DUMUXexud/dumux/dumux/nonlinear/newtonsolver.hh:381]: Linear solver did not converge"
59 Newton solver did not converge with dt = 0.0495 seconds. Retrying with time step of 0.02475 seconds
59 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
80 did  comm_.min(converged) convergedRemote: 0
80 final convergedRemote: 0
80 about to throw Linear solver did not converge
80 Newton::solveLinearSystem : Caught exception: "NumericalProblem [solveLinearSystem:/home/m.giraud/DUMUXexud/dumux/dumux/nonlinear/newtonsolver.hh:381]: Linear solver did not converge when getting convergedRemote "
80 Newton: Caught exception: "NumericalProblem [solveLinearSystem:/home/m.giraud/DUMUXexud/dumux/dumux/nonlinear/newtonsolver.hh:381]: Linear solver did not converge"
80 Newton solver did not converge with dt = 0.0495 seconds. Retrying with time step of 0.02475 seconds
80 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
r.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000279064
87: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000324284
87: communicator.hh::sendRecv,MPI_Testany 4 1 -32766 0 0 0.000328254
87: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 4 1 -32766 0 0
87: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 19
87: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 4 1 -32766 0 0
87: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 19
87: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 4 1 -32766 0 0
87: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 19
87: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 4 1 -32766 0 0
87: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 19
87: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 4 1 -32766 0 0
87: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 19
87: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 4 1 -32766 0 0
87: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 19
87: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 4 1 -32766 0 0
87: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 19
87: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 4 1 -32766 0 0
87: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 19
87: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 4 1 -32766 0 0
87: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 19
87: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 4 1 -32766 0 0
87: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 10 19
87: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 4 1 -32766 0 0
87: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 11 19
87: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 4 1 -32766 0 0
87: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 12 19
87: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 4 1 -32766 0 0
87: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 13 19
87: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 4 1 -32766 0 0
87: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 14 19
87: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 4 1 -32766 0 0
87: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 15 19
87: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 4 1 -32766 0 0
87: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 16 19
87: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 4 1 -32766 0 0
87: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 17 19
87: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 4 1 -32766 0 0
87: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 18 19
87: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 4 1 -32766 0 0
Newton::solveLinearSystem : Caught exception from the linear solver when getting converg6 0 0
Newton::solveLinearSystem : Caught exception from the linear solver when getting converged: "BCRSMatrixError [operator++:/home/m.giraud/DUMUXexud/dune-istl/dune/istl/bcrsmatrix.hh:940]: matrix already built up"
91 converged value: 0 121 1 let s go get convergedRemote
91 to  comm_.min(converged) 
91 did  comm_.min(converged) convergedRemote: 0
91 final convergedRemote: 0
91 about to throw Linear solver did not converge
91 Newton::solveLinearSystem : Caught exception: "NumericalProblem [solveLinearSystem:/home/m.giraud/DUMUXexud/dumux/dumux/nonlinear/newtonsolver.hh:381]: Linear solver did not converge when getting convergedRemote "
91 Newton: Caught exception: "NumericalProblem [solveLinearSystem:/home/m.giraud/DUMUXexud/dumux/dumux/nonlinear/newtonsolver.hh:381]: Linear solver did not converge"
91 Newton solver did not converge with dt = 0.0495 seconds. Retrying with time step of 0.02475 seconds
91 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
nicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000339424
77: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000342204
77: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000344954
77: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000347564
77: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000350174
77: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000352794
77: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000355414
77: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000358294
77: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000387795
77: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000390705
77: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000393325
77: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000395945
77: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000398545
77: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000401455
77: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000404205
77: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000406965
77: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000409755
77: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000412475
77: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000415525
77: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000418625
77: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000467146
77: communicator.hh::sendRecv,MPI_Testany 3 1 -32766 0 0 0.000470826
77: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 3 1 -32766 0 0
77: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 6
77: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 3 1 -32766 0 0
77: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 6
77: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 3 1 -32766 0 0
77: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 6
77: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 3 1 -32766 0 0
77: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 6
77: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 3 1 -32766 0 0
77: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 6
77: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 3 1 -32766 0 0
Newton::solveLinearSystem : Caught exception from the linear solver when getting converged: "BCRSMatrixError [operator++:/home/m.giraud/DUMUXexud/dune-istl/dune/istl/bcrsmatrix.hh:940]: matrix already built up"
77 converged value: 0 121 1 let s go get convergedRemote
77 to  comm_.min(converged) 
77 did  comm_.min(converged) convergedRemote: 0
77 final convergedRemote: 0
77 about to throw Linear solver did not converge
77 Newton::solveLinearSystem : Caught exception: "NumericalProblem [solveLinearSystem:/home/m.giraud/DUMUXexud/dumux/dumux/nonlinear/newtonsolver.hh:381]: Linear solver did not converge when getting convergedRemote "
77 Newton: Caught exception: "NumericalProblem [solveLinearSystem:/home/m.giraud/DUMUXexud/dumux/dumux/nonlinear/newtonsolver.hh:381]: Linear solver did not converge"
77 Newton solver did not converge with dt = 0.0495 seconds. Retrying with time step of 0.02475 seconds
77 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
INED); 8 1 -32766 0 0
97: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 9
97: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 8 1 -32766 0 0
97: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 9
97: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 8 1 -32766 0 0
97: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 9
97: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 8 1 -32766 0 0
97: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 9
97: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 8 1 -32766 0 0
97: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 9
97: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 8 1 -32766 0 0
97: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 9
97: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 8 1 -32766 0 0
Newton::solveLinearSystem : Caught exception from the linear solver when getting converged: "BCRSMatrixError [operator++:/home/m.giraud/DUMUXexud/dune-istl/dune/istl/bcrsmatrix.hh:940]: matrix already built up"
97 converged value: 0 121 1 let s go get convergedRemote
97 to  comm_.min(converged) 
97 did  comm_.min(converged) convergedRemote: 0
97 final convergedRemote: 0
97 about to throw Linear solver did not converge
97 Newton::solveLinearSystem : Caught exception: "NumericalProblem [solveLinearSystem:/home/m.giraud/DUMUXexud/dumux/dumux/nonlinear/newtonsolver.hh:381]: Linear solver did not converge when getting convergedRemote "
97 Newton: Caught exception: "NumericalProblem [solveLinearSystem:/home/m.giraud/DUMUXexud/dumux/dumux/nonlinear/newtonsolver.hh:381]: Linear solver did not converge"
97 Newton solver did not converge with dt = 0.0495 seconds. Retrying with time step of 0.02475 seconds
97 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
cator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000276024
47: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000317594
47: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000321454
47: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000324544
47: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000327484
47: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000330314
47: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000333254
47: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000335964
47: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000339244
47: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000342274
47: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000344944
47: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000347834
47: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000351085
47: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000354345
47: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000359265
47: communicator.hh::sendRecv,MPI_Testany 3 1 -32766 0 0 0.000362775
47: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 3 1 -32766 0 0
47: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 6
47: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 3 1 -32766 0 0
47: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 6
47: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 3 1 -32766 0 0
47: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 6
47: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 3 1 -32766 0 0
47: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 6
47: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 3 1 -32766 0 0
47: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 6
47: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 3 1 -32766 0 0
Newton::solveLinearSystem : Caught exception from the linear solver when getting converged: "BCRSMatrixError [operator++:/home/m.giraud/DUMUXexud/dune-istl/dune/istl/bcrsmatrix.hh:940]: matrix already built up"
47 converged value: 0 121 1 let s go get convergedRemote
47 to  comm_.min(converged) 
47 did  comm_.min(converged) convergedRemote: 0
47 final convergedRemote: 0
47 about to throw Linear solver did not converge
47 Newton::solveLinearSystem : Caught exception: "NumericalProblem [solveLinearSystem:/home/m.giraud/DUMUXexud/dumux/dumux/nonlinear/newtonsolver.hh:381]: Linear solver did not converge when getting convergedRemote "
47 Newton: Caught exception: "NumericalProblem [solveLinearSystem:/home/m.giraud/DUMUXexud/dumux/dumux/nonlinear/newtonsolver.hh:381]: Linear solver did not converge"
47 Newton solver did not converge with dt = 0.0495 seconds. Retrying with time step of 0.02475 seconds
47 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
74 did  comm_.min(converged) convergedRemote: 0
74 final convergedRemote: 0
74 about to throw Linear solver did not converge
74 Newton::solveLinearSystem : Caught exception: "NumericalProblem [solveLinearSystem:/home/m.giraud/DUMUXexud/dumux/dumux/nonlinear/newtonsolver.hh:381]: Linear solver did not converge when getting convergedRemote "
74 Newton: Caught exception: "NumericalProblem [solveLinearSystem:/home/m.giraud/DUMUXexud/dumux/dumux/nonlinear/newtonsolver.hh:381]: Linear solver did not converge"
74 Newton solver did not converge with dt = 0.0495 seconds. Retrying with time step of 0.02475 seconds
74 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
ormation_.size(), recvRequests, &finished, &status); 4 12
21: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 9 1 -32766 0 0
21: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 12
21: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 9 1 -32766 0 0
21: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 12
21: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 9 1 -32766 0 0
21: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 12
21: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 9 1 -32766 0 0
21: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 12
21: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 9 1 -32766 0 0
21: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 12
21: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 9 1 -32766 0 0
21: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 10 12
21: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 9 1 -32766 0 0
21: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 11 12
21: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 9 1 -32766 0 0
Newton::solveLinearSystem : Caught exception from the linear solver when getting converged: "BCRSMatrixError [operator++:/home/m.giraud/DUMUXexud/dune-istl/dune/istl/bcrsmatrix.hh:940]: matrix already built up"
21 converged value: 0 121 1 let s go get convergedRemote
21 to  comm_.min(converged) 
21 did  comm_.min(converged) convergedRemote: 0
21 final convergedRemote: 0
21 about to throw Linear solver did not converge
21 Newton::solveLinearSystem : Caught exception: "NumericalProblem [solveLinearSystem:/home/m.giraud/DUMUXexud/dumux/dumux/nonlinear/newtonsolver.hh:381]: Linear solver did not converge when getting convergedRemote "
21 Newton: Caught exception: "NumericalProblem [solveLinearSystem:/home/m.giraud/DUMUXexud/dumux/dumux/nonlinear/newtonsolver.hh:381]: Linear solver did not converge"
21 Newton solver did not converge with dt = 0.0495 seconds. Retrying with time step of 0.02475 seconds
21 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
m_.min(converged) convergedRemote: 0
19 final convergedRemote: 0
19 about to throw Linear solver did not converge
19 Newton::solveLinearSystem : Caught exception: "NumericalProblem [solveLinearSystem:/home/m.giraud/DUMUXexud/dumux/dumux/nonlinear/newtonsolver.hh:381]: Linear solver did not converge when getting convergedRemote "
19 Newton: Caught exception: "NumericalProblem [solveLinearSystem:/home/m.giraud/DUMUXexud/dumux/dumux/nonlinear/newtonsolver.hh:381]: Linear solver did not converge"
19 Newton solver did not converge with dt = 0.0495 seconds. Retrying with time step of 0.02475 seconds
19 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
d, &status); 18 24
38: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 9 1 -32766 0 0
38: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 19 24
38: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 9 1 -32766 0 0
38: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 20 24
38: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 9 1 -32766 0 0
38: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 21 24
38: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 9 1 -32766 0 0
38: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 22 24
38: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 9 1 -32766 0 0
38: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 23 24
38: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 9 1 -32766 0 0
Newton::solveLinearSystem : Caught exception from the linear solver when getting converged: "BCRSMatrixError [operator++:/home/m.giraud/DUMUXexud/dune-istl/dune/istl/bcrsmatrix.hh:940]: matrix already built up"
38 converged value: 0 121 1 let s go get convergedRemote
38 to  comm_.min(converged) 
38 did  comm_.min(converged) convergedRemote: 0
38 final convergedRemote: 0
38 about to throw Linear solver did not converge
38 Newton::solveLinearSystem : Caught exception: "NumericalProblem [solveLinearSystem:/home/m.giraud/DUMUXexud/dumux/dumux/nonlinear/newtonsolver.hh:381]: Linear solver did not converge when getting convergedRemote "
38 Newton: Caught exception: "NumericalProblem [solveLinearSystem:/home/m.giraud/DUMUXexud/dumux/dumux/nonlinear/newtonsolver.hh:381]: Linear solver did not converge"
38 Newton solver did not converge with dt = 0.0495 seconds. Retrying with time step of 0.02475 seconds
38 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
recvRequests, &finished, &status); 16 17
83: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 9 1 -32766 0 0
Newton::solveLinearSystem : Caught exception from the linear solver when getting converged: "BCRSMatrixError [operator++:/home/m.giraud/DUMUXexud/dune-istl/dune/istl/bcrsmatrix.hh:940]: matrix already built up"
83 converged value: 0 121 1 let s go get convergedRemote
83 to  comm_.min(converged) 
83 did  comm_.min(converged) convergedRemote: 0
83 final convergedRemote: 0
83 about to throw Linear solver did not converge
83 Newton::solveLinearSystem : Caught exception: "NumericalProblem [solveLinearSystem:/home/m.giraud/DUMUXexud/dumux/dumux/nonlinear/newtonsolver.hh:381]: Linear solver did not converge when getting convergedRemote "
83 Newton: Caught exception: "NumericalProblem [solveLinearSystem:/home/m.giraud/DUMUXexud/dumux/dumux/nonlinear/newtonsolver.hh:381]: Linear solver did not converge"
83 Newton solver did not converge with dt = 0.0495 seconds. Retrying with time step of 0.02475 seconds
83 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
 of 0.02475 seconds
118 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
Recv,MPI_Testany -32766 0 -32766 0 0 0.000305174
33: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000307954
33: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000310974
33: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000313704
33: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000316454
33: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000319244
33: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000322224
33: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000325024
33: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000328204
33: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000330914
33: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000334854
33: communicator.hh::sendRecv,MPI_Testany 1 1 -32766 0 0 0.000337674
33: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 1 1 -32766 0 0
33: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 9
33: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 1 1 -32766 0 0
33: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 9
33: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 1 1 -32766 0 0
33: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 9
33: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 1 1 -32766 0 0
33: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 9
33: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 1 1 -32766 0 0
33: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 9
33: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 1 1 -32766 0 0
33: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 9
33: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 1 1 -32766 0 0
33: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 9
33: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 1 1 -32766 0 0
33: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 9
33: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 1 1 -32766 0 0
Newton::solveLinearSystem : Caught exception from the linear solver when getting converged: "BCRSMatrixError [operator++:/home/m.giraud/DUMUXexud/dune-istl/dune/istl/bcrsmatrix.hh:940]: matrix already built up"
33 converged value: 0 121 1 let s go get convergedRemote
33 to  comm_.min(converged) 
33 did  comm_.min(converged) convergedRemote: 0
33 final convergedRemote: 0
33 about to throw Linear solver did not converge
33 Newton::solveLinearSystem : Caught exception: "NumericalProblem [solveLinearSystem:/home/m.giraud/DUMUXexud/dumux/dumux/nonlinear/newtonsolver.hh:381]: Linear solver did not converge when getting convergedRemote "
33 Newton: Caught exception: "NumericalProblem [solveLinearSystem:/home/m.giraud/DUMUXexud/dumux/dumux/nonlinear/newtonsolver.hh:381]: Linear solver did not converge"
33 Newton solver did not converge with dt = 0.0495 seconds. Retrying with time step of 0.02475 seconds
33 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
fore MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 15
48: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 9 1 -32766 0 0
48: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 15
48: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 9 1 -32766 0 0
48: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 10 15
48: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 9 1 -32766 0 0
48: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 11 15
48: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 9 1 -32766 0 0
48: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 12 15
48: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 9 1 -32766 0 0
48: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 13 15
48: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 9 1 -32766 0 0
48: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 14 15
48: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 9 1 -32766 0 0
Newton::solveLinearSystem : Caught exception from the linear solver when getting converged: "BCRSMatrixError [operator++:/home/m.giraud/DUMUXexud/dune-istl/dune/istl/bcrsmatrix.hh:940]: matrix already built up"
48 converged value: 0 121 1 let s go get convergedRemote
48 to  comm_.min(converged) 
48 did  comm_.min(converged) convergedRemote: 0
48 final convergedRemote: 0
48 about to throw Linear solver did not converge
48 Newton::solveLinearSystem : Caught exception: "NumericalProblem [solveLinearSystem:/home/m.giraud/DUMUXexud/dumux/dumux/nonlinear/newtonsolver.hh:381]: Linear solver did not converge when getting convergedRemote "
48 Newton: Caught exception: "NumericalProblem [solveLinearSystem:/home/m.giraud/DUMUXexud/dumux/dumux/nonlinear/newtonsolver.hh:381]: Linear solver did not converge"
48 Newton solver did not converge with dt = 0.0495 seconds. Retrying with time step of 0.02475 seconds
48 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
nds. Retrying with time step of 0.02475 seconds
5 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
= dS/dt + div F - q;   M = grad r
d, &status); 18 26
52: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 4 1 -32766 0 0
52: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 19 26
52: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 4 1 -32766 0 0
52: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 20 26
52: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 4 1 -32766 0 0
52: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 21 26
52: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 4 1 -32766 0 0
52: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 22 26
52: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 4 1 -32766 0 0
52: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 23 26
52: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 4 1 -32766 0 0
52: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 24 26
52: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 4 1 -32766 0 0
52: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 25 26
52: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 4 1 -32766 0 0
Newton::solveLinearSystem : Caught exception from the linear solver when getting converged: "BCRSMatrixError [operator++:/home/m.giraud/DUMUXexud/dune-istl/dune/istl/bcrsmatrix.hh:940]: matrix already built up"
52 converged value: 0 121 1 let s go get convergedRemote
52 to  comm_.min(converged) 
52 did  comm_.min(converged) convergedRemote: 0
52 final convergedRemote: 0
52 about to throw Linear solver did not converge
52 Newton::solveLinearSystem : Caught exception: "NumericalProblem [solveLinearSystem:/home/m.giraud/DUMUXexud/dumux/dumux/nonlinear/newtonsolver.hh:381]: Linear solver did not converge when getting convergedRemote "
52 Newton: Caught exception: "NumericalProblem [solveLinearSystem:/home/m.giraud/DUMUXexud/dumux/dumux/nonlinear/newtonsolver.hh:381]: Linear solver did not converge"
52 Newton solver did not converge with dt = 0.0495 seconds. Retrying with time step of 0.02475 seconds
52 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
home/m.giraud/DUMUXexud/dumux/dumux/nonlinear/newtonsolver.hh:381]: Linear solver did not converge"
66 Newton solver did not converge with dt = 0.0495 seconds. Retrying with time step of 0.02475 seconds
66 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
mmunicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000378565
55: communicator.hh::sendRecv,MPI_Testany 3 1 -32766 0 0 0.000382825
55: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 3 1 -32766 0 0
55: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 11
55: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 3 1 -32766 0 0
55: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 11
55: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 3 1 -32766 0 0
55: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 11
55: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 3 1 -32766 0 0
55: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 11
55: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 3 1 -32766 0 0
55: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 11
55: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 3 1 -32766 0 0
55: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 11
55: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 3 1 -32766 0 0
55: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 11
55: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 3 1 -32766 0 0
55: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 11
55: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 3 1 -32766 0 0
55: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 11
55: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 3 1 -32766 0 0
55: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 10 11
55: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 3 1 -32766 0 0
Newton::solveLinearSystem : Caught exception from the linear solver when getting converged: "BCRSMatrixError [operator++:/home/m.giraud/DUMUXexud/dune-istl/dune/istl/bcrsmatrix.hh:940]: matrix already built up"
55 converged value: 0 121 1 let s go get convergedRemote
55 to  comm_.min(converged) 
55 did  comm_.min(converged) convergedRemote: 0
55 final convergedRemote: 0
55 about to throw Linear solver did not converge
55 Newton::solveLinearSystem : Caught exception: "NumericalProblem [solveLinearSystem:/home/m.giraud/DUMUXexud/dumux/dumux/nonlinear/newtonsolver.hh:381]: Linear solver did not converge when getting convergedRemote "
55 Newton: Caught exception: "NumericalProblem [solveLinearSystem:/home/m.giraud/DUMUXexud/dumux/dumux/nonlinear/newtonsolver.hh:381]: Linear solver did not converge"
55 Newton solver did not converge with dt = 0.0495 seconds. Retrying with time step of 0.02475 seconds
55 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
convergedRemote: 0
69 final convergedRemote: 0
69 about to throw Linear solver did not converge
69 Newton::solveLinearSystem : Caught exception: "NumericalProblem [solveLinearSystem:/home/m.giraud/DUMUXexud/dumux/dumux/nonlinear/newtonsolver.hh:381]: Linear solver did not converge when getting convergedRemote "
69 Newton: Caught exception: "NumericalProblem [solveLinearSystem:/home/m.giraud/DUMUXexud/dumux/dumux/nonlinear/newtonsolver.hh:381]: Linear solver did not converge"
69 Newton solver did not converge with dt = 0.0495 seconds. Retrying with time step of 0.02475 seconds
69 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
UMUXexud/dumux/dumux/nonlinear/newtonsolver.hh:381]: Linear solver did not converge"
40 Newton solver did not converge with dt = 0.0495 seconds. Retrying with time step of 0.02475 seconds
40 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
UMUXexud/dumux/dumux/nonlinear/newtonsolver.hh:381]: Linear solver did not converge"
37 Newton solver did not converge with dt = 0.0495 seconds. Retrying with time step of 0.02475 seconds
37 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
d, &status); 18 26
26: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 9 1 -32766 0 0
26: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 19 26
26: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 9 1 -32766 0 0
26: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 20 26
26: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 9 1 -32766 0 0
26: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 21 26
26: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 9 1 -32766 0 0
26: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 22 26
26: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 9 1 -32766 0 0
26: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 23 26
26: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 9 1 -32766 0 0
26: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 24 26
26: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 9 1 -32766 0 0
26: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 25 26
26: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 9 1 -32766 0 0
Newton::solveLinearSystem : Caught exception from the linear solver when getting converged: "BCRSMatrixError [operator++:/home/m.giraud/DUMUXexud/dune-istl/dune/istl/bcrsmatrix.hh:940]: matrix already built up"
26 converged value: 0 121 1 let s go get convergedRemote
26 to  comm_.min(converged) 
26 did  comm_.min(converged) convergedRemote: 0
26 final convergedRemote: 0
26 about to throw Linear solver did not converge
26 Newton::solveLinearSystem : Caught exception: "NumericalProblem [solveLinearSystem:/home/m.giraud/DUMUXexud/dumux/dumux/nonlinear/newtonsolver.hh:381]: Linear solver did not converge when getting convergedRemote "
26 Newton: Caught exception: "NumericalProblem [solveLinearSystem:/home/m.giraud/DUMUXexud/dumux/dumux/nonlinear/newtonsolver.hh:381]: Linear solver did not converge"
26 Newton solver did not converge with dt = 0.0495 seconds. Retrying with time step of 0.02475 seconds
26 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
xception: "NumericalProblem [solveLinearSystem:/home/m.giraud/DUMUXexud/dumux/dumux/nonlinear/newtonsolver.hh:381]: Linear solver did not converge"
35 Newton solver did not converge with dt = 0.0495 seconds. Retrying with time step of 0.02475 seconds
35 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
MPI_Testany -32766 0 -32766 0 0 0.000258943
93: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000301173
93: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000305024
93: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000308514
93: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000311994
93: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000315454
93: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000318954
93: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000322434
93: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000325984
93: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000329384
93: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000332804
93: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000336214
93: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000339524
93: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000342814
93: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000346394
93: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000349894
93: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000353524
93: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000357034
93: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000360524
93: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000364074
93: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000404315
93: communicator.hh::sendRecv,MPI_Testany 8 1 -32766 0 0 0.000408025
93: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 8 1 -32766 0 0
93: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 13
93: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 8 1 -32766 0 0
93: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 13
93: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 8 1 -32766 0 0
93: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 13
93: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 8 1 -32766 0 0
93: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 13
93: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 8 1 -32766 0 0
93: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 13
93: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 8 1 -32766 0 0
93: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 13
93: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 8 1 -32766 0 0
93: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 13
93: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 8 1 -32766 0 0
93: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 13
93: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 8 1 -32766 0 0
93: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 13
93: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 8 1 -32766 0 0
93: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 10 13
93: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 8 1 -32766 0 0
93: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 11 13
93: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 8 1 -32766 0 0
93: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 12 13
93: communicator.hh::sendReormation_.size(), recvRequests, &finished, &status); 18 24
101: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 6 1 -32766 0 0
101: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 19 24
101: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 6 1 -32766 0 0
101: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 20 24
101: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 6 1 -32766 0 0
101: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 21 24
101: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 6 1 -32766 0 0
101: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 22 24
101: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 6 1 -32766 0 0
101: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 23 24
101: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 6 1 -32766 0 0
Newton::solveLinearSystem : Caught exception from the linear solver when getting converged: "BCRSMatrixError [operator++:/home/m.giraud/DUMUXexud/dune-istl/dune/istl/bcrsmatrix.hh:940]: matrix already built up"
101 converged value: 0 121 1 let s go get convergedRemote
101 to  comm_.min(converged) 
101 did  comm_.min(converged) convergedRemote: 0
101 final convergedRemote: 0
101 about to throw Linear solver did not converge
101 Newton::solveLinearSystem : Caught exception: "NumericalProblem [solveLinearSystem:/home/m.giraud/DUMUXexud/dumux/dumux/nonlinear/newtonsolver.hh:381]: Linear solver did not converge when getting convergedRemote "
101 Newton: Caught exception: "NumericalProblem [solveLinearSystem:/home/m.giraud/DUMUXexud/dumux/dumux/nonlinear/newtonsolver.hh:381]: Linear solver did not converge"
101 Newton solver did not converge with dt = 0.0495 seconds. Retrying with time step of 0.02475 seconds
101 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
s); 7 9
39: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 7 1 -32766 0 0
39: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 9
39: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 7 1 -32766 0 0
Newton::solveLinearSystem : Caught exception from the linear solver when getting converged: "BCRSMatrixError [operator++:/home/m.giraud/DUMUXexud/dune-istl/dune/istl/bcrsmatrix.hh:940]: matrix already built up"
39 converged value: 0 121 1 let s go get convergedRemote
39 to  comm_.min(converged) 
39 did  comm_.min(converged) convergedRemote: 0
39 final convergedRemote: 0
39 about to throw Linear solver did not converge
39 Newton::solveLinearSystem : Caught exception: "NumericalProblem [solveLinearSystem:/home/m.giraud/DUMUXexud/dumux/dumux/nonlinear/newtonsolver.hh:381]: Linear solver did not converge when getting convergedRemote "
39 Newton: Caught exception: "NumericalProblem [solveLinearSystem:/home/m.giraud/DUMUXexud/dumux/dumux/nonlinear/newtonsolver.hh:381]: Linear solver did not converge"
39 Newton solver did not converge with dt = 0.0495 seconds. Retrying with time step of 0.02475 seconds
39 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
earSystem:/home/m.giraud/DUMUXexud/dumux/dumux/nonlinear/newtonsolver.hh:381]: Linear solver did not converge when getting convergedRemote "
79 Newton: Caught exception: "NumericalProblem [solveLinearSystem:/home/m.giraud/DUMUXexud/dumux/dumux/nonlinear/newtonsolver.hh:381]: Linear solver did not converge"
79 Newton solver did not converge with dt = 0.0495 seconds. Retrying with time step of 0.02475 seconds
79 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
Waitany(messageInformation_.size(), recvRequests, &finished, &status); 18 22
117: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 17 1 -32766 0 0
117: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 19 22
117: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 17 1 -32766 0 0
117: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 20 22
117: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 17 1 -32766 0 0
117: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 21 22
117: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 17 1 -32766 0 0
Newton::solveLinearSystem : Caught exception from the linear solver when getting converged: "BCRSMatrixError [operator++:/home/m.giraud/DUMUXexud/dune-istl/dune/istl/bcrsmatrix.hh:940]: matrix already built up"
117 converged value: 0 121 1 let s go get convergedRemote
117 to  comm_.min(converged) 
117 did  comm_.min(converged) convergedRemote: 0
117 final convergedRemote: 0
117 about to throw Linear solver did not converge
117 Newton::solveLinearSystem : Caught exception: "NumericalProblem [solveLinearSystem:/home/m.giraud/DUMUXexud/dumux/dumux/nonlinear/newtonsolver.hh:381]: Linear solver did not converge when getting convergedRemote "
117 Newton: Caught exception: "NumericalProblem [solveLinearSystem:/home/m.giraud/DUMUXexud/dumux/dumux/nonlinear/newtonsolver.hh:381]: Linear solver did not converge"
117 Newton solver did not converge with dt = 0.0495 seconds. Retrying with time step of 0.02475 seconds
117 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
ed, &status); 18 26
12: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 0 1 -32766 0 0
12: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 19 26
12: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 0 1 -32766 0 0
12: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 20 26
12: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 0 1 -32766 0 0
12: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 21 26
12: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 0 1 -32766 0 0
12: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 22 26
12: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 0 1 -32766 0 0
12: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 23 26
12: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 0 1 -32766 0 0
12: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 24 26
12: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 0 1 -32766 0 0
12: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 25 26
12: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 0 1 -32766 0 0
Newton::solveLinearSystem : Caught exception from the linear solver when getting converged: "BCRSMatrixError [operator++:/home/m.giraud/DUMUXexud/dune-istl/dune/istl/bcrsmatrix.hh:940]: matrix already built up"
12 converged value: 0 121 1 let s go get convergedRemote
12 to  comm_.min(converged) 
12 did  comm_.min(converged) convergedRemote: 0
12 final convergedRemote: 0
12 about to throw Linear solver did not converge
12 Newton::solveLinearSystem : Caught exception: "NumericalProblem [solveLinearSystem:/home/m.giraud/DUMUXexud/dumux/dumux/nonlinear/newtonsolver.hh:381]: Linear solver did not converge when getting convergedRemote "
12 Newton: Caught exception: "NumericalProblem [solveLinearSystem:/home/m.giraud/DUMUXexud/dumux/dumux/nonlinear/newtonsolver.hh:381]: Linear solver did not converge"
12 Newton solver did not converge with dt = 0.0495 seconds. Retrying with time step of 0.02475 seconds
12 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
mm_.min(converged) convergedRemote: 0
27 final convergedRemote: 0
27 about to throw Linear solver did not converge
27 Newton::solveLinearSystem : Caught exception: "NumericalProblem [solveLinearSystem:/home/m.giraud/DUMUXexud/dumux/dumux/nonlinear/newtonsolver.hh:381]: Linear solver did not converge when getting convergedRemote "
27 Newton: Caught exception: "NumericalProblem [solveLinearSystem:/home/m.giraud/DUMUXexud/dumux/dumux/nonlinear/newtonsolver.hh:381]: Linear solver did not converge"
27 Newton solver did not converge with dt = 0.0495 seconds. Retrying with time step of 0.02475 seconds
27 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
rged) 
75 did  comm_.min(converged) convergedRemote: 0
75 final convergedRemote: 0
75 about to throw Linear solver did not converge
75 Newton::solveLinearSystem : Caught exception: "NumericalProblem [solveLinearSystem:/home/m.giraud/DUMUXexud/dumux/dumux/nonlinear/newtonsolver.hh:381]: Linear solver did not converge when getting convergedRemote "
75 Newton: Caught exception: "NumericalProblem [solveLinearSystem:/home/m.giraud/DUMUXexud/dumux/dumux/nonlinear/newtonsolver.hh:381]: Linear solver did not converge"
75 Newton solver did not converge with dt = 0.0495 seconds. Retrying with time step of 0.02475 seconds
75 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
trixError [operator++:/home/m.giraud/DUMUXexud/dune-istl/dune/istl/bcrsmatrix.hh:940]: matrix already built up"
54 converged value: 0 121 1 let s go get convergedRemote
54 to  comm_.min(converged) 
54 did  comm_.min(converged) convergedRemote: 0
54 final convergedRemote: 0
54 about to throw Linear solver did not converge
54 Newton::solveLinearSystem : Caught exception: "NumericalProblem [solveLinearSystem:/home/m.giraud/DUMUXexud/dumux/dumux/nonlinear/newtonsolver.hh:381]: Linear solver did not converge when getting convergedRemote "
54 Newton: Caught exception: "NumericalProblem [solveLinearSystem:/home/m.giraud/DUMUXexud/dumux/dumux/nonlinear/newtonsolver.hh:381]: Linear solver did not converge"
54 Newton solver did not converge with dt = 0.0495 seconds. Retrying with time step of 0.02475 seconds
54 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
final convergedRemote: 0
6 about to throw Linear solver did not converge
6 Newton::solveLinearSystem : Caught exception: "NumericalProblem [solveLinearSystem:/home/m.giraud/DUMUXexud/dumux/dumux/nonlinear/newtonsolver.hh:381]: Linear solver did not converge when getting convergedRemote "
6 Newton: Caught exception: "NumericalProblem [solveLinearSystem:/home/m.giraud/DUMUXexud/dumux/dumux/nonlinear/newtonsolver.hh:381]: Linear solver did not converge"
6 Newton solver did not converge with dt = 0.0495 seconds. Retrying with time step of 0.02475 seconds
6 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
nearSystem:/home/m.giraud/DUMUXexud/dumux/dumux/nonlinear/newtonsolver.hh:381]: Linear solver did not converge when getting convergedRemote "
32 Newton: Caught exception: "NumericalProblem [solveLinearSystem:/home/m.giraud/DUMUXexud/dumux/dumux/nonlinear/newtonsolver.hh:381]: Linear solver did not converge"
32 Newton solver did not converge with dt = 0.0495 seconds. Retrying with time step of 0.02475 seconds
32 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
endRecv,MPI_Testany -32766 0 -32766 0 0 0.000291073
89: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000293903
89: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000297113
89: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000299933
89: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000303343
89: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000306433
89: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000309384
89: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000312334
89: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000355384
89: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000359064
89: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000362474
89: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000365864
89: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000369254
89: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000372824
89: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000376624
89: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000380204
89: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000383634
89: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000387824
89: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000391255
89: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000394695
89: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000398405
89: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000401845
89: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000405245
89: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000410235
89: communicator.hh::sendRecv,MPI_Testany 5 1 -32766 0 0 0.000413765
89: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 5 1 -32766 0 0
89: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 11
89: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 5 1 -32766 0 0
89: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 11
89: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 5 1 -32766 0 0
89: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 11
89: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 5 1 -32766 0 0
89: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 11
89: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 5 1 -32766 0 0
89: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 11
89: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 5 1 -32766 0 0
89: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 11
89: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 5 1 -32766 0 0
89: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 11
89: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 5 1 -32766 0 0
89: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 11
89: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 5 1 -32766 0 0
89: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 11
89: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 5 1 -32766 0 0
89: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 10 11
89: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 5 1 -32766 0 0
Newton::solveLinearSystem : Caught exception from the linear solver when getting converged: "BCRSMatrixError [operator++:/h92 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
92: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 16
92: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 4.39e-06
92: communicator.hh::sendRecv,MPI_Testany 8 1 -32766 0 0 9.94e-06
92: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 8 1 -32766 0 0
92: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 16
92: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 8 1 -32766 0 0
92: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 16
92: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 8 1 -32766 0 0
92: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 16
92: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 8 1 -32766 0 0
92: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 16
92: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 8 1 -32766 0 0
92: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 16
92: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 8 1 -32766 0 0
92: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 16
92: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 8 1 -32766 0 0
92: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 16
92: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 8 1 -32766 0 0
92: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 16
92: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 8 1 -32766 0 0
92: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 16
92: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 8 1 -32766 0 0
92: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 10 16
92: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 8 1 -32766 0 0
92: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 11 16
92: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 8 1 -32766 0 0
92: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 12 16
92: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 8 1 -32766 0 0
92: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 13 16
92: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 8 1 -32766 0 0
92: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 14 16
92: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 8 1 -32766 0 0
92: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 15 16
92: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 8 1 -32766 0 0
Newton::solveLinearSystem : Caught exception from the linear solver when getting converged: "BCRSMatrixError [operator++:/home/m.giraud/DUMUXexud/dune-istl/dune/istl/bcrsmatrix.hh:940]: matrix already built up"
92 converged value: 0 121 1 let s go get convergedRemote
92 to  comm_.min(converged) 
92 did  comm_.min(converged) convergedRemote: 0
92 final convergedRemote: 0
92 about to throw Linear solver did not converge
92 Newton::solveLinearSystem : Caught exception: "NumericalProblem [solveLinearSystem:/home/m2 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 8
2: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 2.84e-06
2: communicator.hh::sendRecv,MPI_Testany 0 1 -32766 0 0 8.05e-06
2: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 0 1 -32766 0 0
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 8
2: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 0 1 -32766 0 0
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 8
2: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 0 1 -32766 0 0
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 8
2: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 0 1 -32766 0 0
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 8
2: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 0 1 -32766 0 0
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 8
2: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 0 1 -32766 0 0
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 8
2: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 0 1 -32766 0 0
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 8
2: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 0 1 -32766 0 0
Newton::solveLinearSystem : Caught exception from the linear solver when getting converged: "BCRSMatrixError [operator++:/home/m.giraud/DUMUXexud/dune-istl/dune/istl/bcrsmatrix.hh:940]: matrix already built up"
2 converged value: 0 121 1 let s go get convergedRemote
2 to  comm_.min(converged) 
2 did  comm_.min(converged) convergedRemote: 0
2 final convergedRemote: 0
2 about to throw Linear solver did not converge
2 Newton::solveLinearSystem : Caught exception: "NumericalProblem [solveLinearSystem:/home/m.giraud/DUMUXexud/dumux/dumux/nonlinear/newtonsolver.hh:381]: Linear solver did not converge when getting convergedRemote "
2 Newton: Caught exception: "NumericalProblem [solveLinearSystem:/home/m.giraud/DUMUXexud/dumux/dumux/nonlinear/newtonsolver.hh:381]: Linear solver did not converge"
2 Newton solver did not converge with dt = 0.0495 seconds. Retrying with time step of 0.02475 seconds
2 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
Requests, &finished, &status); 18 31
56: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 11 1 -32766 0 0
56: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 19 31
56: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 11 1 -32766 0 0
56: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 20 31
56: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 11 1 -32766 0 0
56: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 21 31
56: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 11 1 -32766 0 0
56: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 22 31
56: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 11 1 -32766 0 0
56: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 23 31
56: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 11 1 -32766 0 0
56: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 24 31
56: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 11 1 -32766 0 0
56: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 25 31
56: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 11 1 -32766 0 0
56: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 26 31
56: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 11 1 -32766 0 0
56: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 27 31
56: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 11 1 -32766 0 0
56: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 28 31
56: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 11 1 -32766 0 0
56: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 29 31
56: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 11 1 -32766 0 0
56: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 30 31
56: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 11 1 -32766 0 0
Newton::solveLinearSystem : Caught exception from the linear solver when getting converged: "BCRSMatrixError [operator++:/home/m.giraud/DUMUXexud/dune-istl/dune/istl/bcrsmatrix.hh:940]: matrix already built up"
56 converged value: 0 121 1 let s go get convergedRemote
56 to  comm_.min(converged) 
56 did  comm_.min(converged) convergedRemote: 0
56 final convergedRemote: 0
56 about to throw Linear solver did not converge
56 Newton::solveLinearSystem : Caught exception: "NumericalProblem [solveLinearSystem:/home/m.giraud/DUMUXexud/dumux/dumux/nonlinear/newtonsolver.hh:381]: Linear solver did not converge when getting convergedRemote "
56 Newton: Caught exception: "NumericalProblem [solveLinearSystem:/home/m.giraud/DUMUXexud/dumux/dumux/nonlinear/newtonsolver.hh:381]: Linear solver did not converge"
56 Newton solver did not converge with dt = 0.0495 seconds. Retrying with time step of 0.02475 seconds
56 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
107 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
107: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 5
107: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 5e-07
107: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 7.21e-06
107: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 1.091e-05
107: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 1.451e-05
107: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 1.8041e-05
107: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 2.1381e-05
107: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 2.4741e-05
107: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 2.8181e-05
107: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 3.1671e-05
107: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 3.4961e-05
107: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 3.8281e-05
107: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 4.1751e-05
107: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 8.0241e-05
107: communicator.hh::sendRecv,MPI_Testany 2 1 -32766 0 0 8.4031e-05
107: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 2 1 -32766 0 0
107: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 5
107: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 2 1 -32766 0 0
107: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 5
107: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 2 1 -32766 0 0
107: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 5
107: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 2 1 -32766 0 0
107: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 5
107: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 2 1 -32766 0 0
Newton::solveLinearSystem : Caught exception from the linear solver when getting converged: "BCRSMatrixError [operator++:/home/m.giraud/DUMUXexud/dune-istl/dune/istl/bcrsmatrix.hh:940]: matrix already built up"
107 converged value: 0 121 1 let s go get convergedRemote
107 to  comm_.min(converged) 
107 did  comm_.min(converged) convergedRemote: 0
107 final convergedRemote: 0
107 about to throw Linear solver did not converge
107 Newton::solveLinearSystem : Caught exception: "NumericalProblem [solveLinearSystem:/home/m.giraud/DUMUXexud/dumux/dumux/nonlinear/newtonsolver.hh:381]: Linear solver did not converge when getting convergedRemote "
107 Newton: Caught exception: "NumericalProblem [solveLinearSystem:/home/m.giraud/DUMUXexud/dumux/dumux/nonlinear/newtonsolver.hh:381]: Linear solver did not converge"
107 Newton solver did not converge with dt = 0.0495 seconds. Retrying with time step of 0.02475 seconds
107 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
28 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
28: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 18
28: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 9.52e-06
28: communicator.hh::sendRecv,MPI_Testany 3 1 -32766 0 0 4.5711e-05
28: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 3 1 -32766 0 0
28: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 18
28: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 3 1 -32766 0 0
28: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 18
28: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 3 1 -32766 0 0
28: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 18
28: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 3 1 -32766 0 0
28: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 18
28: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 3 1 -32766 0 0
28: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 18
28: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 3 1 -32766 0 0
28: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 18
28: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 3 1 -32766 0 0
28: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 18
28: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 3 1 -32766 0 0
28: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 18
28: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 3 1 -32766 0 0
28: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 18
28: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 3 1 -32766 0 0
28: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 10 18
28: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 3 1 -32766 0 0
28: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 11 18
28: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 3 1 -32766 0 0
28: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 12 18
28: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 3 1 -32766 0 0
28: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 13 18
28: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 3 1 -32766 0 0
28: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 14 18
28: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 3 1 -32766 0 0
28: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 15 18
28: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 3 1 -32766 0 0
28: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 16 18
28: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 3 1 -32766 0 0
28: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 17 18
28: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 3 1 -32766 0 0
Newton::solveLinearSystem : Caught exception from the linear solver when getting converged: "BCRSMaor.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000596588
116: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000599918
116: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000609428
116: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000618668
116: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.00072995
116: communicator.hh::sendRecv,MPI_Testany 2 1 -32766 0 0 0.000808631
116: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 2 1 -32766 0 0
116: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 7
116: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 2 1 -32766 0 0
116: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 7
116: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 2 1 -32766 0 0
116: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 7
116: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 2 1 -32766 0 0
116: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 7
116: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 2 1 -32766 0 0
116: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 7
116: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 2 1 -32766 0 0
116: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 7
116: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 2 1 -32766 0 0
Newton::solveLinearSystem : Caught exception from the linear solver when getting converged: "BCRSMatrixError [operator++:/home/m.giraud/DUMUXexud/dune-istl/dune/istl/bcrsmatrix.hh:940]: matrix already built up"
116 converged value: 0 121 1 let s go get convergedRemote
116 to  comm_.min(converged) 
116 did  comm_.min(converged) convergedRemote: 0
116 final convergedRemote: 0
116 about to throw Linear solver did not converge
116 Newton::solveLinearSystem : Caught exception: "NumericalProblem [solveLinearSystem:/home/m.giraud/DUMUXexud/dumux/dumux/nonlinear/newtonsolver.hh:381]: Linear solver did not converge when getting convergedRemote "
116 Newton: Caught exception: "NumericalProblem [solveLinearSystem:/home/m.giraud/DUMUXexud/dumux/dumux/nonlinear/newtonsolver.hh:381]: Linear solver did not converge"
116 Newton solver did not converge with dt = 0.0495 seconds. Retrying with time step of 0.02475 seconds
116 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
8 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
8: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 20
8: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 6.62e-06
8: communicator.hh::sendRecv,MPI_Testany 0 1 -32766 0 0 1.252e-05
8: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 0 1 -32766 0 0
8: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 20
8: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 0 1 -32766 0 0
8: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 20
8: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 0 1 -32766 0 0
8: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 20
8: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 0 1 -32766 0 0
8: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 20
8: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 0 1 -32766 0 0
8: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 20
8: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 0 1 -32766 0 0
8: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 20
8: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 0 1 -32766 0 0
8: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 20
8: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 0 1 -32766 0 0
8: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 20
8: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 0 1 -32766 0 0
8: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 20
8: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 0 1 -32766 0 0
8: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 10 20
8: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 0 1 -32766 0 0
8: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 11 20
8: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 0 1 -32766 0 0
8: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 12 20
8: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 0 1 -32766 0 0
8: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 13 20
8: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 0 1 -32766 0 0
8: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 14 20
8: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 0 1 -32766 0 0
8: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 15 20
8: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 0 1 -32766 0 0
8: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 16 20
8: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 0 1 -32766 0 0
8: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 17 20
8: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 0 1 -32766 0 0
8: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 18 20
8: communicator.hh::s86 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
86: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 31
86: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 7.69e-06
86: communicator.hh::sendRecv,MPI_Testany 7 1 -32766 0 0 1.333e-05
86: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 7 1 -32766 0 0
86: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 31
86: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 7 1 -32766 0 0
86: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 31
86: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 7 1 -32766 0 0
86: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 31
86: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 7 1 -32766 0 0
86: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 31
86: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 7 1 -32766 0 0
86: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 31
86: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 7 1 -32766 0 0
86: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 31
86: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 7 1 -32766 0 0
86: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 31
86: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 7 1 -32766 0 0
86: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 31
86: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 7 1 -32766 0 0
86: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 31
86: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 7 1 -32766 0 0
86: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 10 31
86: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 7 1 -32766 0 0
86: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 11 31
86: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 7 1 -32766 0 0
86: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 12 31
86: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 7 1 -32766 0 0
86: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 13 31
86: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 7 1 -32766 0 0
86: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 14 31
86: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 7 1 -32766 0 0
86: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 15 31
86: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 7 1 -32766 0 0
86: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 16 31
86: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 7 1 -32766 0 0
86: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 17 31
86: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 7 1 -32766 0 0
86: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finishe MPI_UNDEFINED); 6 1 -32766 0 0
25: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 7
25: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 6 1 -32766 0 0
25: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 7
25: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 6 1 -32766 0 0
25: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 7
25: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 6 1 -32766 0 0
25: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 7
25: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 6 1 -32766 0 0
25: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 7
25: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 6 1 -32766 0 0
Newton::solveLinearSystem : Caught exception from the linear solver when getting converged: "BCRSMatrixError [operator++:/home/m.giraud/DUMUXexud/dune-istl/dune/istl/bcrsmatrix.hh:940]: matrix already built up"
25 converged value: 0 121 1 let s go get convergedRemote
25 to  comm_.min(converged) 
25 did  comm_.min(converged) convergedRemote: 0
25 final convergedRemote: 0
25 about to throw Linear solver did not converge
25 Newton::solveLinearSystem : Caught exception: "NumericalProblem [solveLinearSystem:/home/m.giraud/DUMUXexud/dumux/dumux/nonlinear/newtonsolver.hh:381]: Linear solver did not converge when getting convergedRemote "
25 Newton: Caught exception: "NumericalProblem [solveLinearSystem:/home/m.giraud/DUMUXexud/dumux/dumux/nonlinear/newtonsolver.hh:381]: Linear solver did not converge"
25 Newton solver did not converge with dt = 0.0495 seconds. Retrying with time step of 0.02475 seconds
25 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
ed: "BCRSMatrixError [operator++:/home/m.giraud/DUMUXexud/dune-istl/dune/istl/bcrsmatrix.hh:940]: matrix already built up"
87 converged value: 0 121 1 let s go get convergedRemote
87 to  comm_.min(converged) 
87 did  comm_.min(converged) convergedRemote: 0
87 final convergedRemote: 0
87 about to throw Linear solver did not converge
87 Newton::solveLinearSystem : Caught exception: "NumericalProblem [solveLinearSystem:/home/m.giraud/DUMUXexud/dumux/dumux/nonlinear/newtonsolver.hh:381]: Linear solver did not converge when getting convergedRemote "
87 Newton: Caught exception: "NumericalProblem [solveLinearSystem:/home/m.giraud/DUMUXexud/dumux/dumux/nonlinear/newtonsolver.hh:381]: Linear solver did not converge"
87 Newton solver did not converge with dt = 0.0495 seconds. Retrying with time step of 0.02475 seconds
87 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
cv, before assert(finished != MPI_UNDEFINED); 8 1 -32766 0 0
Newton::solveLinearSystem : Caught exception from the linear solver when getting converged: "BCRSMatrixError [operator++:/home/m.giraud/DUMUXexud/dune-istl/dune/istl/bcrsmatrix.hh:940]: matrix already built up"
93 converged value: 0 121 1 let s go get convergedRemote
93 to  comm_.min(converged) 
93 did  comm_.min(converged) convergedRemote: 0
93 final convergedRemote: 0
93 about to throw Linear solver did not converge
93 Newton::solveLinearSystem : Caught exception: "NumericalProblem [solveLinearSystem:/home/m.giraud/DUMUXexud/dumux/dumux/nonlinear/newtonsolver.hh:381]: Linear solver did not converge when getting convergedRemote "
93 Newton: Caught exception: "NumericalProblem [solveLinearSystem:/home/m.giraud/DUMUXexud/dumux/dumux/nonlinear/newtonsolver.hh:381]: Linear solver did not converge"
93 Newton solver did not converge with dt = 0.0495 seconds. Retrying with time step of 0.02475 seconds
93 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
d, &status); 18 31
86: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 7 1 -32766 0 0
86: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 19 31
86: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 7 1 -32766 0 0
86: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 20 31
86: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 7 1 -32766 0 0
86: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 21 31
86: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 7 1 -32766 0 0
86: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 22 31
86: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 7 1 -32766 0 0
86: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 23 31
86: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 7 1 -32766 0 0
86: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 24 31
86: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 7 1 -32766 0 0
86: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 25 31
86: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 7 1 -32766 0 0
86: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 26 31
86: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 7 1 -32766 0 0
86: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 27 31
86: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 7 1 -32766 0 0
86: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 28 31
86: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 7 1 -32766 0 0
86: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 29 31
86: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 7 1 -32766 0 0
86: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 30 31
86: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 7 1 -32766 0 0
Newton::solveLinearSystem : Caught exception from the linear solver when getting converged: "BCRSMatrixError [operator++:/home/m.giraud/DUMUXexud/dune-istl/dune/istl/bcrsmatrix.hh:940]: matrix already built up"
86 converged value: 0 121 1 let s go get convergedRemote
86 to  comm_.min(converged) 
86 did  comm_.min(converged) convergedRemote: 0
86 final convergedRemote: 0
86 about to throw Linear solver did not converge
86 Newton::solveLinearSystem : Caught exception: "NumericalProblem [solveLinearSystem:/home/m.giraud/DUMUXexud/dumux/dumux/nonlinear/newtonsolver.hh:381]: Linear solver did not converge when getting convergedRemote "
86 Newton: Caught exception: "NumericalProblem [solveLinearSystem:/home/m.giraud/DUMUXexud/dumux/dumux/nonlinear/newtonsolver.hh:381]: Linear solver did not converge"
86 Newton solver did not converge with dt = 0.0495 seconds. Retrying with time step of 0.02475 seconds
86 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
ome/m.giraud/DUMUXexud/dune-istl/dune/istl/bcrsmatrix.hh:940]: matrix already built up"
89 converged value: 0 121 1 let s go get convergedRemote
89 to  comm_.min(converged) 
89 did  comm_.min(converged) convergedRemote: 0
89 final convergedRemote: 0
89 about to throw Linear solver did not converge
89 Newton::solveLinearSystem : Caught exception: "NumericalProblem [solveLinearSystem:/home/m.giraud/DUMUXexud/dumux/dumux/nonlinear/newtonsolver.hh:381]: Linear solver did not converge when getting convergedRemote "
89 Newton: Caught exception: "NumericalProblem [solveLinearSystem:/home/m.giraud/DUMUXexud/dumux/dumux/nonlinear/newtonsolver.hh:381]: Linear solver did not converge"
89 Newton solver did not converge with dt = 0.0495 seconds. Retrying with time step of 0.02475 seconds
89 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
.giraud/DUMUXexud/dumux/dumux/nonlinear/newtonsolver.hh:381]: Linear solver did not converge when getting convergedRemote "
92 Newton: Caught exception: "NumericalProblem [solveLinearSystem:/home/m.giraud/DUMUXexud/dumux/dumux/nonlinear/newtonsolver.hh:381]: Linear solver did not converge"
92 Newton solver did not converge with dt = 0.0495 seconds. Retrying with time step of 0.02475 seconds
92 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
trixError [operator++:/home/m.giraud/DUMUXexud/dune-istl/dune/istl/bcrsmatrix.hh:940]: matrix already built up"
28 converged value: 0 121 1 let s go get convergedRemote
28 to  comm_.min(converged) 
28 did  comm_.min(converged) convergedRemote: 0
28 final convergedRemote: 0
28 about to throw Linear solver did not converge
28 Newton::solveLinearSystem : Caught exception: "NumericalProblem [solveLinearSystem:/home/m.giraud/DUMUXexud/dumux/dumux/nonlinear/newtonsolver.hh:381]: Linear solver did not converge when getting convergedRemote "
28 Newton: Caught exception: "NumericalProblem [solveLinearSystem:/home/m.giraud/DUMUXexud/dumux/dumux/nonlinear/newtonsolver.hh:381]: Linear solver did not converge"
28 Newton solver did not converge with dt = 0.0495 seconds. Retrying with time step of 0.02475 seconds
28 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
endRecv, before assert(finished != MPI_UNDEFINED); 0 1 -32766 0 0
8: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 19 20
8: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 0 1 -32766 0 0
Newton::solveLinearSystem : Caught exception from the linear solver when getting converged: "BCRSMatrixError [operator++:/home/m.giraud/DUMUXexud/dune-istl/dune/istl/bcrsmatrix.hh:940]: matrix already built up"
8 converged value: 0 121 1 let s go get convergedRemote
8 to  comm_.min(converged) 
8 did  comm_.min(converged) convergedRemote: 0
8 final convergedRemote: 0
8 about to throw Linear solver did not converge
8 Newton::solveLinearSystem : Caught exception: "NumericalProblem [solveLinearSystem:/home/m.giraud/DUMUXexud/dumux/dumux/nonlinear/newtonsolver.hh:381]: Linear solver did not converge when getting convergedRemote "
8 Newton: Caught exception: "NumericalProblem [solveLinearSystem:/home/m.giraud/DUMUXexud/dumux/dumux/nonlinear/newtonsolver.hh:381]: Linear solver did not converge"
8 Newton solver did not converge with dt = 0.0495 seconds. Retrying with time step of 0.02475 seconds
8 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
= dS/dt + div F - q;   M = grad r
ion from the linear solver when getting converged: "BCRSMatrixError [operator++:/home/m.giraud/DUMUXexud/dune-istl/dune/istl/bcrsmatrix.hh:940]: matrix already built up"
113 converged value: 0 121 1 let s go get convergedRemote
113 to  comm_.min(converged) 
113 did  comm_.min(converged) convergedRemote: 0
113 final convergedRemote: 0
113 about to throw Linear solver did not converge
113 Newton::solveLinearSystem : Caught exception: "NumericalProblem [solveLinearSystem:/home/m.giraud/DUMUXexud/dumux/dumux/nonlinear/newtonsolver.hh:381]: Linear solver did not converge when getting convergedRemote "
113 Newton: Caught exception: "NumericalProblem [solveLinearSystem:/home/m.giraud/DUMUXexud/dumux/dumux/nonlinear/newtonsolver.hh:381]: Linear solver did not converge"
113 Newton solver did not converge with dt = 0.0495 seconds. Retrying with time step of 0.02475 seconds
113 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
nicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000302973
23: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000305783
23: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000308583
23: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000344364
23: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000347424
23: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000350064
23: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000352704
23: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000355494
23: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000358654
23: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000361554
23: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000364474
23: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000367304
23: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000370184
23: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000373124
23: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000376434
23: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000379704
23: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000384464
23: communicator.hh::sendRecv,MPI_Testany 10 1 -32766 0 0 0.000387944
23: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 10 1 -32766 0 0
23: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 11
23: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 10 1 -32766 0 0
23: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 11
23: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 10 1 -32766 0 0
23: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 11
23: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 10 1 -32766 0 0
23: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 11
23: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 10 1 -32766 0 0
23: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 11
23: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 10 1 -32766 0 0
23: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 11
23: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 10 1 -32766 0 0
23: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 11
23: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 10 1 -32766 0 0
23: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 11
23: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 10 1 -32766 0 0
23: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 11
23: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 10 1 -32766 0 0
23: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 10 11
23: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 10 1 -32766 0 0
Newton::solveLinearSystem : Caught exception from the linear solver when getting converged: "BCRSMatrixError [operator++:/home/m.giraud/DUMUXexud/dune-istl/dune/istl/bcrsmatrix.hh:940]: matrix already built up"
23 converged value: 0 121 1 let s go get convergedRemote
23 to  comm_.min(converged) 
23 did  comm_.min(converged) convergedRemote: 0
23 final convergedRemote: 0
23 about to throw Linear solver did not converge
23 Newton::solveLinearSystem : Caught exception: "NumericalProblem [solveLinearSystem:/home/m.giraud/DUMUXexud/dumux/dumux/nonlinear/newtonsolver.hh:381]: Linear solver did not converge when getecv,MPI_Testany -32766 0 -32766 0 0 0.000288243
73: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000291523
73: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000294993
73: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000298394
73: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000301784
73: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000305164
73: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000308554
73: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000311954
73: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000317004
73: communicator.hh::sendRecv,MPI_Testany 10 1 -32766 0 0 0.000320534
73: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 10 1 -32766 0 0
73: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 11
73: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 10 1 -32766 0 0
73: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 11
73: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 10 1 -32766 0 0
73: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 11
73: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 10 1 -32766 0 0
73: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 11
73: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 10 1 -32766 0 0
73: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 11
73: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 10 1 -32766 0 0
73: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 11
73: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 10 1 -32766 0 0
73: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 11
73: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 10 1 -32766 0 0
73: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 11
73: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 10 1 -32766 0 0
73: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 11
73: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 10 1 -32766 0 0
73: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 10 11
73: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 10 1 -32766 0 0
Newton::solveLinearSystem : Caught exception from the linear solver when getting converged: "BCRSMatrixError [operator++:/home/m.giraud/DUMUXexud/dune-istl/dune/istl/bcrsmatrix.hh:940]: matrix already built up"
73 converged value: 0 121 1 let s go get convergedRemote
73 to  comm_.min(converged) 
73 did  comm_.min(converged) convergedRemote: 0
73 final convergedRemote: 0
73 about to throw Linear solver did not converge
73 Newton::solveLinearSystem : Caught exception: "NumericalProblem [solveLinearSystem:/home/m.giraud/DUMUXexud/dumux/dumux/nonlinear/newtonsolver.hh:381]: Linear solver did not converge when getting convergedRemote "
73 Newton: Caught exception: "NumericalProblem [solveLinearSystem:/home/m.giraud/DUMUXexud/dumux/dumux/nonlinear/newtonsolver.hh:381]: Linear solver did not converge"
73 Newton solver did not converge with dt = 0.0495 seconds. Retrying with time step of 0.02475 seconds
73 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000284804
63: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000287394
63: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000290704
63: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000294094
63: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000297474
63: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000300844
63: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000304084
63: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000307364
63: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000310754
63: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000313994
63: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000317324
63: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000320554
63: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000323784
63: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000327274
63: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000365145
63: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000368765
63: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000372075
63: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000375515
63: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000378815
63: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000382085
63: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000386635
63: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000390025
63: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000393415
63: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000396865
63: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000400165
63: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000403485
63: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000407205
63: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000410875
63: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000414206
63: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000417546
63: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000420886
63: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000424206
63: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000452036
63: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000455646
63: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000832681
63: communicator.hh::sendRecv,MPI_Testany 1 1 -32766 0 0 0.00109293
63: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 1 1 -32766 0 0
63: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 9
63: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 1 1 -32766 0 0
63: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 9
63: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 1 1 -32766 0 0
63: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 9
63: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 1 1 -32766 0 0
63: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 9
63: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 1 1 -32766 0 0
63: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 9
63: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 1 1 -32766 0 0
63: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 9
63: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 1 1 -32766 0 0
63: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 9
63: communicator.hh:: = dS/dt + div F - q;   M = grad r
hh::sendRecv, before assert(finished != MPI_UNDEFINED); 6 1 -32766 0 0
31: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 10
31: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 6 1 -32766 0 0
31: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 10
31: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 6 1 -32766 0 0
31: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 10
31: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 6 1 -32766 0 0
31: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 10
31: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 6 1 -32766 0 0
31: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 10
31: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 6 1 -32766 0 0
31: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 10
31: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 6 1 -32766 0 0
31: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 10
31: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 6 1 -32766 0 0
31: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 10
31: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 6 1 -32766 0 0
31: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 10
31: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 6 1 -32766 0 0
Newton::solveLinearSystem : Caught exception from the linear solver when getting converged: "BCRSMatrixError [operator++:/home/m.giraud/DUMUXexud/dune-istl/dune/istl/bcrsmatrix.hh:940]: matrix already built up"
31 converged value: 0 121 1 let s go get convergedRemote
31 to  comm_.min(converged) 
31 did  comm_.min(converged) convergedRemote: 0
31 final convergedRemote: 0
31 about to throw Linear solver did not converge
31 Newton::solveLinearSystem : Caught exception: "NumericalProblem [solveLinearSystem:/home/m.giraud/DUMUXexud/dumux/dumux/nonlinear/newtonsolver.hh:381]: Linear solver did not converge when getting convergedRemote "
31 Newton: Caught exception: "NumericalProblem [solveLinearSystem:/home/m.giraud/DUMUXexud/dumux/dumux/nonlinear/newtonsolver.hh:381]: Linear solver did not converge"
31 Newton solver did not converge with dt = 0.0495 seconds. Retrying with time step of 0.02475 seconds
31 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
mm_.min(converged) convergedRemote: 0
11 final convergedRemote: 0
11 about to throw Linear solver did not converge
11 Newton::solveLinearSystem : Caught exception: "NumericalProblem [solveLinearSystem:/home/m.giraud/DUMUXexud/dumux/dumux/nonlinear/newtonsolver.hh:381]: Linear solver did not converge when getting convergedRemote "
11 Newton: Caught exception: "NumericalProblem [solveLinearSystem:/home/m.giraud/DUMUXexud/dumux/dumux/nonlinear/newtonsolver.hh:381]: Linear solver did not converge"
11 Newton solver did not converge with dt = 0.0495 seconds. Retrying with time step of 0.02475 seconds
11 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
ed, &status); 18 20
22: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 8 1 -32766 0 0
22: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 19 20
22: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 8 1 -32766 0 0
Newton::solveLinearSystem : Caught exception from the linear solver when getting converged: "BCRSMatrixError [operator++:/home/m.giraud/DUMUXexud/dune-istl/dune/istl/bcrsmatrix.hh:940]: matrix already built up"
22 converged value: 0 121 1 let s go get convergedRemote
22 to  comm_.min(converged) 
22 did  comm_.min(converged) convergedRemote: 0
22 final convergedRemote: 0
22 about to throw Linear solver did not converge
22 Newton::solveLinearSystem : Caught exception: "NumericalProblem [solveLinearSystem:/home/m.giraud/DUMUXexud/dumux/dumux/nonlinear/newtonsolver.hh:381]: Linear solver did not converge when getting convergedRemote "
22 Newton: Caught exception: "NumericalProblem [solveLinearSystem:/home/m.giraud/DUMUXexud/dumux/dumux/nonlinear/newtonsolver.hh:381]: Linear solver did not converge"
22 Newton solver did not converge with dt = 0.0495 seconds. Retrying with time step of 0.02475 seconds
22 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
53 converged value: 0 121 1 let s go get convergedRemote
53 to  comm_.min(converged) 
53 did  comm_.min(converged) convergedRemote: 0
53 final convergedRemote: 0
53 about to throw Linear solver did not converge
53 Newton::solveLinearSystem : Caught exception: "NumericalProblem [solveLinearSystem:/home/m.giraud/DUMUXexud/dumux/dumux/nonlinear/newtonsolver.hh:381]: Linear solver did not converge when getting convergedRemote "
53 Newton: Caught exception: "NumericalProblem [solveLinearSystem:/home/m.giraud/DUMUXexud/dumux/dumux/nonlinear/newtonsolver.hh:381]: Linear solver did not converge"
53 Newton solver did not converge with dt = 0.0495 seconds. Retrying with time step of 0.02475 seconds
53 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
"BCRSMatrixError [operator++:/home/m.giraud/DUMUXexud/dune-istl/dune/istl/bcrsmatrix.hh:940]: matrix already built up"
44 converged value: 0 121 1 let s go get convergedRemote
44 to  comm_.min(converged) 
44 did  comm_.min(converged) convergedRemote: 0
44 final convergedRemote: 0
44 about to throw Linear solver did not converge
44 Newton::solveLinearSystem : Caught exception: "NumericalProblem [solveLinearSystem:/home/m.giraud/DUMUXexud/dumux/dumux/nonlinear/newtonsolver.hh:381]: Linear solver did not converge when getting convergedRemote "
44 Newton: Caught exception: "NumericalProblem [solveLinearSystem:/home/m.giraud/DUMUXexud/dumux/dumux/nonlinear/newtonsolver.hh:381]: Linear solver did not converge"
44 Newton solver did not converge with dt = 0.0495 seconds. Retrying with time step of 0.02475 seconds
44 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
onvergedRemote
9 to  comm_.min(converged) 
9 did  comm_.min(converged) convergedRemote: 0
9 final convergedRemote: 0
9 about to throw Linear solver did not converge
9 Newton::solveLinearSystem : Caught exception: "NumericalProblem [solveLinearSystem:/home/m.giraud/DUMUXexud/dumux/dumux/nonlinear/newtonsolver.hh:381]: Linear solver did not converge when getting convergedRemote "
9 Newton: Caught exception: "NumericalProblem [solveLinearSystem:/home/m.giraud/DUMUXexud/dumux/dumux/nonlinear/newtonsolver.hh:381]: Linear solver did not converge"
9 Newton solver did not converge with dt = 0.0495 seconds. Retrying with time step of 0.02475 seconds
9 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
MPI_Testany -32766 0 -32766 0 0 0.000609038
85: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000612358
85: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000615758
85: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000619188
85: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000622488
85: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000625818
85: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000629648
85: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000917982
85: communicator.hh::sendRecv,MPI_Testany 0 1 -32766 0 0 0.00116741
85: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 0 1 -32766 0 0
85: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 11
85: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 0 1 -32766 0 0
85: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 11
85: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 0 1 -32766 0 0
85: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 11
85: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 0 1 -32766 0 0
85: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 11
85: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 0 1 -32766 0 0
85: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 11
85: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 0 1 -32766 0 0
85: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 11
85: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 0 1 -32766 0 0
85: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 11
85: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 0 1 -32766 0 0
85: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 11
85: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 0 1 -32766 0 0
85: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 11
85: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 0 1 -32766 0 0
85: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 10 11
85: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 0 1 -32766 0 0
Newton::solveLinearSystem : Caught exception from the linear solver when getting converged: "BCRSMatrixError [operator++:/home/m.giraud/DUMUXexud/dune-istl/dune/istl/bcrsmatrix.hh:940]: matrix already built up"
85 converged value: 0 121 1 let s go get convergedRemote
85 to  comm_.min(converged) 
85 did  comm_.min(converged) convergedRemote: 0
85 final convergedRemote: 0
85 about to throw Linear solver did not converge
85 Newton::solveLinearSystem : Caught exception: "NumericalProblem [solveLinearSystem:/home/m.giraud/DUMUXexud/dumux/dumux/nonlinear/newtonsolver.hh:381]: Linear solver did not converge when getting convergedRemote "
85 Newton: Caught exception: "NumericalProblem [solveLinearSystem:/home/m.giraud/DUMUXexud/dumux/dumux/nonlinear/newtonsolver.hh:381]: Linear solver did not converge"
85 Newton solver did not converge with dt = 0.0495 seconds. Retrying with time step of 0.02475 seconds
85 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
convergedRemote: 0
95 about to throw Linear solver did not converge
95 Newton::solveLinearSystem : Caught exception: "NumericalProblem [solveLinearSystem:/home/m.giraud/DUMUXexud/dumux/dumux/nonlinear/newtonsolver.hh:381]: Linear solver did not converge when getting convergedRemote "
95 Newton: Caught exception: "NumericalProblem [solveLinearSystem:/home/m.giraud/DUMUXexud/dumux/dumux/nonlinear/newtonsolver.hh:381]: Linear solver did not converge"
95 Newton solver did not converge with dt = 0.0495 seconds. Retrying with time step of 0.02475 seconds
95 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
v,MPI_Testany -32766 0 -32766 0 0 0.00184825
1: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.00188767
1: communicator.hh::sendRecv,MPI_Testany 4 1 -32766 0 0 0.00189111
1: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 4 1 -32766 0 0
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 5
1: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 4 1 -32766 0 0
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 5
1: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 4 1 -32766 0 0
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 5
1: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 4 1 -32766 0 0
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 5
1: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 4 1 -32766 0 0
Newton::solveLinearSystem : Caught exception from the linear solver when getting converged: "BCRSMatrixError [operator++:/home/m.giraud/DUMUXexud/dune-istl/dune/istl/bcrsmatrix.hh:940]: matrix already built up"
1 converged value: 0 121 1 let s go get convergedRemote
1 to  comm_.min(converged) 
1 did  comm_.min(converged) convergedRemote: 0
1 final convergedRemote: 0
1 about to throw Linear solver did not converge
1 Newton::solveLinearSystem : Caught exception: "NumericalProblem [solveLinearSystem:/home/m.giraud/DUMUXexud/dumux/dumux/nonlinear/newtonsolver.hh:381]: Linear solver did not converge when getting convergedRemote "
1 Newton: Caught exception: "NumericalProblem [solveLinearSystem:/home/m.giraud/DUMUXexud/dumux/dumux/nonlinear/newtonsolver.hh:381]: Linear solver did not converge"
1 Newton solver did not converge with dt = 0.0495 seconds. Retrying with time step of 0.02475 seconds
1 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
ting convergedRemote "
23 Newton: Caught exception: "NumericalProblem [solveLinearSystem:/home/m.giraud/DUMUXexud/dumux/dumux/nonlinear/newtonsolver.hh:381]: Linear solver did not converge"
23 Newton solver did not converge with dt = 0.0495 seconds. Retrying with time step of 0.02475 seconds
23 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
sendRecv, before assert(finished != MPI_UNDEFINED); 1 1 -32766 0 0
63: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 9
63: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 1 1 -32766 0 0
Newton::solveLinearSystem : Caught exception from the linear solver when getting converged: "BCRSMatrixError [operator++:/home/m.giraud/DUMUXexud/dune-istl/dune/istl/bcrsmatrix.hh:940]: matrix already built up"
63 converged value: 0 121 1 let s go get convergedRemote
63 to  comm_.min(converged) 
63 did  comm_.min(converged) convergedRemote: 0
63 final convergedRemote: 0
63 about to throw Linear solver did not converge
63 Newton::solveLinearSystem : Caught exception: "NumericalProblem [solveLinearSystem:/home/m.giraud/DUMUXexud/dumux/dumux/nonlinear/newtonsolver.hh:381]: Linear solver did not converge when getting convergedRemote "
63 Newton: Caught exception: "NumericalProblem [solveLinearSystem:/home/m.giraud/DUMUXexud/dumux/dumux/nonlinear/newtonsolver.hh:381]: Linear solver did not converge"
63 Newton solver did not converge with dt = 0.0495 seconds. Retrying with time step of 0.02475 seconds
63 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
--------------------------------------------------------------------------
Primary job  terminated normally, but 1 process returned
a non-zero exit code. Per user-direction, the job has been aborted.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
mpirun noticed that process rank 30 with PID 2789481 on node node03 exited on signal 11 (Segmentation fault).
--------------------------------------------------------------------------
