16: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
void presmooth(LevelContext& levelContext, size_t steps), before SmootherApplier<typename LevelContext::SmootherType>::preSmooth 
void preSmooth(Smoother& smoother, Domain& v, const Range& d), before smoother.apply(v,d);
29 schwarz.hh:virtual void apply (X& v, const Y& d), before communication.copyOwnerToAll(v,v); 
29 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
29: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 11
29: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 11
29: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 11
29: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 11
29: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 11
29: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 11
29: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 11
29: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 11
47: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 6
47: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 6
47: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 6
47: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 6
118 schwarz.hh:virtual void apply (X& v, const Y& d), before communication.copyOwnerToAll(v,v); 
118 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
118: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 14
118: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 14
118: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 14
118: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 14
118: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 14
118: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 14
118: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 14
118: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 14
118: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 14
118: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 14
118: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 10 14
118: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 11 14
118: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 12 14
118: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 13 14
118: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
112 schwarz.hh:virtual void apply (X& v, const Y& d), before communication.copyOwnerToAll(v,v); 
112 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
112: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 5
112: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 5
112: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 5
112: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 5
112: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 5
112: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
void presmooth(LevelContext& levelContext, size_t steps), before SmootherApplier<typename LevelContext::SmootherType>::preSmooth 
void preSmooth(Smoother& smoother, Domain& v, const Range& d), before smoother.apply(v,d);
23 schwarz.hh:virtual void apply (X& v, const Y& d), before communication.copyOwnerToAll(v,v); 
23 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
23: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 11
23: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 11
23: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 11
23: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 11
23: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 11
23: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 11
23: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 11
23: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 11
23: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 11
23: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 11
23: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 10 11
23: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
38: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 23 24
void presmooth(LevelContext& levelContext, size_t steps), before SmootherApplier<typename LevelContext::SmootherType>::preSmooth 
void preSmooth(Smoother& smoother, Domain& v, const Range& d), before smoother.apply(v,d);
31 schwarz.hh:virtual void apply (X& v, const Y& d), before communication.copyOwnerToAll(v,v); 
31 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
31: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 10
31: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 10
31: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 10
31: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 10
31: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 10
31: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 10
31: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 10
void presmooth(LevelContext& levelContext, size_t steps), before SmootherApplier<typename LevelContext::SmootherType>::preSmooth 
void preSmooth(Smoother& smoother, Domain& v, const Range& d), before smoother.apply(v,d);
46 schwarz.hh:virtual void apply (X& v, const Y& d), before communication.copyOwnerToAll(v,v); 
46 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
46: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 12
46: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 12
46: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 12
46: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 12
46: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 12
46: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 12
46: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 12
46: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 12
void presmooth(LevelContext& levelContext, size_t steps), before SmootherApplier<typename LevelContext::SmootherType>::preSmooth 
void preSmooth(Smoother& smoother, Domain& v, const Range& d), before smoother.apply(v,d);
4 schwarz.hh:virtual void apply (X& v, const Y& d), before communication.copyOwnerToAll(v,v); 
4 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 14
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 14
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 14
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 14
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 14
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 14
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 14
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 14
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 14
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 14
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 10 14
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 11 14
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 12 14
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 13 14
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
void presmooth(LevelContext& levelContext, size_t steps), before SmootherApplier<typename LevelContext::SmootherType>::preSmooth 
void preSmooth(Smoother& smoother, Domain& v, const Range& d), before smoother.apply(v,d);
21 schwarz.hh:virtual void apply (X& v, const Y& d), before communication.copyOwnerToAll(v,v); 
21 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
21: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 12
21: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 12
21: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 12
21: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 12
21: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 12
21: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 12
21: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 12
21: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 12
21: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 12
21: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 12
21: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 10 12
void presmooth(LevelContext& levelContext, size_t steps), before SmootherApplier<typename LevelContext::SmootherType>::preSmooth 
void preSmooth(Smoother& smoother, Domain& v, const Range& d), before smoother.apply(v,d);
94 schwarz.hh:virtual void apply (X& v, const Y& d), before communication.copyOwnerToAll(v,v); 
94 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
94: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 21
94: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 21
94: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 21
94: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 21
94: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 21
94: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 21
94: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 21
94: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 21
94: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 21
94: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 21
94: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 10 21
94: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 11 21
94: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 12 21
94: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 13 21
94: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 14 21
94: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 15 21
94: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 16 21
94: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 17 21
94: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 18 21
94: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 19 21
void presmooth(LevelContext& levelContext, size_t steps), before SmootherApplier<typename LevelContext::SmootherType>::preSmooth 
void preSmooth(Smoother& smoother, Domain& v, const Range& d), before smoother.apply(v,d);
106 schwarz.hh:virtual void apply (X& v, const Y& d), before communication.copyOwnerToAll(v,v); 
106 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
106: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 16
106: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 16
106: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 16
106: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 16
106: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 16
106: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 16
106: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 16
106: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 16
106: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 16
106: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 16
106: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 10 16
106: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 11 16
106: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 12 16
106: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 13 16
106: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 14 16
106: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 15 16
106: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
52: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 24 26
52: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 25 26
void presmooth(LevelContext& levelContext, size_t steps), before SmootherApplier<typename LevelContext::SmootherType>::preSmooth 
void preSmooth(Smoother& smoother, Domain& v, const Range& d), before smoother.apply(v,d);
27 schwarz.hh:virtual void apply (X& v, const Y& d), before communication.copyOwnerToAll(v,v); 
27 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
27: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 17
27: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 17
27: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 17
27: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 17
27: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 17
27: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 17
27: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 17
27: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 17
27: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 17
27: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 17
27: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 10 17
27: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 11 17
113 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
113: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 18
113: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 18
113: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 18
113: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 18
113: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 18
113: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 18
113: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 18
113: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 18
113: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 18
113: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 18
113: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 10 18
113: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 11 18
113: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 12 18
113: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 13 18
113: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 14 18
113: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 15 18
113: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 16 18
113: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 17 18
113: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
120: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 7
120: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 7
120: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 7
120: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 7
120: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
90: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 6
90: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 6
90: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 6
90: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 6
90: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 6
90 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
90: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 6
void presmooth(LevelContext& levelContext, size_t steps), before SmootherApplier<typename LevelContext::SmootherType>::preSmooth 
void preSmooth(Smoother& smoother, Domain& v, const Range& d), before smoother.apply(v,d);
111 schwarz.hh:virtual void apply (X& v, const Y& d), before communication.copyOwnerToAll(v,v); 
111 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
111: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 11
111: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 11
111: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 11
111: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 11
111: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 11
111: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 11
111: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 11
111: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 11
111: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 11
111: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 11
111: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 10 11
111: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
void presmooth(LevelContext& levelContext, size_t steps), before SmootherApplier<typename LevelContext::SmootherType>::preSmooth 
void preSmooth(Smoother& smoother, Domain& v, const Range& d), before smoother.apply(v,d);
10 schwarz.hh:virtual void apply (X& v, const Y& d), before communication.copyOwnerToAll(v,v); 
10 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
10: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 11
10: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 11
10: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 11
10: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 11
10: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 11
10: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 11
10: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 11
10: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 11
10: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 11
10: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 11
10: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 10 11
10: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
15 schwarz.hh:virtual void apply (X& v, const Y& d), before communication.copyOwnerToAll(v,v); 
15 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
15: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 8
15: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 8
15: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 8
15: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 8
15: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 8
15: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 8
15: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 8
15: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 8
15: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
28: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 18
28: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 18
28: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 18
28: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 18
28: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 18
28: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 10 18
28: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 11 18
28: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 12 18
28: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 13 18
28: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 14 18
28: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 15 18
28: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 16 18
28: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 17 18
void presmooth(LevelContext& levelContext, size_t steps), before SmootherApplier<typename LevelContext::SmootherType>::preSmooth 
void preSmooth(Smoother& smoother, Domain& v, const Range& d), before smoother.apply(v,d);
28 schwarz.hh:virtual void apply (X& v, const Y& d), before communication.copyOwnerToAll(v,v); 
28 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
28: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 18
28: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 18
28: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 18
28: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 18
28: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 18
28: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 18
28: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 18
28: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 18
28: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 18
28: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 18
28: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 10 18
28: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 11 18
28: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 12 18
28: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 13 18
28: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 14 18
117: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 18 22
117: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 19 22
117: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 20 22
117: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 21 22
void presmooth(LevelContext& levelContext, size_t steps), before SmootherApplier<typename LevelContext::SmootherType>::preSmooth 
void preSmooth(Smoother& smoother, Domain& v, const Range& d), before smoother.apply(v,d);
117 schwarz.hh:virtual void apply (X& v, const Y& d), before communication.copyOwnerToAll(v,v); 
117 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
117: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 22
117: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 22
117: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 22
117: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 22
117: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 22
117: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 22
117: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 22
117: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 22
117: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 22
117: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 22
117: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 10 22
117: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 11 22
117: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 12 22
117: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 13 22
117: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 14 22
117: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 15 22
117: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 16 22
117: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 17 22
117: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 18 22
117: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 19 22
117: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 20 22
117: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 21 22
117: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
void presmooth(LevelContext& levelContext, size_t steps), before SmootherApplier<typename LevelContext::SmootherType>::preSmooth 
void preSmooth(Smoother& smoother, Domain& v, const Range& d), before smoother.apply(v,d);
109 schwarz.hh:virtual void apply (X& v, const Y& d), before communication.copyOwnerToAll(v,v); 
109 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
109: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 20
109: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 20
109: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 20
109: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 20
109: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 20
109: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 20
109: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 20
109: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 20
109: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 20
109: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 20
109: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 10 20
109: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 11 20
109: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 12 20
109: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 13 20
109: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 14 20
109: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 15 20
109: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 16 20
109: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 17 20
109: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 18 20
109: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 19 20
109: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
14: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 13 17
14: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 14 17
14: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 15 17
14: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 16 17
void presmooth(LevelContext& levelContext, size_t steps), before SmootherApplier<typename LevelContext::SmootherType>::preSmooth 
void preSmooth(Smoother& smoother, Domain& v, const Range& d), before smoother.apply(v,d);
14 schwarz.hh:virtual void apply (X& v, const Y& d), before communication.copyOwnerToAll(v,v); 
14 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
14: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 17
14: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 17
14: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 17
14: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 17
14: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 17
14: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 17
14: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 17
14: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 17
14: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 17
14: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 17
14: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 10 17
14: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 11 17
14: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 12 17
14: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 13 17
110: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 11
110: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 10 11
void presmooth(LevelContext& levelContext, size_t steps), before SmootherApplier<typename LevelContext::SmootherType>::preSmooth 
void preSmooth(Smoother& smoother, Domain& v, const Range& d), before smoother.apply(v,d);
110 schwarz.hh:virtual void apply (X& v, const Y& d), before communication.copyOwnerToAll(v,v); 
110 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
110: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 11
110: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 11
110: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 11
110: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 11
110: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 11
110: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 11
110: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 11
110: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 11
110: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 11
110: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 11
110: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 10 11
110: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
8: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 19 20
void presmooth(LevelContext& levelContext, size_t steps), before SmootherApplier<typename LevelContext::SmootherType>::preSmooth 
void preSmooth(Smoother& smoother, Domain& v, const Range& d), before smoother.apply(v,d);
8 schwarz.hh:virtual void apply (X& v, const Y& d), before communication.copyOwnerToAll(v,v); 
8 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
8: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 20
8: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 20
8: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 20
8: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 20
8: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 20
8: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 20
8: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 20
8: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 20
8: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 20
8: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 20
8: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 10 20
8: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 11 20
8: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 12 20
8: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 13 20
8: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 14 20
8: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 15 20
8: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 16 20
8: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 17 20
8: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 18 20
8: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 19 20
8: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
void presmooth(LevelContext& levelContext, size_t steps), before SmootherApplier<typename LevelContext::SmootherType>::preSmooth 
void preSmooth(Smoother& smoother, Domain& v, const Range& d), before smoother.apply(v,d);
66 schwarz.hh:virtual void apply (X& v, const Y& d), before communication.copyOwnerToAll(v,v); 
66 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
66: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 15
66: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 15
66: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 15
66: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 15
66: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 15
66: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 15
66: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 15
66: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 15
66: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 15
66: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 15
66: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 10 15
116: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 7
116: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 7
116: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 7
116: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 7
116: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 7
116: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 7
116: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
42: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 10 20
42: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 11 20
49: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 15
49: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 10 15
107: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 5
107: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 5
107: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 5
107: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 5
107: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
65: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 9
void presmooth(LevelContext& levelContext, size_t steps), before SmootherApplier<typename LevelContext::SmootherType>::preSmooth 
void preSmooth(Smoother& smoother, Domain& v, const Range& d), before smoother.apply(v,d);
65 schwarz.hh:virtual void apply (X& v, const Y& d), before communication.copyOwnerToAll(v,v); 
65 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
65: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 9
65: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 9
65: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 9
65: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 9
65: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 9
65: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 9
65: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 9
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 5
void presmooth(LevelContext& levelContext, size_t steps), before SmootherApplier<typename LevelContext::SmootherType>::preSmooth 
void preSmooth(Smoother& smoother, Domain& v, const Range& d), before smoother.apply(v,d);
1 schwarz.hh:virtual void apply (X& v, const Y& d), before communication.copyOwnerToAll(v,v); 
1 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 5
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 5
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 5
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 5
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 5
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
79: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 14 16
79: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 15 16
void presmooth(LevelContext& levelContext, size_t steps), before SmootherApplier<typename LevelContext::SmootherType>::preSmooth 
void preSmooth(Smoother& smoother, Domain& v, const Range& d), before smoother.apply(v,d);
79 schwarz.hh:virtual void apply (X& v, const Y& d), before communication.copyOwnerToAll(v,v); 
79 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
79: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 16
79: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 16
79: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 16
79: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 16
79: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 16
79: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 16
79: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 16
79: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 16
79: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 16
79: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 16
79: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 10 16
void presmooth(LevelContext& levelContext, size_t steps), before SmootherApplier<typename LevelContext::SmootherType>::preSmooth 
void preSmooth(Smoother& smoother, Domain& v, const Range& d), before smoother.apply(v,d);
105 schwarz.hh:virtual void apply (X& v, const Y& d), before communication.copyOwnerToAll(v,v); 
105 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
105: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 9
105: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 9
105: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 9
105: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 9
105: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 9
105: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 9
105: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 9
105: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 9
105: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 9
105: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
78: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 10 15
78: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 11 15
18: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 13 14
void presmooth(LevelContext& levelContext, size_t steps), before SmootherApplier<typename LevelContext::SmootherType>::preSmooth 
void preSmooth(Smoother& smoother, Domain& v, const Range& d), before smoother.apply(v,d);
18 schwarz.hh:virtual void apply (X& v, const Y& d), before communication.copyOwnerToAll(v,v); 
18 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
18: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 14
18: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 14
18: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 14
18: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 14
18: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 14
18: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 14
18: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 14
18: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 14
18: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 14
18: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 14
18: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 10 14
18: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 11 14
18: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 12 14
18: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 13 14
18: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
89: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 11
89: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 11
89: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 11
89: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 11
89: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 11
77: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 6
void presmooth(LevelContext& levelContext, size_t steps), before SmootherApplier<typename LevelContext::SmootherType>::preSmooth 
void preSmooth(Smoother& smoother, Domain& v, const Range& d), before smoother.apply(v,d);
77 schwarz.hh:virtual void apply (X& v, const Y& d), before communication.copyOwnerToAll(v,v); 
77 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
77: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 6
26: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 10 26
26: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 11 26
26: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 12 26
26: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 13 26
26: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 14 26
26: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 15 26
26: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 16 26
26: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 17 26
26: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 18 26
26: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 19 26
26: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 20 26
26: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 21 26
26: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 22 26
26: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 23 26
26: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 24 26
26: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 25 26
void presmooth(LevelContext& levelContext, size_t steps), before SmootherApplier<typename LevelContext::SmootherType>::preSmooth 
void preSmooth(Smoother& smoother, Domain& v, const Range& d), before smoother.apply(v,d);
26 schwarz.hh:virtual void apply (X& v, const Y& d), before communication.copyOwnerToAll(v,v); 
26 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
26: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 26
26: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 26
26: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 26
26: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 26
26: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 26
26: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 26
26: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 26
26: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 26
26: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 26
26: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 26
26: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 10 26
26: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 11 26
26: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 12 26
26: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 13 26
26: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(),37: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 11 15
37: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 12 15
37: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 13 15
37: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 14 15
void presmooth(LevelContext& levelContext, size_t steps), before SmootherApplier<typename LevelContext::SmootherType>::preSmooth 
void preSmooth(Smoother& smoother, Domain& v, const Range& d), before smoother.apply(v,d);
37 schwarz.hh:virtual void apply (X& v, const Y& d), before communication.copyOwnerToAll(v,v); 
37 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
37: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 15
37: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 15
37: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 15
37: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 15
37: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 15
37: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 15
37: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 15
37: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 15
37: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 15
37: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 15
51: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 10
51: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 10
51: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 10
51: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 10
64: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 10 14
84: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 10 18
84: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 11 18
84: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 12 18
84: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 13 18
104: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 9
104: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 9
104: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 9
104: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 9
104: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 9
104: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 9
void presmooth(LevelContext& levelContext, size_t steps), before SmootherApplier<typename LevelContext::SmootherType>::preSmooth 
void preSmooth(Smoother& smoother, Domain& v, const Range& d), before smoother.apply(v,d);
104 schwarz.hh:virtual void apply (X& v, const Y& d), before communication.copyOwnerToAll(v,v); 
104 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
104: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 9
104: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 9
104: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 9
104: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 9
104: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 9
104: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 9
32: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 11 16
32: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 12 16
32: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 13 16
32: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 14 16
32: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 15 16
void presmooth(LevelContext& levelContext, size_t steps), before SmootherApplier<typename LevelContext::SmootherType>::preSmooth 
void preSmooth(Smoother& smoother, Domain& v, const Range& d), before smoother.apply(v,d);
32 schwarz.hh:virtual void apply (X& v, const Y& d), before communication.copyOwnerToAll(v,v); 
32 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
32: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 16
32: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 16
32: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 16
32: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 16
32: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 16
32: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 16
32: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 16
32: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 16
32: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 16
32: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 16
void presmooth(LevelContext& levelContext, size_t steps), before SmootherApplier<typename LevelContext::SmootherType>::preSmooth 
void preSmooth(Smoother& smoother, Domain& v, const Range& d), before smoother.apply(v,d);
30 schwarz.hh:virtual void apply (X& v, const Y& d), before communication.copyOwnerToAll(v,v); 
30 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
30: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 6
30: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 6
30: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 6
30: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 6
30: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 6
53: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 17
53: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 17
53: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 17
53: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 17
53: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 10 17
53: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 11 17
53: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 12 17
53: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 13 17
53: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 14 17
53: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 15 17
53: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 16 17
80: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 10
80: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 10
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 13 17
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 14 17
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 15 17
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 16 17
void presmooth(LevelContext& levelContext, size_t steps), before SmootherApplier<typename LevelContext::SmootherType>::preSmooth 
void preSmooth(Smoother& smoother, Domain& v, const Range& d), before smoother.apply(v,d);
6 schwarz.hh:virtual void apply (X& v, const Y& d), before communication.copyOwnerToAll(v,v); 
6 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 17
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 17
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 17
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 17
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 17
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 17
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 17
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 17
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 17
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 17
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 10 17
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 11 17
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 12 17
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 13 17
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 14 17
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 15 17
7: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 10 13
7: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 11 13
7: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 12 13
void presmooth(LevelContext& levelContext, size_t steps), before SmootherApplier<typename LevelContext::SmootherType>::preSmooth 
void preSmooth(Smoother& smoother, Domain& v, const Range& d), before smoother.apply(v,d);
7 schwarz.hh:virtual void apply (X& v, const Y& d), before communication.copyOwnerToAll(v,v); 
7 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
7: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 13
7: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 13
7: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 13
7: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 13
7: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 13
7: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 13
7: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 13
7: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 13
7: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 13
7: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 13
7: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 10 13
7: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 11 13
7: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 12 13
7: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
13: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 11
13: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 11
13: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 11
13: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 10 11
void presmooth(LevelContext& levelContext, size_t steps), before SmootherApplier<typename LevelContext::SmootherType>::preSmooth 
void preSmooth(Smoother& smoother, Domain& v, const Range& d), before smoother.apply(v,d);
13 schwarz.hh:virtual void apply (X& v, const Y& d), before communication.copyOwnerToAll(v,v); 
13 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
13: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 11
13: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 11
13: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 11
13: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 11
13: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 11
13: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 11
13: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 11
13: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 11
13: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 11
13: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 11
void presmooth(LevelContext& levelContext, size_t steps), before SmootherApplier<typename LevelContext::SmootherType>::preSmooth 
void preSmooth(Smoother& smoother, Domain& v, const Range& d), before smoother.apply(v,d);
91 schwarz.hh:virtual void apply (X& v, const Y& d), before communication.copyOwnerToAll(v,v); 
91 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
91: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 9
91: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 9
91: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 9
91: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 9
91: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 9
91: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 9
91: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 9
91: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 9
91: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 9
91: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
19: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 10 17
19: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 11 17
19: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 12 17
19: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 13 17
19: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 14 17
19: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 15 17
19: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 16 17
void presmooth(LevelContext& levelContext, size_t steps), before SmootherApplier<typename LevelContext::SmootherType>::preSmooth 
void preSmooth(Smoother& smoother, Domain& v, const Range& d), before smoother.apply(v,d);
19 schwarz.hh:virtual void apply (X& v, const Y& d), before communication.copyOwnerToAll(v,v); 
19 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
19: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 17
19: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 17
19: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 17
19: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 17
19: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 17
19: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 17
19: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 17
19: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 17
19: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 17
19: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 17
19: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 10 17
19: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 11 17
19: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 12 17
19: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 13 17
19: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 14 17
92: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 14 16
92: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 15 16
void presmooth(LevelContext& levelContext, size_t steps), before SmootherApplier<typename LevelContext::SmootherType>::preSmooth 
void preSmooth(Smoother& smoother, Domain& v, const Range& d), before smoother.apply(v,d);
92 schwarz.hh:virtual void apply (X& v, const Y& d), before communication.copyOwnerToAll(v,v); 
92 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
92: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 16
92: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 16
92: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 16
92: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 16
92: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 16
92: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 16
92: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 16
92: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 16
92: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 16
92: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 16
92: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 10 16
92: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 11 16
92: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 12 16
92: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 13 16
92: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 14 16
92: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 15 16
void presmooth(LevelContext& levelContext, size_t steps), before SmootherApplier<typename LevelContext::SmootherType>::preSmooth 
void preSmooth(Smoother& smoother, Domain& v, const Range& d), before smoother.apply(v,d);
39 schwarz.hh:virtual void apply (X& v, const Y& d), before communication.copyOwnerToAll(v,v); 
39 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
39: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 9
39: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 9
39: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 9
39: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 9
39: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 9
39: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 9
void presmooth(LevelContext& levelContext, size_t steps), before SmootherApplier<typename LevelContext::SmootherType>::preSmooth 
void preSmooth(Smoother& smoother, Domain& v, const Range& d), before smoother.apply(v,d);
20 schwarz.hh:virtual void apply (X& v, const Y& d), before communication.copyOwnerToAll(v,v); 
20 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
20: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 10
20: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 10
20: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 10
20: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 10
20: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 10
20: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 10
20: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 10
20: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 10
20: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 10
20: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 10
20: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
86: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 18 31
86: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 19 31
86: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 20 31
86: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 21 31
86: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 22 31
86: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 23 31
86: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 24 31
67: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 13
67: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 13
void presmooth(LevelContext& levelContext, size_t steps), before SmootherApplier<typename LevelContext::SmootherType>::preSmooth 
void preSmooth(Smoother& smoother, Domain& v, const Range& d), before smoother.apply(v,d);
93 schwarz.hh:virtual void apply (X& v, const Y& d), before communication.copyOwnerToAll(v,v); 
93 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
93: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 13
93: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 13
93: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 13
93: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 13
93: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 13
93: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 13
93: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 13
93: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 13
93: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 13
93: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 13
93: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 10 13
93: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 11 13
93: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 12 13
93: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
12: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 23 26
12: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 24 26
12: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 25 26
void presmooth(LevelContext& levelContext, size_t steps), before SmootherApplier<typename LevelContext::SmootherType>::preSmooth 
void preSmooth(Smoother& smoother, Domain& v, const Range& d), before smoother.apply(v,d);
12 schwarz.hh:virtual void apply (X& v, const Y& d), before communication.copyOwnerToAll(v,v); 
12 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
12: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 26
12: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 26
12: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 26
12: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 26
12: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 26
12: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 26
12: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 26
12: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 26
12: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 26
12: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 26
12: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 10 26
12: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 11 26
12: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 12 26
12: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 13 26
12: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 14 26
12: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 15 26
12: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 16 26
12: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 17 26
12: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 18 26
12: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 19 26
12: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 20 26
12: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 21 26
12: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 22 26
void presmooth(LevelContext& levelContext, size_t steps), before SmootherApplier<typename LevelContext::SmootherType>::preSmooth 
void preSmooth(Smoother& smoother, Domain& v, const Range& d), before smoother.apply(v,d);
95 schwarz.hh:virtual void apply (X& v, const Y& d), before communication.copyOwnerToAll(v,v); 
95 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
95: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 7
95: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 7
95: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 7
95: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 7
95: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 7
95: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 7
95: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 7
95: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
void presmooth(LevelContext& levelContext, size_t steps), before SmootherApplier<typename LevelContext::SmootherType>::preSmooth 
void preSmooth(Smoother& smoother, Domain& v, const Range& d), before smoother.apply(v,d);
115 schwarz.hh:virtual void apply (X& v, const Y& d), before communication.copyOwnerToAll(v,v); 
115 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
115: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 11
115: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 11
115: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 11
115: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 11
115: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 11
115: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 11
115: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 11
115: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 11
115: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 11
115: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 11
115: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 10 11
115: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
101: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 19 24
101: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 20 24
101: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 21 24
101: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 22 24
101: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 23 24
void presmooth(LevelContext& levelContext, size_t steps), before SmootherApplier<typename LevelContext::SmootherType>::preSmooth 
void preSmooth(Smoother& smoother, Domain& v, const Range& d), before smoother.apply(v,d);
101 schwarz.hh:virtual void apply (X& v, const Y& d), before communication.copyOwnerToAll(v,v); 
101 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
101: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 24
101: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 24
101: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 24
101: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 24
101: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 24
101: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 24
101: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 24
101: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 24
101: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 24
101: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 24
101: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 10 24
101: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 11 24
101: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 12 24
101: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 13 24
101: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 14 24
101: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 15 24
101: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 16 24
101: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 17 24
101: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 18 24
101: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 19 24
101: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 20 24
99: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 11 13
99: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 12 13
void presmooth(LevelContext& levelContext, size_t steps), before SmootherApplier<typename LevelContext::SmootherType>::preSmooth 
void preSmooth(Smoother& smoother, Domain& v, const Range& d), before smoother.apply(v,d);
99 schwarz.hh:virtual void apply (X& v, const Y& d), before communication.copyOwnerToAll(v,v); 
99 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
99: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 13
99: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 13
99: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 13
99: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 13
99: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 13
99: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 13
99: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 13
99: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 13
99: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 13
99: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 13
99: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 10 13
99: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 11 13
43: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 11
43: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 11
43: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 11
43: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 11
43: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 11
43: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 11
void presmooth(LevelContext& levelContext, size_t steps), before SmootherApplier<typename LevelContext::SmootherType>::preSmooth 
void preSmooth(Smoother& smoother, Domain& v, const Range& d), before smoother.apply(v,d);
72 schwarz.hh:virtual void apply (X& v, const Y& d), before communication.copyOwnerToAll(v,v); 
72 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
72: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 20
72: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 20
72: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 20
72: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 20
72: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 20
72: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 20
72: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 20
72: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 20
72: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 20
72: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 20
72: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 10 20
72: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 11 20
72: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 12 20
72: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 13 20
 recvRequests, &finished, &status); 14 26
26: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 15 26
26: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 16 26
26: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 17 26
26: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 18 26
26: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 19 26
26: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 20 26
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 26 117
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 27 117
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 28 117
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 29 117
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 30 117
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 31 117
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 32 117
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 33 117
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 34 117
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 35 117
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 36 117
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 37 117
Newton::solveLinearSystem : Caught exception from the linear solver when getting converged: "FMatrixError [luDecomposition:/home/m.giraud/DUMUXexud/dune-common/dune/common/densematrix.hh:909]: matrix is singular"
68 converged value: 0 121 1 let s go get convergedRemote
68 to  comm_.min(converged) 
Newton::solveLinearSystem : Caught exception from the linear solver when getting converged: "FMatrixError [luDecomposition:/home/m.giraud/DUMUXexud/dune-common/dune/common/densematrix.hh:909]: matrix is singular"
62 converged value: 0 121 1 let s go get convergedRemote
62 to  comm_.min(converged) 
Newton::solveLinearSystem : Caught exception from the linear solver when getting converged: "FMatrixError [luDecomposition:/home/m.giraud/DUMUXexud/dune-common/dune/common/densematrix.hh:909]: matrix is singular"
82 converged value: 0 121 1 let s go get convergedRemote
82 to  comm_.min(converged) 
Newton::solveLinearSystem : Caught exception from the linear solver when getting converged: "FMatrixError [luDecomposition:/home/m.giraud/DUMUXexud/dune-common/dune/common/densematrix.hh:909]: matrix is singular"
75 converged value: 0 121 1 let s go get convergedRemote
75 to  comm_.min(converged) 
Newton::solveLinearSystem : Caught exception from the linear solver when getting converged: "FMatrixError [luDecomposition:/home/m.giraud/DUMUXexud/dune-common/dune/common/densematrix.hh:909]: matrix is singular"
70 converged value: 0 121 1 let s go get convergedRemote
70 to  comm_.min(converged) 
Newton::solveLinearSystem : Caught exception from the linear solver when getting converged: "FMatrixError [luDecomposition:/home/m.giraud/DUMUXexud/dune-common/dune/common/densematrix.hh:909]: matrix is singular"
83 converged value: 0 121 1 let s go get convergedRemote
83 to  comm_.min(converged) 
Newton::solveLinearSystem : Caught exception from the linear solver when getting converged: "FMatrixError [luDecomposition:/home/m.giraud/DUMUXexud/dune-common/dune/common/densematrix.hh:909]: matrix is singular"
44 converged value: 0 121 1 let s go get convergedRemote
44 to  comm_.min(converged) 
Newton::solveLinearSystem : Caught exception from the linear solver when getting converged: "FMatrixError [luDecomposition:/home/m.giraud/DUMUXexud/dune-common/dune/common/densematrix.hh:909]: matrix is singular"
57 converged value: 0 121 1 let s go get convergedRemote
57 to  comm_.min(converged) 
Newton::solveLinearSystem : Caught exception from the linear solver when getting converged: "FMatrixError [luDecomposition:/home/m.giraud/DUMUXexud/dune-common/dune/common/densematrix.hh:909]: matrix is singular"
85 converged value: 0 121 1 let s go get convergedRemote
85 to  comm_.min(converged) 
Newton::solveLinearSystem : Caught exception from the linear solver when getting converged: "FMatrixError [luDecomposition:/home/m.giraud/DUMUXexud/dune-common/dune/common/densematrix.hh:909]: matrix is singular"
36 converged value: 0 121 1 let s go get convergedRemote
36 to  comm_.min(converged) 
