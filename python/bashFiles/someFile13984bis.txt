52: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 13 26
52: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 14 26
52: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 15 26
52: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 16 26
52: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 17 26
52: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 18 26
52: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 19 26
52: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 20 26
52: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 21 26
52: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 22 26
52: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 23 26
52: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 24 26
52: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 25 26
52 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
52: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 26
52: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 26
52: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 26
52: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 26
52: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 26
52: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 26
52: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 26
73: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 11
73: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 11
73: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 11
73: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 10 11
73 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
73: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 11
73: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 11
73: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 11
73: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 11
73: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 11
63 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
63: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 9
63: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 9
63: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 9
63: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 9
63: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 9
63: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 9
54: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 10 18
54: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 11 18
54: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 12 18
54: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 13 18
54: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 14 18
54: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 15 18
54: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 16 18
54: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 17 18
54 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
54: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 18
54: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 18
54: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 18
54: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 18
54: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 18
113: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 16 18
113: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 17 18
113 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
113: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 18
113: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 18
113: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 18
113: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 18
113: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 18
113: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 18
113: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 18
113: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 18
113: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 18
113: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 18
113: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 10 18
113: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 11 18
113: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 12 18
113: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 13 18
39 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
39: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 9
39: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 9
39: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 9
39: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 9
39: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 9
39: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 9
39: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 9
15: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 8
15: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 8
15: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 8
15: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 8
15: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 8
15: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 8
15: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 8
15: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 8
111 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
111: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 11
111: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 11
111: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 11
111: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 11
111: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 11
111: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 11
111: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 11
111: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 11
111: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 11
111: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 11
90: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 6
90: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 6
90: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 6
90: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 6
90: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 6
10 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
10: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 11
10: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 11
10: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 11
10: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 11
10: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 11
10: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 11
10: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 11
10: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 11
10: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 11
10: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 11
75: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 15 17
75: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 16 17
75 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
75: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 17
75: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 17
75: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 17
75: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 17
75: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 17
75: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 17
75: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 17
117: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 16 22
117: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 17 22
117: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 18 22
117: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 19 22
117: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 20 22
117: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 21 22
117 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
117: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 22
117: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 22
117: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 22
117: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 22
117: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 22
117: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 22
117: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 22
117: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 22
117: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 22
117: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 22
117: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 10 22
117: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 11 22
117: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 12 22
117: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 13 22
55: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 11
55: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 11
55: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 11
55: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 11
55: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 10 11
55 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
55: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 11
55: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 11
55: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 11
109 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
109: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 20
109: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 20
109: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 20
109: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 20
109: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 20
109: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 20
109: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 20
109: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 20
109: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 20
109: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 20
109: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 10 20
109: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 11 20
109: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 12 20
109: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 13 20
109: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 14 20
109: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 15 20
79 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
79: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 16
79: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 16
79: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 16
79: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 16
79: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 16
79: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 16
79: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 16
79: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 16
79: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 16
79: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 16
79: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 10 16
79: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 11 16
79: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 12 16
14 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
14: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 17
14: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 17
14: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 17
14: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 17
14: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 17
14: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 17
14: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 17
14: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 17
14: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 17
14: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 17
14: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 10 17
14: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 11 17
14: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 12 17
28: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 11 18
28: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 12 18
28: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 13 18
28: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 14 18
28: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 15 18
28: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 16 18
28: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 17 18
28 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
28: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 18
28: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 18
28: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 18
28: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 18
28: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 18
28: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 18
110 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
110: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 11
110: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 11
110: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 11
110: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 11
110: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 11
110: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 11
110: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 11
110: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 11
110: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 11
110: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 11
110: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 10 11
101 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
101: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 24
101: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 24
101: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 24
101: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 24
101: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 24
101: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 24
101: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 24
101: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 24
101: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 24
101: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 24
101: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 10 24
101: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 11 24
101: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 12 24
108 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
108: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 9
108: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 9
108: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 9
108: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 9
108: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 9
108: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 9
108: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 9
108: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 9
108: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 9
23: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 11
23: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 11
23: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 11
23: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 11
23: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 11
23: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 11
23: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 11
23: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 11
83: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 14 17
83: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 15 17
83: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 16 17
83 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
83: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 17
83: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 17
83: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 17
83: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 17
83: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 17
83: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 17
83: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 17
83: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 17
83: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 17
5 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 11
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 11
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 11
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 11
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 11
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 11
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 11
42 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
42: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 20
42: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 20
42: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 20
42: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 20
42: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 20
42: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 20
42: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 20
42: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 20
42: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 20
42: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 20
42: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 10 20
42: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 11 20
42: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 12 20
42: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 13 20
42: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 14 20
44 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
44: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 11
44: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 11
44: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 11
44: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 11
44: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 11
44: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 11
44: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 11
44: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 11
44: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 11
44: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 11
44: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 10 11
116 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
116: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 7
116: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 7
116: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 7
116: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 7
116: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 7
116: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 7
116: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 7
65 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
65: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 9
65: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 9
65: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 9
65: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 9
65: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 9
65: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 9
65: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 9
65: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 9
65: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 9
105 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
105: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 9
105: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 9
105: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 9
105: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 9
105: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 9
105: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 9
105: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 9
105: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 9
85: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 10 11
85 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
85: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 11
85: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 11
85: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 11
85: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 11
85: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 11
85: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 11
89: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 11
89: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 11
89: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 10 11
89 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
89: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 11
89: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 11
89: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 11
89: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 11
26: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 20 26
26: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 21 26
26: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 22 26
26: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 23 26
26: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 24 26
26: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 25 26
26 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
26: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 26
26: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 26
26: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 26
26: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 26
26: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 26
26: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 26
26: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 26
26: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 26
26: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 26
26: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 26
26: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 10 26
26: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 11 26
26: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 12 26
26: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 13 26
26: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 14 26
37: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 14 15
37 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
37: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 15
37: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 15
37: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 15
37: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 15
37: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 15
37: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 15
37: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 15
37: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 15
37: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 15
37: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 15
37: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 10 15
84: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 15 18
84: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 16 18
84: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 17 18
84 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
84: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 18
84: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 18
84: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 18
84: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 18
84: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 18
84: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 18
84: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 18
84: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 18
84: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 18
84: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 18
32: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 13 16
32: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 14 16
32: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 15 16
32 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
32: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 16
32: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 16
32: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 16
32: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 16
32: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 16
32: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 16
32: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 16
32: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 16
80 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
80: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 10
80: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 10
80: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 10
80: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 10
80: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 10
80: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 10
80: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 10
13 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
13: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 11
13: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 11
13: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 11
13: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 11
13: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 11
13: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 11
13: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 11
13: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 11
13: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 11
7 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
7: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 13
7: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 13
7: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 13
7: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 13
7: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 13
7: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 13
7: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 13
7: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 13
7: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 13
7: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 13
7: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 10 13
7: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 11 13
7: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 12 13
91 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
91: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 9
91: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 9
91: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 9
91: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 9
91: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 9
91: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 9
91: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 9
91: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 9
91: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 9
86: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 28 31
86: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 29 31
86: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 30 31
86 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
86: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 31
86: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 31
86: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 31
86: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 31
86: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 31
86: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 31
86: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 31
86: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 31
86: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 31
86: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 31
86: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 10 31
86: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 11 31
86: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 12 31
86: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 13 31
86: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 14 31
86: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 15 31
86: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 16 31
86: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 17 31
86: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 18 31
86: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 19 31
20 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
20: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 10
20: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 10
20: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 10
20: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 10
20: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 10
20: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 10
20: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 10
20: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 10
20: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 10
20: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 10
76 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
76: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 12
76: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 12
76: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 12
76: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 12
76: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 12
76: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 12
76: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 12
67 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
67: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 13
67: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 13
67: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 13
67: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 13
67: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 13
67: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 13
67: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 13
67: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 13
67: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 13
67: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 13
67: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 10 13
67: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 11 13
67: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 12 13
93 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
93: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 13
93: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 13
93: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 13
93: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 13
93: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 13
93: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 13
93: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 13
93: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 13
93: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 13
93: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 13
93: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 10 13
93: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 11 13
93: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 12 13
12 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
12: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 26
12: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 26
12: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 26
12: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 26
12: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 26
12: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 26
12: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 26
12: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 26
12: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 26
12: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 26
12: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 10 26
12: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 11 26
12: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 12 26
12: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 13 26
12: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 14 26
12: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 15 26
12: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 16 26
12: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 17 26
12: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 18 26
12: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 19 26
12: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 20 26
12: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 21 26
12: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 22 26
12: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 23 26
12: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 24 26
70 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
70: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 13
70: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 13
70: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 13
70: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 13
70: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 13
70: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 13
70: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 13
70: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 13
70: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 13
95 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
95: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 7
95: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 7
95: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 7
95: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 7
95: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 7
95: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 7
95: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 7
115 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
115: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 11
115: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 11
115: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 11
115: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 11
115: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 11
115: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 11
115: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 11
115: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 11
115: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 11
115: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 11
115: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 10 11
99 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
99: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 13
99: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 13
99: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 13
99: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 13
99: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 13
99: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 13
99: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 13
99: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 13
99: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 13
43 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
43: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 11
43: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 11
43: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 11
43: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 11
43: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 11
43: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 11
43: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 11
43: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 11
43: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 11
72 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
72: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 20
72: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 20
72: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 20
72: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 20
72: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 20
72: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 20
72: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 20
72: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 20
72: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 20
72: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 20
72: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 10 20
72: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 11 20
72: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 12 20
72: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 13 20
72: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 14 20
72: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 15 20
72: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 16 20
72: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 17 20
98 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
98: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 18
98: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 18
98: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 18
98: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 18
98: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 18
98: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 18
98: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 18
98: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 18
98: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 18
98: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 18
98: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 10 18
98: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 11 18
98: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 12 18
98: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 13 18
98: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 14 18
98: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 15 18
98: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 16 18
120: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 7
120: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 7
120: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 7
120 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
120: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 7
120: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 7
120: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 7
120: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 7
120: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 7
120: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 7
120: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 7
111: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 10 11
10: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 10 11
75: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 17
75: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 17
75: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 17
75: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 10 17
75: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 11 17
75: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 12 17
75: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 13 17
75: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 14 17
75: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 15 17
75: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 16 17
117: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 14 22
117: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 15 22
117: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 16 22
117: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 17 22
117: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 18 22
117: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 19 22
117: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 20 22
117: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 21 22
55: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 11
55: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 11
55: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 11
55: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 11
55: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 11
55: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 11
55: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 11
55: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 10 11
109: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 16 20
109: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 17 20
109: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 18 20
109: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 19 20
79: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 13 16
79: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 14 16
79: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 15 16
14: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 13 17
14: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 14 17
14: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 15 17
14: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 16 17
28: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 18
28: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 18
28: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 18
28: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 18
28: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 10 18
28: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 11 18
28: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 12 18
28: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 13 18
28: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 14 18
28: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 15 18
28: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 16 18
28: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 17 18
101: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 13 24
101: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 14 24
101: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 15 24
101: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 16 24
101: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 17 24
101: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 18 24
101: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 19 24
101: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 20 24
101: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 21 24
101: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 22 24
101: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 23 24
23: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 11
23: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 11
23: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 10 11
83: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 17
83: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 10 17
83: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 11 17
83: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 12 17
83: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 13 17
83: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 14 17
83: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 15 17
83: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 16 17
42: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 15 20
42: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 16 20
42: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 17 20
42: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 18 20
42: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 19 20
49 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
49: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 15
49: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 15
49: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 15
49: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 15
49: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 15
49: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 15
49: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 15
49: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 15
49: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 15
49: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 15
49: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 10 15
49: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 11 15
49: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 12 15
49: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 13 15
49: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 14 15
107 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
107: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 5
107: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 5
107: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 5
107: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 5
107: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 5
57 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
57: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 19
57: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 19
57: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 19
57: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 19
57: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 19
57: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 19
57: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 19
57: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 19
57: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 19
57: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 19
57: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 10 19
57: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 11 19
57: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 12 19
57: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 13 19
57: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 14 19
57: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 15 19
57: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 16 19
57: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 17 19
57: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 18 19
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 5
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 5
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 5
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 5
105: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 9
78 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
78: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 15
78: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 15
78: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 15
78: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 15
78: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 15
78: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 15
78: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 15
78: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 15
78: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 15
78: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 15
78: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 10 15
78: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 11 15
78: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 12 15
78: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 13 15
78: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 14 15
18 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
18: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 14
18: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 14
18: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 14
18: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 14
18: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 14
18: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 14
18: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 14
18: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 14
18: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 14
18: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 14
18: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 10 14
18: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 11 14
18: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 12 14
18: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 13 14
85: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 11
85: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 11
85: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 11
85: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 11
85: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 10 11
89: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 11
89: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 11
89: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 11
89: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 11
89: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 11
89: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 11
89: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 10 11
77 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
77: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 6
77: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 6
77: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 6
77: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 6
77: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 6
77: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 6
36 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
36: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 14
36: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 14
36: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 14
36: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 14
36: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 14
36: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 14
36: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 14
36: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 14
36: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 14
36: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 14
36: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 10 14
36: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 11 14
36: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 12 14
36: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 13 14
26: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 15 26
26: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 16 26
26: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 17 26
26: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 18 26
26: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 19 26
26: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 20 26
26: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 21 26
26: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 22 26
26: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 23 26
26: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 24 26
26: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 25 26
37: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 11 15
37: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 12 15
37: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 13 15
37: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 14 15
51 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
51: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 10
51: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 10
51: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 10
51: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 10
51: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 10
51: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 10
51: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 10
51: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 10
51: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 10
51: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 10
64 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
64: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 14
64: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 14
64: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 14
64: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 14
64: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 14
64: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 14
64: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 14
64: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 14
64: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 14
64: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 14
64: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 10 14
64: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 11 14
64: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 12 14
64: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 13 14
84: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 10 18
84: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 11 18
84: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 12 18
84: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 13 18
84: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 14 18
84: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 15 18
84: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 16 18
84: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 17 18
61 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
61: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 9
61: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 9
61: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 9
61: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 9
61: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 9
61: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 9
61: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 9
61: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 9
61: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 9
104 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
104: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 9
104: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 9
104: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 9
104: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 9
104: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 9
104: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 9
104: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 9
104: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 9
104: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 9
32: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 16
32: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 16
32: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 10 16
32: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 11 16
32: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 12 16
32: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 13 16
32: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 14 16
32: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 15 16
30: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 6
30: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 6
30: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 6
30: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 6
30: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 6
53 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
53: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 17
53: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 17
53: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 17
53: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 17
53: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 17
53: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 17
53: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 17
53: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 17
53: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 17
53: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 17
53: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 10 17
53: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 11 17
53: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 12 17
53: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 13 17
53: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 14 17
53: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 15 17
53: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 16 17
80: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 10
80: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 10
80: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 10
6 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 17
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 17
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 17
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 17
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 17
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 17
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 17
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 17
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 17
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 17
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 10 17
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 11 17
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 12 17
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 13 17
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 14 17
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 15 17
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 16 17
13: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 11
13: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 10 11
19 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
19: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 17
19: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 17
19: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 17
19: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 17
19: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 17
19: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 17
19: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 17
19: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 17
19: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 17
19: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 17
19: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 10 17
19: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 11 17
19: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 12 17
19: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 13 17
19: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 14 17
19: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 15 17
19: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 16 17
92 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
92: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 16
92: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 16
92: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 16
92: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 16
92: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 16
92: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 16
92: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 16
92: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 16
92: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 16
92: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 16
92: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 10 16
92: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 11 16
92: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 12 16
92: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 13 16
92: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 14 16
92: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 15 16
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 3
86: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 20 31
86: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 21 31
86: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 22 31
86: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 23 31
86: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 24 31
86: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 25 31
86: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 26 31
86: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 27 31
86: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 28 31
86: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 29 31
86: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 30 31
76: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 12
76: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 12
76: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 12
76: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 10 12
76: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 11 12
12: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 25 26
70: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 13
70: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 10 13
70: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 11 13
70: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 12 13
99: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 13
99: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 10 13
99: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 11 13
99: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 12 13
43: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 11
43: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 10 11
72: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 18 20
72: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 19 20
98: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 17 18
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 11
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 11
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 11
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 10 11
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 8
60: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 6
60: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 6
60: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 6
60: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 6
22: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 20
22: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 20
22: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 20
22: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 10 20
22: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 11 20
22: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 12 20
22: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 13 20
22: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 14 20
22: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 15 20
22: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 16 20
22: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 17 20
22: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 18 20
22: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 19 20
45: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 17
45: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 17
45: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 17
45: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 17
45: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 17
45: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 17
45: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 17
45: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 17
45: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 10 17
45: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 11 17
45: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 12 17
45: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 13 17
45: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 14 17
45: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 15 17
45: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 16 17
102: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 15
102: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 15
102: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 15
102: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 15
102: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 15
102: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 10 15
102: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 11 15
102: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 12 15
102: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 13 15
102: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 14 15
41: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 20
41: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 20
41: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 20
41: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 10 20
41: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 11 20
41: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 12 20
41: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 13 20
41: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 14 20
41: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 15 20
41: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 16 20
41: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 17 20
41: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 18 20
41: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 19 20
100 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
100: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 6
100: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 6
100: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 6
100: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 6
100: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 6
100: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 6
119: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 13
119: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 13
119: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 13
119: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 13
119: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 10 13
119: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 11 13
119: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 12 13
56: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 31
56: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 31
56: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 31
56: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 31
56: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 31
56: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 31
56: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 31
56: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 31
56: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 10 31
56: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 11 31
56: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 12 31
56: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 13 31
56: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 14 31
56: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 15 31
56: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 16 31
56: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 17 31
56: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 18 31
56: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 19 31
56: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 20 31
56: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 21 31
56: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 22 31
56: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 23 31
56: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 24 31
56: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 25 31
56: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 26 31
56: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 27 31
56: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 28 31
56: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 29 31
56: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 30 31
74: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 11
74: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 11
74: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 11
74: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 11
74: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 11
74: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 11
74: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 11
74: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 10 11
9: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 7
9: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 7
9: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 7
9: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 7
9: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 7
9: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 7
88: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 21
88: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 21
88: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 21
88: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 21
88: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 21
88: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 21
88: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 10 21
88: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 11 21
88: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 12 21
88: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 13 21
88: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 14 21
88: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 15 21
88: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 16 21
88: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 17 21
88: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 18 21
88: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 19 21
88: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 20 21
11 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
11: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 17
11: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 17
11: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 17
11: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 17
11: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 17
11: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 17
11: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 17
11: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 17
11: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 17
11: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 17
11: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 10 17
11: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 11 17
11: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 12 17
11: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 13 17
11: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 14 17
11: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 15 17
11: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 16 17
58: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 21
58: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 21
58: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 21
58: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 21
58: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 21
58: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 21
58: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 21
58: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 21
58: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 21
58: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 10 21
58: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 11 21
58: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 12 21
58: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 13 21
58: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 14 21
58: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 15 21
58: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 16 21
58: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 17 21
58: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 18 21
58: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 19 21
58: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 20 21
87: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 19
87: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 19
87: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 19
87: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 19
87: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 19
87: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 19
87: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 10 19
87: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 11 19
87: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 12 19
87: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 13 19
87: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 14 19
87: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 15 19
87: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 16 19
87: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 17 19
87: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 18 19
97: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 9
97: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 9
97: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 9
97: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 9
114: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 13
114: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 13
114: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 13
114: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 13
114: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 10 13
114: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 11 13
114: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 12 13
34: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 14
34: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 14
34: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 14
34: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 14
34: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 14
34: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 10 14
34: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 11 14
34: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 12 14
34: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 13 14
24: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 14
24: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 14
24: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 10 14
24: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 11 14
24: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 12 14
24: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 13 14
103: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 16
103: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 16
103: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 16
103: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 16
103: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 16
103: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 10 16
103: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 11 16
103: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 12 16
103: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 13 16
103: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 14 16
103: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 15 16
25: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 7
25: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 7
25: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 7
81: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 11
81: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 11
81: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 11
81: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 11
81: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 11
81: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 11
81: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 11
81: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 10 11
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 12 14
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 13 14
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 9
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 9
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 9
118: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 14
118: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 14
118: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 14
118: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 14
118: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 14
118: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 14
118: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 10 14
118: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 11 14
118: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 12 14
118: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 13 14
96: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 12
96: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 12
96: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 12
96: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 12
96: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 12
96: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 12
96: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 12
96: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 10 12
96: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 11 12
50: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 10
50: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 10
50: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 10
50: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 10
50: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 10
50: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 10
50: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 10
50: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 10
50: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 10
68 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
68: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 22
68: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 22
68: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 22
68: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 22
68: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 22
68: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 22
68: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 22
68: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 22
68: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 22
68: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 22
68: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 10 22
68: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 11 22
68: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 12 22
68: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 13 22
68: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 14 22
68: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 15 22
68: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 16 22
68: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 17 22
68: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 18 22
68: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 19 22
68: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 20 22
68: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 21 22
29: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 11
29: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 11
29: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 11
29: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 11
29: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 11
29: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 10 11
47: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 6
47: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 6
47: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 6
47: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 6
47: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 6
71: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 20
71: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 10 20
71: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 11 20
71: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 12 20
71: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 13 20
71: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 14 20
71: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 15 20
71: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 16 20
71: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 17 20
71: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 18 20
71: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 19 20
62: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 15
62: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 15
62: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 15
62: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 15
62: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 15
62: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 10 15
62: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 11 15
62: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 12 15
62: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 13 15
62: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 14 15
66: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 12 15
66: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 13 15
66: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 14 15
112: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 5
112: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 5
112: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 5
38: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 24
38: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 24
38: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 10 24
38: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 11 24
38: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 12 24
38: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 13 24
38: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 14 24
38: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 15 24
38: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 16 24
38: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 17 24
38: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 18 24
38: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 19 24
38: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 20 24
38: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 21 24
38: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 22 24
38: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 23 24
21: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 12
21: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 12
21: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 12
21: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 12
21: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 12
21: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 12
21: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 12
21: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 10 12
21: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 11 12
69: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 7
31: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 10
31: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 10
31: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 10
31: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 10
31: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 10
31: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 10
46: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 12
46: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 12
46: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 12
46: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 12
46: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 12
46: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 12
46: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 12
46: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 12
46: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 12
46: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 10 12
46: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 11 12
8: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 17 20
8: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 18 20
8: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 19 20
40: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 15
40: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 15
40: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 15
40: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 15
40: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 15
40: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 15
40: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 15
40: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 10 15
40: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 11 15
40: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 12 15
40: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 13 15
40: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 14 15
94: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 15 21
94: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 16 21
94: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 17 21
94: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 18 21
94: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 19 21
94: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 20 21
82: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 26
82: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 10 26
82: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 11 26
82: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 12 26
82: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 13 26
82: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 14 26
82: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 15 26
82: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 16 26
82: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 17 26
82: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 18 26
82: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 19 26
82: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 20 26
82: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 21 26
82: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 22 26
82: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 23 26
82: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 24 26
82: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 25 26
35: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 9
35: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 9
33: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 9
33: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 9
33: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 9
59: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 11
59: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 11
59: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 11
59: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 11
59: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 11
59: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 11
59: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 11
59: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 11
59: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 10 11
106: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 16
106: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 10 16
106: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 11 16
106: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 12 16
106: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 13 16
106: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 14 16
106: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 15 16
48: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 15
48: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 15
48: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 15
48: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 15
48: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 15
48: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 15
48: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 15
48: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 15
48: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 10 15
48: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 11 15
48: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 12 15
48: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 13 15
48: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 14 15
27: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 17
27: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 17
27: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 17
27: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 17
27: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 17
27: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 17
27: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 10 17
27: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 11 17
27: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 12 17
27: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 13 17
27: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 14 17
27: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 15 17
27: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 16 17
52: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 26
52: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 26
52: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 26
52: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 10 26
52: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 11 26
52: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 12 26
52: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 13 26
52: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 14 26
52: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 15 26
52: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 16 26
52: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 17 26
52: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 18 26
52: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 19 26
52: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 20 26
52: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 21 26
52: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 22 26
52: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 23 26
52: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 24 26
52: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 25 26
73: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 11
73: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 11
73: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 11
73: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 11
73: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 11
73: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 10 11
63: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 9
63: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 9
63: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 9
54: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 18
54: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 18
54: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 18
54: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 18
54: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 18
54: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 10 18
54: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 11 18
54: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 12 18
54: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 13 18
54: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 14 18
54: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 15 18
54: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 16 18
54: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 17 18
113: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 14 18
113: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 15 18
113: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 16 18
113: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 17 18
39: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 9
39: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 9
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 117
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 117
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 117
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 117
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 117
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 117
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 117
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 117
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 117
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 117
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 10 117
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 11 117
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 12 117
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 13 117
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 14 117
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 15 117
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 16 117
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 17 117
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 18 117
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 19 117
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 20 117
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 21 117
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 22 117
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 23 117
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 24 117
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 25 117
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 26 117
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 27 117
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 28 117
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 29 117
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 30 117
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 31 117
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 32 117
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 33 117
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 34 117
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 35 117
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 36 117
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 37 117
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 38 117
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 39 117
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 40 117
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 41 117
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 42 117
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 43 117
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 44 117
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 45 117
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 46 117
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 47 117
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 48 117
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 49 117
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 50 117
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 51 117
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 52 117
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 53 117
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 54 117
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 55 117
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 56 117
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 57 117
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 58 117
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 59 117
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 60 117
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 61 117
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 62 117
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 63 117
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 64 117
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 65 117
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 66 117
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 67 117
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 68 117
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 69 117
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 70 117
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 71 117
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 72 117
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 73 117
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 74 117
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 75 117
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 76 117
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 77 117
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 78 117
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 79 117
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 80 117
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 81 117
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 82 117
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 83 117
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 84 117
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 85 117
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 86 117
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 87 117
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 88 117
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 89 117
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 90 117
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 91 117
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 92 117
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 93 117
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 94 117
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 95 117
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 96 117
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 97 117
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 98 117
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 99 117
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 100 117
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 101 117
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 102 117
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 103 117
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 104 117
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 105 117
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 106 117
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 107 117
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 108 117
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 109 117
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 110 117
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 111 117
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 112 117
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 113 117
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 114 117
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 115 117
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 116 117
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 117
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 117
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 117
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 117
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 117
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 117
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 117
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 117
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 117
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 117
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 10 117
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 11 117
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 12 117
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 13 117
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 14 117
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 15 117
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 16 117
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 17 117
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 18 117
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 19 117
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 20 117
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 21 117
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 22 117
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 23 117
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 24 117
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 25 117
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 26 117
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 27 117
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 28 117
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 29 117
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 30 117
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 31 117
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 32 117
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 33 117
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 34 117
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 35 117
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 36 117
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 37 117
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 38 117
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 39 117
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 40 117
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 41 117
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 42 117
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 43 117
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 44 117
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 45 117
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 46 117
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 47 117
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 48 117
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 49 117
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 50 117
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 51 117
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 52 117
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 53 117
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 54 117
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 55 117
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 56 117
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 57 117
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 58 117
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 59 117
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 60 117
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 61 117
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 62 117
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 63 117
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 64 117
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 65 117
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 66 117
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 67 117
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 68 117
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 69 117
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 70 117
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 71 117
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 72 117
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 73 117
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 74 117
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 75 117
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 76 117
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 77 117
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 78 117
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 79 117
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 80 117
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 81 117
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 82 117
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 83 117
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 84 117
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 85 117
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 86 117
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 87 117
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 88 117
0: communicator.hh::sendRecv, before MPI_Waitany(messageInf112 121 before Dune::BiCGSTABSolver 
112 Dune::BiCGSTABSolver.apply(x, b, result_) 
112 BiCGSTABSolver::apply 
112 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
112: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 5
112: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 5
112: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 5
112: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 5
38 121 before Dune::BiCGSTABSolver 
38 Dune::BiCGSTABSolver.apply(x, b, result_) 
38 BiCGSTABSolver::apply 
38 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
38: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 24
38: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 24
38: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 24
38: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 24
38: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 24
38: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 24
38: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 24
21 121 before Dune::BiCGSTABSolver 
21 Dune::BiCGSTABSolver.apply(x, b, result_) 
21 BiCGSTABSolver::apply 
21 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
21: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 12
21: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 12
69 121 before Dune::BiCGSTABSolver 
69 Dune::BiCGSTABSolver.apply(x, b, result_) 
69 BiCGSTABSolver::apply 
69 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
69: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 7
69: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 7
69: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 7
31 121 before Dune::BiCGSTABSolver 
31 Dune::BiCGSTABSolver.apply(x, b, result_) 
31 BiCGSTABSolver::apply 
31 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
31: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 10
31: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 10
31: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 10
31: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 10
31: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 10
46 121 before Dune::BiCGSTABSolver 
46 Dune::BiCGSTABSolver.apply(x, b, result_) 
46 BiCGSTABSolver::apply 
46 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
46: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 12
46: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 12
46: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 12
8 121 before Dune::BiCGSTABSolver 
8 Dune::BiCGSTABSolver.apply(x, b, result_) 
8 BiCGSTABSolver::apply 
8 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
8: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 20
8: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 20
8: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 20
8: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 20
8: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 20
8: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 20
8: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 20
8: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 20
8: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 20
8: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 20
8: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 10 20
40 121 before Dune::BiCGSTABSolver 
40 Dune::BiCGSTABSolver.apply(x, b, result_) 
40 BiCGSTABSolver::apply 
40 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
40: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 15
40: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 15
40: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 15
94 121 before Dune::BiCGSTABSolver 
94 Dune::BiCGSTABSolver.apply(x, b, result_) 
94 BiCGSTABSolver::apply 
94 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
94: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 21
94: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 21
94: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 21
94: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 21
94: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 21
94: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 21
94: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 21
94: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 21
94: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 21
94: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 21
94: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 10 21
94: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 11 21
94: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 12 21
82 121 before Dune::BiCGSTABSolver 
82 Dune::BiCGSTABSolver.apply(x, b, result_) 
82 BiCGSTABSolver::apply 
82 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
82: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 26
82: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 26
82: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 26
82: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 26
82: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 26
82: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 26
35 121 before Dune::BiCGSTABSolver 
35 Dune::BiCGSTABSolver.apply(x, b, result_) 
35 BiCGSTABSolver::apply 
35 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
35: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 9
35: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 9
35: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 9
35: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 9
35: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 9
33 121 before Dune::BiCGSTABSolver 
33 Dune::BiCGSTABSolver.apply(x, b, result_) 
33 BiCGSTABSolver::apply 
33 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
33: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 9
33: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 9
33: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 9
33: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 9
33: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 9
59 121 before Dune::BiCGSTABSolver 
59 Dune::BiCGSTABSolver.apply(x, b, result_) 
59 BiCGSTABSolver::apply 
59 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
59: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 11
106 121 before Dune::BiCGSTABSolver 
106 Dune::BiCGSTABSolver.apply(x, b, result_) 
106 BiCGSTABSolver::apply 
106 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
106: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 16
106: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 16
106: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 16
106: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 16
106: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 16
106: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 16
106: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 16
106: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 16
106: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 16
48 121 before Dune::BiCGSTABSolver 
48 Dune::BiCGSTABSolver.apply(x, b, result_) 
48 BiCGSTABSolver::apply 
48 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
48: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 15
48: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 15
48: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 15
48: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 15
48: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 15
27 121 before Dune::BiCGSTABSolver 
27 Dune::BiCGSTABSolver.apply(x, b, result_) 
27 BiCGSTABSolver::apply 
27 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
27: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 17
27: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 17
27: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 17
27: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 17
27: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 17
27: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 17
52 121 before Dune::BiCGSTABSolver 
52 Dune::BiCGSTABSolver.apply(x, b, result_) 
52 BiCGSTABSolver::apply 
52 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
52: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 26
52: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 26
52: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 26
73 121 before Dune::BiCGSTABSolver 
73 Dune::BiCGSTABSolver.apply(x, b, result_) 
73 BiCGSTABSolver::apply 
73 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
73: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 11
73: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 11
73: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 11
73: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 11
73: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 11
63 121 before Dune::BiCGSTABSolver 
63 Dune::BiCGSTABSolver.apply(x, b, result_) 
63 BiCGSTABSolver::apply 
63 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
63: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 9
63: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 9
63: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 9
63: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 9
63: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 9
54 121 before Dune::BiCGSTABSolver 
54 Dune::BiCGSTABSolver.apply(x, b, result_) 
54 BiCGSTABSolver::apply 
54 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
54: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 18
54: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 18
113 121 before Dune::BiCGSTABSolver 
113 Dune::BiCGSTABSolver.apply(x, b, result_) 
113 BiCGSTABSolver::apply 
113 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
113: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 18
113: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 18
113: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 18
113: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 18
113: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 18
113: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 18
113: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 18
113: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 18
113: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 18
113: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 18
113: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 10 18
113: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 11 18
113: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 12 18
113: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 13 18
15 121 before Dune::BiCGSTABSolver 
15 Dune::BiCGSTABSolver.apply(x, b, result_) 
15 BiCGSTABSolver::apply 
15 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
15: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 8
15: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 8
15: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 8
15: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 8
15: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 8
120 121 before Dune::BiCGSTABSolver 
120 Dune::BiCGSTABSolver.apply(x, b, result_) 
120 BiCGSTABSolver::apply 
120 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
120: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 7
120: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 7
120: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 7
120: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 7
120: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 7
120: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 7
90 121 before Dune::BiCGSTABSolver 
90 Dune::BiCGSTABSolver.apply(x, b, result_) 
90 BiCGSTABSolver::apply 
90 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
90: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 6
90: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 6
90: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 6
90: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 6
111 121 before Dune::BiCGSTABSolver 
111 Dune::BiCGSTABSolver.apply(x, b, result_) 
111 BiCGSTABSolver::apply 
111 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
111: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 11
111: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 11
111: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 11
111: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 11
111: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 11
111: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 11
111: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 11
10 121 before Dune::BiCGSTABSolver 
10 Dune::BiCGSTABSolver.apply(x, b, result_) 
10 BiCGSTABSolver::apply 
10 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
10: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 11
10: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 11
10: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 11
10: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 11
10: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 11
75 121 before Dune::BiCGSTABSolver 
75 Dune::BiCGSTABSolver.apply(x, b, result_) 
75 BiCGSTABSolver::apply 
75 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
75: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 17
75: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 17
75: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 17
75: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 17
75: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 17
75: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 17
75: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 17
75: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 17
117 121 before Dune::BiCGSTABSolver 
117 Dune::BiCGSTABSolver.apply(x, b, result_) 
117 BiCGSTABSolver::apply 
117 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
117: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 22
117: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 22
117: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 22
117: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 22
117: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 22
117: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 22
117: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 22
117: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 22
117: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 22
117: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 22
117: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 10 22
117: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 11 22
117: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 12 22
117: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 13 22
55 121 before Dune::BiCGSTABSolver 
55 Dune::BiCGSTABSolver.apply(x, b, result_) 
55 BiCGSTABSolver::apply 
55 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
55: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 11
109 121 before Dune::BiCGSTABSolver 
109 Dune::BiCGSTABSolver.apply(x, b, result_) 
109 BiCGSTABSolver::apply 
109 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
109: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 20
109: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 20
109: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 20
109: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 20
109: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 20
109: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 20
109: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 20
109: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 20
109: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 20
109: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 20
109: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 10 20
109: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 11 20
109: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 12 20
79 121 before Dune::BiCGSTABSolver 
79 Dune::BiCGSTABSolver.apply(x, b, result_) 
79 BiCGSTABSolver::apply 
79 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
79: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 16
79: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 16
79: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 16
79: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 16
79: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 16
79: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 16
79: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 16
79: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 16
79: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 16
79: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 16
79: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 10 16
79: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 11 16
14 121 before Dune::BiCGSTABSolver 
14 Dune::BiCGSTABSolver.apply(x, b, result_) 
14 BiCGSTABSolver::apply 
14 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
14: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 17
14: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 17
14: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 17
14: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 17
14: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 17
14: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 17
14: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 17
14: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 17
14: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 17
14: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 17
110 121 before Dune::BiCGSTABSolver 
110 Dune::BiCGSTABSolver.apply(x, b, result_) 
110 BiCGSTABSolver::apply 
110 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
110: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 11
110: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 11
110: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 11
110: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 11
110: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 11
110: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 11
110: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 11
28 121 before Dune::BiCGSTABSolver 
28 Dune::BiCGSTABSolver.apply(x, b, result_) 
28 BiCGSTABSolver::apply 
28 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
28: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 18
28: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 18
28: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 18
108 121 before Dune::BiCGSTABSolver 
108 Dune::BiCGSTABSolver.apply(x, b, result_) 
108 BiCGSTABSolver::apply 
108 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
108: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 9
108: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 9
108: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 9
108: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 9
108: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 9
108: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 9
101 121 before Dune::BiCGSTABSolver 
101 Dune::BiCGSTABSolver.apply(x, b, result_) 
101 BiCGSTABSolver::apply 
101 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
101: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 24
101: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 24
101: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 24
101: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 24
101: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 24
101: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 24
101: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 24
101: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 24
101: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 24
101: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 24
23 121 before Dune::BiCGSTABSolver 
23 Dune::BiCGSTABSolver.apply(x, b, result_) 
23 BiCGSTABSolver::apply 
23 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
23: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 11
23: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 11
23: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 11
23: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 11
23: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 11
ormation_.size(), recvRequests, &finished, &status); 89 117
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 90 117
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 91 117
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 92 117
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 93 117
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 94 117
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 95 117
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 96 117
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 97 117
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 98 117
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 99 117
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 100 117
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 101 117
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 102 117
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 103 117
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 104 117
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 105 117
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 106 117
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 107 117
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 108 117
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 109 117
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 110 117
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 111 117
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 112 117
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 113 117
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 114 117
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 115 117
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 116 117
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 117
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 117
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 117
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 117
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 117
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 117
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finis78 121 before Dune::BiCGSTABSolver 
78 Dune::BiCGSTABSolver.apply(x, b, result_) 
78 BiCGSTABSolver::apply 
78 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
78: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 15
78: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 15
78: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 15
78: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 15
78: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 15
78: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 15
78: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 15
78: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 15
78: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 15
78: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 15
18 121 before Dune::BiCGSTABSolver 
18 Dune::BiCGSTABSolver.apply(x, b, result_) 
18 BiCGSTABSolver::apply 
18 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
18: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 14
18: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 14
18: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 14
18: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 14
18: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 14
18: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 14
18: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 14
85 121 before Dune::BiCGSTABSolver 
85 Dune::BiCGSTABSolver.apply(x, b, result_) 
85 BiCGSTABSolver::apply 
85 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
85: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 11
85: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 11
85: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 11
85: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 11
89 121 before Dune::BiCGSTABSolver 
89 Dune::BiCGSTABSolver.apply(x, b, result_) 
89 BiCGSTABSolver::apply 
89 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
89: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 11
89: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 11
77 121 before Dune::BiCGSTABSolver 
77 Dune::BiCGSTABSolver.apply(x, b, result_) 
77 BiCGSTABSolver::apply 
77 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
77: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 6
77: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 6
77: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 6
36 121 before Dune::BiCGSTABSolver 
36 Dune::BiCGSTABSolver.apply(x, b, result_) 
36 BiCGSTABSolver::apply 
36 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
36: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 14
36: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 14
36: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 14
36: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 14
36: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 14
36: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 14
36: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 14
36: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 14
36: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 14
36: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 14
36: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 10 14
26 121 before Dune::BiCGSTABSolver 
26 Dune::BiCGSTABSolver.apply(x, b, result_) 
26 BiCGSTABSolver::apply 
26 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
26: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 26
26: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 26
26: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 26
26: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 26
26: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 26
26: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 26
26: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 26
26: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 26
26: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 26
26: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 26
26: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 10 26
26: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 11 26
26: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 12 26
37 121 before Dune::BiCGSTABSolver 
37 Dune::BiCGSTABSolver.apply(x, b, result_) 
37 BiCGSTABSolver::apply 
37 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
37: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 15
37: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 15
37: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 15
37: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 15
37: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 15
37: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 15
37: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 15
51 121 before Dune::BiCGSTABSolver 
51 Dune::BiCGSTABSolver.apply(x, b, result_) 
51 BiCGSTABSolver::apply 
51 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
51: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 10
51: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 10
51: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 10
51: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 10
51: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 10
51: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 10
64 121 before Dune::BiCGSTABSolver 
64 Dune::BiCGSTABSolver.apply(x, b, result_) 
64 BiCGSTABSolver::apply 
64 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
64: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 14
64: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 14
64: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 14
64: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 14
64: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 14
64: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 14
64: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 14
64: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 14
64: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 14
64: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 14
64: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 10 14
84 121 before Dune::BiCGSTABSolver 
84 Dune::BiCGSTABSolver.apply(x, b, result_) 
84 BiCGSTABSolver::apply 
84 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
84: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 18
84: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 18
84: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 18
84: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 18
84: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 18
84: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 18
84: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 18
84: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 18
61 121 before Dune::BiCGSTABSolver 
61 Dune::BiCGSTABSolver.apply(x, b, result_) 
61 BiCGSTABSolver::apply 
61 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
61: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 9
61: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 9
61: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 9
61: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 9
61: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 9
61: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 9
61: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 9
61: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 9
104 121 before Dune::BiCGSTABSolver 
104 Dune::BiCGSTABSolver.apply(x, b, result_) 
104 BiCGSTABSolver::apply 
104 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
104: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 9
104: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 9
104: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 9
104: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 9
104: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 9
104: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 9
32 121 before Dune::BiCGSTABSolver 
32 Dune::BiCGSTABSolver.apply(x, b, result_) 
32 BiCGSTABSolver::apply 
32 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
32: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 16
32: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 16
32: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 16
32: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 16
32: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 16
32: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 16
32: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 16
32: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 16
32: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 16
32: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 16
32: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 10 16
32: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 11 16
30 121 before Dune::BiCGSTABSolver 
30 Dune::BiCGSTABSolver.apply(x, b, result_) 
30 BiCGSTABSolver::apply 
30 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
30: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 6
30: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 6
30: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 6
30: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 6
30: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 6
53 121 before Dune::BiCGSTABSolver 
53 Dune::BiCGSTABSolver.apply(x, b, result_) 
53 BiCGSTABSolver::apply 
53 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
53: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 17
53: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 17
53: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 17
80 121 before Dune::BiCGSTABSolver 
80 Dune::BiCGSTABSolver.apply(x, b, result_) 
80 BiCGSTABSolver::apply 
80 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
80: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 10
80: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 10
80: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 10
80: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 10
80: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 10
80: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 10
6 121 before Dune::BiCGSTABSolver 
6 Dune::BiCGSTABSolver.apply(x, b, result_) 
6 BiCGSTABSolver::apply 
6 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 17
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 17
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 17
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 17
13 121 before Dune::BiCGSTABSolver 
13 Dune::BiCGSTABSolver.apply(x, b, result_) 
13 BiCGSTABSolver::apply 
13 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
13: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 11
13: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 11
13: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 11
13: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 11
13: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 11
7 121 before Dune::BiCGSTABSolver 
7 Dune::BiCGSTABSolver.apply(x, b, result_) 
7 BiCGSTABSolver::apply 
7 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
7: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 13
7: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 13
7: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 13
7: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 13
91 121 before Dune::BiCGSTABSolver 
91 Dune::BiCGSTABSolver.apply(x, b, result_) 
91 BiCGSTABSolver::apply 
91 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
91: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 9
91: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 9
91: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 9
91: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 9
91: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 9
19 121 before Dune::BiCGSTABSolver 
19 Dune::BiCGSTABSolver.apply(x, b, result_) 
19 BiCGSTABSolver::apply 
19 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
19: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 17
19: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 17
19: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 17
19: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 17
19: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 17
19: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 17
19: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 17
92 121 before Dune::BiCGSTABSolver 
92 Dune::BiCGSTABSolver.apply(x, b, result_) 
92 BiCGSTABSolver::apply 
92 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
92: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 16
92: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 16
92: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 16
92: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 16
92: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 16
92: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 16
92: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 16
92: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 16
92: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 16
92: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 16
92: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 10 16
92: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 11 16
92: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 12 16
39 121 before Dune::BiCGSTABSolver 
39 Dune::BiCGSTABSolver.apply(x, b, result_) 
39 BiCGSTABSolver::apply 
39 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
39: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 9
39: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 9
39: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 9
39: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 9
39: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 9
20 121 before Dune::BiCGSTABSolver 
20 Dune::BiCGSTABSolver.apply(x, b, result_) 
20 BiCGSTABSolver::apply 
20 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
20: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 10
20: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 10
20: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 10
20: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 10
86 121 before Dune::BiCGSTABSolver 
86 Dune::BiCGSTABSolver.apply(x, b, result_) 
86 BiCGSTABSolver::apply 
86 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
86: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 31
86: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 31
86: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 31
86: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 31
86: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 31
86: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 31
86: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 31
86: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 31
86: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 31
86: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 31
86: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 10 31
86: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 11 31
86: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 12 31
86: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 13 31
86: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 14 31
86: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 15 31
86: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 16 31
86: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 17 31
86: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 18 31
86: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 19 31
86: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 20 31
86: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 21 31
86: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 22 31
86: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 23 31
86: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 24 31
67 121 before Dune::BiCGSTABSolver 
67 Dune::BiCGSTABSolver.apply(x, b, result_) 
67 BiCGSTABSolver::apply 
67 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
67: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 13
67: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 13
67: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 13
67: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 13
67: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 13
67: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 13
67: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 13
67: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 13
93 121 before Dune::BiCGSTABSolver 
93 Dune::BiCGSTABSolver.apply(x, b, result_) 
93 BiCGSTABSolver::apply 
93 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
93: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 13
93: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 13
93: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 13
93: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 13
93: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 13
76 121 before Dune::BiCGSTABSolver 
76 Dune::BiCGSTABSolver.apply(x, b, result_) 
76 BiCGSTABSolver::apply 
76 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
76: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 12
76: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 12
76: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 12
76: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 12
76: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 12
76: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 12
76: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 12
76: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 12
76: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 12
12 121 before Dune::BiCGSTABSolver 
12 Dune::BiCGSTABSolver.apply(x, b, result_) 
12 BiCGSTABSolver::apply 
12 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
12: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 26
12: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 26
12: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 26
12: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 26
12: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 26
12: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 26
12: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 26
12: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 26
12: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 26
12: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 26
12: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 10 26
12: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 11 26
12: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 12 26
12: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 13 26
12: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 14 26
12: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 15 26
12: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 16 26
12: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 17 26
12: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 18 26
95 121 before Dune::BiCGSTABSolver 
95 Dune::BiCGSTABSolver.apply(x, b, result_) 
95 BiCGSTABSolver::apply 
95 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
95: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 7
95: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 7
95: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 7
95: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 7
115 121 before Dune::BiCGSTABSolver 
115 Dune::BiCGSTABSolver.apply(x, b, result_) 
115 BiCGSTABSolver::apply 
115 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
115: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 11
115: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 11
115: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 11
115: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 11
115: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 11
115: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 11
115: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 11
115: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 11
115: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 11
115: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 11
70 121 before Dune::BiCGSTABSolver 
70 Dune::BiCGSTABSolver.apply(x, b, result_) 
70 BiCGSTABSolver::apply 
70 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
70: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 13
70: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 13
70: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 13
70: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 13
70: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 13
70: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 13
70: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 13
70: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 13
70: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 13
70: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 13
70: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 10 13
99 121 before Dune::BiCGSTABSolver 
99 Dune::BiCGSTABSolver.apply(x, b, result_) 
99 BiCGSTABSolver::apply 
99 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
99: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 13
99: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 13
99: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 13
99: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 13
99: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 13
99: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 13
43 121 before Dune::BiCGSTABSolver 
43 Dune::BiCGSTABSolver.apply(x, b, result_) 
43 BiCGSTABSolver::apply 
43 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
43: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 11
43: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 11
72 121 before Dune::BiCGSTABSolver 
72 Dune::BiCGSTABSolver.apply(x, b, result_) 
72 BiCGSTABSolver::apply 
72 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
72: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 20
72: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 20
72: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 20
72: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 20
72: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 20
72: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 20
72: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 20
72: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 20
72: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 20
72: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 20
72: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 10 20
72: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 11 20
72: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 12 20
72: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 13 20
98 121 before Dune::BiCGSTABSolver 
98 Dune::BiCGSTABSolver.apply(x, b, result_) 
98 BiCGSTABSolver::apply 
98 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
98: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 18
98: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 18
98: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 18
98: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 18
98: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 18
98: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 18
98: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 18
98: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 18
98: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 18
98: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 18
98: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 10 18
98: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 11 18
98: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 12 18
5 121 before Dune::BiCGSTABSolver 
5 Dune::BiCGSTABSolver.apply(x, b, result_) 
5 BiCGSTABSolver::apply 
5 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 11
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 11
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 11
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 11
2 121 before Dune::BiCGSTABSolver 
2 Dune::BiCGSTABSolver.apply(x, b, result_) 
2 BiCGSTABSolver::apply 
2 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 8
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 8
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 8
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 8
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 8
60 121 before Dune::BiCGSTABSolver 
60 Dune::BiCGSTABSolver.apply(x, b, result_) 
60 BiCGSTABSolver::apply 
60 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
60: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 6
60: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 6
60: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 6
60: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 6
60: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 6
22 121 before Dune::BiCGSTABSolver 
22 Dune::BiCGSTABSolver.apply(x, b, result_) 
22 BiCGSTABSolver::apply 
22 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
22: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 20
22: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 20
22: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 20
22: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 20
22: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 20
22: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 20
22: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 20
22: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 20
22: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 20
22: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 20
22: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 10 20
22: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 11 20
22: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 12 20
22: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 13 20
22: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 14 20
45 121 before Dune::BiCGSTABSolver 
45 Dune::BiCGSTABSolver.apply(x, b, result_) 
45 BiCGSTABSolver::apply 
45 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
45: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 17
45: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 17
45: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 17
45: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 17
45: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 17
45: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 17
45: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 17
45: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 17
45: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 17
45: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 17
45: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 10 17
102 121 before Dune::BiCGSTABSolver 
102 Dune::BiCGSTABSolver.apply(x, b, result_) 
102 BiCGSTABSolver::apply 
102 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
102: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 15
102: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 15
102: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 15
102: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 15
102: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 15
102: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 15
102: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 15
102: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 15
102: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 15
102: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 15
102: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 10 15
102: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 11 15
41 121 before Dune::BiCGSTABSolver 
41 Dune::BiCGSTABSolver.apply(x, b, result_) 
41 BiCGSTABSolver::apply 
41 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
41: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 20
41: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 20
41: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 20
41: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 20
41: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 20
41: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 20
41: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 20
41: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 20
41: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 20
41: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 20
41: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 10 20
41: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 11 20
41: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 12 20
41: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 13 20
100 121 before Dune::BiCGSTABSolver 
100 Dune::BiCGSTABSolver.apply(x, b, result_) 
100 BiCGSTABSolver::apply 
100 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
100: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 6
100: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 6
100: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 6
100: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 6
100: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 6
100: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 6
119 121 before Dune::BiCGSTABSolver 
119 Dune::BiCGSTABSolver.apply(x, b, result_) 
119 BiCGSTABSolver::apply 
119 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
119: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 13
119: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 13
119: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 13
119: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 13
119: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 13
119: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 13
119: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 13
119: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 13
119: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 13
119: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 13
119: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 10 13
56 121 before Dune::BiCGSTABSolver 
56 Dune::BiCGSTABSolver.apply(x, b, result_) 
56 BiCGSTABSolver::apply 
56 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
56: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 31
56: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 31
56: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 31
56: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 31
56: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 31
56: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 31
56: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 31
56: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 31
56: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 31
56: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 31
56: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 10 31
56: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 11 31
56: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 12 31
56: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 13 31
56: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 14 31
56: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 15 31
56: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 16 31
56: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 17 31
56: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 18 31
56: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 19 31
56: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 20 31
56: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 21 31
56: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 22 31
56: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 23 31
56: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 24 31
56: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 25 31
56: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 26 31
56: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 27 31
74 121 before Dune::BiCGSTABSolver 
74 Dune::BiCGSTABSolver.apply(x, b, result_) 
74 BiCGSTABSolver::apply 
74 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
74: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 11
74: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 11
74: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 11
74: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 11
74: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 11
74: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 11
74: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 11
74: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 11
74: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 11
9 121 before Dune::BiCGSTABSolver 
9 Dune::BiCGSTABSolver.apply(x, b, result_) 
9 BiCGSTABSolver::apply 
9 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
9: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 7
9: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 7
88 121 before Dune::BiCGSTABSolver 
88 Dune::BiCGSTABSolver.apply(x, b, result_) 
88 BiCGSTABSolver::apply 
88 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
88: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 21
88: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 21
88: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 21
88: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 21
88: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 21
88: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 21
88: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 21
88: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 21
88: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 21
88: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 21
88: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 10 21
88: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 11 21
88: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 12 21
88: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 13 21
88: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 14 21
88: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 15 21
88: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 16 21
11 121 before Dune::BiCGSTABSolver 
11 Dune::BiCGSTABSolver.apply(x, b, result_) 
11 BiCGSTABSolver::apply 
11 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
11: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 17
11: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 17
11: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 17
11: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 17
11: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 17
11: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 17
11: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 17
11: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 17
58 121 before Dune::BiCGSTABSolver 
58 Dune::BiCGSTABSolver.apply(x, b, result_) 
58 BiCGSTABSolver::apply 
58 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
58: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 21
58: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 21
58: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 21
58: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 21
58: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 21
58: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 21
58: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 21
58: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 21
58: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 21
58: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 21
58: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 10 21
58: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 11 21
58: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 12 21
58: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 13 21
58: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 14 21
58: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 15 21
58: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 16 21
17 121 before Dune::BiCGSTABSolver 
17 Dune::BiCGSTABSolver.apply(x, b, result_) 
17 BiCGSTABSolver::apply 
17 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
17: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 3
17: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 3
17: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 3
87 121 before Dune::BiCGSTABSolver 
87 Dune::BiCGSTABSolver.apply(x, b, result_) 
87 BiCGSTABSolver::apply 
87 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
87: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 19
87: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 19
87: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 19
87: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 19
87: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 19
87: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 19
87: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 19
87: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 19
87: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 19
87: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 19
87: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 10 19
87: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 11 19
87: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 12 19
87: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 13 19
87: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 14 19
87: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 15 19
87: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 16 19
97 121 before Dune::BiCGSTABSolver 
97 Dune::BiCGSTABSolver.apply(x, b, result_) 
97 BiCGSTABSolver::apply 
97 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
97: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 9
97: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 9
97: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 9
97: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 9
97: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 9
97: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 9
114 121 before Dune::BiCGSTABSolver 
114 Dune::BiCGSTABSolver.apply(x, b, result_) 
114 BiCGSTABSolver::apply 
114 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
114: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 13
114: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 13
114: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 13
114: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 13
114: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 13
114: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 13
114: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 13
114: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 13
114: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 13
114: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 13
114: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 10 13
114: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 11 13
34 121 before Dune::BiCGSTABSolver 
34 Dune::BiCGSTABSolver.apply(x, b, result_) 
34 BiCGSTABSolver::apply 
34 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
34: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 14
34: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 14
34: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 14
34: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 14
34: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 14
34: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 14
34: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 14
34: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 14
34: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 14
24 121 before Dune::BiCGSTABSolver 
24 Dune::BiCGSTABSolver.apply(x, b, result_) 
24 BiCGSTABSolver::apply 
24 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
24: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 14
24: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 14
24: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 14
24: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 14
24: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 14
24: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 14
24: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 14
24: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 14
24: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 14
24: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 14
24: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 10 14
103 121 before Dune::BiCGSTABSolver 
103 Dune::BiCGSTABSolver.apply(x, b, result_) 
103 BiCGSTABSolver::apply 
103 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
103: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 16
103: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 16
103: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 16
103: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 16
103: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 16
103: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 16
103: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 16
103: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 16
103: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 16
103: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 16
103: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 10 16
103: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 11 16
103: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 12 16
25 121 before Dune::BiCGSTABSolver 
25 Dune::BiCGSTABSolver.apply(x, b, result_) 
25 BiCGSTABSolver::apply 
25 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
25: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 7
25: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 7
25: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 7
25: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 7
25: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 7
25: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 7
25: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 7
81 121 before Dune::BiCGSTABSolver 
81 Dune::BiCGSTABSolver.apply(x, b, result_) 
81 BiCGSTABSolver::apply 
81 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
81: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 11
81: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 11
81: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 11
81: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 11
81: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 11
81: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 11
81: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 11
81: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 11
81: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 11
81: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 11
4 121 before Dune::BiCGSTABSolver 
4 Dune::BiCGSTABSolver.apply(x, b, result_) 
4 BiCGSTABSolver::apply 
4 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 14
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 14
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 14
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 14
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 14
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 14
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 14
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 14
3 121 before Dune::BiCGSTABSolver 
3 Dune::BiCGSTABSolver.apply(x, b, result_) 
3 BiCGSTABSolver::apply 
3 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 9
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 9
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 9
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 9
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 9
118 121 before Dune::BiCGSTABSolver 
118 Dune::BiCGSTABSolver.apply(x, b, result_) 
118 BiCGSTABSolver::apply 
118 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
118: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 14
118: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 14
118: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 14
118: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 14
118: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 14
118: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 14
118: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 14
118: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 14
118: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 14
118: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 14
118: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 10 14
118: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 11 14
118: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 12 14
96 121 before Dune::BiCGSTABSolver 
96 Dune::BiCGSTABSolver.apply(x, b, result_) 
96 BiCGSTABSolver::apply 
96 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
96: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 12
96: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 12
96: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 12
96: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 12
96: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 12
96: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 12
96: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 12
96: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 12
96: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 12
96: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 12
96: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 10 12
96: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 11 12
16 121 before Dune::BiCGSTABSolver 
16 Dune::BiCGSTABSolver.apply(x, b, result_) 
16 BiCGSTABSolver::apply 
16 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
16: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 6
16: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 6
16: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 6
16: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 6
16: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 6
50 121 before Dune::BiCGSTABSolver 
50 Dune::BiCGSTABSolver.apply(x, b, result_) 
50 BiCGSTABSolver::apply 
50 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
50: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 10
50: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 10
50: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 10
50: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 10
50: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 10
50: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 10
50: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 10
50: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 10
50: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 10
50: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 10
68 121 before Dune::BiCGSTABSolver 
68 Dune::BiCGSTABSolver.apply(x, b, result_) 
68 BiCGSTABSolver::apply 
68 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
68: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 22
68: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 22
68: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 22
68: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 22
68: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 22
68: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 22
68: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 22
68: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 22
68: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 22
68: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 22
68: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 10 22
68: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 11 22
68: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 12 22
68: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 13 22
68: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 14 22
68: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 15 22
68: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 16 22
68: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 17 22
68: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 18 22
68: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 19 22
29 121 before Dune::BiCGSTABSolver 
29 Dune::BiCGSTABSolver.apply(x, b, result_) 
29 BiCGSTABSolver::apply 
29 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
29: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 11
29: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 11
29: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 11
29: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 11
29: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 11
29: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 11
29: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 11
29: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 11
29: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 11
29: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 11
29: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 10 11
47 121 before Dune::BiCGSTABSolver 
47 Dune::BiCGSTABSolver.apply(x, b, result_) 
47 BiCGSTABSolver::apply 
47 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
47: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 6
47: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 6
47: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 6
47: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 6
47: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 6
47: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 6
71 121 before Dune::BiCGSTABSolver 
71 Dune::BiCGSTABSolver.apply(x, b, result_) 
71 BiCGSTABSolver::apply 
71 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
71: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 20
71: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 20
71: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 20
71: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 20
71: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 20
71: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 20
71: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 20
71: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 20
71: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 20
71: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 20
71: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 10 20
71: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 11 20
71: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 12 20
71: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 13 20
71: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 14 20
71: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 15 20
71: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 16 20
71: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 17 20
71: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 18 20
62 121 before Dune::BiCGSTABSolver 
62 Dune::BiCGSTABSolver.apply(x, b, result_) 
62 BiCGSTABSolver::apply 
62 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
62: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 15
62: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 15
62: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 15
62: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 15
62: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 15
62: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 15
62: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 15
62: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 15
62: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 15
62: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 15
62: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 10 15
62: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 11 15
62: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 12 15
62: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 13 15
62: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 14 15
66 121 before Dune::BiCGSTABSolver 
66 Dune::BiCGSTABSolver.apply(x, b, result_) 
66 BiCGSTABSolver::apply 
66 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
66: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 15
66: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 15
66: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 15
66: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 15
66: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 15
66: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 15
66: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 15
66: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 15
66: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 15
66: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 15
99: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 13
99: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 13
99: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 13
99: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 13
99: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 10 13
99: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 11 13
99: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 12 13
43: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 11
43: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 11
43: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 11
43: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 11
43: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 11
43: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 11
43: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 11
43: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 11
43: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 10 11
72: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 14 20
72: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 15 20
72: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 16 20
72: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 17 20
72: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 18 20
72: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 19 20
98: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 13 18
98: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 14 18
98: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 15 18
98: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 16 18
98: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 17 18
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 11
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 11
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 11
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 11
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 11
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 11
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 10 11
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 8
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 8
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 8
60: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 6
60 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
60: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 6
22: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 15 20
22: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 16 20
22: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 17 20
22: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 18 20
22: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 19 20
45: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 11 17
45: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 12 17
45: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 13 17
45: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 14 17
45: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 15 17
45: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 16 17
102: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 12 15
102: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 13 15
102: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 14 15
41: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 14 20
41: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 15 20
41: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 16 20
41: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 17 20
41: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 18 20
41: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 19 20
119: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 11 13
119: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 12 13
119 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
119: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 13
119: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 13
56: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 28 31
56: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 29 31
56: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 30 31
74: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 11
74: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 10 11
9: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 7
9: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 7
9: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 7
9: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 7
9: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 7
88: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 17 21
88: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 18 21
88: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 19 21
88: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 20 21
11: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 17
11: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 17
11: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 10 17
11: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 11 17
11: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 12 17
11: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 13 17
11: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 14 17
11: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 15 17
11: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 16 17
58: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 17 21
58: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 18 21
58: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 19 21
58: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 20 21
87: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 17 19
87: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 18 19
97: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 9
97: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 9
97: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 9
114: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 12 13
114 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
114: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 13
114: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 13
114: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 13
34: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 14
34: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 10 14
34: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 11 14
34: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 12 14
34: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 13 14
34 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
34: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 14
24: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 11 14
24: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 12 14
24: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 13 14
103: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 13 16
103: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 14 16
103: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 15 16
81: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 10 11
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 14
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 14
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 10 14
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 11 14
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 12 14
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 13 14
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 9
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 9
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 9
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 9
118: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 13 14
118 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
118: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 14
118: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 14
16: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 6
68: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 20 22
68: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 21 22
47 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
47: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 6
71: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 19 20
71 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
71: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 20
71: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 20
62 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
62: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 15
23: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 11
23: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 11
23: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 11
23: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 11
23: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 11
23: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 10 11
112: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 5
38: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 24
38: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 24
38: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 24
38: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 10 24
38: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 11 24
38: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 12 24
38: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 13 24
38: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 14 24
38: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 15 24
38: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 16 24
38: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 17 24
38: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 18 24
38: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 19 24
38: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 20 24
38: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 21 24
38: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 22 24
38: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 23 24
21: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 12
21: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 12
21: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 12
21: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 12
21: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 12
21: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 12
21: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 12
21: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 12
21: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 10 12
21: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 11 12
69: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 7
69: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 7
69: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 7
69: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 7
31: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 10
31: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 10
31: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 10
31: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 10
31: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 10
31 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
31: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 10
46: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 12
46: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 12
46: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 12
46: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 12
46: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 12
46: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 12
46: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 12
46: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 10 12
46: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 11 12
8: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 11 20
8: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 12 20
8: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 13 20
8: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 14 20
8: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 15 20
8: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 16 20
8: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 17 20
8: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 18 20
8: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 19 20
40: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 15
40: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 15
40: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 15
40: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 15
40: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 15
40: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 15
40: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 15
40: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 10 15
40: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 11 15
40: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 12 15
40: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 13 15
40: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 14 15
94: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 13 21
94: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 14 21
94: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 15 21
94: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 16 21
94: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 17 21
94: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 18 21
94: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 19 21
94: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 20 21
82: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 26
82: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 26
82: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 26
82: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 26
82: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 10 26
82: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 11 26
82: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 12 26
82: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 13 26
82: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 14 26
82: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 15 26
82: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 16 26
82: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 17 26
82: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 18 26
82: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 19 26
82: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 20 26
82: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 21 26
82: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 22 26
82: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 23 26
82: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 24 26
82: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 25 26
35: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 9
35: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 9
35: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 9
35: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 9
35 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
35: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 9
33: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 9
33: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 9
33: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 9
33: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 9
59: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 11
59: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 11
59: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 11
59: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 11
59: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 11
59: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 11
59: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 11
59: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 11
59: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 11
59: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 10 11
106: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 16
106: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 10 16
106: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 11 16
106: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 12 16
106: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 13 16
106: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 14 16
106: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 15 16
48: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 15
48: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 15
48: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 15
48: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 15
48: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 15
48: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 10 15
48: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 11 15
48: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 12 15
48: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 13 15
48: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 14 15
27: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 17
27: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 17
27: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 17
27: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 17
27: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 10 17
27: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 11 17
27: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 12 17
27: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 13 17
27: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 14 17
27: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 15 17
27: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 16 17
52: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 26
52: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 26
52: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 26
52: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 26
52: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 26
52: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 26
52: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 26
52: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 10 26
52: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 11 26
52: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 12 26
52: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 13 26
52: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 14 26
52: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 15 26
52: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 16 26
52: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 17 26
52: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 18 26
52: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 19 26
52: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 20 26
52: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 21 26
52: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 22 26
52: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 23 26
52: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 24 26
52: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 25 26
73: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 11
73: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 11
73: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 11
73: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 11
73: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 11
73: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 10 11
63: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 9
63: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 9
63: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 9
63: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 9
63 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
63: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 9
63: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 9
54: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 18
54: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 18
54: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 18
54: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 18
54: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 18
54: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 18
54: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 18
54: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 18
54: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 10 18
54: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 11 18
54: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 12 18
54: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 13 18
54: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 14 18
54: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 15 18
54: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 16 18
54: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 17 18
113: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 14 18
113: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 15 18
113: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 16 18
113: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 17 18
15: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 8
15: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 8
15: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 8
15 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
15: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 8
120: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 7
120 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
120: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 7
120: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 7
90: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 6
90: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 6
90 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
90: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 6
111: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 11
111: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 11
111: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 11
111: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 10 11
111 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
111: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 11
10: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 11
10: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 11
10: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 11
10: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 11
10: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 11
10: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 10 11
75: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 17
75: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 17
75: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 10 17
75: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 11 17
75: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 12 17
75: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 13 17
75: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 14 17
75: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 15 17
75: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 16 17
117: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 14 22
117: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 15 22
117: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 16 22
117: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 17 22
117: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 18 22
117: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 19 22
117: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 20 22
117: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 21 22
55: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 11
55: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 11
55: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 11
55: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 11
55: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 11
55: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 11
55: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 11
55: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 11
55: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 11
55: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 10 11
55 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
55: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 11
109: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 13 20
109: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 14 20
109: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 15 20
109: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 16 20
109: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 17 20
109: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 18 20
109: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 19 20
79: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 12 16
79: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 13 16
79: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 14 16
79: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 15 16
79 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
79: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 16
79: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 16
79: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 16
14: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 10 17
14: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 11 17
14: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 12 17
14: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 13 17
14: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 14 17
14: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 15 17
14: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 16 17
110: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 11
110: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 11
110: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 11
110: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 10 11
28: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 18
28: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 18
28: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 18
28: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 18
28: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 18
28: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 18
28: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 18
28: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 10 18
28: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 11 18
28: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 12 18
28: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 13 18
28: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 14 18
28: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 15 18
28: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 16 18
28: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 17 18
108: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 9
108: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 9
108: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 9
101: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 10 24
101: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 11 24
101: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 12 24
101: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 13 24
101: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 14 24
101: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 15 24
101: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 16 24
101: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 17 24
101: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 18 24
101: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 19 24
101: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 20 24
101: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 21 24
101: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 22 24
101: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 23 24
66: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 10 15
66: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 11 15
66: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 12 15
66: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 13 15
66: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 14 15
83 121 before Dune::BiCGSTABSolver 
83 Dune::BiCGSTABSolver.apply(x, b, result_) 
83 BiCGSTABSolver::apply 
83 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
83: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 17
83: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 17
83: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 17
83: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 17
83: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 17
83: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 17
83: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 17
83: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 17
83: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 17
83: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 17
83: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 10 17
83: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 11 17
83: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 12 17
83: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 13 17
83: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 14 17
83: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 15 17
83: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 16 17
83 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
83: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 17
44 121 before Dune::BiCGSTABSolver 
44 Dune::BiCGSTABSolver.apply(x, b, result_) 
44 BiCGSTABSolver::apply 
44 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
44: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 11
44: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 11
44: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 11
44: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 11
44: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 11
44: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 11
44: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 11
44: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 11
44: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 11
44: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 11
44: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 10 11
116 121 before Dune::BiCGSTABSolver 
116 Dune::BiCGSTABSolver.apply(x, b, result_) 
116 BiCGSTABSolver::apply 
116 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
116: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 7
116: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 7
116: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 7
116: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 7
116: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 7
116: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 7
116: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 7
116 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
116: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 7
116: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 7
116: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 7
116: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 7
116: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 7
116: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 7
42 121 before Dune::BiCGSTABSolver 
42 Dune::BiCGSTABSolver.apply(x, b, result_) 
42 BiCGSTABSolver::apply 
42 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
42: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 20
42: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 20
42: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 20
42: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 20
42: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 20
42: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 20
42: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 20
42: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 20
42: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 20
42: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 20
42: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 10 20
42: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 11 20
42: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 12 20
42: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 13 20
42: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 14 20
42: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 15 20
42: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 16 20
42: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 17 20
42: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 18 20
42: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 19 20
49 121 before Dune::BiCGSTABSolver 
49 Dune::BiCGSTABSolver.apply(x, b, result_) 
49 BiCGSTABSolver::apply 
49 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
49: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 15
49: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 15
49: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 15
49: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 15
49: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 15
49: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 15
49: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 15
49: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 15
49: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 15
49: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 15
49: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 10 15
49: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 11 15
49: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 12 15
49: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 13 15
49: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 14 15
107 121 before Dune::BiCGSTABSolver 
107 Dune::BiCGSTABSolver.apply(x, b, result_) 
107 BiCGSTABSolver::apply 
107 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
107: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 5
107: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 5
107: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 5
107: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 5
107: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 5
107 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
107: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 5
107: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 5
65 121 before Dune::BiCGSTABSolver 
65 Dune::BiCGSTABSolver.apply(x, b, result_) 
65 BiCGSTABSolver::apply 
65 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
65: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 9
65: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 9
65: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 9
65: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 9
65: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 9
65: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 9
65: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 9
65: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 9
65: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 9
57 121 before Dune::BiCGSTABSolver 
57 Dune::BiCGSTABSolver.apply(x, b, result_) 
57 BiCGSTABSolver::apply 
57 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
57: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 19
57: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 19
57: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 19
57: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 19
57: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 19
57: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 19
57: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 19
57: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 19
57: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 19
57: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 19
57: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 10 19
57: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 11 19
57: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 12 19
57: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 13 19
57: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 14 19
57: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 15 19
57: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 16 19
57: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 17 19
57: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 18 19
57 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
57: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 19
1 121 before Dune::BiCGSTABSolver 
1 Dune::BiCGSTABSolver.apply(x, b, result_) 
1 BiCGSTABSolver::apply 
1 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 5
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 5
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 5
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 5
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 5
105 121 before Dune::BiCGSTABSolver 
105 Dune::BiCGSTABSolver.apply(x, b, result_) 
105 BiCGSTABSolver::apply 
105 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
105: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 9
105: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 9
105: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 9
105: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 9
105: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 9
105: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 9
105: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 9
105: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 9
105: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 9
105 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
105: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 9
hed, &status); 6 117
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 117
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 117
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 117
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 10 117
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 11 117
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 12 117
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 13 117
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 14 117
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 15 117
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 16 117
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 17 117
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 18 117
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 19 117
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 20 117
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 21 117
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 22 117
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 23 117
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 24 117
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 25 117
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 26 117
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 27 117
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 28 117
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 29 117
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 30 117
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 31 117
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 32 117
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 33 117
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 34 117
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 35 117
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 36 117
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 37 117
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 38 117
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 39 117
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 40 117
0: communicator.hh::sendRecv, b78: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 10 15
78: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 11 15
78: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 12 15
78: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 13 15
78: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 14 15
78 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
78: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 15
78: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 15
78: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 15
18: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 14
18: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 14
18: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 14
18: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 10 14
18: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 11 14
18: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 12 14
18: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 13 14
85: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 11
85: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 11
85: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 11
85: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 11
85: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 11
85: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 11
85: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 10 11
85 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
85: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 11
89: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 11
89: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 11
89: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 11
89: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 11
89: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 11
89: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 11
89: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 11
89: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 11
89: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 10 11
89 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
89: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 11
77: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 6
77: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 6
77: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 6
77 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
77: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 6
36: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 11 14
36: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 12 14
36: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 13 14
26: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 13 26
26: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 14 26
26: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 15 26
26: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 16 26
26: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 17 26
26: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 18 26
26: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 19 26
26: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 20 26
26: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 21 26
26: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 22 26
26: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 23 26
26: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 24 26
26: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 25 26
37: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 15
37: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 15
37: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 15
37: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 10 15
37: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 11 15
37: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 12 15
37: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 13 15
37: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 14 15
37 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
37: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 15
37: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 15
51: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 10
51: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 10
51: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 10
51: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 10
51 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
51: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 10
51: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 10
64: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 11 14
64: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 12 14
64: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 13 14
64 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
64: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 14
64: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 14
64: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 14
64: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 14
64: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 14
64: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 14
84: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 18
84: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 18
84: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 10 18
84: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 11 18
84: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 12 18
84: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 13 18
84: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 14 18
84: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 15 18
84: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 16 18
84: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 17 18
84 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
84: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 18
84: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 18
61: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 9
61 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
61: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 9
61: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 9
61: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 9
61: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 9
61: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 9
61: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 9
104: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 9
104: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 9
104: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 9
104 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
104: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 9
32: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 12 16
32: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 13 16
32: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 14 16
32: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 15 16
32 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
32: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 16
32: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 16
32: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 16
32: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 16
30: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 6
30 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
30: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 6
30: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 6
53: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 17
53: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 17
53: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 17
53: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 17
53: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 17
53: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 17
53: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 17
53: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 10 17
53: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 11 17
53: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 12 17
53: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 13 17
53: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 14 17
53: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 15 17
53: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 16 17
53 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
53: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 17
80: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 10
80: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 10
80: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 10
80: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 10
80 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
80: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 10
80: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 10
80: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 10
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 17
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 17
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 17
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 17
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 17
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 17
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 10 17
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 11 17
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 12 17
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 13 17
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 14 17
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 15 17
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 16 17
6 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 17
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 17
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 17
13: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 11
13: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 11
13: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 11
13: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 11
13: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 11
13: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 10 11
13 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
13: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 11
13: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 11
7: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 13
7: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 13
7: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 13
7: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 13
7: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 13
7: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 13
7: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 10 13
7: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 11 13
7: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 12 13
91: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 9
91: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 9
91: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 9
91: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 9
91 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
91: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 9
91: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 9
19: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 17
19: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 17
19: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 17
19: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 10 17
19: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 11 17
19: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 12 17
19: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 13 17
19: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 14 17
19: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 15 17
19: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 16 17
19 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
19: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 17
19: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 17
92: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 13 16
92: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 14 16
92: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 15 16
92 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
92: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 16
92: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 16
92: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 16
92: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 16
39: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 9
39: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 9
39: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 9
39: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 9
39 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
39: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 9
20: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 10
20: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 10
20: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 10
20: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 10
20: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 10
20: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 10
20 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
20: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 10
20: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 10
20: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 10
86: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 25 31
86: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 26 31
86: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 27 31
86: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 28 31
86: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 29 31
86: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 30 31
86 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
86: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 31
86: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 31
86: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 31
86: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 31
67: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 13
67: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 13
67: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 10 13
67: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 11 13
67: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 12 13
67 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
67: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 13
67: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 13
67: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 13
67: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 13
67: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 13
67: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 13
93: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 13
93: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 13
93: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 13
93: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 13
93: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 13
93: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 10 13
93: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 11 13
93: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 12 13
93 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
93: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 13
93: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 13
76: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 12
76: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 10 12
76: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 11 12
76 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
76: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 12
76: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 12
76: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 12
12: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 19 26
12: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 20 26
12: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 21 26
12: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 22 26
12: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 23 26
12: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 24 26
12: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 25 26
12 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
12: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 26
12: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 26
12: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 26
12: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 26
12: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 26
95: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 7
95: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 7
95: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 7
95 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
95: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 7
95: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 7
115: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 10 11
115 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
115: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 11
115: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 11
115: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 11
115: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 11
115: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 11
115: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 11
115: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 11
70: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 11 13
70: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 12 13
70 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
70: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 13
70: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 13
70: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 13
70: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 13
70: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 13
97 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
97: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 9
97: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 9
97: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 9
97: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 9
97: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 9
97: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 9
97: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 9
97: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 9
97: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 9
114: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 13
114: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 13
114: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 13
114: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 13
114: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 13
114: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 13
114: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 13
114: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 10 13
114: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 11 13
34: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 14
34: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 14
34: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 14
34: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 14
34: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 14
34: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 14
34: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 14
34: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 14
34: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 14
34: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 10 14
34: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 11 14
34: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 12 14
34: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 13 14
24 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
24: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 14
24: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 14
24: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 14
24: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 14
24: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 14
24: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 14
24: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 14
24: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 14
24: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 14
24: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 14
24: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 10 14
24: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 11 14
24: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 12 14
24: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 13 14
25 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
25: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 7
25: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 7
25: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 7
25: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 7
25: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 7
25: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 7
25: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 7
103 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
103: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 16
103: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 16
103: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 16
103: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 16
103: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 16
103: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 16
103: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 16
103: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 16
103: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 16
103: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 16
103: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 10 16
103: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 11 16
103: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 12 16
103: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 13 16
103: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 14 16
103: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 15 16
81 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
81: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 11
81: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 11
81: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 11
81: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 11
81: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 11
81: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 11
81: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 11
81: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 11
81: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 11
81: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 11
4 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 14
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 14
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 14
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 14
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 14
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 14
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 14
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 14
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 14
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 14
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 10 14
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 11 14
96 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
96: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 12
96: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 12
96: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 12
96: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 12
96: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 12
96: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 12
96: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 12
96: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 12
96: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 12
96: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 12
96: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 10 12
96: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 11 12
118: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 14
118: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 14
118: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 14
118: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 14
118: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 14
118: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 14
118: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 14
118: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 14
118: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 10 14
118: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 11 14
50 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
50: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 10
50: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 10
50: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 10
50: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 10
50: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 10
50: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 10
50: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 10
50: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 10
50: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 10
50: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 10
16 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
16: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 6
16: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 6
16: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 6
16: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 6
16: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 6
16: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 6
29 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
29: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 11
29: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 11
29: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 11
29: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 11
29: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 11
29: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 11
29: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 11
29: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 11
29: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 11
29: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 11
29: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 10 11
68 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
68: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 22
68: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 22
68: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 22
68: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 22
68: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 22
68: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 22
68: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 22
68: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 22
68: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 22
68: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 22
68: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 10 22
68: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 11 22
68: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 12 22
68: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 13 22
68: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 14 22
68: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 15 22
68: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 16 22
68: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 17 22
68: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 18 22
68: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 19 22
68: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 20 22
68: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 21 22
47: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 6
47: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 6
47: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 6
47: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 6
47: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 6
71: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 20
71: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 20
71: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 20
71: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 20
71: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 20
71: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 20
71: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 20
71: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 20
71: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 10 20
71: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 11 20
71: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 12 20
71: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 13 20
71: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 14 20
71: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 15 20
71: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 16 20
71: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 17 20
71: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 18 20
62: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 15
62: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 15
62: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 15
62: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 15
62: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 15
62: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 15
62: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 15
62: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 15
62: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 15
62: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 10 15
62: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 11 15
62: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 12 15
62: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 13 15
62: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 14 15
23 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
23: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 11
23: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 11
23: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 11
23: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 11
23: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 11
23: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 11
23: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 11
23: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 11
23: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 11
23: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 11
112 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
112: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 5
112: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 5
112: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 5
112: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 5
112: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 5
38 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
38: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 24
38: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 24
38: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 24
38: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 24
38: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 24
38: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 24
38: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 24
38: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 24
38: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 24
38: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 24
38: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 10 24
38: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 11 24
38: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 12 24
38: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 13 24
38: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 14 24
38: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 15 24
38: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 16 24
38: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 17 24
38: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 18 24
38: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 19 24
38: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 20 24
38: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 21 24
38: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 22 24
21 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
21: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 12
21: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 12
21: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 12
21: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 12
21: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 12
21: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 12
21: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 12
21: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 12
21: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 12
69 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
69: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 7
69: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 7
69: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 7
69: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 7
69: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 7
69: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 7
69: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 7
31: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 10
31: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 10
31: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 10
31: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 10
31: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 10
31: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 10
31: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 10
31: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 10
31: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 10
46 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
46: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 12
46: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 12
46: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 12
46: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 12
46: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 12
46: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 12
46: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 12
46: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 12
46: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 12
46: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 12
46: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 10 12
46: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 11 12
8 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
8: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 20
8: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 20
8: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 20
8: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 20
8: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 20
8: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 20
8: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 20
8: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 20
8: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 20
8: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 20
8: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 10 20
8: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 11 20
8: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 12 20
8: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 13 20
8: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 14 20
8: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 15 20
8: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 16 20
8: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 17 20
40 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
40: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 15
40: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 15
40: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 15
40: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 15
40: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 15
40: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 15
40: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 15
40: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 15
40: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 15
40: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 15
40: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 10 15
40: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 11 15
40: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 12 15
40: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 13 15
94 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
94: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 21
94: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 21
94: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 21
94: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 21
94: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 21
94: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 21
94: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 21
94: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 21
94: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 21
94: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 21
94: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 10 21
94: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 11 21
94: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 12 21
94: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 13 21
94: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 14 21
94: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 15 21
94: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 16 21
94: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 17 21
94: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 18 21
94: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 19 21
94: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 20 21
82 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
82: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 26
82: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 26
82: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 26
82: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 26
82: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 26
82: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 26
82: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 26
82: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 26
82: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 26
82: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 26
82: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 10 26
82: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 11 26
82: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 12 26
82: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 13 26
82: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 14 26
82: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 15 26
82: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 16 26
82: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 17 26
82: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 18 26
82: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 19 26
82: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 20 26
82: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 21 26
82: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 22 26
82: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 23 26
82: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 24 26
82: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 25 26
35: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 9
35: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 9
35: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 9
35: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 9
35: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 9
35: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 9
35: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 9
35: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 9
33 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
33: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 9
33: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 9
33: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 9
33: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 9
33: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 9
33: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 9
33: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 9
33: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 9
33: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 9
59 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
59: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 11
59: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 11
59: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 11
59: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 11
59: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 11
59: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 11
59: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 11
59: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 11
59: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 11
59: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 11
59: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 10 11
106 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
106: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 16
106: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 16
106: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 16
106: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 16
106: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 16
106: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 16
106: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 16
106: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 16
106: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 16
106: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 16
106: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 10 16
106: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 11 16
106: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 12 16
106: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 13 16
106: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 14 16
106: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 15 16
48 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
48: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 15
48: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 15
48: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 15
48: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 15
48: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 15
48: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 15
48: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 15
48: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 15
48: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 15
48: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 15
48: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 10 15
48: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 11 15
48: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 12 15
48: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 13 15
48: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 14 15
27 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
27: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 17
27: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 17
27: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 17
27: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 17
27: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 17
27: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 17
27: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 17
27: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 17
27: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 17
27: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 17
27: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 10 17
27: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 11 17
27: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 12 17
27: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 13 17
27: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 14 17
52 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
52: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 26
52: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 26
52: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 26
52: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 26
52: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 26
52: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 26
52: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 26
52: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 26
52: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 26
52: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 26
52: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 10 26
52: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 11 26
52: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 12 26
52: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 13 26
52: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 14 26
52: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 15 26
52: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 16 26
52: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 17 26
52: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 18 26
52: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 19 26
52: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 20 26
52: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 21 26
52: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 22 26
52: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 23 26
52: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 24 26
52: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 25 26
73 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
73: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 11
73: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 11
73: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 11
73: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 11
73: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 11
73: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 11
73: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 11
73: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 11
73: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 11
73: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 11
73: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 10 11
63: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 9
63: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 9
63: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 9
63: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 9
63: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 9
63: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 9
63: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 9
54 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
54: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 18
54: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 18
54: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 18
54: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 18
54: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 18
54: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 18
54: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 18
54: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 18
54: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 18
54: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 18
54: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 10 18
54: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 11 18
54: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 12 18
54: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 13 18
54: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 14 18
54: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 15 18
54: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 16 18
54: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 17 18
113 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
113: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 18
113: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 18
113: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 18
113: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 18
113: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 18
113: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 18
113: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 18
113: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 18
113: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 18
113: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 18
113: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 10 18
113: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 11 18
113: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 12 18
113: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 13 18
15: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 8
15: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 8
15: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 8
15: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 8
15: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 8
120: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 7
120: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 7
120: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 7
120: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 7
120: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 7
90: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 6
90: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 6
90: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 6
90: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 6
90: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 6
111: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 11
111: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 11
111: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 11
111: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 11
111: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 11
111: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 11
111: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 11
111: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 11
111: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 11
111: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 10 11
10 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
10: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 11
10: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 11
10: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 11
10: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 11
10: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 11
10: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 11
10: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 11
10: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 11
10: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 11
10: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 11
10: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 10 11
75 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
75: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 17
75: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 17
75: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 17
75: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 17
75: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 17
75: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 17
75: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 17
75: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 17
75: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 17
75: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 17
75: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 10 17
75: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 11 17
75: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 12 17
75: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 13 17
75: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 14 17
75: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 15 17
117 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
117: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 22
117: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 22
117: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 22
117: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 22
117: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 22
117: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 22
117: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 22
117: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 22
117: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 22
117: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 22
117: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 10 22
117: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 11 22
117: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 12 22
117: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 13 22
117: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 14 22
117: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 15 22
117: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 16 22
117: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 17 22
117: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 18 22
55: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 11
55: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 11
55: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 11
55: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 11
55: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 11
55: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 11
55: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 11
55: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 11
55: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 11
55: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 10 11
109 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
109: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 20
109: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 20
109: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 20
109: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 20
109: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 20
109: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 20
109: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 20
109: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 20
109: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 20
109: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 20
109: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 10 20
109: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 11 20
109: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 12 20
109: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 13 20
109: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 14 20
109: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 15 20
109: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 16 20
79: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 16
79: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 16
79: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 16
79: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 16
79: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 16
79: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 16
79: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 16
79: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 10 16
79: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 11 16
79: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 12 16
79: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 13 16
79: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 14 16
14 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
14: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 17
14: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 17
14: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 17
14: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 17
14: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 17
14: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 17
14: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 17
14: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 17
14: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 17
14: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 17
14: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 10 17
14: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 11 17
14: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 12 17
14: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 13 17
14: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 14 17
14: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 15 17
14: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 16 17
110 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
110: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 11
110: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 11
110: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 11
110: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 11
110: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 11
110: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 11
110: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 11
110: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 11
110: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 11
110: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 11
110: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 10 11
28 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
28: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 18
28: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 18
28: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 18
28: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 18
28: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 18
28: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 18
28: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 18
28: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 18
28: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 18
28: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 18
28: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 10 18
28: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 11 18
28: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 12 18
28: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 13 18
28: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 14 18
28: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 15 18
28: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 16 18
28: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 17 18
108 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
108: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 9
108: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 9
108: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 9
108: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 9
108: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 9
108: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 9
108: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 9
108: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 9
108: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 9
70: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 13
70: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 13
70: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 13
70: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 13
70: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 13
70: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 10 13
70: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 11 13
70: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 12 13
66 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
66: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 15
66: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 15
66: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 15
66: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 15
66: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 15
66: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 15
66: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 15
66: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 15
66: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 15
66: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 15
66: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 10 15
66: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 11 15
66: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 12 15
66: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 13 15
66: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 14 15
83: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 17
83: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 17
83: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 17
83: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 17
83: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 17
83: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 17
83: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 17
83: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 17
83: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 17
83: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 10 17
83: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 11 17
83: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 12 17
83: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 13 17
83: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 14 17
83: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 15 17
83: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 16 17
44 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
44: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 11
44: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 11
44: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 11
44: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 11
44: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 11
44: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 11
44: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 11
44: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 11
44: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 11
44: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 11
44: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 10 11
116: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 7
42 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
42: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 20
42: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 20
42: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 20
42: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 20
42: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 20
42: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 20
42: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 20
42: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 20
42: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 20
42: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 20
42: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 10 20
42: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 11 20
42: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 12 20
42: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 13 20
42: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 14 20
42: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 15 20
42: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 16 20
42: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 17 20
42: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 18 20
42: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 19 20
49 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
49: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 15
49: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 15
49: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 15
49: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 15
49: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 15
49: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 15
49: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 15
49: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 15
49: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 15
49: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 15
49: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 10 15
49: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 11 15
49: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 12 15
49: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 13 15
49: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 14 15
107: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 5
107: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 5
107: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 5
65 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
65: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 9
65: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 9
65: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 9
65: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 9
65: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 9
65: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 9
65: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 9
65: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 9
65: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 9
57: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 19
57: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 19
57: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 19
57: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 19
57: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 19
57: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 19
57: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 19
57: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 19
57: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 19
57: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 10 19
57: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 11 19
57: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 12 19
57: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 13 19
57: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 14 19
57: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 15 19
57: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 16 19
105: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 9
105: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 9
105: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 9
105: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 9
105: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 9
105: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 9
105: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 9
105: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 9
efore MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 41 117
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 42 117
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 43 117
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 44 117
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 45 117
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 46 117
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 47 117
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 48 117
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 49 117
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 50 117
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 51 117
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 52 117
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 53 117
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 54 117
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 55 117
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 56 117
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 57 117
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 58 117
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 59 117
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 60 117
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 61 117
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 62 117
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 63 117
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 64 117
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 65 117
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 66 117
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 67 117
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 68 117
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 69 117
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 70 117
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 71 117
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 72 117
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 73 117
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 74 117
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), rec78: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 15
78: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 15
78: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 15
78: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 15
78: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 15
78: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 15
78: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 15
78: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 10 15
78: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 11 15
78: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 12 15
78: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 13 15
78: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 14 15
18 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
18: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 14
18: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 14
18: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 14
18: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 14
18: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 14
18: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 14
18: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 14
18: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 14
18: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 14
18: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 14
18: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 10 14
18: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 11 14
18: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 12 14
18: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 13 14
85: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 11
85: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 11
85: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 11
85: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 11
85: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 11
85: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 11
85: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 11
85: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 11
85: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 11
85: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 10 11
89: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 11
89: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 11
89: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 11
89: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 11
89: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 11
89: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 11
89: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 11
89: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 11
89: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 11
89: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 10 11
77: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 6
77: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 6
77: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 6
77: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 6
77: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 6
36 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
36: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 14
36: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 14
36: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 14
36: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 14
36: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 14
36: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 14
36: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 14
36: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 14
36: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 14
36: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 14
36: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 10 14
36: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 11 14
36: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 12 14
36: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 13 14
26 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
26: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 26
26: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 26
26: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 26
26: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 26
26: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 26
26: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 26
26: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 26
26: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 26
26: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 26
26: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 26
26: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 10 26
26: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 11 26
26: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 12 26
26: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 13 26
26: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 14 26
26: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 15 26
26: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 16 26
26: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 17 26
26: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 18 26
26: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 19 26
26: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 20 26
26: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 21 26
26: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 22 26
26: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 23 26
26: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 24 26
26: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 25 26
37: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 15
37: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 15
37: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 15
37: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 15
37: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 15
37: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 15
37: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 15
37: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 15
37: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 10 15
37: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 11 15
37: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 12 15
37: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 13 15
37: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 14 15
51: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 10
51: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 10
51: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 10
51: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 10
51: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 10
51: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 10
51: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 10
51: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 10
64: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 14
64: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 14
64: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 14
64: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 14
64: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 10 14
64: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 11 14
64: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 12 14
64: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 13 14
84: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 18
84: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 18
84: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 18
84: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 18
84: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 18
84: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 18
84: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 18
84: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 18
84: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 10 18
84: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 11 18
84: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 12 18
84: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 13 18
84: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 14 18
84: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 15 18
84: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 16 18
84: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 17 18
104: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 9
104: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 9
104: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 9
104: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 9
104: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 9
104: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 9
104: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 9
104: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 9
32: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 16
32: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 16
32: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 16
32: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 16
32: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 16
32: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 16
32: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 10 16
32: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 11 16
32: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 12 16
32: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 13 16
32: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 14 16
32: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 15 16
30: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 6
30: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 6
30: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 6
30: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 6
53: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 17
53: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 17
53: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 17
53: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 17
53: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 17
53: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 17
53: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 17
53: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 17
53: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 17
53: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 10 17
53: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 11 17
53: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 12 17
53: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 13 17
53: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 14 17
53: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 15 17
53: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 16 17
80: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 10
80: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 10
80: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 10
80: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 10
80: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 10
80: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 10
80: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 10
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 17
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 17
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 17
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 17
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 17
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 17
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 17
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 10 17
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 11 17
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 12 17
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 13 17
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 14 17
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 15 17
13: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 11
13: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 11
13: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 11
13: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 11
13: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 11
13: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 11
13: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 11
13: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 11
13: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 10 11
91: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 9
91: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 9
91: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 9
91: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 9
91: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 9
91: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 9
91: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 9
19: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 17
19: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 17
19: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 17
19: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 17
19: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 17
19: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 17
19: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 17
19: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 17
19: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 10 17
19: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 11 17
19: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 12 17
19: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 13 17
19: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 14 17
19: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 15 17
19: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 16 17
92: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 16
92: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 16
92: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 16
92: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 16
92: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 16
92: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 16
92: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 10 16
92: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 11 16
92: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 12 16
92: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 13 16
92: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 14 16
92: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 15 16
39: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 9
39: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 9
39: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 9
39: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 9
39: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 9
39: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 9
39: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 9
39: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 9
20: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 10
20: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 10
20: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 10
20: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 10
20: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 10
20: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 10
20: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 10
86: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 31
86: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 31
86: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 31
86: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 31
86: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 31
86: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 31
86: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 10 31
86: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 11 31
86: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 12 31
86: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 13 31
86: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 14 31
86: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 15 31
86: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 16 31
86: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 17 31
86: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 18 31
86: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 19 31
86: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 20 31
86: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 21 31
86: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 22 31
86: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 23 31
86: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 24 31
86: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 25 31
86: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 26 31
86: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 27 31
86: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 28 31
86: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 29 31
86: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 30 31
67: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 13
67: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 13
67: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 13
67: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 13
67: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 10 13
67: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 11 13
67: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 12 13
93: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 13
93: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 13
93: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 13
93: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 13
93: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 13
93: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 13
93: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 13
93: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 13
93: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 10 13
93: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 11 13
93: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 12 13
76: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 12
76: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 12
76: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 12
76: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 12
76: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 12
76: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 12
76: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 12
76: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 10 12
76: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 11 12
12: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 26
12: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 26
12: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 26
12: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 26
12: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 26
12: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 10 26
12: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 11 26
12: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 12 26
12: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 13 26
12: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 14 26
12: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 15 26
12: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 16 26
12: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 17 26
12: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 18 26
12: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 19 26
12: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 20 26
12: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 21 26
12: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 22 26
12: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 23 26
12: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 24 26
95: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 7
95: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 7
95: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 7
95: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 7
95: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 7
115: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 11
115: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 11
115: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 11
115: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 10 11
101 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
101: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 24
101: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 24
101: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 24
101: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 24
101: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 24
101: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 24
101: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 24
101: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 24
101: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 24
101: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 24
101: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 10 24
101: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 11 24
101: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 12 24
101: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 13 24
101: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 14 24
101: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 15 24
101: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 16 24
101: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 17 24
101: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 18 24
101: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 19 24
101: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 20 24
101: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 21 24
101: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 22 24
101: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 23 24
99 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
99: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 13
99: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 13
99: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 13
99: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 13
99: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 13
99: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 13
99: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 13
99: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 13
99: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 13
99: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 13
99: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 10 13
99: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 11 13
99: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 12 13
43 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
43: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 11
43: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 11
43: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 11
43: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 11
43: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 11
43: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 11
43: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 11
43: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 11
43: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 11
43: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 11
43: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 10 11
72 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
72: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 20
72: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 20
72: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 20
72: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 20
72: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 20
72: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 20
72: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 20
72: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 20
72: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 20
72: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 20
72: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 10 20
72: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 11 20
72: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 12 20
72: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 13 20
72: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 14 20
72: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 15 20
72: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 16 20
72: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 17 20
72: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 18 20
72: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 19 20
98 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
98: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 18
98: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 18
98: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 18
98: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 18
98: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 18
98: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 18
98: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 18
98: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 18
98: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 18
98: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 18
98: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 10 18
98: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 11 18
98: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 12 18
98: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 13 18
98: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 14 18
98: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 15 18
98: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 16 18
98: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 17 18
5 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 11
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 11
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 11
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 11
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 11
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 11
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 11
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 11
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 11
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 11
2 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 8
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 8
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 8
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 8
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 8
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 8
60: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 6
60: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 6
60: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 6
60: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 6
60: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 6
22 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
22: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 20
22: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 20
22: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 20
22: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 20
22: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 20
22: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 20
22: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 20
22: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 20
22: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 20
22: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 20
22: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 10 20
22: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 11 20
22: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 12 20
22: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 13 20
22: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 14 20
22: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 15 20
22: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 16 20
22: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 17 20
22: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 18 20
22: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 19 20
45 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
45: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 17
45: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 17
45: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 17
45: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 17
45: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 17
45: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 17
45: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 17
45: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 17
45: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 17
45: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 17
45: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 10 17
45: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 11 17
45: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 12 17
45: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 13 17
45: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 14 17
45: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 15 17
45: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 16 17
102 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
102: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 15
102: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 15
102: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 15
102: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 15
102: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 15
102: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 15
102: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 15
102: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 15
102: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 15
102: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 15
102: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 10 15
102: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 11 15
102: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 12 15
102: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 13 15
102: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 14 15
100 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
100: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 6
100: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 6
100: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 6
100: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 6
100: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 6
100: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 6
41 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
41: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 20
41: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 20
41: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 20
41: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 20
41: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 20
41: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 20
41: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 20
41: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 20
41: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 20
41: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 20
41: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 10 20
41: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 11 20
41: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 12 20
41: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 13 20
41: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 14 20
41: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 15 20
41: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 16 20
41: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 17 20
41: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 18 20
41: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 19 20
119: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 13
119: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 13
119: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 13
119: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 13
119: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 13
119: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 13
119: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 13
119: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 13
119: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 10 13
119: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 11 13
119: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 12 13
56 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
56: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 31
56: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 31
56: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 31
56: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 31
56: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 31
56: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 31
56: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 31
56: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 31
56: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 31
56: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 31
56: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 10 31
56: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 11 31
56: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 12 31
56: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 13 31
56: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 14 31
56: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 15 31
56: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 16 31
56: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 17 31
56: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 18 31
56: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 19 31
56: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 20 31
56: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 21 31
56: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 22 31
56: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 23 31
56: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 24 31
56: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 25 31
56: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 26 31
56: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 27 31
56: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 28 31
56: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 29 31
56: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 30 31
74 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
74: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 11
74: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 11
74: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 11
74: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 11
74: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 11
74: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 11
74: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 11
74: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 11
74: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 11
74: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 11
74: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 10 11
9 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
9: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 7
9: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 7
9: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 7
9: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 7
9: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 7
9: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 7
9: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 7
88 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
88: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 21
88: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 21
88: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 21
88: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 21
88: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 21
88: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 21
88: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 21
88: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 21
88: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 21
88: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 21
88: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 10 21
88: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 11 21
88: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 12 21
88: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 13 21
88: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 14 21
88: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 15 21
88: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 16 21
88: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 17 21
88: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 18 21
88: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 19 21
88: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 20 21
11 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
11: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 17
11: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 17
11: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 17
11: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 17
11: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 17
11: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 17
11: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 17
11: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 17
11: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 17
11: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 17
11: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 10 17
11: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 11 17
11: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 12 17
11: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 13 17
11: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 14 17
11: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 15 17
17 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
17: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 3
17: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 3
17: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 3
58 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
58: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 21
58: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 21
58: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 21
58: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 21
58: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 21
58: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 21
58: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 21
58: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 21
58: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 21
58: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 21
58: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 10 21
58: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 11 21
58: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 12 21
58: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 13 21
58: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 14 21
58: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 15 21
58: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 16 21
58: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 17 21
58: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 18 21
58: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 19 21
58: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 20 21
87 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
87: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 19
87: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 19
87: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 19
87: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 19
87: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 19
87: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 19
87: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 19
87: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 19
87: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 19
87: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 19
87: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 10 19
87: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 11 19
87: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 12 19
87: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 13 19
87: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 14 19
87: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 15 19
87: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 16 19
87: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 17 19
87: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 18 19
114: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 12 13
81: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 10 11
118: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 12 14
118: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 13 14
71: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 19 20
23: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 10 11
38: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 23 24
21: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 12
21: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 10 12
21: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 11 12
40: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 14 15
27: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 15 17
27: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 16 17
113: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 14 18
113: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 15 18
113: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 16 18
113: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 17 18
15: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 8
15: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 8
75: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 16 17
117: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 19 22
117: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 20 22
117: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 21 22
109: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 17 20
109: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 18 20
109: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 19 20
79: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 15 16
57: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 17 19
57: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 18 19
vRequests, &finished, &status); 75 117
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 76 117
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 77 117
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 78 117
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 79 117
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 80 117
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 81 117
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 82 117
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 83 117
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 84 117
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 85 117
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 86 117
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 87 117
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 88 117
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 89 117
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 90 117
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 91 117
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 92 117
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 93 117
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 94 117
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 95 117
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 96 117
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 97 117
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 98 117
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 99 117
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 100 117
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 101 117
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 102 117
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 103 117
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 104 117
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 105 117
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 106 117
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 107 117
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 108 117
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 109 117
61: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 9
61: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 9
61: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 9
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 110 117
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 111 117
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 112 117
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 113 117
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 114 117
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 115 117
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 116 117
Using a direct coarse solver (SuperLU)
Building hierarchy of 2 levels (inclusive coarse solver) took 0.0257018 seconds.
0 121 before Dune::BiCGSTABSolver 
0 Dune::BiCGSTABSolver.apply(x, b, result_) 
0 BiCGSTABSolver::apply 
0 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 3
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 3
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 3
0 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 3
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 3
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 3
3 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 9
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 9
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 9
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 9
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 9
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 9
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 9
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 9
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 9
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 12 14
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 13 14
8: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 18 20
8: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 19 20
117 === BiCGSTABSolver
 Iter          Defect            Rate
    0      1.17536e-06
117  (it = 0.5; it < _maxit; it+=.5) 0.5 250
117  before p = r 
117  before  y = W^-1 * p 
117  before _prec->apply(y,p); 0.5 250
117 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
void AMG<M,X,S,PI,A>::mgc(LevelContext& levelContext), before presmooth(levelContext, preSteps_) 
void presmooth(LevelContext& levelContext, size_t steps), before SmootherApplier<typename LevelContext::SmootherType>::preSmooth 
void preSmooth(Smoother& smoother, Domain& v, const Range& d), before smoother.apply(v,d);
117 schwarz.hh:virtual void apply (X& v, const Y& d), before communication.copyOwnerToAll(v,v); 
117 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
117: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 22
117: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 22
117: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 22
117: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 22
117: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 22
117: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 22
117: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 22
117: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 22
117: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 22
117: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 22
117: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 10 22
117: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 11 22
117: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 12 22
117: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 13 22
117: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 14 22
117: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 15 22
117: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 16 22
117: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 17 22
109 === BiCGSTABSolver
 Iter          Defect            Rate
    0      1.17536e-06
109  (it = 0.5; it < _maxit; it+=.5) 0.5 250
109  before p = r 
109  before  y = W^-1 * p 
109  before _prec->apply(y,p); 0.5 250
109 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
void AMG<M,X,S,PI,A>::mgc(LevelContext& levelContext), before presmooth(levelContext, preSteps_) 
void presmooth(LevelContext& levelContext, size_t steps), before SmootherApplier<typename LevelContext::SmootherType>::preSmooth 
void preSmooth(Smoother& smoother, Domain& v, const Range& d), before smoother.apply(v,d);
109 schwarz.hh:virtual void apply (X& v, const Y& d), before communication.copyOwnerToAll(v,v); 
109 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
109: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 20
109: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 20
109: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 20
109: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 20
109: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 20
109: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 20
109: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 20
109: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 20
109: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 20
109: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 20
109: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 10 20
109: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 11 20
109: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 12 20
109: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 13 20
109: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 14 20
109: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 15 20
109: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 16 20
109: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 17 20
109: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 18 20
109: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 19 20
14 === BiCGSTABSolver
 Iter          Defect            Rate
    0      1.17536e-06
14  (it = 0.5; it < _maxit; it+=.5) 0.5 250
14  before p = r 
14  before  y = W^-1 * p 
14  before _prec->apply(y,p); 0.5 250
14 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
void AMG<M,X,S,PI,A>::mgc(LevelContext& levelContext), before presmooth(levelContext, preSteps_) 
void presmooth(LevelContext& levelContext, size_t steps), before SmootherApplier<typename LevelContext::SmootherType>::preSmooth 
void preSmooth(Smoother& smoother, Domain& v, const Range& d), before smoother.apply(v,d);
14 schwarz.hh:virtual void apply (X& v, const Y& d), before communication.copyOwnerToAll(v,v); 
14 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
14: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 17
14: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 17
14: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 17
14: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 17
14: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 17
14: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 17
14: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 17
14: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 17
14: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 17
14: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 17
14: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 10 17
14: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 11 17
14: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 12 17
110 === BiCGSTABSolver
 Iter          Defect            Rate
    0      1.17536e-06
110  (it = 0.5; it < _maxit; it+=.5) 0.5 250
110  before p = r 
110  before  y = W^-1 * p 
110  before _prec->apply(y,p); 0.5 250
110 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
void AMG<M,X,S,PI,A>::mgc(LevelContext& levelContext), before presmooth(levelContext, preSteps_) 
void presmooth(LevelContext& levelContext, size_t steps), before SmootherApplier<typename LevelContext::SmootherType>::preSmooth 
void preSmooth(Smoother& smoother, Domain& v, const Range& d), before smoother.apply(v,d);
110 schwarz.hh:virtual void apply (X& v, const Y& d), before communication.copyOwnerToAll(v,v); 
110 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
110: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 11
110: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 11
110: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 11
110: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 11
110: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 11
110: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 11
110: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 11
110: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 11
110: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 11
28 === BiCGSTABSolver
 Iter          Defect            Rate
    0      1.17536e-06
28  (it = 0.5; it < _maxit; it+=.5) 0.5 250
28  before p = r 
28  before  y = W^-1 * p 
28  before _prec->apply(y,p); 0.5 250
28 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
void AMG<M,X,S,PI,A>::mgc(LevelContext& levelContext), before presmooth(levelContext, preSteps_) 
void presmooth(LevelContext& levelContext, size_t steps), before SmootherApplier<typename LevelContext::SmootherType>::preSmooth 
void preSmooth(Smoother& smoother, Domain& v, const Range& d), before smoother.apply(v,d);
28 schwarz.hh:virtual void apply (X& v, const Y& d), before communication.copyOwnerToAll(v,v); 
28 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
28: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 18
28: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 18
28: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 18
28: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 18
28: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 18
8 === BiCGSTABSolver
 Iter          Defect            Rate
    0      1.17536e-06
8  (it = 0.5; it < _maxit; it+=.5) 0.5 250
8  before p = r 
8  before  y = W^-1 * p 
8  before _prec->apply(y,p); 0.5 250
8 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
void AMG<M,X,S,PI,A>::mgc(LevelContext& levelContext), before presmooth(levelContext, preSteps_) 
void presmooth(LevelContext& levelContext, size_t steps), before SmootherApplier<typename LevelContext::SmootherType>::preSmooth 
void preSmooth(Smoother& smoother, Domain& v, const Range& d), before smoother.apply(v,d);
8 schwarz.hh:virtual void apply (X& v, const Y& d), before communication.copyOwnerToAll(v,v); 
8 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
8: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 20
8: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 20
8: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 20
8: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 20
8: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 20
8: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 20
8: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 20
8: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 20
8: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 20
8: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 20
8: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 10 20
8: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 11 20
8: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 12 20
8: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 13 20
8: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 14 20
8: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 15 20
8: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 16 20
8: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 17 20
8: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 18 20
70 === BiCGSTABSolver
 Iter          Defect            Rate
    0      1.17536e-06
70  (it = 0.5; it < _maxit; it+=.5) 0.5 250
70  before p = r 
70  before  y = W^-1 * p 
70  before _prec->apply(y,p); 0.5 250
70 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
void AMG<M,X,S,PI,A>::mgc(LevelContext& levelContext), before presmooth(levelContext, preSteps_) 
void presmooth(LevelContext& levelContext, size_t steps), before SmootherApplier<typename LevelContext::SmootherType>::preSmooth 
void preSmooth(Smoother& smoother, Domain& v, const Range& d), before smoother.apply(v,d);
66 === BiCGSTABSolver
 Iter          Defect            Rate
    0      1.17536e-06
66  (it = 0.5; it < _maxit; it+=.5) 0.5 250
66  before p = r 
66  before  y = W^-1 * p 
66  before _prec->apply(y,p); 0.5 250
66 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
void AMG<M,X,S,PI,A>::mgc(LevelContext& levelContext), before presmooth(levelContext, preSteps_) 
void presmooth(LevelContext& levelContext, size_t steps), before SmootherApplier<typename LevelContext::SmootherType>::preSmooth 
void preSmooth(Smoother& smoother, Domain& v, const Range& d), before smoother.apply(v,d);
66 schwarz.hh:virtual void apply (X& v, const Y& d), before communication.copyOwnerToAll(v,v); 
66 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
66: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 15
66: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 15
66: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 15
66: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 15
66: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 15
66: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 15
66: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 15
66: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 15
66: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 15
66: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 15
66: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 10 15
66: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 11 15
66: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 12 15
66: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 13 15
66: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 14 15
83 === BiCGSTABSolver
 Iter          Defect            Rate
    0      1.17536e-06
83  (it = 0.5; it < _maxit; it+=.5) 0.5 250
83  before p = r 
83  before  y = W^-1 * p 
83  before _prec->apply(y,p); 0.5 250
83 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
void AMG<M,X,S,PI,A>::mgc(LevelContext& levelContext), before presmooth(levelContext, preSteps_) 
void presmooth(LevelContext& levelContext, size_t steps), before SmootherApplier<typename LevelContext::SmootherType>::preSmooth 
void preSmooth(Smoother& smoother, Domain& v, const Range& d), before smoother.apply(v,d);
44 === BiCGSTABSolver
 Iter          Defect            Rate
    0      1.17536e-06
44  (it = 0.5; it < _maxit; it+=.5) 0.5 250
44  before p = r 
44  before  y = W^-1 * p 
44  before _prec->apply(y,p); 0.5 250
44 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
void AMG<M,X,S,PI,A>::mgc(LevelContext& levelContext), before presmooth(levelContext, preSteps_) 
void presmooth(LevelContext& levelContext, size_t steps), before SmootherApplier<typename LevelContext::SmootherType>::preSmooth 
void preSmooth(Smoother& smoother, Domain& v, const Range& d), before smoother.apply(v,d);
116 === BiCGSTABSolver
 Iter          Defect            Rate
    0      1.17536e-06
116  (it = 0.5; it < _maxit; it+=.5) 0.5 250
116  before p = r 
116  before  y = W^-1 * p 
116  before _prec->apply(y,p); 0.5 250
116 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
void AMG<M,X,S,PI,A>::mgc(LevelContext& levelContext), before presmooth(levelContext, preSteps_) 
void presmooth(LevelContext& levelContext, size_t steps), before SmootherApplier<typename LevelContext::SmootherType>::preSmooth 
void preSmooth(Smoother& smoother, Domain& v, const Range& d), before smoother.apply(v,d);
116 schwarz.hh:virtual void apply (X& v, const Y& d), before communication.copyOwnerToAll(v,v); 
116 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
116: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 7
116: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 7
116: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 7
116: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 7
116: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 7
116: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 7
116: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 7
void presmooth(LevelContext& levelContext, size_t steps), before SmootherApplier<typename LevelContext::SmootherType>::preSmooth 
void preSmooth(Smoother& smoother, Domain& v, const Range& d), before smoother.apply(v,d);
116 schwarz.hh:virtual void apply (X& v, const Y& d), before communication.copyOwnerToAll(v,v); 
116 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
116: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 7
42 === BiCGSTABSolver
 Iter          Defect            Rate
    0      1.17536e-06
42  (it = 0.5; it < _maxit; it+=.5) 0.5 250
42  before p = r 
42  before  y = W^-1 * p 
42  before _prec->apply(y,p); 0.5 250
42 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
void AMG<M,X,S,PI,A>::mgc(LevelContext& levelContext), before presmooth(levelContext, preSteps_) 
void presmooth(LevelContext& levelContext, size_t steps), before SmootherApplier<typename LevelContext::SmootherType>::preSmooth 
void preSmooth(Smoother& smoother, Domain& v, const Range& d), before smoother.apply(v,d);
42 schwarz.hh:virtual void apply (X& v, const Y& d), before communication.copyOwnerToAll(v,v); 
42 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
42: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 20
42: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 20
42: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 20
42: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 20
42: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 20
42: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 20
42: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 20
42: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 20
42: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 20
42: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 20
49 === BiCGSTABSolver
 Iter          Defect            Rate
    0      1.17536e-06
49  (it = 0.5; it < _maxit; it+=.5) 0.5 250
49  before p = r 
49  before  y = W^-1 * p 
49  before _prec->apply(y,p); 0.5 250
49 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
void AMG<M,X,S,PI,A>::mgc(LevelContext& levelContext), before presmooth(levelContext, preSteps_) 
void presmooth(LevelContext& levelContext, size_t steps), before SmootherApplier<typename LevelContext::SmootherType>::preSmooth 
void preSmooth(Smoother& smoother, Domain& v, const Range& d), before smoother.apply(v,d);
49 schwarz.hh:virtual void apply (X& v, const Y& d), before communication.copyOwnerToAll(v,v); 
49 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
49: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 15
49: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 15
49: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 15
49: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 15
49: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 15
49: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 15
49: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 15
49: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 15
49: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 15
107 === BiCGSTABSolver
 Iter          Defect            Rate
    0      1.17536e-06
107  (it = 0.5; it < _maxit; it+=.5) 0.5 250
107  before p = r 
107  before  y = W^-1 * p 
107  before _prec->apply(y,p); 0.5 250
107 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
void AMG<M,X,S,PI,A>::mgc(LevelContext& levelContext), before presmooth(levelContext, preSteps_) 
void presmooth(LevelContext& levelContext, size_t steps), before SmootherApplier<typename LevelContext::SmootherType>::preSmooth 
void preSmooth(Smoother& smoother, Domain& v, const Range& d), before smoother.apply(v,d);
107 schwarz.hh:virtual void apply (X& v, const Y& d), before communication.copyOwnerToAll(v,v); 
107 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
107: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 5
107: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 5
107: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 5
107: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 5
107: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 5
void presmooth(LevelContext& levelContext, size_t steps), before SmootherApplier<typename LevelContext::SmootherType>::preSmooth 
void preSmooth(Smoother& smoother, Domain& v, const Range& d), before smoother.apply(v,d);
107 schwarz.hh:virtual void apply (X& v, const Y& d), before communication.copyOwnerToAll(v,v); 
107 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
107: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 5
65 === BiCGSTABSolver
 Iter          Defect            Rate
    0      1.17536e-06
65  (it = 0.5; it < _maxit; it+=.5) 0.5 250
65  before p = r 
65  before  y = W^-1 * p 
65  before _prec->apply(y,p); 0.5 250
65 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
void AMG<M,X,S,PI,A>::mgc(LevelContext& levelContext), before presmooth(levelContext, preSteps_) 
void presmooth(LevelContext& levelContext, size_t steps), before SmootherApplier<typename LevelContext::SmootherType>::preSmooth 
void preSmooth(Smoother& smoother, Domain& v, const Range& d), before smoother.apply(v,d);
65 schwarz.hh:virtual void apply (X& v, const Y& d), before communication.copyOwnerToAll(v,v); 
65 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
65: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 9
65: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 9
65: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 9
65: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 9
65: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 9
65: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 9
65: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 9
65: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 9
1 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 5
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 5
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 5
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 5
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 5
1 === BiCGSTABSolver
 Iter          Defect            Rate
    0      1.17536e-06
1  (it = 0.5; it < _maxit; it+=.5) 0.5 250
1  before p = r 
1  before  y = W^-1 * p 
1  before _prec->apply(y,p); 0.5 250
1 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
void AMG<M,X,S,PI,A>::mgc(LevelContext& levelContext), before presmooth(levelContext, preSteps_) 
void presmooth(LevelContext& levelContext, size_t steps), before SmootherApplier<typename LevelContext::SmootherType>::preSmooth 
void preSmooth(Smoother& smoother, Domain& v, const Range& d), before smoother.apply(v,d);
1 schwarz.hh:virtual void apply (X& v, const Y& d), before communication.copyOwnerToAll(v,v); 
1 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 5
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 5
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 5
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 5
79 === BiCGSTABSolver
 Iter          Defect            Rate
    0      1.17536e-06
79  (it = 0.5; it < _maxit; it+=.5) 0.5 250
79  before p = r 
79  before  y = W^-1 * p 
79  before _prec->apply(y,p); 0.5 250
79 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
void AMG<M,X,S,PI,A>::mgc(LevelContext& levelContext), before presmooth(levelContext, preSteps_) 
void presmooth(LevelContext& levelContext, size_t steps), before SmootherApplier<typename LevelContext::SmootherType>::preSmooth 
void preSmooth(Smoother& smoother, Domain& v, const Range& d), before smoother.apply(v,d);
79 schwarz.hh:virtual void apply (X& v, const Y& d), before communication.copyOwnerToAll(v,v); 
79 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
79: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 16
79: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 16
79: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 16
79: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 16
79: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 16
79: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 16
79: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 16
79: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 16
79: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 16
79: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 16
79: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 10 16
79: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 11 16
79: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 12 16
79: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 13 16
105 === BiCGSTABSolver
 Iter          Defect            Rate
    0      1.17536e-06
105  (it = 0.5; it < _maxit; it+=.5) 0.5 250
105  before p = r 
105  before  y = W^-1 * p 
105  before _prec->apply(y,p); 0.5 250
105 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
void AMG<M,X,S,PI,A>::mgc(LevelContext& levelContext), before presmooth(levelContext, preSteps_) 
void presmooth(LevelContext& levelContext, size_t steps), before SmootherApplier<typename LevelContext::SmootherType>::preSmooth 
void preSmooth(Smoother& smoother, Domain& v, const Range& d), before smoother.apply(v,d);
105 schwarz.hh:virtual void apply (X& v, const Y& d), before communication.copyOwnerToAll(v,v); 
105 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
105: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 9
105: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 9
105: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 9
105: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 9
105: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 9
105: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 9
105: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 9
105: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 9
105: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 9
57 === BiCGSTABSolver
 Iter          Defect            Rate
    0      1.17536e-06
57  (it = 0.5; it < _maxit; it+=.5) 0.5 250
57  before p = r 
57  before  y = W^-1 * p 
57  before _prec->apply(y,p); 0.5 250
57 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
void AMG<M,X,S,PI,A>::mgc(LevelContext& levelContext), before presmooth(levelContext, preSteps_) 
void presmooth(LevelContext& levelContext, size_t steps), before SmootherApplier<typename LevelContext::SmootherType>::preSmooth 
void preSmooth(Smoother& smoother, Domain& v, const Range& d), before smoother.apply(v,d);
78 === BiCGSTABSolver
 Iter          Defect            Rate
    0      1.17536e-06
78  (it = 0.5; it < _maxit; it+=.5) 0.5 250
78  before p = r 
78  before  y = W^-1 * p 
78  before _prec->apply(y,p); 0.5 250
78 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
void AMG<M,X,S,PI,A>::mgc(LevelContext& levelContext), before presmooth(levelContext, preSteps_) 
void presmooth(LevelContext& levelContext, size_t steps), before SmootherApplier<typename LevelContext::SmootherType>::preSmooth 
void preSmooth(Smoother& smoother, Domain& v, const Range& d), before smoother.apply(v,d);
78 schwarz.hh:virtual void apply (X& v, const Y& d), before communication.copyOwnerToAll(v,v); 
78 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
78: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 15
78: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 15
78: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 15
78: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 15
78: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 15
78: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 15
78: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 15
78: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 15
78: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 15
78: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 15
18 === BiCGSTABSolver
 Iter          Defect            Rate
    0      1.17536e-06
18  (it = 0.5; it < _maxit; it+=.5) 0.5 250
18  before p = r 
18  before  y = W^-1 * p 
18  before _prec->apply(y,p); 0.5 250
18 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
void AMG<M,X,S,PI,A>::mgc(LevelContext& levelContext), before presmooth(levelContext, preSteps_) 
void presmooth(LevelContext& levelContext, size_t steps), before SmootherApplier<typename LevelContext::SmootherType>::preSmooth 
void preSmooth(Smoother& smoother, Domain& v, const Range& d), before smoother.apply(v,d);
18 schwarz.hh:virtual void apply (X& v, const Y& d), before communication.copyOwnerToAll(v,v); 
18 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
18: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 14
18: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 14
18: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 14
18: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 14
18: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 14
18: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 14
18: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 14
18: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 14
18: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 14
18: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 14
18: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 10 14
18: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 11 14
18: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 12 14
85 === BiCGSTABSolver
 Iter          Defect            Rate
    0      1.17536e-06
85  (it = 0.5; it < _maxit; it+=.5) 0.5 250
85  before p = r 
85  before  y = W^-1 * p 
85  before _prec->apply(y,p); 0.5 250
85 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
void AMG<M,X,S,PI,A>::mgc(LevelContext& levelContext), before presmooth(levelContext, preSteps_) 
void presmooth(LevelContext& levelContext, size_t steps), before SmootherApplier<typename LevelContext::SmootherType>::preSmooth 
void preSmooth(Smoother& smoother, Domain& v, const Range& d), before smoother.apply(v,d);
89 === BiCGSTABSolver
 Iter          Defect            Rate
    0      1.17536e-06
89  (it = 0.5; it < _maxit; it+=.5) 0.5 250
89  before p = r 
89  before  y = W^-1 * p 
89  before _prec->apply(y,p); 0.5 250
89 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
void AMG<M,X,S,PI,A>::mgc(LevelContext& levelContext), before presmooth(levelContext, preSteps_) 
void presmooth(LevelContext& levelContext, size_t steps), before SmootherApplier<typename LevelContext::SmootherType>::preSmooth 
void preSmooth(Smoother& smoother, Domain& v, const Range& d), before smoother.apply(v,d);
89 schwarz.hh:virtual void apply (X& v, const Y& d), before communication.copyOwnerToAll(v,v); 
89 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
89: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 11
89: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 11
89: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 11
89: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 11
77 === BiCGSTABSolver
 Iter          Defect            Rate
    0      1.17536e-06
77  (it = 0.5; it < _maxit; it+=.5) 0.5 250
77  before p = r 
77  before  y = W^-1 * p 
77  before _prec->apply(y,p); 0.5 250
77 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
void AMG<M,X,S,PI,A>::mgc(LevelContext& levelContext), before presmooth(levelContext, preSteps_) 
void presmooth(LevelContext& levelContext, size_t steps), before SmootherApplier<typename LevelContext::SmootherType>::preSmooth 
void preSmooth(Smoother& smoother, Domain& v, const Range& d), before smoother.apply(v,d);
77 schwarz.hh:virtual void apply (X& v, const Y& d), before communication.copyOwnerToAll(v,v); 
77 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
77: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 6
77: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 6
77: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 6
77: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 6
77: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 6
36 === BiCGSTABSolver
 Iter          Defect            Rate
    0      1.17536e-06
36  (it = 0.5; it < _maxit; it+=.5) 0.5 250
36  before p = r 
36  before  y = W^-1 * p 
36  before _prec->apply(y,p); 0.5 250
36 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
void AMG<M,X,S,PI,A>::mgc(LevelContext& levelContext), before presmooth(levelContext, preSteps_) 
void presmooth(LevelContext& levelContext, size_t steps), before SmootherApplier<typename LevelContext::SmootherType>::preSmooth 
void preSmooth(Smoother& smoother, Domain& v, const Range& d), before smoother.apply(v,d);
26 === BiCGSTABSolver
 Iter          Defect            Rate
    0      1.17536e-06
26  (it = 0.5; it < _maxit; it+=.5) 0.5 250
26  before p = r 
26  before  y = W^-1 * p 
26  before _prec->apply(y,p); 0.5 250
26 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
void AMG<M,X,S,PI,A>::mgc(LevelContext& levelContext), before presmooth(levelContext, preSteps_) 
void presmooth(LevelContext& levelContext, size_t steps), before SmootherApplier<typename LevelContext::SmootherType>::preSmooth 
void preSmooth(Smoother& smoother, Domain& v, const Range& d), before smoother.apply(v,d);
26 schwarz.hh:virtual void apply (X& v, const Y& d), before communication.copyOwnerToAll(v,v); 
26 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
26: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 26
26: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 26
26: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 26
26: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 26
26: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 26
26: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 26
26: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 26
26: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 26
26: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 26
26: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 26
37 === BiCGSTABSolver
 Iter          Defect            Rate
    0      1.17536e-06
37  (it = 0.5; it < _maxit; it+=.5) 0.5 250
37  before p = r 
37  before  y = W^-1 * p 
37  before _prec->apply(y,p); 0.5 250
37 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
void AMG<M,X,S,PI,A>::mgc(LevelContext& levelContext), before presmooth(levelContext, preSteps_) 
void presmooth(LevelContext& levelContext, size_t steps), before SmootherApplier<typename LevelContext::SmootherType>::preSmooth 
void preSmooth(Smoother& smoother, Domain& v, const Range& d), before smoother.apply(v,d);
37 schwarz.hh:virtual void apply (X& v, const Y& d), before communication.copyOwnerToAll(v,v); 
37 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
37: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 15
37: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 15
37: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 15
37: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 15
37: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 15
37: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 15
37: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 15
37: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 15
37: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 15
37: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 15
37: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 10 15
51 === BiCGSTABSolver
 Iter          Defect            Rate
    0      1.17536e-06
51  (it = 0.5; it < _maxit; it+=.5) 0.5 250
51  before p = r 
51  before  y = W^-1 * p 
51  before _prec->apply(y,p); 0.5 250
51 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
void AMG<M,X,S,PI,A>::mgc(LevelContext& levelContext), before presmooth(levelContext, preSteps_) 
void presmooth(LevelContext& levelContext, size_t steps), before SmootherApplier<typename LevelContext::SmootherType>::preSmooth 
void preSmooth(Smoother& smoother, Domain& v, const Range& d), before smoother.apply(v,d);
51 schwarz.hh:virtual void apply (X& v, const Y& d), before communication.copyOwnerToAll(v,v); 
51 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
51: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 10
51: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 10
51: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 10
51: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 10
51: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 10
64 === BiCGSTABSolver
 Iter          Defect            Rate
    0      1.17536e-06
64  (it = 0.5; it < _maxit; it+=.5) 0.5 250
64  before p = r 
64  before  y = W^-1 * p 
64  before _prec->apply(y,p); 0.5 250
64 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
void AMG<M,X,S,PI,A>::mgc(LevelContext& levelContext), before presmooth(levelContext, preSteps_) 
void presmooth(LevelContext& levelContext, size_t steps), before SmootherApplier<typename LevelContext::SmootherType>::preSmooth 
void preSmooth(Smoother& smoother, Domain& v, const Range& d), before smoother.apply(v,d);
64 schwarz.hh:virtual void apply (X& v, const Y& d), before communication.copyOwnerToAll(v,v); 
64 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
64: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 14
64: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 14
64: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 14
64: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 14
64: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 14
64: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 14
64: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 14
64: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 14
64: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 14
64: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 14
61 === BiCGSTABSolver
 Iter          Defect            Rate
    0      1.17536e-06
61  (it = 0.5; it < _maxit; it+=.5) 0.5 250
61  before p = r 
61  before  y = W^-1 * p 
61  before _prec->apply(y,p); 0.5 250
61 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
void AMG<M,X,S,PI,A>::mgc(LevelContext& levelContext), before presmooth(levelContext, preSteps_) 
void presmooth(LevelContext& levelContext, size_t steps), before SmootherApplier<typename LevelContext::SmootherType>::preSmooth 
void preSmooth(Smoother& smoother, Domain& v, const Range& d), before smoother.apply(v,d);
61 schwarz.hh:virtual void apply (X& v, const Y& d), before communication.copyOwnerToAll(v,v); 
61 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
61: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 9
61: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 9
61: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 9
61: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 9
61: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 9
61: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 9
61: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 9
84 === BiCGSTABSolver
 Iter          Defect            Rate
    0      1.17536e-06
84  (it = 0.5; it < _maxit; it+=.5) 0.5 250
84  before p = r 
84  before  y = W^-1 * p 
84  before _prec->apply(y,p); 0.5 250
84 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
void AMG<M,X,S,PI,A>::mgc(LevelContext& levelContext), before presmooth(levelContext, preSteps_) 
void presmooth(LevelContext& levelContext, size_t steps), before SmootherApplier<typename LevelContext::SmootherType>::preSmooth 
void preSmooth(Smoother& smoother, Domain& v, const Range& d), before smoother.apply(v,d);
84 schwarz.hh:virtual void apply (X& v, const Y& d), before communication.copyOwnerToAll(v,v); 
84 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
84: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 18
84: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 18
84: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 18
84: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 18
84: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 18
84: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 18
84: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 18
84: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 18
84: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 18
84: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 18
104 === BiCGSTABSolver
 Iter          Defect            Rate
    0      1.17536e-06
104  (it = 0.5; it < _maxit; it+=.5) 0.5 250
104  before p = r 
104  before  y = W^-1 * p 
104  before _prec->apply(y,p); 0.5 250
104 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
void AMG<M,X,S,PI,A>::mgc(LevelContext& levelContext), before presmooth(levelContext, preSteps_) 
void presmooth(LevelContext& levelContext, size_t steps), before SmootherApplier<typename LevelContext::SmootherType>::preSmooth 
void preSmooth(Smoother& smoother, Domain& v, const Range& d), before smoother.apply(v,d);
104 schwarz.hh:virtual void apply (X& v, const Y& d), before communication.copyOwnerToAll(v,v); 
104 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
104: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 9
104: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 9
104: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 9
32 === BiCGSTABSolver
 Iter          Defect            Rate
    0      1.17536e-06
32  (it = 0.5; it < _maxit; it+=.5) 0.5 250
32  before p = r 
32  before  y = W^-1 * p 
32  before _prec->apply(y,p); 0.5 250
32 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
void AMG<M,X,S,PI,A>::mgc(LevelContext& levelContext), before presmooth(levelContext, preSteps_) 
void presmooth(LevelContext& levelContext, size_t steps), before SmootherApplier<typename LevelContext::SmootherType>::preSmooth 
void preSmooth(Smoother& smoother, Domain& v, const Range& d), before smoother.apply(v,d);
32 schwarz.hh:virtual void apply (X& v, const Y& d), before communication.copyOwnerToAll(v,v); 
32 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
32: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 16
32: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 16
32: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 16
32: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 16
32: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 16
32: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 16
32: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 16
32: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 16
32: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 16
32: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 16
32: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 10 16
30 === BiCGSTABSolver
 Iter          Defect            Rate
    0      1.17536e-06
30  (it = 0.5; it < _maxit; it+=.5) 0.5 250
30  before p = r 
30  before  y = W^-1 * p 
30  before _prec->apply(y,p); 0.5 250
30 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
void AMG<M,X,S,PI,A>::mgc(LevelContext& levelContext), before presmooth(levelContext, preSteps_) 
void presmooth(LevelContext& levelContext, size_t steps), before SmootherApplier<typename LevelContext::SmootherType>::preSmooth 
void preSmooth(Smoother& smoother, Domain& v, const Range& d), before smoother.apply(v,d);
30 schwarz.hh:virtual void apply (X& v, const Y& d), before communication.copyOwnerToAll(v,v); 
30 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
30: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 6
30: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 6
30: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 6
30: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 6
30: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 6
30: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 6
53 === BiCGSTABSolver
 Iter          Defect            Rate
    0      1.17536e-06
53  (it = 0.5; it < _maxit; it+=.5) 0.5 250
53  before p = r 
53  before  y = W^-1 * p 
53  before _prec->apply(y,p); 0.5 250
53 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
void AMG<M,X,S,PI,A>::mgc(LevelContext& levelContext), before presmooth(levelContext, preSteps_) 
void presmooth(LevelContext& levelContext, size_t steps), before SmootherApplier<typename LevelContext::SmootherType>::preSmooth 
void preSmooth(Smoother& smoother, Domain& v, const Range& d), before smoother.apply(v,d);
53 schwarz.hh:virtual void apply (X& v, const Y& d), before communication.copyOwnerToAll(v,v); 
53 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
53: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 17
53: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 17
53: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 17
53: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 17
53: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 17
53: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 17
80 === BiCGSTABSolver
 Iter          Defect            Rate
    0      1.17536e-06
80  (it = 0.5; it < _maxit; it+=.5) 0.5 250
80  before p = r 
80  before  y = W^-1 * p 
80  before _prec->apply(y,p); 0.5 250
80 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
void AMG<M,X,S,PI,A>::mgc(LevelContext& levelContext), before presmooth(levelContext, preSteps_) 
void presmooth(LevelContext& levelContext, size_t steps), before SmootherApplier<typename LevelContext::SmootherType>::preSmooth 
void preSmooth(Smoother& smoother, Domain& v, const Range& d), before smoother.apply(v,d);
80 schwarz.hh:virtual void apply (X& v, const Y& d), before communication.copyOwnerToAll(v,v); 
80 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
80: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 10
80: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 10
80: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 10
80: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 10
80: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 10
80: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 10
80: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 10
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 16 17
6 === BiCGSTABSolver
 Iter          Defect            Rate
    0      1.17536e-06
6  (it = 0.5; it < _maxit; it+=.5) 0.5 250
6  before p = r 
6  before  y = W^-1 * p 
6  before _prec->apply(y,p); 0.5 250
6 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
void AMG<M,X,S,PI,A>::mgc(LevelContext& levelContext), before presmooth(levelContext, preSteps_) 
void presmooth(LevelContext& levelContext, size_t steps), before SmootherApplier<typename LevelContext::SmootherType>::preSmooth 
void preSmooth(Smoother& smoother, Domain& v, const Range& d), before smoother.apply(v,d);
6 schwarz.hh:virtual void apply (X& v, const Y& d), before communication.copyOwnerToAll(v,v); 
6 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 17
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 17
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 17
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 17
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 17
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 17
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 17
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 17
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 17
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 17
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 10 17
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 11 17
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 12 17
7 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
7: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 13
7: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 13
7: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 13
7: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 13
7: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 13
7: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 13
7: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 13
7: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 13
7: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 13
7: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 13
7: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 10 13
7: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 11 13
7: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 12 13
7 === BiCGSTABSolver
 Iter          Defect            Rate
    0      1.17536e-06
7  (it = 0.5; it < _maxit; it+=.5) 0.5 250
7  before p = r 
7  before  y = W^-1 * p 
7  before _prec->apply(y,p); 0.5 250
7 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
void AMG<M,X,S,PI,A>::mgc(LevelContext& levelContext), before presmooth(levelContext, preSteps_) 
void presmooth(LevelContext& levelContext, size_t steps), before SmootherApplier<typename LevelContext::SmootherType>::preSmooth 
void preSmooth(Smoother& smoother, Domain& v, const Range& d), before smoother.apply(v,d);
7 schwarz.hh:virtual void apply (X& v, const Y& d), before communication.copyOwnerToAll(v,v); 
7 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
7: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 13
7: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 13
7: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 13
7: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 13
7: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 13
7: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 13
7: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 13
7: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 13
7: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 13
7: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 13
13 === BiCGSTABSolver
 Iter          Defect            Rate
    0      1.17536e-06
13  (it = 0.5; it < _maxit; it+=.5) 0.5 250
13  before p = r 
13  before  y = W^-1 * p 
13  before _prec->apply(y,p); 0.5 250
13 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
void AMG<M,X,S,PI,A>::mgc(LevelContext& levelContext), before presmooth(levelContext, preSteps_) 
void presmooth(LevelContext& levelContext, size_t steps), before SmootherApplier<typename LevelContext::SmootherType>::preSmooth 
void preSmooth(Smoother& smoother, Domain& v, const Range& d), before smoother.apply(v,d);
13 schwarz.hh:virtual void apply (X& v, const Y& d), before communication.copyOwnerToAll(v,v); 
13 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
13: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 11
13: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 11
13: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 11
13: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 11
13: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 11
13: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 11
13: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 11
91 === BiCGSTABSolver
 Iter          Defect            Rate
    0      1.17536e-06
91  (it = 0.5; it < _maxit; it+=.5) 0.5 250
91  before p = r 
91  before  y = W^-1 * p 
91  before _prec->apply(y,p); 0.5 250
91 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
void AMG<M,X,S,PI,A>::mgc(LevelContext& levelContext), before presmooth(levelContext, preSteps_) 
void presmooth(LevelContext& levelContext, size_t steps), before SmootherApplier<typename LevelContext::SmootherType>::preSmooth 
void preSmooth(Smoother& smoother, Domain& v, const Range& d), before smoother.apply(v,d);
91 schwarz.hh:virtual void apply (X& v, const Y& d), before communication.copyOwnerToAll(v,v); 
91 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
91: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 9
91: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 9
91: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 9
91: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 9
91: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 9
91: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 9
91: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 9
91: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 9
91: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 9
19 === BiCGSTABSolver
 Iter          Defect            Rate
    0      1.17536e-06
19  (it = 0.5; it < _maxit; it+=.5) 0.5 250
19  before p = r 
19  before  y = W^-1 * p 
19  before _prec->apply(y,p); 0.5 250
19 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
void AMG<M,X,S,PI,A>::mgc(LevelContext& levelContext), before presmooth(levelContext, preSteps_) 
void presmooth(LevelContext& levelContext, size_t steps), before SmootherApplier<typename LevelContext::SmootherType>::preSmooth 
void preSmooth(Smoother& smoother, Domain& v, const Range& d), before smoother.apply(v,d);
19 schwarz.hh:virtual void apply (X& v, const Y& d), before communication.copyOwnerToAll(v,v); 
19 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
19: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 17
19: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 17
19: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 17
19: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 17
19: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 17
19: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 17
19: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 17
19: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 17
19: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 17
19: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 17
92 === BiCGSTABSolver
 Iter          Defect            Rate
    0      1.17536e-06
92  (it = 0.5; it < _maxit; it+=.5) 0.5 250
92  before p = r 
92  before  y = W^-1 * p 
92  before _prec->apply(y,p); 0.5 250
92 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
void AMG<M,X,S,PI,A>::mgc(LevelContext& levelContext), before presmooth(levelContext, preSteps_) 
void presmooth(LevelContext& levelContext, size_t steps), before SmootherApplier<typename LevelContext::SmootherType>::preSmooth 
void preSmooth(Smoother& smoother, Domain& v, const Range& d), before smoother.apply(v,d);
92 schwarz.hh:virtual void apply (X& v, const Y& d), before communication.copyOwnerToAll(v,v); 
92 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
92: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 16
92: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 16
92: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 16
92: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 16
92: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 16
92: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 16
92: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 16
92: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 16
92: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 16
92: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 16
92: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 10 16
92: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 11 16
92: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 12 16
92: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 13 16
39 === BiCGSTABSolver
 Iter          Defect            Rate
    0      1.17536e-06
39  (it = 0.5; it < _maxit; it+=.5) 0.5 250
39  before p = r 
39  before  y = W^-1 * p 
39  before _prec->apply(y,p); 0.5 250
39 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
void AMG<M,X,S,PI,A>::mgc(LevelContext& levelContext), before presmooth(levelContext, preSteps_) 
void presmooth(LevelContext& levelContext, size_t steps), before SmootherApplier<typename LevelContext::SmootherType>::preSmooth 
void preSmooth(Smoother& smoother, Domain& v, const Range& d), before smoother.apply(v,d);
39 schwarz.hh:virtual void apply (X& v, const Y& d), before communication.copyOwnerToAll(v,v); 
39 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
39: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 9
39: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 9
39: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 9
39: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 9
39: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 9
39: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 9
39: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 9
39: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 9
39: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 9
20 === BiCGSTABSolver
 Iter          Defect            Rate
    0      1.17536e-06
20  (it = 0.5; it < _maxit; it+=.5) 0.5 250
20  before p = r 
20  before  y = W^-1 * p 
20  before _prec->apply(y,p); 0.5 250
20 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
void AMG<M,X,S,PI,A>::mgc(LevelContext& levelContext), before presmooth(levelContext, preSteps_) 
void presmooth(LevelContext& levelContext, size_t steps), before SmootherApplier<typename LevelContext::SmootherType>::preSmooth 
void preSmooth(Smoother& smoother, Domain& v, const Range& d), before smoother.apply(v,d);
20 schwarz.hh:virtual void apply (X& v, const Y& d), before communication.copyOwnerToAll(v,v); 
20 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
20: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 10
20: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 10
20: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 10
20: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 10
20: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 10
20: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 10
20: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 10
20: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 10
20: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 10
20: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 10
86 === BiCGSTABSolver
 Iter          Defect            Rate
    0      1.17536e-06
86  (it = 0.5; it < _maxit; it+=.5) 0.5 250
86  before p = r 
86  before  y = W^-1 * p 
86  before _prec->apply(y,p); 0.5 250
86 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
void AMG<M,X,S,PI,A>::mgc(LevelContext& levelContext), before presmooth(levelContext, preSteps_) 
void presmooth(LevelContext& levelContext, size_t steps), before SmootherApplier<typename LevelContext::SmootherType>::preSmooth 
void preSmooth(Smoother& smoother, Domain& v, const Range& d), before smoother.apply(v,d);
86 schwarz.hh:virtual void apply (X& v, const Y& d), before communication.copyOwnerToAll(v,v); 
86 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
86: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 31
86: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 31
86: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 31
86: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 31
86: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 31
86: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 31
86: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 31
86: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 31
86: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 31
86: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 31
86: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 10 31
86: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 11 31
86: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 12 31
86: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 13 31
86: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 14 31
86: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 15 31
86: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 16 31
86: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 17 31
67 === BiCGSTABSolver
 Iter          Defect            Rate
    0      1.17536e-06
67  (it = 0.5; it < _maxit; it+=.5) 0.5 250
67  before p = r 
67  before  y = W^-1 * p 
67  before _prec->apply(y,p); 0.5 250
67 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
void AMG<M,X,S,PI,A>::mgc(LevelContext& levelContext), before presmooth(levelContext, preSteps_) 
void presmooth(LevelContext& levelContext, size_t steps), before SmootherApplier<typename LevelContext::SmootherType>::preSmooth 
void preSmooth(Smoother& smoother, Domain& v, const Range& d), before smoother.apply(v,d);
67 schwarz.hh:virtual void apply (X& v, const Y& d), before communication.copyOwnerToAll(v,v); 
67 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
67: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 13
67: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 13
67: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 13
67: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 13
67: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 13
67: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 13
67: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 13
93 === BiCGSTABSolver
 Iter          Defect            Rate
    0      1.17536e-06
93  (it = 0.5; it < _maxit; it+=.5) 0.5 250
93  before p = r 
93  before  y = W^-1 * p 
93  before _prec->apply(y,p); 0.5 250
93 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
void AMG<M,X,S,PI,A>::mgc(LevelContext& levelContext), before presmooth(levelContext, preSteps_) 
void presmooth(LevelContext& levelContext, size_t steps), before SmootherApplier<typename LevelContext::SmootherType>::preSmooth 
void preSmooth(Smoother& smoother, Domain& v, const Range& d), before smoother.apply(v,d);
93 schwarz.hh:virtual void apply (X& v, const Y& d), before communication.copyOwnerToAll(v,v); 
93 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
93: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 13
93: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 13
93: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 13
93: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 13
93: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 13
93: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 13
93: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 13
93: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 13
93: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 13
93: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 13
93: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 10 13
93: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 11 13
93: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 12 13
76 === BiCGSTABSolver
 Iter          Defect            Rate
    0      1.17536e-06
76  (it = 0.5; it < _maxit; it+=.5) 0.5 250
76  before p = r 
76  before  y = W^-1 * p 
76  before _prec->apply(y,p); 0.5 250
76 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
void AMG<M,X,S,PI,A>::mgc(LevelContext& levelContext), before presmooth(levelContext, preSteps_) 
void presmooth(LevelContext& levelContext, size_t steps), before SmootherApplier<typename LevelContext::SmootherType>::preSmooth 
void preSmooth(Smoother& smoother, Domain& v, const Range& d), before smoother.apply(v,d);
76 schwarz.hh:virtual void apply (X& v, const Y& d), before communication.copyOwnerToAll(v,v); 
76 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
76: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 12
76: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 12
76: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 12
76: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 12
76: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 12
76: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 12
76: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 12
76: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 12
76: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 12
76: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 12
76: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 10 12
12: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 25 26
12 === BiCGSTABSolver
 Iter          Defect            Rate
    0      1.17536e-06
12  (it = 0.5; it < _maxit; it+=.5) 0.5 250
12  before p = r 
12  before  y = W^-1 * p 
12  before _prec->apply(y,p); 0.5 250
12 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
void AMG<M,X,S,PI,A>::mgc(LevelContext& levelContext), before presmooth(levelContext, preSteps_) 
void presmooth(LevelContext& levelContext, size_t steps), before SmootherApplier<typename LevelContext::SmootherType>::preSmooth 
void preSmooth(Smoother& smoother, Domain& v, const Range& d), before smoother.apply(v,d);
12 schwarz.hh:virtual void apply (X& v, const Y& d), before communication.copyOwnerToAll(v,v); 
12 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
12: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 26
12: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 26
12: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 26
12: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 26
12: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 26
12: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 26
12: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 26
12: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 26
12: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 26
12: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 26
12: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 10 26
12: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 11 26
12: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 12 26
12: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 13 26
12: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 14 26
12: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 15 26
12: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 16 26
12: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 17 26
12: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 18 26
12: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 19 26
12: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 20 26
12: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 21 26
12: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 22 26
95 === BiCGSTABSolver
 Iter          Defect            Rate
    0      1.17536e-06
95  (it = 0.5; it < _maxit; it+=.5) 0.5 250
95  before p = r 
95  before  y = W^-1 * p 
95  before _prec->apply(y,p); 0.5 250
95 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
void AMG<M,X,S,PI,A>::mgc(LevelContext& levelContext), before presmooth(levelContext, preSteps_) 
void presmooth(LevelContext& levelContext, size_t steps), before SmootherApplier<typename LevelContext::SmootherType>::preSmooth 
void preSmooth(Smoother& smoother, Domain& v, const Range& d), before smoother.apply(v,d);
95 schwarz.hh:virtual void apply (X& v, const Y& d), before communication.copyOwnerToAll(v,v); 
95 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
95: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 7
95: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 7
95: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 7
95: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 7
95: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 7
95: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 7
95: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 7
115 === BiCGSTABSolver
 Iter          Defect            Rate
    0      1.17536e-06
115  (it = 0.5; it < _maxit; it+=.5) 0.5 250
115  before p = r 
115  before  y = W^-1 * p 
115  before _prec->apply(y,p); 0.5 250
115 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
void AMG<M,X,S,PI,A>::mgc(LevelContext& levelContext), before presmooth(levelContext, preSteps_) 
void presmooth(LevelContext& levelContext, size_t steps), before SmootherApplier<typename LevelContext::SmootherType>::preSmooth 
void preSmooth(Smoother& smoother, Domain& v, const Range& d), before smoother.apply(v,d);
115 schwarz.hh:virtual void apply (X& v, const Y& d), before communication.copyOwnerToAll(v,v); 
115 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
115: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 11
115: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 11
115: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 11
115: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 11
115: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 11
115: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 11
115: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 11
115: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 11
115: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 11
115: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 11
115: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 10 11
101 === BiCGSTABSolver
 Iter          Defect            Rate
    0      1.17536e-06
101  (it = 0.5; it < _maxit; it+=.5) 0.5 250
101  before p = r 
101  before  y = W^-1 * p 
101  before _prec->apply(y,p); 0.5 250
101 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
void AMG<M,X,S,PI,A>::mgc(LevelContext& levelContext), before presmooth(levelContext, preSteps_) 
void presmooth(LevelContext& levelContext, size_t steps), before SmootherApplier<typename LevelContext::SmootherType>::preSmooth 
void preSmooth(Smoother& smoother, Domain& v, const Range& d), before smoother.apply(v,d);
101 schwarz.hh:virtual void apply (X& v, const Y& d), before communication.copyOwnerToAll(v,v); 
101 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
101: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 24
101: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 24
101: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 24
101: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 24
101: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 24
101: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 24
101: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 24
101: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 24
101: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 24
101: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 24
101: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 10 24
101: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 11 24
101: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 12 24
101: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 13 24
101: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 14 24
101: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 15 24
101: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 16 24
101: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 17 24
101: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 18 24
99 === BiCGSTABSolver
 Iter          Defect            Rate
    0      1.17536e-06
99  (it = 0.5; it < _maxit; it+=.5) 0.5 250
99  before p = r 
99  before  y = W^-1 * p 
99  before _prec->apply(y,p); 0.5 250
99 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
void AMG<M,X,S,PI,A>::mgc(LevelContext& levelContext), before presmooth(levelContext, preSteps_) 
void presmooth(LevelContext& levelContext, size_t steps), before SmootherApplier<typename LevelContext::SmootherType>::preSmooth 
void preSmooth(Smoother& smoother, Domain& v, const Range& d), before smoother.apply(v,d);
99 schwarz.hh:virtual void apply (X& v, const Y& d), before communication.copyOwnerToAll(v,v); 
99 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
99: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 13
99: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 13
99: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 13
99: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 13
99: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 13
99: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 13
99: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 13
99: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 13
99: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 13
99: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 13
99: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 10 13
43 === BiCGSTABSolver
 Iter          Defect            Rate
    0      1.17536e-06
43  (it = 0.5; it < _maxit; it+=.5) 0.5 250
43  before p = r 
43  before  y = W^-1 * p 
43  before _prec->apply(y,p); 0.5 250
43 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
void AMG<M,X,S,PI,A>::mgc(LevelContext& levelContext), before presmooth(levelContext, preSteps_) 
void presmooth(LevelContext& levelContext, size_t steps), before SmootherApplier<typename LevelContext::SmootherType>::preSmooth 
void preSmooth(Smoother& smoother, Domain& v, const Range& d), before smoother.apply(v,d);
43 schwarz.hh:virtual void apply (X& v, const Y& d), before communication.copyOwnerToAll(v,v); 
43 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
43: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 11
43: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 11
43: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 11
43: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 11
72 === BiCGSTABSolver
 Iter          Defect            Rate
    0      1.17536e-06
72  (it = 0.5; it < _maxit; it+=.5) 0.5 250
72  before p = r 
72  before  y = W^-1 * p 
72  before _prec->apply(y,p); 0.5 250
72 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
void AMG<M,X,S,PI,A>::mgc(LevelContext& levelContext), before presmooth(levelContext, preSteps_) 
void presmooth(LevelContext& levelContext, size_t steps), before SmootherApplier<typename LevelContext::SmootherType>::preSmooth 
void preSmooth(Smoother& smoother, Domain& v, const Range& d), before smoother.apply(v,d);
72 schwarz.hh:virtual void apply (X& v, const Y& d), before communication.copyOwnerToAll(v,v); 
72 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
72: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 20
72: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 20
72: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 20
72: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 20
72: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 20
72: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 20
72: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 20
72: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 20
72: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 20
72: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 20
72: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 10 20
72: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 11 20
72: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 12 20
72: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 13 20
72: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 14 20
72: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 15 20
72: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 16 20
72: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 17 20
72: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 18 20
72: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 19 20
98 === BiCGSTABSolver
 Iter          Defect            Rate
    0      1.17536e-06
98  (it = 0.5; it < _maxit; it+=.5) 0.5 250
98  before p = r 
98  before  y = W^-1 * p 
98  before _prec->apply(y,p); 0.5 250
98 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
void AMG<M,X,S,PI,A>::mgc(LevelContext& levelContext), before presmooth(levelContext, preSteps_) 
void presmooth(LevelContext& levelContext, size_t steps), before SmootherApplier<typename LevelContext::SmootherType>::preSmooth 
void preSmooth(Smoother& smoother, Domain& v, const Range& d), before smoother.apply(v,d);
98 schwarz.hh:virtual void apply (X& v, const Y& d), before communication.copyOwnerToAll(v,v); 
98 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
98: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 18
98: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 18
98: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 18
98: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 18
98: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 18
98: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 18
98: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 18
98: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 18
98: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 18
98: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 18
98: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 10 18
98: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 11 18
98: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 12 18
98: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 13 18
98: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 14 18
98: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 15 18
98: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 16 18
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 10 11
5 === BiCGSTABSolver
 Iter          Defect            Rate
    0      1.17536e-06
5  (it = 0.5; it < _maxit; it+=.5) 0.5 250
5  before p = r 
5  before  y = W^-1 * p 
5  before _prec->apply(y,p); 0.5 250
5 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
void AMG<M,X,S,PI,A>::mgc(LevelContext& levelContext), before presmooth(levelContext, preSteps_) 
void presmooth(LevelContext& levelContext, size_t steps), before SmootherApplier<typename LevelContext::SmootherType>::preSmooth 
void preSmooth(Smoother& smoother, Domain& v, const Range& d), before smoother.apply(v,d);
5 schwarz.hh:virtual void apply (X& v, const Y& d), before communication.copyOwnerToAll(v,v); 
5 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 11
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 11
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 11
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 11
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 11
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 11
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 8
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 8
2 === BiCGSTABSolver
 Iter          Defect            Rate
    0      1.17536e-06
2  (it = 0.5; it < _maxit; it+=.5) 0.5 250
2  before p = r 
2  before  y = W^-1 * p 
2  before _prec->apply(y,p); 0.5 250
2 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
void AMG<M,X,S,PI,A>::mgc(LevelContext& levelContext), before presmooth(levelContext, preSteps_) 
void presmooth(LevelContext& levelContext, size_t steps), before SmootherApplier<typename LevelContext::SmootherType>::preSmooth 
void preSmooth(Smoother& smoother, Domain& v, const Range& d), before smoother.apply(v,d);
2 schwarz.hh:virtual void apply (X& v, const Y& d), before communication.copyOwnerToAll(v,v); 
2 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 8
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 8
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 8
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 8
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 8
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 8
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 8
60 === BiCGSTABSolver
 Iter          Defect            Rate
    0      1.17536e-06
60  (it = 0.5; it < _maxit; it+=.5) 0.5 250
60  before p = r 
60  before  y = W^-1 * p 
60  before _prec->apply(y,p); 0.5 250
60 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
void AMG<M,X,S,PI,A>::mgc(LevelContext& levelContext), before presmooth(levelContext, preSteps_) 
void presmooth(LevelContext& levelContext, size_t steps), before SmootherApplier<typename LevelContext::SmootherType>::preSmooth 
void preSmooth(Smoother& smoother, Domain& v, const Range& d), before smoother.apply(v,d);
60 schwarz.hh:virtual void apply (X& v, const Y& d), before communication.copyOwnerToAll(v,v); 
60 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
60: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 6
60: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 6
60: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 6
60: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 6
60: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 6
60: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 6
void presmooth(LevelContext& levelContext, size_t steps), before SmootherApplier<typename LevelContext::SmootherType>::preSmooth 
void preSmooth(Smoother& smoother, Domain& v, const Range& d), before smoother.apply(v,d);
60 schwarz.hh:virtual void apply (X& v, const Y& d), before communication.copyOwnerToAll(v,v); 
60 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
60: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 6
22 === BiCGSTABSolver
 Iter          Defect            Rate
    0      1.17536e-06
22  (it = 0.5; it < _maxit; it+=.5) 0.5 250
22  before p = r 
22  before  y = W^-1 * p 
22  before _prec->apply(y,p); 0.5 250
22 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
void AMG<M,X,S,PI,A>::mgc(LevelContext& levelContext), before presmooth(levelContext, preSteps_) 
void presmooth(LevelContext& levelContext, size_t steps), before SmootherApplier<typename LevelContext::SmootherType>::preSmooth 
void preSmooth(Smoother& smoother, Domain& v, const Range& d), before smoother.apply(v,d);
22 schwarz.hh:virtual void apply (X& v, const Y& d), before communication.copyOwnerToAll(v,v); 
22 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
22: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 20
22: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 20
22: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 20
22: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 20
22: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 20
22: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 20
22: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 20
22: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 20
22: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 20
22: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 20
22: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 10 20
22: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 11 20
22: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 12 20
22: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 13 20
22: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 14 20
22: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 15 20
22: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 16 20
22: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 17 20
22: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 18 20
22: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 19 20
45 === BiCGSTABSolver
 Iter          Defect            Rate
    0      1.17536e-06
45  (it = 0.5; it < _maxit; it+=.5) 0.5 250
45  before p = r 
45  before  y = W^-1 * p 
45  before _prec->apply(y,p); 0.5 250
45 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
void AMG<M,X,S,PI,A>::mgc(LevelContext& levelContext), before presmooth(levelContext, preSteps_) 
void presmooth(LevelContext& levelContext, size_t steps), before SmootherApplier<typename LevelContext::SmootherType>::preSmooth 
void preSmooth(Smoother& smoother, Domain& v, const Range& d), before smoother.apply(v,d);
45 schwarz.hh:virtual void apply (X& v, const Y& d), before communication.copyOwnerToAll(v,v); 
45 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
45: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 17
45: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 17
45: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 17
45: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 17
45: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 17
45: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 17
102 === BiCGSTABSolver
 Iter          Defect            Rate
    0      1.17536e-06
102  (it = 0.5; it < _maxit; it+=.5) 0.5 250
102  before p = r 
102  before  y = W^-1 * p 
102  before _prec->apply(y,p); 0.5 250
102 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
void AMG<M,X,S,PI,A>::mgc(LevelContext& levelContext), before presmooth(levelContext, preSteps_) 
void presmooth(LevelContext& levelContext, size_t steps), before SmootherApplier<typename LevelContext::SmootherType>::preSmooth 
void preSmooth(Smoother& smoother, Domain& v, const Range& d), before smoother.apply(v,d);
102 schwarz.hh:virtual void apply (X& v, const Y& d), before communication.copyOwnerToAll(v,v); 
102 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
102: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 15
102: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 15
102: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 15
102: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 15
102: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 15
102: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 15
102: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 15
102: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 15
102: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 15
102: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 15
102: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 10 15
102: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 11 15
102: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 12 15
102: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 13 15
100 === BiCGSTABSolver
 Iter          Defect            Rate
    0      1.17536e-06
100  (it = 0.5; it < _maxit; it+=.5) 0.5 250
100  before p = r 
100  before  y = W^-1 * p 
100  before _prec->apply(y,p); 0.5 250
100 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
void AMG<M,X,S,PI,A>::mgc(LevelContext& levelContext), before presmooth(levelContext, preSteps_) 
void presmooth(LevelContext& levelContext, size_t steps), before SmootherApplier<typename LevelContext::SmootherType>::preSmooth 
void preSmooth(Smoother& smoother, Domain& v, const Range& d), before smoother.apply(v,d);
100 schwarz.hh:virtual void apply (X& v, const Y& d), before communication.copyOwnerToAll(v,v); 
100 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
100: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 6
100: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 6
100: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 6
100: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 6
100: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 6
100: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 6
41 === BiCGSTABSolver
 Iter          Defect            Rate
    0      1.17536e-06
41  (it = 0.5; it < _maxit; it+=.5) 0.5 250
41  before p = r 
41  before  y = W^-1 * p 
41  before _prec->apply(y,p); 0.5 250
41 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
void AMG<M,X,S,PI,A>::mgc(LevelContext& levelContext), before presmooth(levelContext, preSteps_) 
void presmooth(LevelContext& levelContext, size_t steps), before SmootherApplier<typename LevelContext::SmootherType>::preSmooth 
void preSmooth(Smoother& smoother, Domain& v, const Range& d), before smoother.apply(v,d);
41 schwarz.hh:virtual void apply (X& v, const Y& d), before communication.copyOwnerToAll(v,v); 
41 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
41: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 20
41: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 20
41: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 20
41: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 20
41: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 20
41: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 20
41: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 20
41: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 20
41: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 20
41: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 20
41: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 10 20
41: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 11 20
41: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 12 20
41: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 13 20
119 === BiCGSTABSolver
 Iter          Defect            Rate
    0      1.17536e-06
119  (it = 0.5; it < _maxit; it+=.5) 0.5 250
119  before p = r 
119  before  y = W^-1 * p 
119  before _prec->apply(y,p); 0.5 250
119 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
void AMG<M,X,S,PI,A>::mgc(LevelContext& levelContext), before presmooth(levelContext, preSteps_) 
void presmooth(LevelContext& levelContext, size_t steps), before SmootherApplier<typename LevelContext::SmootherType>::preSmooth 
void preSmooth(Smoother& smoother, Domain& v, const Range& d), before smoother.apply(v,d);
119 schwarz.hh:virtual void apply (X& v, const Y& d), before communication.copyOwnerToAll(v,v); 
119 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
119: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 13
119: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 13
119: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 13
119: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 13
119: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 13
119: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 13
119: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 13
119: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 13
119: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 13
119: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 13
119: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 10 13
119: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 11 13
119: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 12 13
56 === BiCGSTABSolver
 Iter          Defect            Rate
    0      1.17536e-06
56  (it = 0.5; it < _maxit; it+=.5) 0.5 250
56  before p = r 
56  before  y = W^-1 * p 
56  before _prec->apply(y,p); 0.5 250
56 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
void AMG<M,X,S,PI,A>::mgc(LevelContext& levelContext), before presmooth(levelContext, preSteps_) 
void presmooth(LevelContext& levelContext, size_t steps), before SmootherApplier<typename LevelContext::SmootherType>::preSmooth 
void preSmooth(Smoother& smoother, Domain& v, const Range& d), before smoother.apply(v,d);
56 schwarz.hh:virtual void apply (X& v, const Y& d), before communication.copyOwnerToAll(v,v); 
56 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
56: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 31
56: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 31
56: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 31
56: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 31
56: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 31
56: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 31
56: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 31
56: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 31
56: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 31
56: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 31
56: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 10 31
56: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 11 31
56: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 12 31
56: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 13 31
56: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 14 31
56: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 15 31
56: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 16 31
56: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 17 31
56: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 18 31
56: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 19 31
56: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 20 31
74 === BiCGSTABSolver
 Iter          Defect            Rate
    0      1.17536e-06
74  (it = 0.5; it < _maxit; it+=.5) 0.5 250
74  before p = r 
74  before  y = W^-1 * p 
74  before _prec->apply(y,p); 0.5 250
74 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
void AMG<M,X,S,PI,A>::mgc(LevelContext& levelContext), before presmooth(levelContext, preSteps_) 
void presmooth(LevelContext& levelContext, size_t steps), before SmootherApplier<typename LevelContext::SmootherType>::preSmooth 
void preSmooth(Smoother& smoother, Domain& v, const Range& d), before smoother.apply(v,d);
74 schwarz.hh:virtual void apply (X& v, const Y& d), before communication.copyOwnerToAll(v,v); 
74 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
74: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 11
74: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 11
74: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 11
74: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 11
74: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 11
74: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 11
74: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 11
74: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 11
74: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 11
74: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 11
74: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 10 11
9 === BiCGSTABSolver
 Iter          Defect            Rate
    0      1.17536e-06
9  (it = 0.5; it < _maxit; it+=.5) 0.5 250
9  before p = r 
9  before  y = W^-1 * p 
9  before _prec->apply(y,p); 0.5 250
9 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
void AMG<M,X,S,PI,A>::mgc(LevelContext& levelContext), before presmooth(levelContext, preSteps_) 
void presmooth(LevelContext& levelContext, size_t steps), before SmootherApplier<typename LevelContext::SmootherType>::preSmooth 
void preSmooth(Smoother& smoother, Domain& v, const Range& d), before smoother.apply(v,d);
9 schwarz.hh:virtual void apply (X& v, const Y& d), before communication.copyOwnerToAll(v,v); 
9 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
9: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 7
9: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 7
9: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 7
9: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 7
9: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 7
9: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 7
9: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 7
88 === BiCGSTABSolver
 Iter          Defect            Rate
    0      1.17536e-06
88  (it = 0.5; it < _maxit; it+=.5) 0.5 250
88  before p = r 
88  before  y = W^-1 * p 
88  before _prec->apply(y,p); 0.5 250
88 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
void AMG<M,X,S,PI,A>::mgc(LevelContext& levelContext), before presmooth(levelContext, preSteps_) 
void presmooth(LevelContext& levelContext, size_t steps), before SmootherApplier<typename LevelContext::SmootherType>::preSmooth 
void preSmooth(Smoother& smoother, Domain& v, const Range& d), before smoother.apply(v,d);
88 schwarz.hh:virtual void apply (X& v, const Y& d), before communication.copyOwnerToAll(v,v); 
88 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
88: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 21
88: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 21
88: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 21
88: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 21
88: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 21
88: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 21
88: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 21
88: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 21
88: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 21
88: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 21
88: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 10 21
88: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 11 21
88: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 12 21
88: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 13 21
88: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 14 21
88: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 15 21
88: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 16 21
88: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 17 21
11: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 16 17
11 === BiCGSTABSolver
 Iter          Defect            Rate
    0      1.17536e-06
11  (it = 0.5; it < _maxit; it+=.5) 0.5 250
11  before p = r 
11  before  y = W^-1 * p 
11  before _prec->apply(y,p); 0.5 250
11 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
void AMG<M,X,S,PI,A>::mgc(LevelContext& levelContext), before presmooth(levelContext, preSteps_) 
void presmooth(LevelContext& levelContext, size_t steps), before SmootherApplier<typename LevelContext::SmootherType>::preSmooth 
void preSmooth(Smoother& smoother, Domain& v, const Range& d), before smoother.apply(v,d);
11 schwarz.hh:virtual void apply (X& v, const Y& d), before communication.copyOwnerToAll(v,v); 
11 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
11: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 17
11: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 17
11: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 17
11: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 17
11: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 17
11: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 17
11: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 17
11: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 17
11: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 17
11: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 17
11: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 10 17
11: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 11 17
17 === BiCGSTABSolver
 Iter          Defect            Rate
    0      1.17536e-06
17  (it = 0.5; it < _maxit; it+=.5) 0.5 250
17  before p = r 
17  before  y = W^-1 * p 
17  before _prec->apply(y,p); 0.5 250
17 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
void AMG<M,X,S,PI,A>::mgc(LevelContext& levelContext), before presmooth(levelContext, preSteps_) 
void presmooth(LevelContext& levelContext, size_t steps), before SmootherApplier<typename LevelContext::SmootherType>::preSmooth 
void preSmooth(Smoother& smoother, Domain& v, const Range& d), before smoother.apply(v,d);
17 schwarz.hh:virtual void apply (X& v, const Y& d), before communication.copyOwnerToAll(v,v); 
17 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
17: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 3
17: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 3
17: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 3
void presmooth(LevelContext& levelContext, size_t steps), before SmootherApplier<typename LevelContext::SmootherType>::preSmooth 
void preSmooth(Smoother& smoother, Domain& v, const Range& d), before smoother.apply(v,d);
17 schwarz.hh:virtual void apply (X& v, const Y& d), before communication.copyOwnerToAll(v,v); 
17 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
17: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 3
58 === BiCGSTABSolver
 Iter          Defect            Rate
    0      1.17536e-06
58  (it = 0.5; it < _maxit; it+=.5) 0.5 250
58  before p = r 
58  before  y = W^-1 * p 
58  before _prec->apply(y,p); 0.5 250
58 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
void AMG<M,X,S,PI,A>::mgc(LevelContext& levelContext), before presmooth(levelContext, preSteps_) 
void presmooth(LevelContext& levelContext, size_t steps), before SmootherApplier<typename LevelContext::SmootherType>::preSmooth 
void preSmooth(Smoother& smoother, Domain& v, const Range& d), before smoother.apply(v,d);
58 schwarz.hh:virtual void apply (X& v, const Y& d), before communication.copyOwnerToAll(v,v); 
58 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
58: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 21
58: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 21
58: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 21
58: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 21
58: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 21
58: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 21
58: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 21
58: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 21
58: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 21
58: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 21
58: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 10 21
58: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 11 21
58: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 12 21
58: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 13 21
108 === BiCGSTABSolver
 Iter          Defect            Rate
    0      1.17536e-06
108  (it = 0.5; it < _maxit; it+=.5) 0.5 250
108  before p = r 
108  before  y = W^-1 * p 
108  before _prec->apply(y,p); 0.5 250
108 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
void AMG<M,X,S,PI,A>::mgc(LevelContext& levelContext), before presmooth(levelContext, preSteps_) 
void presmooth(LevelContext& levelContext, size_t steps), before SmootherApplier<typename LevelContext::SmootherType>::preSmooth 
void preSmooth(Smoother& smoother, Domain& v, const Range& d), before smoother.apply(v,d);
108 schwarz.hh:virtual void apply (X& v, const Y& d), before communication.copyOwnerToAll(v,v); 
108 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
108: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 9
108: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 9
108: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 9
108: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 9
108: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 9
108: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 9
108: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 9
108: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 9
108: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 9
97 === BiCGSTABSolver
 Iter          Defect            Rate
    0      1.17536e-06
97  (it = 0.5; it < _maxit; it+=.5) 0.5 250
97  before p = r 
97  before  y = W^-1 * p 
97  before _prec->apply(y,p); 0.5 250
97 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
void AMG<M,X,S,PI,A>::mgc(LevelContext& levelContext), before presmooth(levelContext, preSteps_) 
void presmooth(LevelContext& levelContext, size_t steps), before SmootherApplier<typename LevelContext::SmootherType>::preSmooth 
void preSmooth(Smoother& smoother, Domain& v, const Range& d), before smoother.apply(v,d);
97 schwarz.hh:virtual void apply (X& v, const Y& d), before communication.copyOwnerToAll(v,v); 
97 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
97: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 9
97: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 9
97: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 9
97: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 9
97: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 9
97: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 9
97: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 9
97: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 9
97: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 9
void presmooth(LevelContext& levelContext, size_t steps), before SmootherApplier<typename LevelContext::SmootherType>::preSmooth 
void preSmooth(Smoother& smoother, Domain& v, const Range& d), before smoother.apply(v,d);
87 === BiCGSTABSolver
 Iter          Defect            Rate
    0      1.17536e-06
87  (it = 0.5; it < _maxit; it+=.5) 0.5 250
87  before p = r 
87  before  y = W^-1 * p 
87  before _prec->apply(y,p); 0.5 250
87 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
void AMG<M,X,S,PI,A>::mgc(LevelContext& levelContext), before presmooth(levelContext, preSteps_) 
void presmooth(LevelContext& levelContext, size_t steps), before SmootherApplier<typename LevelContext::SmootherType>::preSmooth 
void preSmooth(Smoother& smoother, Domain& v, const Range& d), before smoother.apply(v,d);
87 schwarz.hh:virtual void apply (X& v, const Y& d), before communication.copyOwnerToAll(v,v); 
87 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
87: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 19
87: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 19
87: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 19
87: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 19
87: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 19
87: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 19
87: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 19
87: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 19
34 === BiCGSTABSolver
 Iter          Defect            Rate
    0      1.17536e-06
34  (it = 0.5; it < _maxit; it+=.5) 0.5 250
34  before p = r 
34  before  y = W^-1 * p 
34  before _prec->apply(y,p); 0.5 250
34 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
void AMG<M,X,S,PI,A>::mgc(LevelContext& levelContext), before presmooth(levelContext, preSteps_) 
void presmooth(LevelContext& levelContext, size_t steps), before SmootherApplier<typename LevelContext::SmootherType>::preSmooth 
void preSmooth(Smoother& smoother, Domain& v, const Range& d), before smoother.apply(v,d);
34 schwarz.hh:virtual void apply (X& v, const Y& d), before communication.copyOwnerToAll(v,v); 
34 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
34: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 14
34: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 14
34: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 14
34: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 14
34: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 14
34: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 14
34: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 14
34: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 14
34: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 14
34: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 14
34: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 10 14
34: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 11 14
34: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 12 14
34: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 13 14
24 === BiCGSTABSolver
 Iter          Defect            Rate
    0      1.17536e-06
24  (it = 0.5; it < _maxit; it+=.5) 0.5 250
24  before p = r 
24  before  y = W^-1 * p 
24  before _prec->apply(y,p); 0.5 250
24 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
void AMG<M,X,S,PI,A>::mgc(LevelContext& levelContext), before presmooth(levelContext, preSteps_) 
void presmooth(LevelContext& levelContext, size_t steps), before SmootherApplier<typename LevelContext::SmootherType>::preSmooth 
void preSmooth(Smoother& smoother, Domain& v, const Range& d), before smoother.apply(v,d);
24 schwarz.hh:virtual void apply (X& v, const Y& d), before communication.copyOwnerToAll(v,v); 
24 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
24: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 14
24: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 14
24: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 14
24: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 14
24: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 14
24: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 14
24: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 14
24: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 14
24: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 14
24: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 14
24: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 10 14
24: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 11 14
24: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 12 14
24: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 13 14
25 === BiCGSTABSolver
 Iter          Defect            Rate
    0      1.17536e-06
25  (it = 0.5; it < _maxit; it+=.5) 0.5 250
25  before p = r 
25  before  y = W^-1 * p 
25  before _prec->apply(y,p); 0.5 250
25 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
void AMG<M,X,S,PI,A>::mgc(LevelContext& levelContext), before presmooth(levelContext, preSteps_) 
void presmooth(LevelContext& levelContext, size_t steps), before SmootherApplier<typename LevelContext::SmootherType>::preSmooth 
void preSmooth(Smoother& smoother, Domain& v, const Range& d), before smoother.apply(v,d);
25 schwarz.hh:virtual void apply (X& v, const Y& d), before communication.copyOwnerToAll(v,v); 
25 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
25: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 7
25: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 7
25: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 7
25: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 7
25: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 7
25: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 7
25: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 7
103 === BiCGSTABSolver
 Iter          Defect            Rate
    0      1.17536e-06
103  (it = 0.5; it < _maxit; it+=.5) 0.5 250
103  before p = r 
103  before  y = W^-1 * p 
103  before _prec->apply(y,p); 0.5 250
103 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
void AMG<M,X,S,PI,A>::mgc(LevelContext& levelContext), before presmooth(levelContext, preSteps_) 
void presmooth(LevelContext& levelContext, size_t steps), before SmootherApplier<typename LevelContext::SmootherType>::preSmooth 
void preSmooth(Smoother& smoother, Domain& v, const Range& d), before smoother.apply(v,d);
103 schwarz.hh:virtual void apply (X& v, const Y& d), before communication.copyOwnerToAll(v,v); 
103 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
103: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 16
103: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 16
103: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 16
103: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 16
103: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 16
103: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 16
103: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 16
103: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 16
103: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 16
103: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 16
103: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 10 16
103: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 11 16
103: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 12 16
103: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 13 16
103: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 14 16
103: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 15 16
114 === BiCGSTABSolver
 Iter          Defect            Rate
    0      1.17536e-06
114  (it = 0.5; it < _maxit; it+=.5) 0.5 250
114  before p = r 
114  before  y = W^-1 * p 
114  before _prec->apply(y,p); 0.5 250
114 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
void AMG<M,X,S,PI,A>::mgc(LevelContext& levelContext), before presmooth(levelContext, preSteps_) 
void presmooth(LevelContext& levelContext, size_t steps), before SmootherApplier<typename LevelContext::SmootherType>::preSmooth 
void preSmooth(Smoother& smoother, Domain& v, const Range& d), before smoother.apply(v,d);
114 schwarz.hh:virtual void apply (X& v, const Y& d), before communication.copyOwnerToAll(v,v); 
114 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
114: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 13
114: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 13
114: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 13
114: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 13
114: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 13
114: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 13
114: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 13
114: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 13
114: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 13
114: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 13
114: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 10 13
114: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 11 13
114: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 12 13
0 === BiCGSTABSolver
 Iter          Defect            Rate
    0      1.17536e-06
0  (it = 0.5; it < _maxit; it+=.5) 0.5 250
0  before p = r 
0  before  y = W^-1 * p 
0  before _prec->apply(y,p); 0.5 250
0 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
void AMG<M,X,S,PI,A>::mgc(LevelContext& levelContext), before presmooth(levelContext, preSteps_) 
void presmooth(LevelContext& levelContext, size_t steps), before SmootherApplier<typename LevelContext::SmootherType>::preSmooth 
void preSmooth(Smoother& smoother, Domain& v, const Range& d), before smoother.apply(v,d);
0 schwarz.hh:virtual void apply (X& v, const Y& d), before communication.copyOwnerToAll(v,v); 
0 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 3
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 3
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 3
void presmooth(LevelContext& levelContext, size_t steps), before SmootherApplier<typename LevelContext::SmootherType>::preSmooth 
void preSmooth(Smoother& smoother, Domain& v, const Range& d), before smoother.apply(v,d);
0 schwarz.hh:virtual void apply (X& v, const Y& d), before communication.copyOwnerToAll(v,v); 
0 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 3
3 === BiCGSTABSolver
 Iter          Defect            Rate
    0      1.17536e-06
3  (it = 0.5; it < _maxit; it+=.5) 0.5 250
3  before p = r 
3  before  y = W^-1 * p 
3  before _prec->apply(y,p); 0.5 250
3 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
void AMG<M,X,S,PI,A>::mgc(LevelContext& levelContext), before presmooth(levelContext, preSteps_) 
void presmooth(LevelContext& levelContext, size_t steps), before SmootherApplier<typename LevelContext::SmootherType>::preSmooth 
void preSmooth(Smoother& smoother, Domain& v, const Range& d), before smoother.apply(v,d);
3 schwarz.hh:virtual void apply (X& v, const Y& d), before communication.copyOwnerToAll(v,v); 
3 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 9
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 9
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 9
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 9
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 9
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 9
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 9
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 9
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 9
void presmooth(LevelContext& levelContext, size_t steps), before SmootherApplier<typename LevelContext::SmootherType>::preSmooth 
void preSmooth(Smoother& smoother, Domain& v, const Range& d), before smoother.apply(v,d);
3 schwarz.hh:virtual void apply (X& v, const Y& d), before communication.copyOwnerToAll(v,v); 
3 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
96 === BiCGSTABSolver
 Iter          Defect            Rate
    0      1.17536e-06
96  (it = 0.5; it < _maxit; it+=.5) 0.5 250
96  before p = r 
96  before  y = W^-1 * p 
96  before _prec->apply(y,p); 0.5 250
96 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
void AMG<M,X,S,PI,A>::mgc(LevelContext& levelContext), before presmooth(levelContext, preSteps_) 
void presmooth(LevelContext& levelContext, size_t steps), before SmootherApplier<typename LevelContext::SmootherType>::preSmooth 
void preSmooth(Smoother& smoother, Domain& v, const Range& d), before smoother.apply(v,d);
96 schwarz.hh:virtual void apply (X& v, const Y& d), before communication.copyOwnerToAll(v,v); 
96 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
96: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 12
96: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 12
96: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 12
96: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 12
96: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 12
96: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 12
96: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 12
96: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 12
96: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 12
96: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 12
96: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 10 12
96: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 11 12
81 === BiCGSTABSolver
 Iter          Defect            Rate
    0      1.17536e-06
81  (it = 0.5; it < _maxit; it+=.5) 0.5 250
81  before p = r 
81  before  y = W^-1 * p 
81  before _prec->apply(y,p); 0.5 250
81 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
void AMG<M,X,S,PI,A>::mgc(LevelContext& levelContext), before presmooth(levelContext, preSteps_) 
void presmooth(LevelContext& levelContext, size_t steps), before SmootherApplier<typename LevelContext::SmootherType>::preSmooth 
void preSmooth(Smoother& smoother, Domain& v, const Range& d), before smoother.apply(v,d);
81 schwarz.hh:virtual void apply (X& v, const Y& d), before communication.copyOwnerToAll(v,v); 
81 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
81: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 11
81: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 11
81: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 11
81: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 11
81: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 11
81: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 11
81: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 11
81: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 11
81: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 11
81: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 11
81: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 10 11
void presmooth(LevelContext& levelContext, size_t steps), before SmootherApplier<typename LevelContext::SmootherType>::preSmooth 
void preSmooth(Smoother& smoother, Domain& v, const Range& d), before smoother.apply(v,d);
50 === BiCGSTABSolver
 Iter          Defect            Rate
    0      1.17536e-06
50  (it = 0.5; it < _maxit; it+=.5) 0.5 250
50  before p = r 
50  before  y = W^-1 * p 
50  before _prec->apply(y,p); 0.5 250
50 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
void AMG<M,X,S,PI,A>::mgc(LevelContext& levelContext), before presmooth(levelContext, preSteps_) 
void presmooth(LevelContext& levelContext, size_t steps), before SmootherApplier<typename LevelContext::SmootherType>::preSmooth 
void preSmooth(Smoother& smoother, Domain& v, const Range& d), before smoother.apply(v,d);
50 schwarz.hh:virtual void apply (X& v, const Y& d), before communication.copyOwnerToAll(v,v); 
50 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
50: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 10
50: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 10
50: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 10
50: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 10
50: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 10
50: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 10
50: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 10
50: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 10
50: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 10
50: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 10
16 === BiCGSTABSolver
 Iter          Defect            Rate
    0      1.17536e-06
16  (it = 0.5; it < _maxit; it+=.5) 0.5 250
16  before p = r 
16  before  y = W^-1 * p 
16  before _prec->apply(y,p); 0.5 250
16 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
void AMG<M,X,S,PI,A>::mgc(LevelContext& levelContext), before presmooth(levelContext, preSteps_) 
void presmooth(LevelContext& levelContext, size_t steps), before SmootherApplier<typename LevelContext::SmootherType>::preSmooth 
void preSmooth(Smoother& smoother, Domain& v, const Range& d), before smoother.apply(v,d);
16 schwarz.hh:virtual void apply (X& v, const Y& d), before communication.copyOwnerToAll(v,v); 
16 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
16: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 6
16: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 6
16: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 6
16: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 6
16: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 6
16: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 6
29 === BiCGSTABSolver
 Iter          Defect            Rate
    0      1.17536e-06
29  (it = 0.5; it < _maxit; it+=.5) 0.5 250
29  before p = r 
29  before  y = W^-1 * p 
29  before _prec->apply(y,p); 0.5 250
29 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
void AMG<M,X,S,PI,A>::mgc(LevelContext& levelContext), before presmooth(levelContext, preSteps_) 
void presmooth(LevelContext& levelContext, size_t steps), before SmootherApplier<typename LevelContext::SmootherType>::preSmooth 
void preSmooth(Smoother& smoother, Domain& v, const Range& d), before smoother.apply(v,d);
29 schwarz.hh:virtual void apply (X& v, const Y& d), before communication.copyOwnerToAll(v,v); 
29 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
29: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 11
29: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 11
29: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 11
29: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 11
29: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 11
29: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 11
29: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 11
29: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 11
29: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 11
29: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 11
29: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 10 11
68 === BiCGSTABSolver
 Iter          Defect            Rate
    0      1.17536e-06
68  (it = 0.5; it < _maxit; it+=.5) 0.5 250
68  before p = r 
68  before  y = W^-1 * p 
68  before _prec->apply(y,p); 0.5 250
68 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
void AMG<M,X,S,PI,A>::mgc(LevelContext& levelContext), before presmooth(levelContext, preSteps_) 
void presmooth(LevelContext& levelContext, size_t steps), before SmootherApplier<typename LevelContext::SmootherType>::preSmooth 
void preSmooth(Smoother& smoother, Domain& v, const Range& d), before smoother.apply(v,d);
47 === BiCGSTABSolver
 Iter          Defect            Rate
    0      1.17536e-06
47  (it = 0.5; it < _maxit; it+=.5) 0.5 250
47  before p = r 
47  before  y = W^-1 * p 
47  before _prec->apply(y,p); 0.5 250
47 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
void AMG<M,X,S,PI,A>::mgc(LevelContext& levelContext), before presmooth(levelContext, preSteps_) 
void presmooth(LevelContext& levelContext, size_t steps), before SmootherApplier<typename LevelContext::SmootherType>::preSmooth 
void preSmooth(Smoother& smoother, Domain& v, const Range& d), before smoother.apply(v,d);
47 schwarz.hh:virtual void apply (X& v, const Y& d), before communication.copyOwnerToAll(v,v); 
47 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
47: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 6
47: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 6
47: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 6
47: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 6
47: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 6
47: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 6
void presmooth(LevelContext& levelContext, size_t steps), before SmootherApplier<typename LevelContext::SmootherType>::preSmooth 
void preSmooth(Smoother& smoother, Domain& v, const Range& d), before smoother.apply(v,d);
47 schwarz.hh:virtual void apply (X& v, const Y& d), before communication.copyOwnerToAll(v,v); 
47 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
47: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 6
118 === BiCGSTABSolver
 Iter          Defect            Rate
    0      1.17536e-06
118  (it = 0.5; it < _maxit; it+=.5) 0.5 250
118  before p = r 
118  before  y = W^-1 * p 
118  before _prec->apply(y,p); 0.5 250
118 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
void AMG<M,X,S,PI,A>::mgc(LevelContext& levelContext), before presmooth(levelContext, preSteps_) 
void presmooth(LevelContext& levelContext, size_t steps), before SmootherApplier<typename LevelContext::SmootherType>::preSmooth 
void preSmooth(Smoother& smoother, Domain& v, const Range& d), before smoother.apply(v,d);
118 schwarz.hh:virtual void apply (X& v, const Y& d), before communication.copyOwnerToAll(v,v); 
118 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
118: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 14
118: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 14
118: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 14
118: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 14
118: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 14
118: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 14
118: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 14
118: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 14
118: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 14
118: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 14
118: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 10 14
118: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 11 14
118: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 12 14
118: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 13 14
void presmooth(LevelContext& levelContext, size_t steps), before SmootherApplier<typename LevelContext::SmootherType>::preSmooth 
void preSmooth(Smoother& smoother, Domain& v, const Range& d), before smoother.apply(v,d);
62 === BiCGSTABSolver
 Iter          Defect            Rate
    0      1.17536e-06
62  (it = 0.5; it < _maxit; it+=.5) 0.5 250
62  before p = r 
62  before  y = W^-1 * p 
62  before _prec->apply(y,p); 0.5 250
62 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
void AMG<M,X,S,PI,A>::mgc(LevelContext& levelContext), before presmooth(levelContext, preSteps_) 
void presmooth(LevelContext& levelContext, size_t steps), before SmootherApplier<typename LevelContext::SmootherType>::preSmooth 
void preSmooth(Smoother& smoother, Domain& v, const Range& d), before smoother.apply(v,d);
71 === BiCGSTABSolver
 Iter          Defect            Rate
    0      1.17536e-06
71  (it = 0.5; it < _maxit; it+=.5) 0.5 250
71  before p = r 
71  before  y = W^-1 * p 
71  before _prec->apply(y,p); 0.5 250
71 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
void AMG<M,X,S,PI,A>::mgc(LevelContext& levelContext), before presmooth(levelContext, preSteps_) 
void presmooth(LevelContext& levelContext, size_t steps), before SmootherApplier<typename LevelContext::SmootherType>::preSmooth 
void preSmooth(Smoother& smoother, Domain& v, const Range& d), before smoother.apply(v,d);
71 schwarz.hh:virtual void apply (X& v, const Y& d), before communication.copyOwnerToAll(v,v); 
71 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
71: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 20
71: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 20
71: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 20
71: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 20
71: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 20
71: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 20
71: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 20
71: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 20
71: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 20
71: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 20
71: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 10 20
71: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 11 20
71: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 12 20
71: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 13 20
71: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 14 20
112 === BiCGSTABSolver
 Iter          Defect            Rate
    0      1.17536e-06
112  (it = 0.5; it < _maxit; it+=.5) 0.5 250
112  before p = r 
112  before  y = W^-1 * p 
112  before _prec->apply(y,p); 0.5 250
112 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
void AMG<M,X,S,PI,A>::mgc(LevelContext& levelContext), before presmooth(levelContext, preSteps_) 
void presmooth(LevelContext& levelContext, size_t steps), before SmootherApplier<typename LevelContext::SmootherType>::preSmooth 
void preSmooth(Smoother& smoother, Domain& v, const Range& d), before smoother.apply(v,d);
112 schwarz.hh:virtual void apply (X& v, const Y& d), before communication.copyOwnerToAll(v,v); 
112 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
112: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 5
112: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 5
112: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 5
112: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 5
112: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 5
void presmooth(LevelContext& levelContext, size_t steps), before SmootherApplier<typename LevelContext::SmootherType>::preSmooth 
void preSmooth(Smoother& smoother, Domain& v, const Range& d), before smoother.apply(v,d);
23 === BiCGSTABSolver
 Iter          Defect            Rate
    0      1.17536e-06
23  (it = 0.5; it < _maxit; it+=.5) 0.5 250
23  before p = r 
23  before  y = W^-1 * p 
23  before _prec->apply(y,p); 0.5 250
23 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
void AMG<M,X,S,PI,A>::mgc(LevelContext& levelContext), before presmooth(levelContext, preSteps_) 
void presmooth(LevelContext& levelContext, size_t steps), before SmootherApplier<typename LevelContext::SmootherType>::preSmooth 
void preSmooth(Smoother& smoother, Domain& v, const Range& d), before smoother.apply(v,d);
23 schwarz.hh:virtual void apply (X& v, const Y& d), before communication.copyOwnerToAll(v,v); 
23 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
23: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 11
23: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 11
23: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 11
23: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 11
23: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 11
23: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 11
23: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 11
23: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 11
23: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 11
23: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 11
23: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 10 11
38 === BiCGSTABSolver
 Iter          Defect            Rate
    0      1.17536e-06
38  (it = 0.5; it < _maxit; it+=.5) 0.5 250
38  before p = r 
38  before  y = W^-1 * p 
38  before _prec->apply(y,p); 0.5 250
38 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
void AMG<M,X,S,PI,A>::mgc(LevelContext& levelContext), before presmooth(levelContext, preSteps_) 
void presmooth(LevelContext& levelContext, size_t steps), before SmootherApplier<typename LevelContext::SmootherType>::preSmooth 
void preSmooth(Smoother& smoother, Domain& v, const Range& d), before smoother.apply(v,d);
38 schwarz.hh:virtual void apply (X& v, const Y& d), before communication.copyOwnerToAll(v,v); 
38 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
38: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 24
38: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 24
38: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 24
38: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 24
38: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 24
38: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 24
38: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 24
38: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 24
38: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 24
38: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 24
38: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 10 24
38: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 11 24
38: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 12 24
38: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 13 24
38: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 14 24
38: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 15 24
38: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 16 24
38: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 17 24
38: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 18 24
38: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 19 24
38: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 20 24
38: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 21 24
38: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 22 24
69 === BiCGSTABSolver
 Iter          Defect            Rate
    0      1.17536e-06
69  (it = 0.5; it < _maxit; it+=.5) 0.5 250
69  before p = r 
69  before  y = W^-1 * p 
69  before _prec->apply(y,p); 0.5 250
69 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
void AMG<M,X,S,PI,A>::mgc(LevelContext& levelContext), before presmooth(levelContext, preSteps_) 
void presmooth(LevelContext& levelContext, size_t steps), before SmootherApplier<typename LevelContext::SmootherType>::preSmooth 
void preSmooth(Smoother& smoother, Domain& v, const Range& d), before smoother.apply(v,d);
69 schwarz.hh:virtual void apply (X& v, const Y& d), before communication.copyOwnerToAll(v,v); 
69 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
69: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 7
69: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 7
69: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 7
69: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 7
69: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 7
31 === BiCGSTABSolver
 Iter          Defect            Rate
    0      1.17536e-06
31  (it = 0.5; it < _maxit; it+=.5) 0.5 250
31  before p = r 
31  before  y = W^-1 * p 
31  before _prec->apply(y,p); 0.5 250
31 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
void AMG<M,X,S,PI,A>::mgc(LevelContext& levelContext), before presmooth(levelContext, preSteps_) 
void presmooth(LevelContext& levelContext, size_t steps), before SmootherApplier<typename LevelContext::SmootherType>::preSmooth 
void preSmooth(Smoother& smoother, Domain& v, const Range& d), before smoother.apply(v,d);
31 schwarz.hh:virtual void apply (X& v, const Y& d), before communication.copyOwnerToAll(v,v); 
31 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
31: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 10
31: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 10
31: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 10
31: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 10
31: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 10
31: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 10
31: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 10
31: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 10
31: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 10
31: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 10
46 === BiCGSTABSolver
 Iter          Defect            Rate
    0      1.17536e-06
46  (it = 0.5; it < _maxit; it+=.5) 0.5 250
46  before p = r 
46  before  y = W^-1 * p 
46  before _prec->apply(y,p); 0.5 250
46 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
void AMG<M,X,S,PI,A>::mgc(LevelContext& levelContext), before presmooth(levelContext, preSteps_) 
void presmooth(LevelContext& levelContext, size_t steps), before SmootherApplier<typename LevelContext::SmootherType>::preSmooth 
void preSmooth(Smoother& smoother, Domain& v, const Range& d), before smoother.apply(v,d);
46 schwarz.hh:virtual void apply (X& v, const Y& d), before communication.copyOwnerToAll(v,v); 
46 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
46: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 12
46: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 12
46: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 12
46: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 12
46: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 12
46: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 12
46: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 12
46: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 12
46: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 12
46: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 12
46: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 10 12
46: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 11 12
4 === BiCGSTABSolver
 Iter          Defect            Rate
    0      1.17536e-06
4  (it = 0.5; it < _maxit; it+=.5) 0.5 250
4  before p = r 
4  before  y = W^-1 * p 
4  before _prec->apply(y,p); 0.5 250
4 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
void AMG<M,X,S,PI,A>::mgc(LevelContext& levelContext), before presmooth(levelContext, preSteps_) 
void presmooth(LevelContext& levelContext, size_t steps), before SmootherApplier<typename LevelContext::SmootherType>::preSmooth 
void preSmooth(Smoother& smoother, Domain& v, const Range& d), before smoother.apply(v,d);
4 schwarz.hh:virtual void apply (X& v, const Y& d), before communication.copyOwnerToAll(v,v); 
4 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 14
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 14
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 14
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 14
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 14
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 14
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 14
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 14
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 14
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 14
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 10 14
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 11 14
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 12 14
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 13 14
21 === BiCGSTABSolver
 Iter          Defect            Rate
    0      1.17536e-06
21  (it = 0.5; it < _maxit; it+=.5) 0.5 250
21  before p = r 
21  before  y = W^-1 * p 
21  before _prec->apply(y,p); 0.5 250
21 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
void AMG<M,X,S,PI,A>::mgc(LevelContext& levelContext), before presmooth(levelContext, preSteps_) 
void presmooth(LevelContext& levelContext, size_t steps), before SmootherApplier<typename LevelContext::SmootherType>::preSmooth 
void preSmooth(Smoother& smoother, Domain& v, const Range& d), before smoother.apply(v,d);
21 schwarz.hh:virtual void apply (X& v, const Y& d), before communication.copyOwnerToAll(v,v); 
21 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
21: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 12
21: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 12
21: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 12
21: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 12
21: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 12
21: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 12
21: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 12
21: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 12
21: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 12
21: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 12
21: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 10 12
21: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 11 12
94 === BiCGSTABSolver
 Iter          Defect            Rate
    0      1.17536e-06
94  (it = 0.5; it < _maxit; it+=.5) 0.5 250
94  before p = r 
94  before  y = W^-1 * p 
94  before _prec->apply(y,p); 0.5 250
94 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
void AMG<M,X,S,PI,A>::mgc(LevelContext& levelContext), before presmooth(levelContext, preSteps_) 
void presmooth(LevelContext& levelContext, size_t steps), before SmootherApplier<typename LevelContext::SmootherType>::preSmooth 
void preSmooth(Smoother& smoother, Domain& v, const Range& d), before smoother.apply(v,d);
94 schwarz.hh:virtual void apply (X& v, const Y& d), before communication.copyOwnerToAll(v,v); 
94 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
94: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 21
94: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 21
94: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 21
94: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 21
94: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 21
94: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 21
94: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 21
94: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 21
94: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 21
94: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 21
94: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 10 21
94: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 11 21
94: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 12 21
94: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 13 21
94: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 14 21
94: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 15 21
94: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 16 21
94: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 17 21
94: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 18 21
94: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 19 21
94: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 20 21
82 === BiCGSTABSolver
 Iter          Defect            Rate
    0      1.17536e-06
82  (it = 0.5; it < _maxit; it+=.5) 0.5 250
82  before p = r 
82  before  y = W^-1 * p 
82  before _prec->apply(y,p); 0.5 250
82 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
void AMG<M,X,S,PI,A>::mgc(LevelContext& levelContext), before presmooth(levelContext, preSteps_) 
void presmooth(LevelContext& levelContext, size_t steps), before SmootherApplier<typename LevelContext::SmootherType>::preSmooth 
void preSmooth(Smoother& smoother, Domain& v, const Range& d), before smoother.apply(v,d);
35 === BiCGSTABSolver
 Iter          Defect            Rate
    0      1.17536e-06
35  (it = 0.5; it < _maxit; it+=.5) 0.5 250
35  before p = r 
35  before  y = W^-1 * p 
35  before _prec->apply(y,p); 0.5 250
35 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
void AMG<M,X,S,PI,A>::mgc(LevelContext& levelContext), before presmooth(levelContext, preSteps_) 
void presmooth(LevelContext& levelContext, size_t steps), before SmootherApplier<typename LevelContext::SmootherType>::preSmooth 
void preSmooth(Smoother& smoother, Domain& v, const Range& d), before smoother.apply(v,d);
35 schwarz.hh:virtual void apply (X& v, const Y& d), before communication.copyOwnerToAll(v,v); 
35 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
35: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 9
35: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 9
35: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 9
35: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 9
35: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 9
35: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 9
35: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 9
33 === BiCGSTABSolver
 Iter          Defect            Rate
    0      1.17536e-06
33  (it = 0.5; it < _maxit; it+=.5) 0.5 250
33  before p = r 
33  before  y = W^-1 * p 
33  before _prec->apply(y,p); 0.5 250
33 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
void AMG<M,X,S,PI,A>::mgc(LevelContext& levelContext), before presmooth(levelContext, preSteps_) 
void presmooth(LevelContext& levelContext, size_t steps), before SmootherApplier<typename LevelContext::SmootherType>::preSmooth 
void preSmooth(Smoother& smoother, Domain& v, const Range& d), before smoother.apply(v,d);
33 schwarz.hh:virtual void apply (X& v, const Y& d), before communication.copyOwnerToAll(v,v); 
33 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
33: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 9
33: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 9
33: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 9
33: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 9
33: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 9
33: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 9
33: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 9
33: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 9
33: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 9
59 === BiCGSTABSolver
 Iter          Defect            Rate
    0      1.17536e-06
59  (it = 0.5; it < _maxit; it+=.5) 0.5 250
59  before p = r 
59  before  y = W^-1 * p 
59  before _prec->apply(y,p); 0.5 250
59 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
void AMG<M,X,S,PI,A>::mgc(LevelContext& levelContext), before presmooth(levelContext, preSteps_) 
void presmooth(LevelContext& levelContext, size_t steps), before SmootherApplier<typename LevelContext::SmootherType>::preSmooth 
void preSmooth(Smoother& smoother, Domain& v, const Range& d), before smoother.apply(v,d);
59 schwarz.hh:virtual void apply (X& v, const Y& d), before communication.copyOwnerToAll(v,v); 
59 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
59: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 11
59: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 11
59: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 11
59: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 11
59: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 11
59: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 11
59: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 11
59: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 11
106 === BiCGSTABSolver
 Iter          Defect            Rate
    0      1.17536e-06
106  (it = 0.5; it < _maxit; it+=.5) 0.5 250
106  before p = r 
106  before  y = W^-1 * p 
106  before _prec->apply(y,p); 0.5 250
106 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
void AMG<M,X,S,PI,A>::mgc(LevelContext& levelContext), before presmooth(levelContext, preSteps_) 
void presmooth(LevelContext& levelContext, size_t steps), before SmootherApplier<typename LevelContext::SmootherType>::preSmooth 
void preSmooth(Smoother& smoother, Domain& v, const Range& d), before smoother.apply(v,d);
106 schwarz.hh:virtual void apply (X& v, const Y& d), before communication.copyOwnerToAll(v,v); 
106 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
106: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 16
106: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 16
106: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 16
106: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 16
106: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 16
106: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 16
106: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 16
106: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 16
106: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 16
106: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 16
106: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 10 16
106: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 11 16
106: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 12 16
106: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 13 16
106: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 14 16
106: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 15 16
48 === BiCGSTABSolver
 Iter          Defect            Rate
    0      1.17536e-06
48  (it = 0.5; it < _maxit; it+=.5) 0.5 250
48  before p = r 
48  before  y = W^-1 * p 
48  before _prec->apply(y,p); 0.5 250
48 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
void AMG<M,X,S,PI,A>::mgc(LevelContext& levelContext), before presmooth(levelContext, preSteps_) 
void presmooth(LevelContext& levelContext, size_t steps), before SmootherApplier<typename LevelContext::SmootherType>::preSmooth 
void preSmooth(Smoother& smoother, Domain& v, const Range& d), before smoother.apply(v,d);
48 schwarz.hh:virtual void apply (X& v, const Y& d), before communication.copyOwnerToAll(v,v); 
48 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
48: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 15
48: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 15
48: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 15
48: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 15
48: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 15
48: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 15
48: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 15
48: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 15
48: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 15
48: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 15
48: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 10 15
48: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 11 15
48: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 12 15
48: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 13 15
48: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 14 15
40 === BiCGSTABSolver
 Iter          Defect            Rate
    0      1.17536e-06
40  (it = 0.5; it < _maxit; it+=.5) 0.5 250
40  before p = r 
40  before  y = W^-1 * p 
40  before _prec->apply(y,p); 0.5 250
40 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
void AMG<M,X,S,PI,A>::mgc(LevelContext& levelContext), before presmooth(levelContext, preSteps_) 
void presmooth(LevelContext& levelContext, size_t steps), before SmootherApplier<typename LevelContext::SmootherType>::preSmooth 
void preSmooth(Smoother& smoother, Domain& v, const Range& d), before smoother.apply(v,d);
40 schwarz.hh:virtual void apply (X& v, const Y& d), before communication.copyOwnerToAll(v,v); 
40 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
40: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 15
40: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 15
40: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 15
40: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 15
40: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 15
40: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 15
40: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 15
40: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 15
40: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 15
40: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 15
40: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 10 15
40: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 11 15
40: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 12 15
40: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 13 15
40: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 14 15
52 === BiCGSTABSolver
 Iter          Defect            Rate
    0      1.17536e-06
52  (it = 0.5; it < _maxit; it+=.5) 0.5 250
52  before p = r 
52  before  y = W^-1 * p 
52  before _prec->apply(y,p); 0.5 250
52 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
void AMG<M,X,S,PI,A>::mgc(LevelContext& levelContext), before presmooth(levelContext, preSteps_) 
void presmooth(LevelContext& levelContext, size_t steps), before SmootherApplier<typename LevelContext::SmootherType>::preSmooth 
void preSmooth(Smoother& smoother, Domain& v, const Range& d), before smoother.apply(v,d);
52 schwarz.hh:virtual void apply (X& v, const Y& d), before communication.copyOwnerToAll(v,v); 
52 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
52: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 26
52: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 26
52: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 26
52: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 26
52: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 26
52: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 26
52: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 26
52: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 26
52: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 26
52: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 26
52: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 10 26
52: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 11 26
52: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 12 26
52: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 13 26
52: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 14 26
52: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 15 26
52: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 16 26
52: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 17 26
52: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 18 26
52: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 19 26
52: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 20 26
52: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 21 26
52: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 22 26
52: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 23 26
73 === BiCGSTABSolver
 Iter          Defect            Rate
    0      1.17536e-06
73  (it = 0.5; it < _maxit; it+=.5) 0.5 250
73  before p = r 
73  before  y = W^-1 * p 
73  before _prec->apply(y,p); 0.5 250
73 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
void AMG<M,X,S,PI,A>::mgc(LevelContext& levelContext), before presmooth(levelContext, preSteps_) 
void presmooth(LevelContext& levelContext, size_t steps), before SmootherApplier<typename LevelContext::SmootherType>::preSmooth 
void preSmooth(Smoother& smoother, Domain& v, const Range& d), before smoother.apply(v,d);
73 schwarz.hh:virtual void apply (X& v, const Y& d), before communication.copyOwnerToAll(v,v); 
73 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
73: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 11
73: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 11
73: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 11
73: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 11
73: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 11
73: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 11
73: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 11
73: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 11
63 === BiCGSTABSolver
 Iter          Defect            Rate
    0      1.17536e-06
63  (it = 0.5; it < _maxit; it+=.5) 0.5 250
63  before p = r 
63  before  y = W^-1 * p 
63  before _prec->apply(y,p); 0.5 250
63 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
void AMG<M,X,S,PI,A>::mgc(LevelContext& levelContext), before presmooth(levelContext, preSteps_) 
void presmooth(LevelContext& levelContext, size_t steps), before SmootherApplier<typename LevelContext::SmootherType>::preSmooth 
void preSmooth(Smoother& smoother, Domain& v, const Range& d), before smoother.apply(v,d);
63 schwarz.hh:virtual void apply (X& v, const Y& d), before communication.copyOwnerToAll(v,v); 
63 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
63: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 9
63: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 9
63: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 9
63: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 9
63: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 9
63: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 9
63: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 9
63: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 9
54 === BiCGSTABSolver
 Iter          Defect            Rate
    0      1.17536e-06
54  (it = 0.5; it < _maxit; it+=.5) 0.5 250
54  before p = r 
54  before  y = W^-1 * p 
54  before _prec->apply(y,p); 0.5 250
54 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
void AMG<M,X,S,PI,A>::mgc(LevelContext& levelContext), before presmooth(levelContext, preSteps_) 
void presmooth(LevelContext& levelContext, size_t steps), before SmootherApplier<typename LevelContext::SmootherType>::preSmooth 
void preSmooth(Smoother& smoother, Domain& v, const Range& d), before smoother.apply(v,d);
54 schwarz.hh:virtual void apply (X& v, const Y& d), before communication.copyOwnerToAll(v,v); 
54 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
54: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 18
54: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 18
54: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 18
54: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 18
54: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 18
54: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 18
54: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 18
54: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 18
54: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 18
54: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 18
54: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 10 18
54: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 11 18
54: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 12 18
54: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 13 18
54: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 14 18
54: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 15 18
54: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 16 18
54: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 17 18
27 === BiCGSTABSolver
 Iter          Defect            Rate
    0      1.17536e-06
27  (it = 0.5; it < _maxit; it+=.5) 0.5 250
27  before p = r 
27  before  y = W^-1 * p 
27  before _prec->apply(y,p); 0.5 250
27 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
void AMG<M,X,S,PI,A>::mgc(LevelContext& levelContext), before presmooth(levelContext, preSteps_) 
void presmooth(LevelContext& levelContext, size_t steps), before SmootherApplier<typename LevelContext::SmootherType>::preSmooth 
void preSmooth(Smoother& smoother, Domain& v, const Range& d), before smoother.apply(v,d);
27 schwarz.hh:virtual void apply (X& v, const Y& d), before communication.copyOwnerToAll(v,v); 
27 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
27: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 17
27: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 17
27: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 17
27: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 17
27: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 17
27: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 17
27: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 17
27: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 17
27: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 17
27: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 17
27: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 10 17
27: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 11 17
27: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 12 17
27: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 13 17
27: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 14 17
27: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 15 17
27: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 16 17
113 === BiCGSTABSolver
 Iter          Defect            Rate
    0      1.17536e-06
113  (it = 0.5; it < _maxit; it+=.5) 0.5 250
113  before p = r 
113  before  y = W^-1 * p 
113  before _prec->apply(y,p); 0.5 250
113 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
void AMG<M,X,S,PI,A>::mgc(LevelContext& levelContext), before presmooth(levelContext, preSteps_) 
void presmooth(LevelContext& levelContext, size_t steps), before SmootherApplier<typename LevelContext::SmootherType>::preSmooth 
void preSmooth(Smoother& smoother, Domain& v, const Range& d), before smoother.apply(v,d);
113 schwarz.hh:virtual void apply (X& v, const Y& d), before communication.copyOwnerToAll(v,v); 
113 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
113: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 18
113: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 18
113: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 18
113: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 18
113: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 18
113: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 18
113: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 18
113: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 18
113: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 18
113: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 18
113: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 10 18
113: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 11 18
113: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 12 18
113: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 13 18
113: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 14 18
113: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 15 18
113: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 16 18
113: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 17 18
void presmooth(LevelContext& levelContext, size_t steps), before SmootherApplier<typename LevelContext::SmootherType>::preSmooth 
void preSmooth(Smoother& smoother, Domain& v, const Range& d), before smoother.apply(v,d);
113 schwarz.hh:virtual void apply (X& v, const Y& d), before communication.copyOwnerToAll(v,v); 
120 === BiCGSTABSolver
 Iter          Defect            Rate
    0      1.17536e-06
120  (it = 0.5; it < _maxit; it+=.5) 0.5 250
120  before p = r 
120  before  y = W^-1 * p 
120  before _prec->apply(y,p); 0.5 250
120 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
void AMG<M,X,S,PI,A>::mgc(LevelContext& levelContext), before presmooth(levelContext, preSteps_) 
void presmooth(LevelContext& levelContext, size_t steps), before SmootherApplier<typename LevelContext::SmootherType>::preSmooth 
void preSmooth(Smoother& smoother, Domain& v, const Range& d), before smoother.apply(v,d);
120 schwarz.hh:virtual void apply (X& v, const Y& d), before communication.copyOwnerToAll(v,v); 
120 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
120: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 7
120: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 7
120: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 7
120: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 7
120: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 7
120: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 7
120: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 7
void presmooth(LevelContext& levelContext, size_t steps), before SmootherApplier<typename LevelContext::SmootherType>::preSmooth 
void preSmooth(Smoother& smoother, Domain& v, const Range& d), before smoother.apply(v,d);
120 schwarz.hh:virtual void apply (X& v, const Y& d), before communication.copyOwnerToAll(v,v); 
120 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
120: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 7
120: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 7
120: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 7
90 === BiCGSTABSolver
 Iter          Defect            Rate
    0      1.17536e-06
90  (it = 0.5; it < _maxit; it+=.5) 0.5 250
90  before p = r 
90  before  y = W^-1 * p 
90  before _prec->apply(y,p); 0.5 250
90 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
void AMG<M,X,S,PI,A>::mgc(LevelContext& levelContext), before presmooth(levelContext, preSteps_) 
void presmooth(LevelContext& levelContext, size_t steps), before SmootherApplier<typename LevelContext::SmootherType>::preSmooth 
void preSmooth(Smoother& smoother, Domain& v, const Range& d), before smoother.apply(v,d);
90 schwarz.hh:virtual void apply (X& v, const Y& d), before communication.copyOwnerToAll(v,v); 
90 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
90: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 6
90: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 6
90: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 6
90: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 6
90: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 6
90: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 6
void presmooth(LevelContext& levelContext, size_t steps), before SmootherApplier<typename LevelContext::SmootherType>::preSmooth 
void preSmooth(Smoother& smoother, Domain& v, const Range& d), before smoother.apply(v,d);
90 schwarz.hh:virtual void apply (X& v, const Y& d), before communication.copyOwnerToAll(v,v); 
90 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
90: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 6
111 === BiCGSTABSolver
 Iter          Defect            Rate
    0      1.17536e-06
111  (it = 0.5; it < _maxit; it+=.5) 0.5 250
111  before p = r 
111  before  y = W^-1 * p 
111  before _prec->apply(y,p); 0.5 250
111 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
void AMG<M,X,S,PI,A>::mgc(LevelContext& levelContext), before presmooth(levelContext, preSteps_) 
void presmooth(LevelContext& levelContext, size_t steps), before SmootherApplier<typename LevelContext::SmootherType>::preSmooth 
void preSmooth(Smoother& smoother, Domain& v, const Range& d), before smoother.apply(v,d);
111 schwarz.hh:virtual void apply (X& v, const Y& d), before communication.copyOwnerToAll(v,v); 
111 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
111: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 11
111: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 11
111: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 11
111: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 11
111: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 11
111: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 11
111: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 11
111: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 11
111: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 11
111: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 11
111: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 10 11
10 === BiCGSTABSolver
 Iter          Defect            Rate
    0      1.17536e-06
10  (it = 0.5; it < _maxit; it+=.5) 0.5 250
10  before p = r 
10  before  y = W^-1 * p 
10  before _prec->apply(y,p); 0.5 250
10 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
void AMG<M,X,S,PI,A>::mgc(LevelContext& levelContext), before presmooth(levelContext, preSteps_) 
void presmooth(LevelContext& levelContext, size_t steps), before SmootherApplier<typename LevelContext::SmootherType>::preSmooth 
void preSmooth(Smoother& smoother, Domain& v, const Range& d), before smoother.apply(v,d);
10 schwarz.hh:virtual void apply (X& v, const Y& d), before communication.copyOwnerToAll(v,v); 
10 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
10: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 11
10: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 11
10: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 11
10: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 11
10: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 11
10: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 11
10: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 11
10: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 11
10: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 11
10: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 11
10: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 10 11
15 === BiCGSTABSolver
 Iter          Defect            Rate
    0      1.17536e-06
15  (it = 0.5; it < _maxit; it+=.5) 0.5 250
15  before p = r 
15  before  y = W^-1 * p 
15  before _prec->apply(y,p); 0.5 250
15 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
void AMG<M,X,S,PI,A>::mgc(LevelContext& levelContext), before presmooth(levelContext, preSteps_) 
void presmooth(LevelContext& levelContext, size_t steps), before SmootherApplier<typename LevelContext::SmootherType>::preSmooth 
void preSmooth(Smoother& smoother, Domain& v, const Range& d), before smoother.apply(v,d);
15 schwarz.hh:virtual void apply (X& v, const Y& d), before communication.copyOwnerToAll(v,v); 
15 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
15: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 8
15: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 8
15: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 8
15: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 8
15: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 8
15: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 8
15: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 8
15: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 8
void presmooth(LevelContext& levelContext, size_t steps), before SmootherApplier<typename LevelContext::SmootherType>::preSmooth 
void preSmooth(Smoother& smoother, Domain& v, const Range& d), before smoother.apply(v,d);
75 === BiCGSTABSolver
 Iter          Defect            Rate
    0      1.17536e-06
75  (it = 0.5; it < _maxit; it+=.5) 0.5 250
75  before p = r 
75  before  y = W^-1 * p 
75  before _prec->apply(y,p); 0.5 250
75 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
void AMG<M,X,S,PI,A>::mgc(LevelContext& levelContext), before presmooth(levelContext, preSteps_) 
void presmooth(LevelContext& levelContext, size_t steps), before SmootherApplier<typename LevelContext::SmootherType>::preSmooth 
void preSmooth(Smoother& smoother, Domain& v, const Range& d), before smoother.apply(v,d);
55 === BiCGSTABSolver
 Iter          Defect            Rate
    0      1.17536e-06
55  (it = 0.5; it < _maxit; it+=.5) 0.5 250
55  before p = r 
55  before  y = W^-1 * p 
55  before _prec->apply(y,p); 0.5 250
55 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
void AMG<M,X,S,PI,A>::mgc(LevelContext& levelContext), before presmooth(levelContext, preSteps_) 
void presmooth(LevelContext& levelContext, size_t steps), before SmootherApplier<typename LevelContext::SmootherType>::preSmooth 
void preSmooth(Smoother& smoother, Domain& v, const Range& d), before smoother.apply(v,d);
55 schwarz.hh:virtual void apply (X& v, const Y& d), before communication.copyOwnerToAll(v,v); 
55 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
55: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 11
55: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 11
55: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 11
55: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 11
55: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 11
55: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 11
55: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 11
55: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 11
55: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 11
55: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 11
55: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 10 11
98: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 17 18
void presmooth(LevelContext& levelContext, size_t steps), before SmootherApplier<typename LevelContext::SmootherType>::preSmooth 
void preSmooth(Smoother& smoother, Domain& v, const Range& d), before smoother.apply(v,d);
98 schwarz.hh:virtual void apply (X& v, const Y& d), before communication.copyOwnerToAll(v,v); 
98 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
98: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 18
98: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 18
98: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 18
98: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 18
98: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 18
98: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 18
98: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 18
98: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 18
98: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 18
98: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 18
98: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 10 18
98: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 11 18
98: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 12 18
98: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 13 18
98: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 14 18
98: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 15 18
98: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 16 18
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 11
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 11
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 11
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 11
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 10 11
void presmooth(LevelContext& levelContext, size_t steps), before SmootherApplier<typename LevelContext::SmootherType>::preSmooth 
void preSmooth(Smoother& smoother, Domain& v, const Range& d), before smoother.apply(v,d);
5 schwarz.hh:virtual void apply (X& v, const Y& d), before communication.copyOwnerToAll(v,v); 
5 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 11
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 11
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 11
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 11
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 11
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 11
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 11
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 11
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 11
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 11
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 10 11
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 8
void presmooth(LevelContext& levelContext, size_t steps), before SmootherApplier<typename LevelContext::SmootherType>::preSmooth 
void preSmooth(Smoother& smoother, Domain& v, const Range& d), before smoother.apply(v,d);
2 schwarz.hh:virtual void apply (X& v, const Y& d), before communication.copyOwnerToAll(v,v); 
2 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 8
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 8
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 8
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 8
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 8
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 8
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 8
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 8
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
void presmooth(LevelContext& levelContext, size_t steps), before SmootherApplier<typename LevelContext::SmootherType>::preSmooth 
void preSmooth(Smoother& smoother, Domain& v, const Range& d), before smoother.apply(v,d);
22 schwarz.hh:virtual void apply (X& v, const Y& d), before communication.copyOwnerToAll(v,v); 
22 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
22: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 20
22: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 20
22: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 20
22: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 20
22: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 20
22: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 20
22: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 20
22: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 20
22: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 20
22: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 20
22: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 10 20
22: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 11 20
22: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 12 20
22: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 13 20
22: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 14 20
22: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 15 20
22: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 16 20
22: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 17 20
22: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 18 20
22: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 19 20
22: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
45: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 17
45: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 17
45: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 17
45: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 17
45: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 10 17
45: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 11 17
45: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 12 17
45: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 13 17
45: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 14 17
45: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 15 17
45: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 16 17
void presmooth(LevelContext& levelContext, size_t steps), before SmootherApplier<typename LevelContext::SmootherType>::preSmooth 
void preSmooth(Smoother& smoother, Domain& v, const Range& d), before smoother.apply(v,d);
45 schwarz.hh:virtual void apply (X& v, const Y& d), before communication.copyOwnerToAll(v,v); 
45 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
45: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 17
45: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 17
45: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 17
45: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 17
45: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 17
45: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 17
45: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 17
45: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 17
45: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 17
45: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 17
102: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 14 15
void presmooth(LevelContext& levelContext, size_t steps), before SmootherApplier<typename LevelContext::SmootherType>::preSmooth 
void preSmooth(Smoother& smoother, Domain& v, const Range& d), before smoother.apply(v,d);
102 schwarz.hh:virtual void apply (X& v, const Y& d), before communication.copyOwnerToAll(v,v); 
102 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
102: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 15
102: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 15
102: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 15
102: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 15
102: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 15
102: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 15
102: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 15
102: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 15
102: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 15
102: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 15
102: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 10 15
102: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 11 15
void presmooth(LevelContext& levelContext, size_t steps), before SmootherApplier<typename LevelContext::SmootherType>::preSmooth 
void preSmooth(Smoother& smoother, Domain& v, const Range& d), before smoother.apply(v,d);
100 schwarz.hh:virtual void apply (X& v, const Y& d), before communication.copyOwnerToAll(v,v); 
100 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
100: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 6
100: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 6
100: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 6
100: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 6
100: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 6
100: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 6
41: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 14 20
41: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 15 20
41: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 16 20
41: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 17 20
void presmooth(LevelContext& levelContext, size_t steps), before SmootherApplier<typename LevelContext::SmootherType>::preSmooth 
void preSmooth(Smoother& smoother, Domain& v, const Range& d), before smoother.apply(v,d);
119 schwarz.hh:virtual void apply (X& v, const Y& d), before communication.copyOwnerToAll(v,v); 
119 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
119: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 13
119: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 13
119: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 13
119: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 13
119: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 13
119: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 13
119: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 13
119: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 13
119: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 13
119: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 13
119: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 10 13
119: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 11 13
119: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 12 13
119: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
56: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 21 31
void presmooth(LevelContext& levelContext, size_t steps), before SmootherApplier<typename LevelContext::SmootherType>::preSmooth 
void preSmooth(Smoother& smoother, Domain& v, const Range& d), before smoother.apply(v,d);
74 schwarz.hh:virtual void apply (X& v, const Y& d), before communication.copyOwnerToAll(v,v); 
74 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
74: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 11
74: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 11
74: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 11
74: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 11
74: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 11
74: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 11
74: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 11
74: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 11
void presmooth(LevelContext& levelContext, size_t steps), before SmootherApplier<typename LevelContext::SmootherType>::preSmooth 
void preSmooth(Smoother& smoother, Domain& v, const Range& d), before smoother.apply(v,d);
9 schwarz.hh:virtual void apply (X& v, const Y& d), before communication.copyOwnerToAll(v,v); 
9 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
9: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 7
9: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 7
9: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 7
9: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 7
9: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 7
9: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 7
9: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 7
9: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
11: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 12 17
11: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 13 17
11: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 14 17
11: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 15 17
11: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 16 17
void presmooth(LevelContext& levelContext, size_t steps), before SmootherApplier<typename LevelContext::SmootherType>::preSmooth 
void preSmooth(Smoother& smoother, Domain& v, const Range& d), before smoother.apply(v,d);
11 schwarz.hh:virtual void apply (X& v, const Y& d), before communication.copyOwnerToAll(v,v); 
11 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
11: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 17
11: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 17
11: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 17
11: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 17
11: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 17
11: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 17
11: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 17
11: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 17
11: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 17
11: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 17
11: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 10 17
11: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 11 17
11: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 12 17
11: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 13 17
11: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 14 17
11: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 15 17
17: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 3
17: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 3
17: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
58: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 14 21
58: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 15 21
58: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 16 21
void presmooth(LevelContext& levelContext, size_t steps), before SmootherApplier<typename LevelContext::SmootherType>::preSmooth 
void preSmooth(Smoother& smoother, Domain& v, const Range& d), before smoother.apply(v,d);
108 schwarz.hh:virtual void apply (X& v, const Y& d), before communication.copyOwnerToAll(v,v); 
108 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
108: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 9
108: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 9
108: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 9
108: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 9
108: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 9
108: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 9
108: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 9
108: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 9
108: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 9
108: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
97 schwarz.hh:virtual void apply (X& v, const Y& d), before communication.copyOwnerToAll(v,v); 
97 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
97: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 9
97: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 9
97: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 9
97: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 9
97: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 9
97: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 9
97: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 9
97: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 9
97: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 9
97: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
87: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 19
87: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 19
87: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 10 19
87: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 11 19
87: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 12 19
87: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 13 19
87: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 14 19
void presmooth(LevelContext& levelContext, size_t steps), before SmootherApplier<typename LevelContext::SmootherType>::preSmooth 
void preSmooth(Smoother& smoother, Domain& v, const Range& d), before smoother.apply(v,d);
24 schwarz.hh:virtual void apply (X& v, const Y& d), before communication.copyOwnerToAll(v,v); 
24 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
24: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 14
24: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 14
24: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 14
24: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 14
24: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 14
24: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 14
24: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 14
24: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 14
24: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 14
24: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 14
24: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 10 14
24: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 11 14
24: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 12 14
24: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 13 14
24: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
void presmooth(LevelContext& levelContext, size_t steps), before SmootherApplier<typename LevelContext::SmootherType>::preSmooth 
void preSmooth(Smoother& smoother, Domain& v, const Range& d), before smoother.apply(v,d);
25 schwarz.hh:virtual void apply (X& v, const Y& d), before communication.copyOwnerToAll(v,v); 
25 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
25: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 7
25: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 7
25: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 7
25: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 7
25: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 7
25: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 7
25: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 7
25: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
void presmooth(LevelContext& levelContext, size_t steps), before SmootherApplier<typename LevelContext::SmootherType>::preSmooth 
void preSmooth(Smoother& smoother, Domain& v, const Range& d), before smoother.apply(v,d);
103 schwarz.hh:virtual void apply (X& v, const Y& d), before communication.copyOwnerToAll(v,v); 
103 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
103: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 16
103: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 16
103: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 16
103: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 16
103: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 16
103: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 16
103: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 16
103: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 16
103: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 16
103: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 16
103: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 10 16
103: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 11 16
103: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 12 16
void presmooth(LevelContext& levelContext, size_t steps), before SmootherApplier<typename LevelContext::SmootherType>::preSmooth 
void preSmooth(Smoother& smoother, Domain& v, const Range& d), before smoother.apply(v,d);
114 schwarz.hh:virtual void apply (X& v, const Y& d), before communication.copyOwnerToAll(v,v); 
114 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
114: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 13
114: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 13
114: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 13
114: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 13
114: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 13
114: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 13
114: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 13
114: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 13
114: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 13
114: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 13
114: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 10 13
114: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 11 13
114: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 12 13
114: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 3
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 3
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 117
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 117
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 117
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 117
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 117
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 117
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 117
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 117
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 117
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 117
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 10 117
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 11 117
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 12 117
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 13 117
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 14 117
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 15 117
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 16 117
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 17 117
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 18 117
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 19 117
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 20 117
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 21 117
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 22 117
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 23 117
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 24 117
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 25 117
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 9
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 9
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 9
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 9
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 9
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 9
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 9
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 9
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 9
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
void presmooth(LevelContext& levelContext, size_t steps), before SmootherApplier<typename LevelContext::SmootherType>::preSmooth 
void preSmooth(Smoother& smoother, Domain& v, const Range& d), before smoother.apply(v,d);
96 schwarz.hh:virtual void apply (X& v, const Y& d), before communication.copyOwnerToAll(v,v); 
96 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
96: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 12
96: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 12
96: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 12
96: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 12
96: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 12
96: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 12
96: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 12
96: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 12
96: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 12
96: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 12
96: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 10 12
81 schwarz.hh:virtual void apply (X& v, const Y& d), before communication.copyOwnerToAll(v,v); 
81 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
81: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 11
81: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 11
81: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 11
81: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 11
81: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 11
81: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 11
81: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 11
81: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 11
void presmooth(LevelContext& levelContext, size_t steps), before SmootherApplier<typename LevelContext::SmootherType>::preSmooth 
void preSmooth(Smoother& smoother, Domain& v, const Range& d), before smoother.apply(v,d);
50 schwarz.hh:virtual void apply (X& v, const Y& d), before communication.copyOwnerToAll(v,v); 
50 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
50: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 10
50: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 10
50: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 10
50: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 10
void presmooth(LevelContext& levelContext, size_t steps), before SmootherApplier<typename LevelContext::SmootherType>::preSmooth 
void preSmooth(Smoother& smoother, Domain& v, const Range& d), before smoother.apply(v,d);
16 schwarz.hh:virtual void apply (X& v, const Y& d), before communication.copyOwnerToAll(v,v); 
16 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
16: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 6
16: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 6
16: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 6
16: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 6
16: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 6
16: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 6
16: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
void presmooth(LevelContext& levelContext, size_t steps), before SmootherApplier<typename LevelContext::SmootherType>::preSmooth 
void preSmooth(Smoother& smoother, Domain& v, const Range& d), before smoother.apply(v,d);
29 schwarz.hh:virtual void apply (X& v, const Y& d), before communication.copyOwnerToAll(v,v); 
29 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
29: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 11
29: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 11
29: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 11
29: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 11
29: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 11
29: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 11
29: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 11
29: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 11
47: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 6
47: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 6
47: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 6
47: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 6
118 schwarz.hh:virtual void apply (X& v, const Y& d), before communication.copyOwnerToAll(v,v); 
118 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
118: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 14
118: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 14
118: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 14
118: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 14
118: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 14
118: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 14
118: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 14
118: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 14
118: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 14
118: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 14
118: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 10 14
118: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 11 14
118: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 12 14
118: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 13 14
118: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
112 schwarz.hh:virtual void apply (X& v, const Y& d), before communication.copyOwnerToAll(v,v); 
112 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
112: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 5
112: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 5
112: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 5
112: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 5
112: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 5
112: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
void presmooth(LevelContext& levelContext, size_t steps), before SmootherApplier<typename LevelContext::SmootherType>::preSmooth 
void preSmooth(Smoother& smoother, Domain& v, const Range& d), before smoother.apply(v,d);
23 schwarz.hh:virtual void apply (X& v, const Y& d), before communication.copyOwnerToAll(v,v); 
23 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
23: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 11
23: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 11
23: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 11
23: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 11
23: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 11
23: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 11
23: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 11
23: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 11
23: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 11
23: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 11
23: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 10 11
23: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
38: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 23 24
void presmooth(LevelContext& levelContext, size_t steps), before SmootherApplier<typename LevelContext::SmootherType>::preSmooth 
void preSmooth(Smoother& smoother, Domain& v, const Range& d), before smoother.apply(v,d);
31 schwarz.hh:virtual void apply (X& v, const Y& d), before communication.copyOwnerToAll(v,v); 
31 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
31: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 10
31: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 10
31: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 10
31: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 10
31: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 10
31: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 10
31: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 10
void presmooth(LevelContext& levelContext, size_t steps), before SmootherApplier<typename LevelContext::SmootherType>::preSmooth 
void preSmooth(Smoother& smoother, Domain& v, const Range& d), before smoother.apply(v,d);
46 schwarz.hh:virtual void apply (X& v, const Y& d), before communication.copyOwnerToAll(v,v); 
46 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
46: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 12
46: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 12
46: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 12
46: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 12
46: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 12
46: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 12
46: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 12
46: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 12
void presmooth(LevelContext& levelContext, size_t steps), before SmootherApplier<typename LevelContext::SmootherType>::preSmooth 
void preSmooth(Smoother& smoother, Domain& v, const Range& d), before smoother.apply(v,d);
4 schwarz.hh:virtual void apply (X& v, const Y& d), before communication.copyOwnerToAll(v,v); 
4 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 14
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 14
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 14
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 14
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 14
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 14
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 14
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 14
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 14
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 14
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 10 14
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 11 14
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 12 14
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 13 14
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
void presmooth(LevelContext& levelContext, size_t steps), before SmootherApplier<typename LevelContext::SmootherType>::preSmooth 
void preSmooth(Smoother& smoother, Domain& v, const Range& d), before smoother.apply(v,d);
21 schwarz.hh:virtual void apply (X& v, const Y& d), before communication.copyOwnerToAll(v,v); 
21 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
21: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 12
21: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 12
21: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 12
21: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 12
21: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 12
21: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 12
21: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 12
21: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 12
21: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 12
21: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 12
21: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 10 12
void presmooth(LevelContext& levelContext, size_t steps), before SmootherApplier<typename LevelContext::SmootherType>::preSmooth 
void preSmooth(Smoother& smoother, Domain& v, const Range& d), before smoother.apply(v,d);
94 schwarz.hh:virtual void apply (X& v, const Y& d), before communication.copyOwnerToAll(v,v); 
94 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
94: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 21
94: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 21
94: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 21
94: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 21
94: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 21
94: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 21
94: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 21
94: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 21
94: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 21
94: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 21
94: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 10 21
94: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 11 21
94: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 12 21
94: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 13 21
94: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 14 21
94: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 15 21
94: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 16 21
94: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 17 21
94: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 18 21
94: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 19 21
void presmooth(LevelContext& levelContext, size_t steps), before SmootherApplier<typename LevelContext::SmootherType>::preSmooth 
void preSmooth(Smoother& smoother, Domain& v, const Range& d), before smoother.apply(v,d);
106 schwarz.hh:virtual void apply (X& v, const Y& d), before communication.copyOwnerToAll(v,v); 
106 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
106: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 16
106: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 16
106: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 16
106: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 16
106: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 16
106: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 16
106: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 16
106: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 16
106: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 16
106: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 16
106: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 10 16
106: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 11 16
106: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 12 16
106: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 13 16
106: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 14 16
106: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 15 16
106: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
52: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 24 26
52: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 25 26
void presmooth(LevelContext& levelContext, size_t steps), before SmootherApplier<typename LevelContext::SmootherType>::preSmooth 
void preSmooth(Smoother& smoother, Domain& v, const Range& d), before smoother.apply(v,d);
27 schwarz.hh:virtual void apply (X& v, const Y& d), before communication.copyOwnerToAll(v,v); 
27 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
27: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 17
27: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 17
27: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 17
27: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 17
27: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 17
27: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 17
27: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 17
27: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 17
27: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 17
27: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 17
27: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 10 17
27: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 11 17
113 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
113: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 18
113: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 18
113: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 18
113: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 18
113: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 18
113: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 18
113: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 18
113: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 18
113: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 18
113: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 18
113: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 10 18
113: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 11 18
113: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 12 18
113: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 13 18
113: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 14 18
113: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 15 18
113: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 16 18
113: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 17 18
113: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
120: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 7
120: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 7
120: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 7
120: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 7
120: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
90: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 6
90: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 6
90: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 6
90: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 6
90: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 6
90 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
90: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 6
void presmooth(LevelContext& levelContext, size_t steps), before SmootherApplier<typename LevelContext::SmootherType>::preSmooth 
void preSmooth(Smoother& smoother, Domain& v, const Range& d), before smoother.apply(v,d);
111 schwarz.hh:virtual void apply (X& v, const Y& d), before communication.copyOwnerToAll(v,v); 
111 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
111: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 11
111: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 11
111: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 11
111: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 11
111: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 11
111: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 11
111: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 11
111: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 11
111: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 11
111: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 11
111: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 10 11
111: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
void presmooth(LevelContext& levelContext, size_t steps), before SmootherApplier<typename LevelContext::SmootherType>::preSmooth 
void preSmooth(Smoother& smoother, Domain& v, const Range& d), before smoother.apply(v,d);
10 schwarz.hh:virtual void apply (X& v, const Y& d), before communication.copyOwnerToAll(v,v); 
10 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
10: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 11
10: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 11
10: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 11
10: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 11
10: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 11
10: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 11
10: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 11
10: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 11
10: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 11
10: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 11
10: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 10 11
10: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
15 schwarz.hh:virtual void apply (X& v, const Y& d), before communication.copyOwnerToAll(v,v); 
15 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
15: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 8
15: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 8
15: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 8
15: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 8
15: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 8
15: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 8
15: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 8
15: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 8
15: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
28: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 18
28: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 18
28: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 18
28: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 18
28: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 18
28: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 10 18
28: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 11 18
28: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 12 18
28: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 13 18
28: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 14 18
28: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 15 18
28: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 16 18
28: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 17 18
void presmooth(LevelContext& levelContext, size_t steps), before SmootherApplier<typename LevelContext::SmootherType>::preSmooth 
void preSmooth(Smoother& smoother, Domain& v, const Range& d), before smoother.apply(v,d);
28 schwarz.hh:virtual void apply (X& v, const Y& d), before communication.copyOwnerToAll(v,v); 
28 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
28: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 18
28: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 18
28: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 18
28: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 18
28: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 18
28: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 18
28: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 18
28: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 18
28: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 18
28: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 18
28: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 10 18
28: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 11 18
28: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 12 18
28: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 13 18
28: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 14 18
117: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 18 22
117: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 19 22
117: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 20 22
117: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 21 22
void presmooth(LevelContext& levelContext, size_t steps), before SmootherApplier<typename LevelContext::SmootherType>::preSmooth 
void preSmooth(Smoother& smoother, Domain& v, const Range& d), before smoother.apply(v,d);
117 schwarz.hh:virtual void apply (X& v, const Y& d), before communication.copyOwnerToAll(v,v); 
117 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
117: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 22
117: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 22
117: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 22
117: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 22
117: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 22
117: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 22
117: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 22
117: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 22
117: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 22
117: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 22
117: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 10 22
117: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 11 22
117: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 12 22
117: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 13 22
117: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 14 22
117: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 15 22
117: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 16 22
117: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 17 22
117: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 18 22
117: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 19 22
117: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 20 22
117: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 21 22
117: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
void presmooth(LevelContext& levelContext, size_t steps), before SmootherApplier<typename LevelContext::SmootherType>::preSmooth 
void preSmooth(Smoother& smoother, Domain& v, const Range& d), before smoother.apply(v,d);
109 schwarz.hh:virtual void apply (X& v, const Y& d), before communication.copyOwnerToAll(v,v); 
109 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
109: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 20
109: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 20
109: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 20
109: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 20
109: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 20
109: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 20
109: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 20
109: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 20
109: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 20
109: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 20
109: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 10 20
109: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 11 20
109: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 12 20
109: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 13 20
109: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 14 20
109: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 15 20
109: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 16 20
109: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 17 20
109: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 18 20
109: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 19 20
109: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
14: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 13 17
14: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 14 17
14: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 15 17
14: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 16 17
void presmooth(LevelContext& levelContext, size_t steps), before SmootherApplier<typename LevelContext::SmootherType>::preSmooth 
void preSmooth(Smoother& smoother, Domain& v, const Range& d), before smoother.apply(v,d);
14 schwarz.hh:virtual void apply (X& v, const Y& d), before communication.copyOwnerToAll(v,v); 
14 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
14: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 17
14: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 17
14: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 17
14: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 17
14: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 17
14: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 17
14: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 17
14: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 17
14: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 17
14: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 17
14: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 10 17
14: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 11 17
14: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 12 17
14: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 13 17
110: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 11
110: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 10 11
void presmooth(LevelContext& levelContext, size_t steps), before SmootherApplier<typename LevelContext::SmootherType>::preSmooth 
void preSmooth(Smoother& smoother, Domain& v, const Range& d), before smoother.apply(v,d);
110 schwarz.hh:virtual void apply (X& v, const Y& d), before communication.copyOwnerToAll(v,v); 
110 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
110: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 11
110: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 11
110: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 11
110: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 11
110: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 11
110: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 11
110: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 11
110: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 11
110: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 11
110: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 11
110: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 10 11
110: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
8: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 19 20
void presmooth(LevelContext& levelContext, size_t steps), before SmootherApplier<typename LevelContext::SmootherType>::preSmooth 
void preSmooth(Smoother& smoother, Domain& v, const Range& d), before smoother.apply(v,d);
8 schwarz.hh:virtual void apply (X& v, const Y& d), before communication.copyOwnerToAll(v,v); 
8 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
8: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 20
8: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 20
8: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 20
8: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 20
8: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 20
8: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 20
8: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 20
8: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 20
8: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 20
8: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 20
8: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 10 20
8: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 11 20
8: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 12 20
8: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 13 20
8: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 14 20
8: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 15 20
8: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 16 20
8: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 17 20
8: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 18 20
8: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 19 20
8: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
void presmooth(LevelContext& levelContext, size_t steps), before SmootherApplier<typename LevelContext::SmootherType>::preSmooth 
void preSmooth(Smoother& smoother, Domain& v, const Range& d), before smoother.apply(v,d);
66 schwarz.hh:virtual void apply (X& v, const Y& d), before communication.copyOwnerToAll(v,v); 
66 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
66: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 15
66: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 15
66: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 15
66: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 15
66: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 15
66: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 15
66: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 15
66: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 15
66: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 15
66: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 15
66: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 10 15
116: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 7
116: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 7
116: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 7
116: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 7
116: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 7
116: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 7
116: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
42: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 10 20
42: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 11 20
49: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 15
49: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 10 15
107: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 5
107: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 5
107: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 5
107: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 5
107: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
65: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 9
void presmooth(LevelContext& levelContext, size_t steps), before SmootherApplier<typename LevelContext::SmootherType>::preSmooth 
void preSmooth(Smoother& smoother, Domain& v, const Range& d), before smoother.apply(v,d);
65 schwarz.hh:virtual void apply (X& v, const Y& d), before communication.copyOwnerToAll(v,v); 
65 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
65: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 9
65: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 9
65: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 9
65: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 9
65: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 9
65: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 9
65: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 9
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 5
void presmooth(LevelContext& levelContext, size_t steps), before SmootherApplier<typename LevelContext::SmootherType>::preSmooth 
void preSmooth(Smoother& smoother, Domain& v, const Range& d), before smoother.apply(v,d);
1 schwarz.hh:virtual void apply (X& v, const Y& d), before communication.copyOwnerToAll(v,v); 
1 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 5
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 5
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 5
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 5
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 5
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
79: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 14 16
79: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 15 16
void presmooth(LevelContext& levelContext, size_t steps), before SmootherApplier<typename LevelContext::SmootherType>::preSmooth 
void preSmooth(Smoother& smoother, Domain& v, const Range& d), before smoother.apply(v,d);
79 schwarz.hh:virtual void apply (X& v, const Y& d), before communication.copyOwnerToAll(v,v); 
79 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
79: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 16
79: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 16
79: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 16
79: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 16
79: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 16
79: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 16
79: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 16
79: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 16
79: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 16
79: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 16
79: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 10 16
void presmooth(LevelContext& levelContext, size_t steps), before SmootherApplier<typename LevelContext::SmootherType>::preSmooth 
void preSmooth(Smoother& smoother, Domain& v, const Range& d), before smoother.apply(v,d);
105 schwarz.hh:virtual void apply (X& v, const Y& d), before communication.copyOwnerToAll(v,v); 
105 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
105: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 9
105: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 9
105: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 9
105: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 9
105: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 9
105: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 9
105: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 9
105: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 9
105: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 9
105: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
78: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 10 15
78: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 11 15
18: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 13 14
void presmooth(LevelContext& levelContext, size_t steps), before SmootherApplier<typename LevelContext::SmootherType>::preSmooth 
void preSmooth(Smoother& smoother, Domain& v, const Range& d), before smoother.apply(v,d);
18 schwarz.hh:virtual void apply (X& v, const Y& d), before communication.copyOwnerToAll(v,v); 
18 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
18: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 14
18: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 14
18: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 14
18: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 14
18: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 14
18: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 14
18: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 14
18: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 14
18: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 14
18: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 14
18: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 10 14
18: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 11 14
18: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 12 14
18: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 13 14
18: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
89: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 11
89: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 11
89: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 11
89: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 11
89: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 11
77: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 6
void presmooth(LevelContext& levelContext, size_t steps), before SmootherApplier<typename LevelContext::SmootherType>::preSmooth 
void preSmooth(Smoother& smoother, Domain& v, const Range& d), before smoother.apply(v,d);
77 schwarz.hh:virtual void apply (X& v, const Y& d), before communication.copyOwnerToAll(v,v); 
77 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
77: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 6
26: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 10 26
26: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 11 26
26: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 12 26
26: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 13 26
26: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 14 26
26: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 15 26
26: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 16 26
26: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 17 26
26: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 18 26
26: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 19 26
26: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 20 26
26: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 21 26
26: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 22 26
26: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 23 26
26: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 24 26
26: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 25 26
void presmooth(LevelContext& levelContext, size_t steps), before SmootherApplier<typename LevelContext::SmootherType>::preSmooth 
void preSmooth(Smoother& smoother, Domain& v, const Range& d), before smoother.apply(v,d);
26 schwarz.hh:virtual void apply (X& v, const Y& d), before communication.copyOwnerToAll(v,v); 
26 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
26: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 26
26: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 26
26: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 26
26: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 26
26: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 26
26: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 26
26: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 26
26: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 26
26: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 26
26: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 26
26: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 10 26
26: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 11 26
26: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 12 26
26: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 13 26
26: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(),37: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 11 15
37: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 12 15
37: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 13 15
37: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 14 15
void presmooth(LevelContext& levelContext, size_t steps), before SmootherApplier<typename LevelContext::SmootherType>::preSmooth 
void preSmooth(Smoother& smoother, Domain& v, const Range& d), before smoother.apply(v,d);
37 schwarz.hh:virtual void apply (X& v, const Y& d), before communication.copyOwnerToAll(v,v); 
37 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
37: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 15
37: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 15
37: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 15
37: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 15
37: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 15
37: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 15
37: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 15
37: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 15
37: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 15
37: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 15
51: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 10
51: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 10
51: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 10
51: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 10
64: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 10 14
84: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 10 18
84: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 11 18
84: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 12 18
84: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 13 18
104: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 9
104: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 9
104: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 9
104: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 9
104: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 9
104: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 9
void presmooth(LevelContext& levelContext, size_t steps), before SmootherApplier<typename LevelContext::SmootherType>::preSmooth 
void preSmooth(Smoother& smoother, Domain& v, const Range& d), before smoother.apply(v,d);
104 schwarz.hh:virtual void apply (X& v, const Y& d), before communication.copyOwnerToAll(v,v); 
104 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
104: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 9
104: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 9
104: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 9
104: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 9
104: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 9
104: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 9
32: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 11 16
32: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 12 16
32: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 13 16
32: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 14 16
32: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 15 16
void presmooth(LevelContext& levelContext, size_t steps), before SmootherApplier<typename LevelContext::SmootherType>::preSmooth 
void preSmooth(Smoother& smoother, Domain& v, const Range& d), before smoother.apply(v,d);
32 schwarz.hh:virtual void apply (X& v, const Y& d), before communication.copyOwnerToAll(v,v); 
32 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
32: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 16
32: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 16
32: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 16
32: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 16
32: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 16
32: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 16
32: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 16
32: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 16
32: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 16
32: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 16
void presmooth(LevelContext& levelContext, size_t steps), before SmootherApplier<typename LevelContext::SmootherType>::preSmooth 
void preSmooth(Smoother& smoother, Domain& v, const Range& d), before smoother.apply(v,d);
30 schwarz.hh:virtual void apply (X& v, const Y& d), before communication.copyOwnerToAll(v,v); 
30 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
30: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 6
30: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 6
30: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 6
30: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 6
30: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 6
53: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 17
53: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 17
53: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 17
53: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 17
53: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 10 17
53: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 11 17
53: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 12 17
53: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 13 17
53: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 14 17
53: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 15 17
53: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 16 17
80: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 10
80: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 10
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 13 17
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 14 17
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 15 17
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 16 17
void presmooth(LevelContext& levelContext, size_t steps), before SmootherApplier<typename LevelContext::SmootherType>::preSmooth 
void preSmooth(Smoother& smoother, Domain& v, const Range& d), before smoother.apply(v,d);
6 schwarz.hh:virtual void apply (X& v, const Y& d), before communication.copyOwnerToAll(v,v); 
6 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 17
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 17
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 17
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 17
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 17
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 17
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 17
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 17
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 17
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 17
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 10 17
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 11 17
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 12 17
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 13 17
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 14 17
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 15 17
7: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 10 13
7: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 11 13
7: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 12 13
void presmooth(LevelContext& levelContext, size_t steps), before SmootherApplier<typename LevelContext::SmootherType>::preSmooth 
void preSmooth(Smoother& smoother, Domain& v, const Range& d), before smoother.apply(v,d);
7 schwarz.hh:virtual void apply (X& v, const Y& d), before communication.copyOwnerToAll(v,v); 
7 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
7: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 13
7: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 13
7: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 13
7: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 13
7: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 13
7: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 13
7: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 13
7: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 13
7: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 13
7: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 13
7: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 10 13
7: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 11 13
7: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 12 13
7: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
13: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 11
13: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 11
13: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 11
13: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 10 11
void presmooth(LevelContext& levelContext, size_t steps), before SmootherApplier<typename LevelContext::SmootherType>::preSmooth 
void preSmooth(Smoother& smoother, Domain& v, const Range& d), before smoother.apply(v,d);
13 schwarz.hh:virtual void apply (X& v, const Y& d), before communication.copyOwnerToAll(v,v); 
13 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
13: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 11
13: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 11
13: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 11
13: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 11
13: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 11
13: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 11
13: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 11
13: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 11
13: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 11
13: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 11
void presmooth(LevelContext& levelContext, size_t steps), before SmootherApplier<typename LevelContext::SmootherType>::preSmooth 
void preSmooth(Smoother& smoother, Domain& v, const Range& d), before smoother.apply(v,d);
91 schwarz.hh:virtual void apply (X& v, const Y& d), before communication.copyOwnerToAll(v,v); 
91 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
91: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 9
91: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 9
91: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 9
91: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 9
91: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 9
91: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 9
91: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 9
91: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 9
91: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 9
91: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
19: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 10 17
19: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 11 17
19: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 12 17
19: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 13 17
19: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 14 17
19: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 15 17
19: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 16 17
void presmooth(LevelContext& levelContext, size_t steps), before SmootherApplier<typename LevelContext::SmootherType>::preSmooth 
void preSmooth(Smoother& smoother, Domain& v, const Range& d), before smoother.apply(v,d);
19 schwarz.hh:virtual void apply (X& v, const Y& d), before communication.copyOwnerToAll(v,v); 
19 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
19: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 17
19: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 17
19: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 17
19: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 17
19: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 17
19: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 17
19: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 17
19: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 17
19: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 17
19: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 17
19: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 10 17
19: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 11 17
19: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 12 17
19: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 13 17
19: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 14 17
92: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 14 16
92: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 15 16
void presmooth(LevelContext& levelContext, size_t steps), before SmootherApplier<typename LevelContext::SmootherType>::preSmooth 
void preSmooth(Smoother& smoother, Domain& v, const Range& d), before smoother.apply(v,d);
92 schwarz.hh:virtual void apply (X& v, const Y& d), before communication.copyOwnerToAll(v,v); 
92 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
92: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 16
92: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 16
92: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 16
92: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 16
92: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 16
92: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 16
92: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 16
92: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 16
92: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 16
92: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 16
92: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 10 16
92: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 11 16
92: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 12 16
92: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 13 16
92: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 14 16
92: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 15 16
void presmooth(LevelContext& levelContext, size_t steps), before SmootherApplier<typename LevelContext::SmootherType>::preSmooth 
void preSmooth(Smoother& smoother, Domain& v, const Range& d), before smoother.apply(v,d);
39 schwarz.hh:virtual void apply (X& v, const Y& d), before communication.copyOwnerToAll(v,v); 
39 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
39: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 9
39: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 9
39: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 9
39: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 9
39: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 9
39: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 9
void presmooth(LevelContext& levelContext, size_t steps), before SmootherApplier<typename LevelContext::SmootherType>::preSmooth 
void preSmooth(Smoother& smoother, Domain& v, const Range& d), before smoother.apply(v,d);
20 schwarz.hh:virtual void apply (X& v, const Y& d), before communication.copyOwnerToAll(v,v); 
20 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
20: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 10
20: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 10
20: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 10
20: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 10
20: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 10
20: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 10
20: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 10
20: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 10
20: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 10
20: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 10
20: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
86: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 18 31
86: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 19 31
86: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 20 31
86: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 21 31
86: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 22 31
86: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 23 31
86: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 24 31
67: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 13
67: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 13
void presmooth(LevelContext& levelContext, size_t steps), before SmootherApplier<typename LevelContext::SmootherType>::preSmooth 
void preSmooth(Smoother& smoother, Domain& v, const Range& d), before smoother.apply(v,d);
93 schwarz.hh:virtual void apply (X& v, const Y& d), before communication.copyOwnerToAll(v,v); 
93 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
93: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 13
93: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 13
93: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 13
93: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 13
93: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 13
93: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 13
93: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 13
93: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 13
93: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 13
93: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 13
93: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 10 13
93: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 11 13
93: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 12 13
93: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
12: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 23 26
12: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 24 26
12: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 25 26
void presmooth(LevelContext& levelContext, size_t steps), before SmootherApplier<typename LevelContext::SmootherType>::preSmooth 
void preSmooth(Smoother& smoother, Domain& v, const Range& d), before smoother.apply(v,d);
12 schwarz.hh:virtual void apply (X& v, const Y& d), before communication.copyOwnerToAll(v,v); 
12 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
12: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 26
12: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 26
12: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 26
12: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 26
12: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 26
12: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 26
12: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 26
12: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 26
12: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 26
12: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 26
12: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 10 26
12: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 11 26
12: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 12 26
12: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 13 26
12: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 14 26
12: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 15 26
12: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 16 26
12: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 17 26
12: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 18 26
12: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 19 26
12: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 20 26
12: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 21 26
12: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 22 26
void presmooth(LevelContext& levelContext, size_t steps), before SmootherApplier<typename LevelContext::SmootherType>::preSmooth 
void preSmooth(Smoother& smoother, Domain& v, const Range& d), before smoother.apply(v,d);
95 schwarz.hh:virtual void apply (X& v, const Y& d), before communication.copyOwnerToAll(v,v); 
95 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
95: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 7
95: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 7
95: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 7
95: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 7
95: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 7
95: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 7
95: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 7
95: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
void presmooth(LevelContext& levelContext, size_t steps), before SmootherApplier<typename LevelContext::SmootherType>::preSmooth 
void preSmooth(Smoother& smoother, Domain& v, const Range& d), before smoother.apply(v,d);
115 schwarz.hh:virtual void apply (X& v, const Y& d), before communication.copyOwnerToAll(v,v); 
115 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
115: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 11
115: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 11
115: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 11
115: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 11
115: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 11
115: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 11
115: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 11
115: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 11
115: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 11
115: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 11
115: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 10 11
115: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
101: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 19 24
101: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 20 24
101: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 21 24
101: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 22 24
101: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 23 24
void presmooth(LevelContext& levelContext, size_t steps), before SmootherApplier<typename LevelContext::SmootherType>::preSmooth 
void preSmooth(Smoother& smoother, Domain& v, const Range& d), before smoother.apply(v,d);
101 schwarz.hh:virtual void apply (X& v, const Y& d), before communication.copyOwnerToAll(v,v); 
101 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
101: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 24
101: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 24
101: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 24
101: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 24
101: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 24
101: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 24
101: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 24
101: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 24
101: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 24
101: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 24
101: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 10 24
101: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 11 24
101: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 12 24
101: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 13 24
101: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 14 24
101: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 15 24
101: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 16 24
101: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 17 24
101: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 18 24
101: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 19 24
101: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 20 24
99: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 11 13
99: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 12 13
void presmooth(LevelContext& levelContext, size_t steps), before SmootherApplier<typename LevelContext::SmootherType>::preSmooth 
void preSmooth(Smoother& smoother, Domain& v, const Range& d), before smoother.apply(v,d);
99 schwarz.hh:virtual void apply (X& v, const Y& d), before communication.copyOwnerToAll(v,v); 
99 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
99: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 13
99: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 13
99: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 13
99: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 13
99: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 13
99: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 13
99: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 13
99: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 13
99: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 13
99: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 13
99: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 10 13
99: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 11 13
43: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 11
43: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 11
43: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 11
43: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 11
43: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 11
43: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 11
void presmooth(LevelContext& levelContext, size_t steps), before SmootherApplier<typename LevelContext::SmootherType>::preSmooth 
void preSmooth(Smoother& smoother, Domain& v, const Range& d), before smoother.apply(v,d);
72 schwarz.hh:virtual void apply (X& v, const Y& d), before communication.copyOwnerToAll(v,v); 
72 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
72: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 20
72: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 20
72: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 20
72: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 20
72: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 20
72: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 20
72: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 20
72: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 20
72: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 20
72: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 20
72: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 10 20
72: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 11 20
72: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 12 20
72: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 13 20
 recvRequests, &finished, &status); 14 26
26: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 15 26
26: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 16 26
26: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 17 26
26: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 18 26
26: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 19 26
26: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 20 26
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 26 117
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 27 117
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 28 117
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 29 117
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 30 117
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 31 117
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 32 117
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 33 117
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 34 117
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 35 117
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 36 117
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 37 117
Newton::solveLinearSystem : Caught exception from the linear solver when getting converged: "FMatrixError [luDecomposition:/home/m.giraud/DUMUXexud/dune-common/dune/common/densematrix.hh:909]: matrix is singular"
68 converged value: 0 121 1 let s go get convergedRemote
68 to  comm_.min(converged) 
Newton::solveLinearSystem : Caught exception from the linear solver when getting converged: "FMatrixError [luDecomposition:/home/m.giraud/DUMUXexud/dune-common/dune/common/densematrix.hh:909]: matrix is singular"
62 converged value: 0 121 1 let s go get convergedRemote
62 to  comm_.min(converged) 
Newton::solveLinearSystem : Caught exception from the linear solver when getting converged: "FMatrixError [luDecomposition:/home/m.giraud/DUMUXexud/dune-common/dune/common/densematrix.hh:909]: matrix is singular"
82 converged value: 0 121 1 let s go get convergedRemote
82 to  comm_.min(converged) 
Newton::solveLinearSystem : Caught exception from the linear solver when getting converged: "FMatrixError [luDecomposition:/home/m.giraud/DUMUXexud/dune-common/dune/common/densematrix.hh:909]: matrix is singular"
75 converged value: 0 121 1 let s go get convergedRemote
75 to  comm_.min(converged) 
Newton::solveLinearSystem : Caught exception from the linear solver when getting converged: "FMatrixError [luDecomposition:/home/m.giraud/DUMUXexud/dune-common/dune/common/densematrix.hh:909]: matrix is singular"
70 converged value: 0 121 1 let s go get convergedRemote
70 to  comm_.min(converged) 
Newton::solveLinearSystem : Caught exception from the linear solver when getting converged: "FMatrixError [luDecomposition:/home/m.giraud/DUMUXexud/dune-common/dune/common/densematrix.hh:909]: matrix is singular"
83 converged value: 0 121 1 let s go get convergedRemote
83 to  comm_.min(converged) 
Newton::solveLinearSystem : Caught exception from the linear solver when getting converged: "FMatrixError [luDecomposition:/home/m.giraud/DUMUXexud/dune-common/dune/common/densematrix.hh:909]: matrix is singular"
44 converged value: 0 121 1 let s go get convergedRemote
44 to  comm_.min(converged) 
Newton::solveLinearSystem : Caught exception from the linear solver when getting converged: "FMatrixError [luDecomposition:/home/m.giraud/DUMUXexud/dune-common/dune/common/densematrix.hh:909]: matrix is singular"
57 converged value: 0 121 1 let s go get convergedRemote
57 to  comm_.min(converged) 
Newton::solveLinearSystem : Caught exception from the linear solver when getting converged: "FMatrixError [luDecomposition:/home/m.giraud/DUMUXexud/dune-common/dune/common/densematrix.hh:909]: matrix is singular"
85 converged value: 0 121 1 let s go get convergedRemote
85 to  comm_.min(converged) 
Newton::solveLinearSystem : Caught exception from the linear solver when getting converged: "FMatrixError [luDecomposition:/home/m.giraud/DUMUXexud/dune-common/dune/common/densematrix.hh:909]: matrix is singular"
36 converged value: 0 121 1 let s go get convergedRemote
36 to  comm_.min(converged) 
