27: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 17
27: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 17
27: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 17
27: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 17
27: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 17
27: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 17
27: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 17
27: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 17
27: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 17
27: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 17
27: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 10 17
27: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 11 17
27: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 12 17
27: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 13 17
27: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 14 17
27: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 15 17
27: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 16 17
27: communicator.hh::sendRecv, left MPI_Wait
66 121 before Dune::BiCGSTABSolver 
66 Dune::BiCGSTABSolver.apply(x, b, result_) 
66 BiCGSTABSolver::apply 
66 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
66: communicator.hh::BufferedCommunicator::forward, enter sendRecv
66: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 15
66: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 15
66: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 15
66: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 15
66: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 15
66: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 15
66: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 15
66: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 15
58 121 before Dune::BiCGSTABSolver 
58 Dune::BiCGSTABSolver.apply(x, b, result_) 
58 BiCGSTABSolver::apply 
58 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
58: communicator.hh::BufferedCommunicator::forward, enter sendRecv
58: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 21
58: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 21
58: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 21
58: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 21
58: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 21
58: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 21
58: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 21
58: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 21
58: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 21
58: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 21
58: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 10 21
58: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 11 21
58: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 12 21
58: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 13 21
58: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 14 21
58: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 15 21
58: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 16 21
58: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 17 21
58: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 18 21
58: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 19 21
58: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 20 21
34 121 before Dune::BiCGSTABSolver 
34 Dune::BiCGSTABSolver.apply(x, b, result_) 
34 BiCGSTABSolver::apply 
34 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
34: communicator.hh::BufferedCommunicator::forward, enter sendRecv
34: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 14
34: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 14
34: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 14
34: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 14
34: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 14
34: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 14
34: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 14
34: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 14
34: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 14
34: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 14
34: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 10 14
34: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 11 14
34: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 12 14
34: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 13 14
91 121 before Dune::BiCGSTABSolver 
91 Dune::BiCGSTABSolver.apply(x, b, result_) 
91 BiCGSTABSolver::apply 
91 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
91: communicator.hh::BufferedCommunicator::forward, enter sendRecv
91: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 9
91: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 9
91: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 9
91: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 9
91: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 9
91: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 9
76 121 before Dune::BiCGSTABSolver 
76 Dune::BiCGSTABSolver.apply(x, b, result_) 
76 BiCGSTABSolver::apply 
76 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
76: communicator.hh::BufferedCommunicator::forward, enter sendRecv
76: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 12
76: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 12
76: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 12
76: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 12
76: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 12
76: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 12
76: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 12
76: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 12
76: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 12
76: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 12
76: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 10 12
74 121 before Dune::BiCGSTABSolver 
74 Dune::BiCGSTABSolver.apply(x, b, result_) 
74 BiCGSTABSolver::apply 
74 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
74: communicator.hh::BufferedCommunicator::forward, enter sendRecv
74: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 11
74: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 11
74: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 11
74: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 11
74: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 11
74: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 11
74: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 11
74: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 11
8 121 before Dune::BiCGSTABSolver 
8 Dune::BiCGSTABSolver.apply(x, b, result_) 
8 BiCGSTABSolver::apply 
8 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
8: communicator.hh::BufferedCommunicator::forward, enter sendRecv
8: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 20
8: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 20
8: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 20
8: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 20
8: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 20
8: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 20
8: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 20
8: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 20
8: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 20
8: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 20
8: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 10 20
8: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 11 20
8: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 12 20
8: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 13 20
8: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 14 20
39 121 before Dune::BiCGSTABSolver 
39 Dune::BiCGSTABSolver.apply(x, b, result_) 
39 BiCGSTABSolver::apply 
39 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
39: communicator.hh::BufferedCommunicator::forward, enter sendRecv
39: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 9
39: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 9
39: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 9
39: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 9
39: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 9
39: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 9
39: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 9
39: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 9
108 121 before Dune::BiCGSTABSolver 
108 Dune::BiCGSTABSolver.apply(x, b, result_) 
108 BiCGSTABSolver::apply 
108 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
108: communicator.hh::BufferedCommunicator::forward, enter sendRecv
108: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 9
108: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 9
108: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 9
108: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 9
108: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 9
108: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 9
108: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 9
108: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 9
108: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 9
108: communicator.hh::sendRecv, left MPI_Wait
78 121 before Dune::BiCGSTABSolver 
78 Dune::BiCGSTABSolver.apply(x, b, result_) 
78 BiCGSTABSolver::apply 
78 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
78: communicator.hh::BufferedCommunicator::forward, enter sendRecv
78: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 15
78: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 15
78: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 15
78: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 15
78: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 15
78: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 15
78: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 15
78: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 15
78: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 15
78: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 15
78: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 10 15
78: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 11 15
78: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 12 15
78: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 13 15
78: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 14 15
95 121 before Dune::BiCGSTABSolver 
95 Dune::BiCGSTABSolver.apply(x, b, result_) 
95 BiCGSTABSolver::apply 
95 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
95: communicator.hh::BufferedCommunicator::forward, enter sendRecv
95: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 7
95: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 7
95: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 7
95: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 7
95: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 7
95: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 7
114 121 before Dune::BiCGSTABSolver 
114 Dune::BiCGSTABSolver.apply(x, b, result_) 
114 BiCGSTABSolver::apply 
114 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
114: communicator.hh::BufferedCommunicator::forward, enter sendRecv
114: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 13
114: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 13
114: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 13
114: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 13
114: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 13
114: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 13
114: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 13
114: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 13
114: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 13
114: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 13
114: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 10 13
114: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 11 13
114: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 12 13
62 121 before Dune::BiCGSTABSolver 
62 Dune::BiCGSTABSolver.apply(x, b, result_) 
62 BiCGSTABSolver::apply 
62 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
62: communicator.hh::BufferedCommunicator::forward, enter sendRecv
62: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 15
62: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 15
62: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 15
62: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 15
62: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 15
62: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 15
62: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 15
62: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 15
62: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 15
62: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 15
62: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 10 15
62: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 11 15
62: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 12 15
62: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 13 15
104 121 before Dune::BiCGSTABSolver 
104 Dune::BiCGSTABSolver.apply(x, b, result_) 
104 BiCGSTABSolver::apply 
104 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
104: communicator.hh::BufferedCommunicator::forward, enter sendRecv
104: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 9
104: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 9
104: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 9
104: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 9
104: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 9
104: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 9
104: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 9
104: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 9
79 121 before Dune::BiCGSTABSolver 
79 Dune::BiCGSTABSolver.apply(x, b, result_) 
79 BiCGSTABSolver::apply 
79 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
79: communicator.hh::BufferedCommunicator::forward, enter sendRecv
79: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 16
79: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 16
79: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 16
79: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 16
79: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 16
79: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 16
79: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 16
79: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 16
79: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 16
79: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 16
79: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 10 16
79: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 11 16
79: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 12 16
79: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 13 16
79: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 14 16
43 121 before Dune::BiCGSTABSolver 
43 Dune::BiCGSTABSolver.apply(x, b, result_) 
43 BiCGSTABSolver::apply 
43 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
43: communicator.hh::BufferedCommunicator::forward, enter sendRecv
43: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 11
43: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 11
43: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 11
43: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 11
43: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 11
43: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 11
43: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 11
43: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 11
43: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 11
43: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 11
43: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 10 11
48 121 before Dune::BiCGSTABSolver 
48 Dune::BiCGSTABSolver.apply(x, b, result_) 
48 BiCGSTABSolver::apply 
48 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
48: communicator.hh::BufferedCommunicator::forward, enter sendRecv
48: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 15
48: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 15
48: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 15
48: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 15
48: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 15
48: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 15
48: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 15
48: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 15
48: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 15
48: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 15
48: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 10 15
48: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 11 15
48: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 12 15
48: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 13 15
48: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 14 15
48: communicator.hh::sendRecv, left MPI_Wait
80 121 before Dune::BiCGSTABSolver 
80 Dune::BiCGSTABSolver.apply(x, b, result_) 
80 BiCGSTABSolver::apply 
80 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
80: communicator.hh::BufferedCommunicator::forward, enter sendRecv
80: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 10
80: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 10
80: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 10
80: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 10
80: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 10
80: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 10
80: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 10
80: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 10
80: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 10
80: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 10
80: communicator.hh::sendRecv, left MPI_Wait
17 121 before Dune::BiCGSTABSolver 
17 Dune::BiCGSTABSolver.apply(x, b, result_) 
17 BiCGSTABSolver::apply 
17 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
17: communicator.hh::BufferedCommunicator::forward, enter sendRecv
17: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 3
17: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 3
17: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 3
84 121 before Dune::BiCGSTABSolver 
84 Dune::BiCGSTABSolver.apply(x, b, result_) 
84 BiCGSTABSolver::apply 
84 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
84: communicator.hh::BufferedCommunicator::forward, enter sendRecv
84: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 18
84: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 18
84: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 18
84: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 18
84: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 18
84: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 18
84: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 18
84: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 18
84: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 18
84: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 18
84: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 10 18
84: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 11 18
84: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 12 18
84: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 13 18
84: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 14 18
84: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 15 18
84: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 16 18
84: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 17 18
84: communicator.hh::sendRecv, left MPI_Wait
60 121 before Dune::BiCGSTABSolver 
60 Dune::BiCGSTABSolver.apply(x, b, result_) 
60 BiCGSTABSolver::apply 
60 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
60: communicator.hh::BufferedCommunicator::forward, enter sendRecv
60: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 6
60: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 6
60: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 6
60: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 6
60: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 6
60: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 6
60: communicator.hh::sendRecv, left MPI_Wait
60: leave BufferedCommunicator::sendRecv
60: communicator.hh::BufferedCommunicator::forward, left sendRecv
60 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), after communicator.free();
60 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
60: communicator.hh::BufferedCommunicator::forward, enter sendRecv
60: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 6
82 121 before Dune::BiCGSTABSolver 
82 Dune::BiCGSTABSolver.apply(x, b, result_) 
82 BiCGSTABSolver::apply 
82 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
82: communicator.hh::BufferedCommunicator::forward, enter sendRecv
82: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 26
82: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 26
82: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 26
82: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 26
82: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 26
82: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 26
82: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 26
82: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 26
82: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 26
82: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 26
82: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 10 26
82: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 11 26
82: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 12 26
82: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 13 26
82: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 14 26
82: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 15 26
82: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 16 26
82: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 17 26
82: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 18 26
82: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 19 26
82: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 20 26
82: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 21 26
82: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 22 26
82: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 23 26
82: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 24 26
82: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 25 26
109 121 before Dune::BiCGSTABSolver 
109 Dune::BiCGSTABSolver.apply(x, b, result_) 
109 BiCGSTABSolver::apply 
109 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
109: communicator.hh::BufferedCommunicator::forward, enter sendRecv
109: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 20
109: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 20
109: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 20
109: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 20
109: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 20
109: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 20
109: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 20
109: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 20
109: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 20
109: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 20
109: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 10 20
109: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 11 20
109: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 12 20
109: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 13 20
109: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 14 20
109: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 15 20
109: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 16 20
33 121 before Dune::BiCGSTABSolver 
33 Dune::BiCGSTABSolver.apply(x, b, result_) 
33 BiCGSTABSolver::apply 
33 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
33: communicator.hh::BufferedCommunicator::forward, enter sendRecv
33: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 9
33: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 9
33: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 9
33: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 9
33: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 9
33: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 9
33: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 9
33: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 9
33: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 9
33: communicator.hh::sendRecv, left MPI_Wait
29 121 before Dune::BiCGSTABSolver 
29 Dune::BiCGSTABSolver.apply(x, b, result_) 
29 BiCGSTABSolver::apply 
29 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
29: communicator.hh::BufferedCommunicator::forward, enter sendRecv
29: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 11
29: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 11
29: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 11
29: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 11
29: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 11
29: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 11
29: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 11
29: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 11
29: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 11
29: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 11
29: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 10 11
29: communicator.hh::sendRecv, left MPI_Wait
69 121 before Dune::BiCGSTABSolver 
69 Dune::BiCGSTABSolver.apply(x, b, result_) 
69 BiCGSTABSolver::apply 
69 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
69: communicator.hh::BufferedCommunicator::forward, enter sendRecv
69: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 7
69: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 7
69: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 7
69: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 7
69: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 7
69: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 7
69: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 7
31 121 before Dune::BiCGSTABSolver 
31 Dune::BiCGSTABSolver.apply(x, b, result_) 
31 BiCGSTABSolver::apply 
31 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
31: communicator.hh::BufferedCommunicator::forward, enter sendRecv
31: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 10
31: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 10
31: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 10
31: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 10
31: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 10
31: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 10
31: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 10
31: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 10
31: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 10
31: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 10
14 121 before Dune::BiCGSTABSolver 
14 Dune::BiCGSTABSolver.apply(x, b, result_) 
14 BiCGSTABSolver::apply 
14 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
14: communicator.hh::BufferedCommunicator::forward, enter sendRecv
14: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 17
14: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 17
14: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 17
14: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 17
14: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 17
14: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 17
14: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 17
14: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 17
14: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 17
14: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 17
14: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 10 17
14: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 11 17
14: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 12 17
14: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 13 17
14: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 14 17
14: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 15 17
14: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 16 17
14: communicator.hh::sendRecv, left MPI_Wait
6 121 before Dune::BiCGSTABSolver 
6 Dune::BiCGSTABSolver.apply(x, b, result_) 
6 BiCGSTABSolver::apply 
6 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
6: communicator.hh::BufferedCommunicator::forward, enter sendRecv
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 17
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 17
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 17
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 17
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 17
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 17
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 17
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 17
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 17
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 17
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 10 17
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 11 17
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 12 17
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 13 17
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 14 17
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 15 17
86 121 before Dune::BiCGSTABSolver 
86 Dune::BiCGSTABSolver.apply(x, b, result_) 
86 BiCGSTABSolver::apply 
86 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
86: communicator.hh::BufferedCommunicator::forward, enter sendRecv
86: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 31
86: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 31
86: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 31
86: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 31
86: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 31
86: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 31
86: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 31
86: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 31
86: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 31
86: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 31
86: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 10 31
86: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 11 31
86: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 12 31
86: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 13 31
86: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 14 31
86: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 15 31
86: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 16 31
86: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 17 31
86: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 18 31
86: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 19 31
86: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 20 31
86: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 21 31
86: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 22 31
86: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 23 31
86: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 24 31
86: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 25 31
86: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 26 31
86: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 27 31
86: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 28 31
86: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 29 31
86: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 30 31
86: communicator.hh::sendRecv, left MPI_Wait
55 121 before Dune::BiCGSTABSolver 
55 Dune::BiCGSTABSolver.apply(x, b, result_) 
55 BiCGSTABSolver::apply 
55 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
55: communicator.hh::BufferedCommunicator::forward, enter sendRecv
55: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 11
55: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 11
55: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 11
55: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 11
55: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 11
55: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 11
55: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 11
55: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 11
55: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 11
55: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 11
55: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 10 11
55: communicator.hh::sendRecv, left MPI_Wait
99 121 before Dune::BiCGSTABSolver 
99 Dune::BiCGSTABSolver.apply(x, b, result_) 
99 BiCGSTABSolver::apply 
99 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
99: communicator.hh::BufferedCommunicator::forward, enter sendRecv
99: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 13
99: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 13
99: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 13
99: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 13
99: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 13
99: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 13
99: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 13
99: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 13
99: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 13
99: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 13
99: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 10 13
99: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 11 13
99: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 12 13
99: communicator.hh::sendRecv, left MPI_Wait
61 121 before Dune::BiCGSTABSolver 
61 Dune::BiCGSTABSolver.apply(x, b, result_) 
61 BiCGSTABSolver::apply 
61 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
61: communicator.hh::BufferedCommunicator::forward, enter sendRecv
61: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 9
61: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 9
61: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 9
61: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 9
61: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 9
61: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 9
61: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 9
61: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 9
61: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 9
61: communicator.hh::sendRecv, left MPI_Wait
90 121 before Dune::BiCGSTABSolver 
90 Dune::BiCGSTABSolver.apply(x, b, result_) 
90 BiCGSTABSolver::apply 
90 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
90: communicator.hh::BufferedCommunicator::forward, enter sendRecv
90: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 6
90: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 6
90: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 6
90: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 6
90: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 6
90: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 6
90: communicator.hh::sendRecv, left MPI_Wait
90: leave BufferedCommunicator::sendRecv
90: communicator.hh::BufferedCommunicator::forward, left sendRecv
90 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), after communicator.free();
90 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
90: communicator.hh::BufferedCommunicator::forward, enter sendRecv
90: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 6
7 121 before Dune::BiCGSTABSolver 
7 Dune::BiCGSTABSolver.apply(x, b, result_) 
7 BiCGSTABSolver::apply 
7 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
7: communicator.hh::BufferedCommunicator::forward, enter sendRecv
7: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 13
7: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 13
7: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 13
7: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 13
7: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 13
7: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 13
7: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 13
7: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 13
7: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 13
7: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 13
7: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 10 13
7: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 11 13
7: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 12 13
7: communicator.hh::sendRecv, left MPI_Wait
102 121 before Dune::BiCGSTABSolver 
102 Dune::BiCGSTABSolver.apply(x, b, result_) 
102 BiCGSTABSolver::apply 
102 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
102: communicator.hh::BufferedCommunicator::forward, enter sendRecv
102: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 15
102: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 15
102: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 15
102: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 15
102: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 15
102: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 15
102: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 15
102: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 15
102: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 15
102: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 15
102: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 10 15
102: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 11 15
102: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 12 15
102: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 13 15
102: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 14 15
102: communicator.hh::sendRecv, left MPI_Wait
107 121 before Dune::BiCGSTABSolver 
107 Dune::BiCGSTABSolver.apply(x, b, result_) 
107 BiCGSTABSolver::apply 
107 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
107: communicator.hh::BufferedCommunicator::forward, enter sendRecv
107: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 5
107: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 5
107: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 5
107: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 5
107: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 5
107: communicator.hh::sendRecv, left MPI_Wait
107: leave BufferedCommunicator::sendRecv
107: communicator.hh::BufferedCommunicator::forward, left sendRecv
107 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), after communicator.free();
107 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
107: communicator.hh::BufferedCommunicator::forward, enter sendRecv
107: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 5
16 121 before Dune::BiCGSTABSolver 
16 Dune::BiCGSTABSolver.apply(x, b, result_) 
16 BiCGSTABSolver::apply 
16 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
16: communicator.hh::BufferedCommunicator::forward, enter sendRecv
16: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 6
16: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 6
16: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 6
16: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 6
16: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 6
16: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 6
16: communicator.hh::sendRecv, left MPI_Wait
73 121 before Dune::BiCGSTABSolver 
73 Dune::BiCGSTABSolver.apply(x, b, result_) 
73 BiCGSTABSolver::apply 
73 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
73: communicator.hh::BufferedCommunicator::forward, enter sendRecv
73: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 11
73: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 11
73: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 11
73: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 11
73: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 11
73: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 11
73: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 11
73: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 11
73: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 11
73: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 11
73: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 10 11
73: communicator.hh::sendRecv, left MPI_Wait
73: leave BufferedCommunicator::sendRecv
73: communicator.hh::BufferedCommunicator::forward, left sendRecv
73 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), after communicator.free();
73 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
73: communicator.hh::BufferedCommunicator::forward, enter sendRecv
73: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 11
30 121 before Dune::BiCGSTABSolver 
30 Dune::BiCGSTABSolver.apply(x, b, result_) 
30 BiCGSTABSolver::apply 
30 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
30: communicator.hh::BufferedCommunicator::forward, enter sendRecv
30: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 6
30: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 6
30: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 6
30: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 6
30: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 6
30: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 6
30: communicator.hh::sendRecv, left MPI_Wait
30: leave BufferedCommunicator::sendRecv
30: communicator.hh::BufferedCommunicator::forward, left sendRecv
30 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), after communicator.free();
30 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
30: communicator.hh::BufferedCommunicator::forward, enter sendRecv
30: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 6
37 121 before Dune::BiCGSTABSolver 
37 Dune::BiCGSTABSolver.apply(x, b, result_) 
37 BiCGSTABSolver::apply 
37 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
37: communicator.hh::BufferedCommunicator::forward, enter sendRecv
37: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 15
37: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 15
37: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 15
37: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 15
37: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 15
37: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 15
37: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 15
37: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 15
37: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 15
37: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 15
37: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 10 15
37: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 11 15
37: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 12 15
37: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 13 15
37: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 14 15
1 121 before Dune::BiCGSTABSolver 
1 Dune::BiCGSTABSolver.apply(x, b, result_) 
1 BiCGSTABSolver::apply 
1 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
1: communicator.hh::BufferedCommunicator::forward, enter sendRecv
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 5
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 5
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 5
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 5
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 5
1: communicator.hh::sendRecv, left MPI_Wait
42 121 before Dune::BiCGSTABSolver 
42 Dune::BiCGSTABSolver.apply(x, b, result_) 
42 BiCGSTABSolver::apply 
42 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
42: communicator.hh::BufferedCommunicator::forward, enter sendRecv
42: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 20
42: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 20
42: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 20
42: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 20
42: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 20
42: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 20
42: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 20
42: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 20
42: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 20
42: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 20
42: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 10 20
42: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 11 20
42: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 12 20
42: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 13 20
42: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 14 20
42: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 15 20
42: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 16 20
42: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 17 20
42: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 18 20
42: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 19 20
42: communicator.hh::sendRecv, left MPI_Wait
100 121 before Dune::BiCGSTABSolver 
100 Dune::BiCGSTABSolver.apply(x, b, result_) 
100 BiCGSTABSolver::apply 
100 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
100: communicator.hh::BufferedCommunicator::forward, enter sendRecv
100: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 6
100: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 6
100: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 6
100: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 6
100: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 6
100: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 6
100: communicator.hh::sendRecv, left MPI_Wait
100: leave BufferedCommunicator::sendRecv
100: communicator.hh::BufferedCommunicator::forward, left sendRecv
100 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), after communicator.free();
100 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
100: communicator.hh::BufferedCommunicator::forward, enter sendRecv
100: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 6
97 121 before Dune::BiCGSTABSolver 
97 Dune::BiCGSTABSolver.apply(x, b, result_) 
97 BiCGSTABSolver::apply 
97 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
97: communicator.hh::BufferedCommunicator::forward, enter sendRecv
97: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 9
97: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 9
97: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 9
97: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 9
97: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 9
97: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 9
97: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 9
97: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 9
97: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 9
97: communicator.hh::sendRecv, left MPI_Wait
89 121 before Dune::BiCGSTABSolver 
89 Dune::BiCGSTABSolver.apply(x, b, result_) 
89 BiCGSTABSolver::apply 
89 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
89: communicator.hh::BufferedCommunicator::forward, enter sendRecv
89: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 11
89: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 11
89: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 11
89: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 11
89: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 11
89: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 11
89: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 11
89: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 11
89: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 11
89: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 11
89: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 10 11
89: communicator.hh::sendRecv, left MPI_Wait
59: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 11
59: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 11
59: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 11
59: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 10 11
59: communicator.hh::sendRecv, left MPI_Wait
59: leave BufferedCommunicator::sendRecv
59: communicator.hh::BufferedCommunicator::forward, left sendRecv
59 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), after communicator.free();
59 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
59: communicator.hh::BufferedCommunicator::forward, enter sendRecv
59: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 11
59: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 11
59: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 11
59: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 11
59: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 11
117: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 18 22
117: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 19 22
117: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 20 22
117: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 21 22
117: communicator.hh::sendRecv, left MPI_Wait
117: leave BufferedCommunicator::sendRecv
117: communicator.hh::BufferedCommunicator::forward, left sendRecv
117 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), after communicator.free();
117 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
117: communicator.hh::BufferedCommunicator::forward, enter sendRecv
117: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 22
117: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 22
117: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 22
117: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 22
117: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 22
117: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 22
117: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 22
117: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 22
117: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 22
117: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 22
117: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 10 22
117: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 11 22
117: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 12 22
117: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 13 22
20: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 10
20: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 10
20: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 10
20: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 10
20: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 10
20: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 10
20: communicator.hh::sendRecv, left MPI_Wait
20: leave BufferedCommunicator::sendRecv
20: communicator.hh::BufferedCommunicator::forward, left sendRecv
20 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), after communicator.free();
20 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
20: communicator.hh::BufferedCommunicator::forward, enter sendRecv
20: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 10
19: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 14 17
19: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 15 17
19: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 16 17
19: communicator.hh::sendRecv, left MPI_Wait
19: leave BufferedCommunicator::sendRecv
19: communicator.hh::BufferedCommunicator::forward, left sendRecv
19 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), after communicator.free();
19 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
19: communicator.hh::BufferedCommunicator::forward, enter sendRecv
19: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 17
19: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 17
19: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 17
19: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 17
19: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 17
19: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 17
38: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 19 24
38: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 20 24
38: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 21 24
38: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 22 24
38: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 23 24
38: communicator.hh::sendRecv, left MPI_Wait
38: leave BufferedCommunicator::sendRecv
38: communicator.hh::BufferedCommunicator::forward, left sendRecv
38 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), after communicator.free();
38 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
38: communicator.hh::BufferedCommunicator::forward, enter sendRecv
38: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 24
38: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 24
38: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 24
38: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 24
38: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 24
38: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 24
38: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 24
38: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 24
38: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 24
120: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 7
120: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 7
120: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 7
120: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 7
120: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 7
120: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 7
83: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 13 17
83: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 14 17
83: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 15 17
83: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 16 17
83: communicator.hh::sendRecv, left MPI_Wait
83: leave BufferedCommunicator::sendRecv
83: communicator.hh::BufferedCommunicator::forward, left sendRecv
83 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), after communicator.free();
83 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
83: communicator.hh::BufferedCommunicator::forward, enter sendRecv
83: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 17
83: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 17
83: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 17
83: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 17
83: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 17
83: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 17
83: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 17
83: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 17
21: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 11 12
21: communicator.hh::sendRecv, left MPI_Wait
21: leave BufferedCommunicator::sendRecv
21: communicator.hh::BufferedCommunicator::forward, left sendRecv
21 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), after communicator.free();
21 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
21: communicator.hh::BufferedCommunicator::forward, enter sendRecv
21: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 12
21: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 12
21: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 12
21: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 12
21: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 12
21: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 12
21: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 12
21: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 12
24: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 14
24: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 14
24: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 10 14
24: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 11 14
24: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 12 14
24: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 13 14
24: communicator.hh::sendRecv, left MPI_Wait
24: leave BufferedCommunicator::sendRecv
24: communicator.hh::BufferedCommunicator::forward, left sendRecv
24 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), after communicator.free();
24 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
24: communicator.hh::BufferedCommunicator::forward, enter sendRecv
24: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 14
24: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 14
24: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 14
24: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 14
85: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 11
85: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 11
85: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 10 11
85: communicator.hh::sendRecv, left MPI_Wait
85: leave BufferedCommunicator::sendRecv
85: communicator.hh::BufferedCommunicator::forward, left sendRecv
85 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), after communicator.free();
85 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
85: communicator.hh::BufferedCommunicator::forward, enter sendRecv
85: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 11
85: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 11
85: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 11
85: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 11
85: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 11
85: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 11
47: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 6
47: communicator.hh::sendRecv, left MPI_Wait
47: leave BufferedCommunicator::sendRecv
47: communicator.hh::BufferedCommunicator::forward, left sendRecv
47 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), after communicator.free();
47 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
47: communicator.hh::BufferedCommunicator::forward, enter sendRecv
47: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 6
47: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 6
47: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 6
47: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 6
72: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 20
72: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 10 20
72: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 11 20
72: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 12 20
72: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 13 20
72: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 14 20
72: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 15 20
72: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 16 20
72: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 17 20
72: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 18 20
72: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 19 20
72: communicator.hh::sendRecv, left MPI_Wait
72: leave BufferedCommunicator::sendRecv
72: communicator.hh::BufferedCommunicator::forward, left sendRecv
72 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), after communicator.free();
72 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
72: communicator.hh::BufferedCommunicator::forward, enter sendRecv
72: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 20
72: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 20
72: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 20
72: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 20
98: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 13 18
98: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 14 18
98: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 15 18
98: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 16 18
98: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 17 18
98: communicator.hh::sendRecv, left MPI_Wait
98: leave BufferedCommunicator::sendRecv
98: communicator.hh::BufferedCommunicator::forward, left sendRecv
98 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), after communicator.free();
98 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
98: communicator.hh::BufferedCommunicator::forward, enter sendRecv
98: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 18
98: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 18
98: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 18
98: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 18
98: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 18
98: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 18
98: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 18
28: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 13 18
28: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 14 18
28: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 15 18
28: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 16 18
28: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 17 18
28: communicator.hh::sendRecv, left MPI_Wait
28: leave BufferedCommunicator::sendRecv
28: communicator.hh::BufferedCommunicator::forward, left sendRecv
28 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), after communicator.free();
28 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
28: communicator.hh::BufferedCommunicator::forward, enter sendRecv
28: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 18
28: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 18
28: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 18
28: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 18
28: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 18
28: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 18
28: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 18
28: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 18
28: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 18
28: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 18
28: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 10 18
28: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 11 18
28: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 12 18
118: communicator.hh::sendRecv, left MPI_Wait
118: leave BufferedCommunicator::sendRecv
118: communicator.hh::BufferedCommunicator::forward, left sendRecv
118 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), after communicator.free();
118 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
118: communicator.hh::BufferedCommunicator::forward, enter sendRecv
118: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 14
118: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 14
118: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 14
118: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 14
118: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 14
118: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 14
118: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 14
118: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 14
118: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 14
118: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 14
27: leave BufferedCommunicator::sendRecv
27: communicator.hh::BufferedCommunicator::forward, left sendRecv
27 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), after communicator.free();
27 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
27: communicator.hh::BufferedCommunicator::forward, enter sendRecv
27: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 17
27: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 17
27: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 17
27: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 17
27: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 17
27: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 17
27: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 17
27: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 17
66: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 15
66: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 15
66: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 10 15
66: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 11 15
66: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 12 15
66: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 13 15
66: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 14 15
66: communicator.hh::sendRecv, left MPI_Wait
66: leave BufferedCommunicator::sendRecv
66: communicator.hh::BufferedCommunicator::forward, left sendRecv
66 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), after communicator.free();
66 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
66: communicator.hh::BufferedCommunicator::forward, enter sendRecv
66: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 15
66: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 15
58: communicator.hh::sendRecv, left MPI_Wait
58: leave BufferedCommunicator::sendRecv
58: communicator.hh::BufferedCommunicator::forward, left sendRecv
58 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), after communicator.free();
58 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
58: communicator.hh::BufferedCommunicator::forward, enter sendRecv
58: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 21
58: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 21
58: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 21
58: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 21
58: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 21
58: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 21
58: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 21
58: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 21
58: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 21
58: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 21
58: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 10 21
34: communicator.hh::sendRecv, left MPI_Wait
34: leave BufferedCommunicator::sendRecv
34: communicator.hh::BufferedCommunicator::forward, left sendRecv
34 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), after communicator.free();
34 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
34: communicator.hh::BufferedCommunicator::forward, enter sendRecv
34: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 14
34: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 14
34: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 14
34: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 14
91: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 9
91: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 9
91: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 9
91: communicator.hh::sendRecv, left MPI_Wait
91: leave BufferedCommunicator::sendRecv
91: communicator.hh::BufferedCommunicator::forward, left sendRecv
91 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), after communicator.free();
91 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
91: communicator.hh::BufferedCommunicator::forward, enter sendRecv
91: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 9
91: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 9
91: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 9
76: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 11 12
76: communicator.hh::sendRecv, left MPI_Wait
76: leave BufferedCommunicator::sendRecv
76: communicator.hh::BufferedCommunicator::forward, left sendRecv
76 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), after communicator.free();
76 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
76: communicator.hh::BufferedCommunicator::forward, enter sendRecv
76: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 12
76: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 12
76: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 12
74: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 11
74: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 11
74: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 10 11
74: communicator.hh::sendRecv, left MPI_Wait
74: leave BufferedCommunicator::sendRecv
74: communicator.hh::BufferedCommunicator::forward, left sendRecv
74 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), after communicator.free();
74 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
74: communicator.hh::BufferedCommunicator::forward, enter sendRecv
74: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 11
74: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 11
74: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 11
74: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 11
8: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 15 20
8: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 16 20
8: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 17 20
8: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 18 20
8: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 19 20
8: communicator.hh::sendRecv, left MPI_Wait
8: leave BufferedCommunicator::sendRecv
8: communicator.hh::BufferedCommunicator::forward, left sendRecv
8 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), after communicator.free();
8 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
8: communicator.hh::BufferedCommunicator::forward, enter sendRecv
8: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 20
8: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 20
8: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 20
8: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 20
8: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 20
8: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 20
39: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 9
39: communicator.hh::sendRecv, left MPI_Wait
39: leave BufferedCommunicator::sendRecv
39: communicator.hh::BufferedCommunicator::forward, left sendRecv
39 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), after communicator.free();
39 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
39: communicator.hh::BufferedCommunicator::forward, enter sendRecv
39: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 9
39: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 9
39: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 9
108: leave BufferedCommunicator::sendRecv
108: communicator.hh::BufferedCommunicator::forward, left sendRecv
108 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), after communicator.free();
108 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
108: communicator.hh::BufferedCommunicator::forward, enter sendRecv
108: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 9
108: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 9
108: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 9
108: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 9
108: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 9
108: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 9
108: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 9
108: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 9
78: communicator.hh::sendRecv, left MPI_Wait
78: leave BufferedCommunicator::sendRecv
78: communicator.hh::BufferedCommunicator::forward, left sendRecv
78 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), after communicator.free();
78 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
78: communicator.hh::BufferedCommunicator::forward, enter sendRecv
78: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 15
78: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 15
78: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 15
78: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 15
78: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 15
78: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 15
95: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 7
95: communicator.hh::sendRecv, left MPI_Wait
95: leave BufferedCommunicator::sendRecv
95: communicator.hh::BufferedCommunicator::forward, left sendRecv
95 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), after communicator.free();
95 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
95: communicator.hh::BufferedCommunicator::forward, enter sendRecv
95: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 7
95: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 7
95: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 7
114: communicator.hh::sendRecv, left MPI_Wait
114: leave BufferedCommunicator::sendRecv
114: communicator.hh::BufferedCommunicator::forward, left sendRecv
114 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), after communicator.free();
114 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
114: communicator.hh::BufferedCommunicator::forward, enter sendRecv
114: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 13
114: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 13
114: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 13
114: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 13
114: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 13
114: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 13
114: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 13
114: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 13
62: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 14 15
62: communicator.hh::sendRecv, left MPI_Wait
62: leave BufferedCommunicator::sendRecv
62: communicator.hh::BufferedCommunicator::forward, left sendRecv
62 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), after communicator.free();
62 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
62: communicator.hh::BufferedCommunicator::forward, enter sendRecv
62: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 15
62: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 15
62: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 15
62: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 15
62: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 15
104: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 9
104: communicator.hh::sendRecv, left MPI_Wait
104: leave BufferedCommunicator::sendRecv
104: communicator.hh::BufferedCommunicator::forward, left sendRecv
104 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), after communicator.free();
104 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
104: communicator.hh::BufferedCommunicator::forward, enter sendRecv
104: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 9
104: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 9
104: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 9
104: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 9
79: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 15 16
79: communicator.hh::sendRecv, left MPI_Wait
79: leave BufferedCommunicator::sendRecv
79: communicator.hh::BufferedCommunicator::forward, left sendRecv
79 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), after communicator.free();
79 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
79: communicator.hh::BufferedCommunicator::forward, enter sendRecv
79: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 16
79: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 16
79: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 16
79: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 16
79: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 16
79: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 16
43: communicator.hh::sendRecv, left MPI_Wait
43: leave BufferedCommunicator::sendRecv
43: communicator.hh::BufferedCommunicator::forward, left sendRecv
43 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), after communicator.free();
43 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
43: communicator.hh::BufferedCommunicator::forward, enter sendRecv
43: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 11
43: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 11
43: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 11
43: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 11
43: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 11
43: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 11
48: leave BufferedCommunicator::sendRecv
48: communicator.hh::BufferedCommunicator::forward, left sendRecv
48 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), after communicator.free();
48 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
48: communicator.hh::BufferedCommunicator::forward, enter sendRecv
48: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 15
48: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 15
48: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 15
48: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 15
48: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 15
48: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 15
48: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 15
80: leave BufferedCommunicator::sendRecv
80: communicator.hh::BufferedCommunicator::forward, left sendRecv
80 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), after communicator.free();
80 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
80: communicator.hh::BufferedCommunicator::forward, enter sendRecv
80: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 10
80: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 10
80: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 10
80: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 10
80: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 10
80: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 10
80: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 10
17: communicator.hh::sendRecv, left MPI_Wait
17: leave BufferedCommunicator::sendRecv
17: communicator.hh::BufferedCommunicator::forward, left sendRecv
17 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), after communicator.free();
17 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
17: communicator.hh::BufferedCommunicator::forward, enter sendRecv
17: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 3
17: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 3
84: leave BufferedCommunicator::sendRecv
84: communicator.hh::BufferedCommunicator::forward, left sendRecv
84 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), after communicator.free();
84 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
84: communicator.hh::BufferedCommunicator::forward, enter sendRecv
84: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 18
84: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 18
84: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 18
84: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 18
84: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 18
84: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 18
84: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 18
84: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 18
84: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 18
84: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 18
84: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 10 18
84: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 11 18
60: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 6
60: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 6
82: communicator.hh::sendRecv, left MPI_Wait
82: leave BufferedCommunicator::sendRecv
82: communicator.hh::BufferedCommunicator::forward, left sendRecv
82 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), after communicator.free();
82 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
82: communicator.hh::BufferedCommunicator::forward, enter sendRecv
82: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 26
82: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 26
82: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 26
82: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 26
82: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 26
82: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 26
82: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 26
82: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 26
82: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 26
82: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 26
82: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 10 26
82: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 11 26
82: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 12 26
82: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 13 26
82: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 14 26
109: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 17 20
109: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 18 20
109: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 19 20
109: communicator.hh::sendRecv, left MPI_Wait
109: leave BufferedCommunicator::sendRecv
109: communicator.hh::BufferedCommunicator::forward, left sendRecv
109 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), after communicator.free();
109 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
109: communicator.hh::BufferedCommunicator::forward, enter sendRecv
109: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 20
109: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 20
109: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 20
109: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 20
109: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 20
109: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 20
109: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 20
109: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 20
109: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 20
109: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 20
33: leave BufferedCommunicator::sendRecv
33: communicator.hh::BufferedCommunicator::forward, left sendRecv
33 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), after communicator.free();
33 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
33: communicator.hh::BufferedCommunicator::forward, enter sendRecv
33: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 9
33: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 9
33: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 9
33: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 9
33: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 9
29: leave BufferedCommunicator::sendRecv
29: communicator.hh::BufferedCommunicator::forward, left sendRecv
29 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), after communicator.free();
29 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
29: communicator.hh::BufferedCommunicator::forward, enter sendRecv
29: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 11
29: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 11
29: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 11
29: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 11
29: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 11
29: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 11
29: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 11
29: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 11
29: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 11
69: communicator.hh::sendRecv, left MPI_Wait
69: leave BufferedCommunicator::sendRecv
69: communicator.hh::BufferedCommunicator::forward, left sendRecv
69 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), after communicator.free();
69 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
69: communicator.hh::BufferedCommunicator::forward, enter sendRecv
69: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 7
69: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 7
69: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 7
69: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 7
31: communicator.hh::sendRecv, left MPI_Wait
31: leave BufferedCommunicator::sendRecv
31: communicator.hh::BufferedCommunicator::forward, left sendRecv
31 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), after communicator.free();
31 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
31: communicator.hh::BufferedCommunicator::forward, enter sendRecv
31: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 10
31: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 10
31: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 10
31: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 10
31: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 10
14: leave BufferedCommunicator::sendRecv
14: communicator.hh::BufferedCommunicator::forward, left sendRecv
14 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), after communicator.free();
14 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
14: communicator.hh::BufferedCommunicator::forward, enter sendRecv
14: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 17
14: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 17
14: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 17
14: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 17
14: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 17
14: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 17
14: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 17
14: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 17
14: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 17
14: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 17
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 16 17
6: communicator.hh::sendRecv, left MPI_Wait
6: leave BufferedCommunicator::sendRecv
6: communicator.hh::BufferedCommunicator::forward, left sendRecv
6 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), after communicator.free();
6 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
6: communicator.hh::BufferedCommunicator::forward, enter sendRecv
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 17
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 17
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 17
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 17
86: leave BufferedCommunicator::sendRecv
86: communicator.hh::BufferedCommunicator::forward, left sendRecv
86 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), after communicator.free();
86 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
86: communicator.hh::BufferedCommunicator::forward, enter sendRecv
86: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 31
86: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 31
86: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 31
86: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 31
86: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 31
86: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 31
86: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 31
86: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 31
86: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 31
86: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 31
86: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 10 31
86: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 11 31
86: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 12 31
86: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 13 31
86: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 14 31
86: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 15 31
86: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 16 31
86: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 17 31
86: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 18 31
55: leave BufferedCommunicator::sendRecv
55: communicator.hh::BufferedCommunicator::forward, left sendRecv
55 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), after communicator.free();
55 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
55: communicator.hh::BufferedCommunicator::forward, enter sendRecv
55: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 11
55: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 11
55: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 11
55: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 11
55: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 11
55: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 11
55: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 11
55: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 11
55: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 11
55: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 11
55: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 10 11
99: leave BufferedCommunicator::sendRecv
99: communicator.hh::BufferedCommunicator::forward, left sendRecv
99 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), after communicator.free();
99 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
99: communicator.hh::BufferedCommunicator::forward, enter sendRecv
99: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 13
99: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 13
99: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 13
99: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 13
99: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 13
99: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 13
99: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 13
99: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 13
99: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 13
61: leave BufferedCommunicator::sendRecv
61: communicator.hh::BufferedCommunicator::forward, left sendRecv
61 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), after communicator.free();
61 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
61: communicator.hh::BufferedCommunicator::forward, enter sendRecv
61: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 9
61: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 9
61: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 9
61: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 9
90: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 6
90: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 6
102: leave BufferedCommunicator::sendRecv
102: communicator.hh::BufferedCommunicator::forward, left sendRecv
102 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), after communicator.free();
102 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
102: communicator.hh::BufferedCommunicator::forward, enter sendRecv
102: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 15
102: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 15
102: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 15
102: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 15
102: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 15
102: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 15
102: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 15
102: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 15
107: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 5
107: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 5
107: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 5
107: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 5
107: communicator.hh::sendRecv, left MPI_Wait
107: leave BufferedCommunicator::sendRecv
107: communicator.hh::BufferedCommunicator::forward, left sendRecv
107 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), after communicator.free();
16: leave BufferedCommunicator::sendRecv
16: communicator.hh::BufferedCommunicator::forward, left sendRecv
16 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), after communicator.free();
16 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
16: communicator.hh::BufferedCommunicator::forward, enter sendRecv
16: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 6
16: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 6
16: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 6
16: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 6
16: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 6
16: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 6
30: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 6
30: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 6
37: communicator.hh::sendRecv, left MPI_Wait
37: leave BufferedCommunicator::sendRecv
37: communicator.hh::BufferedCommunicator::forward, left sendRecv
37 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), after communicator.free();
37 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
37: communicator.hh::BufferedCommunicator::forward, enter sendRecv
37: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 15
37: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 15
37: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 15
37: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 15
37: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 15
37: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 15
37: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 15
37: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 15
37: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 15
37: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 15
37: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 10 15
37: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 11 15
42: leave BufferedCommunicator::sendRecv
42: communicator.hh::BufferedCommunicator::forward, left sendRecv
42 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), after communicator.free();
42 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
42: communicator.hh::BufferedCommunicator::forward, enter sendRecv
42: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 20
42: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 20
42: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 20
42: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 20
42: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 20
42: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 20
42: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 20
42: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 20
42: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 20
100: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 6
100: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 6
100: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 6
89: leave BufferedCommunicator::sendRecv
89: communicator.hh::BufferedCommunicator::forward, left sendRecv
89 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), after communicator.free();
89 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
89: communicator.hh::BufferedCommunicator::forward, enter sendRecv
89: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 11
89: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 11
89: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 11
89: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 11
89: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 11
89: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 11
89: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 11
89: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 11
89: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 11
89: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 11
22 121 before Dune::BiCGSTABSolver 
22 Dune::BiCGSTABSolver.apply(x, b, result_) 
22 BiCGSTABSolver::apply 
22 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
22: communicator.hh::BufferedCommunicator::forward, enter sendRecv
22: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 20
22: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 20
22: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 20
22: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 20
22: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 20
22: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 20
22: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 20
22: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 20
22: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 20
22: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 20
22: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 10 20
22: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 11 20
22: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 12 20
22: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 13 20
22: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 14 20
22: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 15 20
22: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 16 20
22: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 17 20
22: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 18 20
22: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 19 20
22: communicator.hh::sendRecv, left MPI_Wait
22: leave BufferedCommunicator::sendRecv
22: communicator.hh::BufferedCommunicator::forward, left sendRecv
22 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), after communicator.free();
22 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
22: communicator.hh::BufferedCommunicator::forward, enter sendRecv
22: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 20
22: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 20
22: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 20
22: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 20
22: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 20
22: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 20
22: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 20
22: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 20
22: communi116: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 7
116: communicator.hh::sendRecv, left MPI_Wait
116: leave BufferedCommunicator::sendRecv
116: communicator.hh::BufferedCommunicator::forward, left sendRecv
116 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), after communicator.free();
116 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
116: communicator.hh::BufferedCommunicator::forward, enter sendRecv
116: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 7
116: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 7
116: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 7
116: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 7
116: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 7
116: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 7
116: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 7
116: communicator.hh::sendRecv, left MPI_Wait
116: leave BufferedCommunicator::sendRecv
116: communicator.hh::BufferedCommunicator::forward, left sendRecv
116 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), after communicator.free();
101: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 11 24
101: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 12 24
101: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 13 24
101: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 14 24
101: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 15 24
101: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 16 24
101: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 17 24
101: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 18 24
101: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 19 24
101: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 20 24
101: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 21 24
101: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 22 24
101: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 23 24
101: communicator.hh::sendRecv, left MPI_Wait
101: leave BufferedCommunicator::sendRecv
101: communicator.hh::BufferedCommunicator::forward, left sendRecv
101 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), after communicator.free();
101 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
101: communicator.hh::BufferedCommunicator::forward, enter sendRecv
101: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 24
101: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 24
101: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 24
101: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 24
101: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 24
101: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 24
101: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 24
101: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 24
101: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 24
101: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 24
101: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 10 24
101: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 11 24
101: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 12 24
101: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 13 24
101: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 14 24
101: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 15 24
103: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 16
103: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 10 16
103: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 11 16
103: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 12 16
103: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 13 16
103: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 14 16
103: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 15 16
103: communicator.hh::sendRecv, left MPI_Wait
103: leave BufferedCommunicator::sendRecv
103: communicator.hh::BufferedCommunicator::forward, left sendRecv
103 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), after communicator.free();
103 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
103: communicator.hh::BufferedCommunicator::forward, enter sendRecv
103: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 16
103: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 16
103: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 16
103: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 16
103: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 16
103: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 16
103: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 16
103: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 16
103: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 16
103: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 16
103: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 10 16
12: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 18 26
12: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 19 26
12: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 20 26
12: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 21 26
12: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 22 26
12: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 23 26
12: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 24 26
12: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 25 26
12: communicator.hh::sendRecv, left MPI_Wait
12: leave BufferedCommunicator::sendRecv
12: communicator.hh::BufferedCommunicator::forward, left sendRecv
12 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), after communicator.free();
12 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
12: communicator.hh::BufferedCommunicator::forward, enter sendRecv
12: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 26
12: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 26
12: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 26
12: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 26
12: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 26
12: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 26
12: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 26
12: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 26
12: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 26
12: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 26
12: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 10 26
12: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 11 26
12: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 12 26
12: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 13 26
12: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 14 26
12: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 15 26
12: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 16 26
12: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 17 26
12: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 18 26
12: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 19 26
12: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 20 26
12: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 21 26
12: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), rec75: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 17
75: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 17
75: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 17
75: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 17
75: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 17
75: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 17
75: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 17
75: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 10 17
75: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 11 17
75: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 12 17
75: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 13 17
75: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 14 17
75: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 15 17
75: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 16 17
75: communicator.hh::sendRecv, left MPI_Wait
75: leave BufferedCommunicator::sendRecv
75: communicator.hh::BufferedCommunicator::forward, left sendRecv
75 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), after communicator.free();
75 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
75: communicator.hh::BufferedCommunicator::forward, enter sendRecv
75: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 17
75: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 17
75: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 17
75: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 17
75: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 17
75: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 17
75: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 17
75: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 17
75: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 17
75: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 17
75: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 10 17
71: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 10 20
71: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 11 20
71: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 12 20
71: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 13 20
71: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 14 20
71: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 15 20
71: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 16 20
71: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 17 20
71: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 18 20
71: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 19 20
71: communicator.hh::sendRecv, left MPI_Wait
71: leave BufferedCommunicator::sendRecv
71: communicator.hh::BufferedCommunicator::forward, left sendRecv
71 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), after communicator.free();
71 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
71: communicator.hh::BufferedCommunicator::forward, enter sendRecv
71: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 20
71: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 20
71: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 20
71: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 20
71: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 20
71: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 20
71: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 20
71: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 20
71: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 20
71: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 20
71: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 10 20
71: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 11 20
71: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 12 20
71: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 13 20
71: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 14 20
71: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 15 20
71: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 16 20
(), recvRequests, &finished, &status); 2 117
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 117
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 117
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 117
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 117
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 117
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 117
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 117
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 10 117
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 11 117
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 12 117
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 13 117
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 14 117
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 15 117
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 16 117
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 17 117
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 18 117
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 19 117
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 20 117
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 21 117
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 22 117
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 23 117
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 24 117
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 25 117
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 26 117
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 27 117
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 28 117
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 29 117
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 30 117
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 31 117
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 32 117
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 33 117
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 34 117
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 35 117
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 36 117
0: communic111: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 11
111: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 11
111: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 10 11
111: communicator.hh::sendRecv, left MPI_Wait
111: leave BufferedCommunicator::sendRecv
111: communicator.hh::BufferedCommunicator::forward, left sendRecv
111 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), after communicator.free();
111 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
111: communicator.hh::BufferedCommunicator::forward, enter sendRecv
111: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 11
111: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 11
111: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 11
111: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 11
111: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 11
111: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 11
111: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 11
111: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 11
111: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 11
111: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 11
111: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 10 11
111: communicator.hh::sendRecv, left MPI_Wait
56: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 12 31
56: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 13 31
56: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 14 31
56: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 15 31
56: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 16 31
56: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 17 31
56: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 18 31
56: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 19 31
56: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 20 31
56: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 21 31
56: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 22 31
56: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 23 31
56: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 24 31
56: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 25 31
56: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 26 31
56: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 27 31
56: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 28 31
56: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 29 31
56: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 30 31
56: communicator.hh::sendRecv, left MPI_Wait
56: leave BufferedCommunicator::sendRecv
56: communicator.hh::BufferedCommunicator::forward, left sendRecv
56 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), after communicator.free();
56 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
56: communicator.hh::BufferedCommunicator::forward, enter sendRecv
56: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 31
56: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 31
56: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 31
56: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 31
56: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 31
56: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 31
56: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 31
56: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 31
56: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 31
56: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 31
56: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 10 31
56: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), rec67: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 13
67: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 10 13
67: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 11 13
67: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 12 13
67: communicator.hh::sendRecv, left MPI_Wait
67: leave BufferedCommunicator::sendRecv
67: communicator.hh::BufferedCommunicator::forward, left sendRecv
67 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), after communicator.free();
67 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
67: communicator.hh::BufferedCommunicator::forward, enter sendRecv
67: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 13
67: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 13
67: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 13
67: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 13
67: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 13
67: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 13
67: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 13
67: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 13
67: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 13
67: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 13
67: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 10 13
32: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 16
32: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 16
32: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 16
32: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 16
32: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 10 16
32: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 11 16
32: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 12 16
32: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 13 16
32: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 14 16
32: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 15 16
32: communicator.hh::sendRecv, left MPI_Wait
32: leave BufferedCommunicator::sendRecv
32: communicator.hh::BufferedCommunicator::forward, left sendRecv
32 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), after communicator.free();
32 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
32: communicator.hh::BufferedCommunicator::forward, enter sendRecv
32: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 16
32: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 16
32: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 16
32: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 16
32: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 16
32: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 16
32: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 16
32: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 16
32: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 16
32: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 16
32: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 10 16
32: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 11 16
32: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 12 16
32: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 13 16
32: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 14 16
32: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 15 16
115: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 10 11
115: communicator.hh::sendRecv, left MPI_Wait
115: leave BufferedCommunicator::sendRecv
115: communicator.hh::BufferedCommunicator::forward, left sendRecv
115 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), after communicator.free();
115 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
115: communicator.hh::BufferedCommunicator::forward, enter sendRecv
115: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 11
115: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 11
115: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 11
115: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 11
115: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 11
115: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 11
115: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 11
115: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 11
115: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 11
115: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 11
115: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 10 11
10: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 11
10: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 11
10: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 11
10: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 11
10: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 10 11
10: communicator.hh::sendRecv, left MPI_Wait
10: leave BufferedCommunicator::sendRecv
10: communicator.hh::BufferedCommunicator::forward, left sendRecv
10 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), after communicator.free();
10 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
10: communicator.hh::BufferedCommunicator::forward, enter sendRecv
10: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 11
10: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 11
10: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 11
10: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 11
10: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 11
10: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 11
10: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 11
10: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 11
10: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 11
10: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 11
10: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 10 11
105: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 9
105: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 9
105: communicator.hh::sendRecv, left MPI_Wait
105: leave BufferedCommunicator::sendRecv
105: communicator.hh::BufferedCommunicator::forward, left sendRecv
105 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), after communicator.free();
105 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
105: communicator.hh::BufferedCommunicator::forward, enter sendRecv
105: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 9
105: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 9
105: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 9
105: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 9
105: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 9
105: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 9
105: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 9
105: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 9
105: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 9
105: communicator.hh::sendRecv, left MPI_Wait
105: leave BufferedCommunicator::sendRecv
105: communicator.hh::BufferedCommunicator::forward, left sendRecv
105 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), after communicator.free();
11: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 17
11: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 17
11: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 17
11: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 17
11: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 10 17
11: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 11 17
11: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 12 17
11: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 13 17
11: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 14 17
11: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 15 17
11: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 16 17
11: communicator.hh::sendRecv, left MPI_Wait
11: leave BufferedCommunicator::sendRecv
11: communicator.hh::BufferedCommunicator::forward, left sendRecv
11 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), after communicator.free();
11 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
11: communicator.hh::BufferedCommunicator::forward, enter sendRecv
11: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 17
11: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 17
11: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 17
11: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 17
11: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 17
11: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 17
11: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 17
11: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 17
11: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 17
11: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 17
11: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 10 17
11: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 11 17
11: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 12 17
11: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 13 17
11: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 14 17
11: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 15 17
53: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 17
53: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 17
53: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 17
53: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 17
53: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 17
53: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 17
53: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 10 17
53: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 11 17
53: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 12 17
53: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 13 17
53: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 14 17
53: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 15 17
53: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 16 17
53: communicator.hh::sendRecv, left MPI_Wait
53: leave BufferedCommunicator::sendRecv
53: communicator.hh::BufferedCommunicator::forward, left sendRecv
53 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), after communicator.free();
53 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
53: communicator.hh::BufferedCommunicator::forward, enter sendRecv
53: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 17
53: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 17
53: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 17
53: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 17
53: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 17
53: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 17
53: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 17
53: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 17
53: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 17
53: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 17
53: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 10 17
53: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 11 17
53: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 12 17
53: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 13 17
53: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 14 17
53: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 15 17
53: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 16 17
53: communicator.hh::sendRecv, left MPI_Wait
77: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 6
77: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 6
77: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 6
77: communicator.hh::sendRecv, left MPI_Wait
77: leave BufferedCommunicator::sendRecv
77: communicator.hh::BufferedCommunicator::forward, left sendRecv
77 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), after communicator.free();
77 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
77: communicator.hh::BufferedCommunicator::forward, enter sendRecv
77: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 6
77: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 6
77: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 6
77: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 6
77: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 6
77: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 6
77: communicator.hh::sendRecv, left MPI_Wait
88: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 21
88: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 10 21
88: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 11 21
88: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 12 21
88: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 13 21
88: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 14 21
88: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 15 21
88: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 16 21
88: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 17 21
88: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 18 21
88: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 19 21
88: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 20 21
88: communicator.hh::sendRecv, left MPI_Wait
88: leave BufferedCommunicator::sendRecv
88: communicator.hh::BufferedCommunicator::forward, left sendRecv
88 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), after communicator.free();
88 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
88: communicator.hh::BufferedCommunicator::forward, enter sendRecv
88: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 21
88: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 21
88: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 21
88: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 21
88: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 21
88: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 21
88: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 21
88: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 21
88: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 21
88: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 21
88: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 10 21
88: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 11 21
88: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 12 21
88: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 13 21
88: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 14 21
88: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 15 21
88: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 16 21
88: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 17 21
88: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recv113: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 15 18
113: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 16 18
113: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 17 18
113: communicator.hh::sendRecv, left MPI_Wait
113: leave BufferedCommunicator::sendRecv
113: communicator.hh::BufferedCommunicator::forward, left sendRecv
113 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), after communicator.free();
113 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
113: communicator.hh::BufferedCommunicator::forward, enter sendRecv
113: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 18
113: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 18
113: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 18
113: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 18
113: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 18
113: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 18
113: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 18
113: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 18
113: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 18
113: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 18
113: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 10 18
113: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 11 18
113: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 12 18
113: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 13 18
113: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 14 18
113: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 15 18
113: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 16 18
113: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 17 18
113: communicator.hh::sendRecv, left MPI_Wait
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 14
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 14
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 14
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 14
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 14
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 10 14
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 11 14
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 12 14
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 13 14
4: communicator.hh::sendRecv, left MPI_Wait
4: leave BufferedCommunicator::sendRecv
4: communicator.hh::BufferedCommunicator::forward, left sendRecv
4 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), after communicator.free();
4 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
4: communicator.hh::BufferedCommunicator::forward, enter sendRecv
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 14
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 14
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 14
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 14
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 14
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 14
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 14
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 14
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 14
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 14
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 10 14
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 11 14
112: leave BufferedCommunicator::sendRecv
112: communicator.hh::BufferedCommunicator::forward, left sendRecv
112 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), after communicator.free();
112 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
112: communicator.hh::BufferedCommunicator::forward, enter sendRecv
112: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 5
112: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 5
112: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 5
112: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 5
112: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 5
112: communicator.hh::sendRecv, left MPI_Wait
35: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 9
35: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 9
35: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 9
35: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 9
35: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 9
35: communicator.hh::sendRecv, left MPI_Wait
35: leave BufferedCommunicator::sendRecv
35: communicator.hh::BufferedCommunicator::forward, left sendRecv
35 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), after communicator.free();
35 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
35: communicator.hh::BufferedCommunicator::forward, enter sendRecv
35: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 9
35: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 9
35: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 9
35: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 9
35: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 9
35: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 9
35: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 9
35: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 9
35: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 9
35: communicator.hh::sendRecv, left MPI_Wait
87: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 12 19
87: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 13 19
87: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 14 19
87: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 15 19
87: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 16 19
87: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 17 19
87: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 18 19
87: communicator.hh::sendRecv, left MPI_Wait
87: leave BufferedCommunicator::sendRecv
87: communicator.hh::BufferedCommunicator::forward, left sendRecv
87 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), after communicator.free();
87 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
87: communicator.hh::BufferedCommunicator::forward, enter sendRecv
87: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 19
87: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 19
87: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 19
87: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 19
87: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 19
87: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 19
87: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 19
87: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 19
87: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 19
87: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 19
87: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 10 19
87: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 11 19
87: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 12 19
87: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 13 19
87: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 14 19
87: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 15 19
87: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 16 19
87: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 17 19
87: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 18 19
87: communicator.hh::sendRecv, left MPI_Wait
70: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 13
70: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 10 13
70: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 11 13
70: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 12 13
70: communicator.hh::sendRecv, left MPI_Wait
70: leave BufferedCommunicator::sendRecv
70: communicator.hh::BufferedCommunicator::forward, left sendRecv
70 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), after communicator.free();
70 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
70: communicator.hh::BufferedCommunicator::forward, enter sendRecv
70: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 13
70: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 13
70: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 13
70: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 13
70: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 13
70: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 13
70: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 13
70: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 13
70: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 13
70: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 13
70: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 10 13
70: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 11 13
70: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 12 13
70: communicator.hh::sendRecv, left MPI_Wait
94: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 10 21
94: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 11 21
94: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 12 21
94: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 13 21
94: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 14 21
94: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 15 21
94: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 16 21
94: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 17 21
94: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 18 21
94: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 19 21
94: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 20 21
94: communicator.hh::sendRecv, left MPI_Wait
94: leave BufferedCommunicator::sendRecv
94: communicator.hh::BufferedCommunicator::forward, left sendRecv
94 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), after communicator.free();
94 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
94: communicator.hh::BufferedCommunicator::forward, enter sendRecv
94: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 21
94: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 21
94: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 21
94: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 21
94: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 21
94: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 21
94: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 21
94: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 21
94: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 21
94: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 21
94: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 10 21
94: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 11 21
94: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 12 21
94: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 13 21
94: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 14 21
94: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 15 21
94: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 16 21
94: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 17 21
94: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 18 21
94: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), rec119: communicator.hh::sendRecv, left MPI_Wait
119: leave BufferedCommunicator::sendRecv
119: communicator.hh::BufferedCommunicator::forward, left sendRecv
119 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), after communicator.free();
119 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
119: communicator.hh::BufferedCommunicator::forward, enter sendRecv
119: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 13
119: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 13
119: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 13
119: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 13
119: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 13
119: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 13
119: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 13
119: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 13
119: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 13
119: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 13
119: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 10 13
119: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 11 13
119: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 12 13
119: communicator.hh::sendRecv, left MPI_Wait
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 8
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 8
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 8
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 8
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 8
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 8
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 8
2: communicator.hh::sendRecv, left MPI_Wait
2: leave BufferedCommunicator::sendRecv
2: communicator.hh::BufferedCommunicator::forward, left sendRecv
2 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), after communicator.free();
2 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
2: communicator.hh::BufferedCommunicator::forward, enter sendRecv
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 8
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 8
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 8
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 8
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 8
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 8
18: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 14
18: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 10 14
18: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 11 14
18: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 12 14
18: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 13 14
18: communicator.hh::sendRecv, left MPI_Wait
18: leave BufferedCommunicator::sendRecv
18: communicator.hh::BufferedCommunicator::forward, left sendRecv
18 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), after communicator.free();
18 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
18: communicator.hh::BufferedCommunicator::forward, enter sendRecv
18: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 14
18: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 14
18: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 14
18: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 14
18: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 14
18: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 14
18: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 14
18: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 14
18: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 14
18: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 14
18: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 10 14
18: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 11 14
18: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 12 14
18: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 13 14
18: communicator.hh::sendRecv, left MPI_Wait
9: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 7
9: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 7
9: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 7
9: communicator.hh::sendRecv, left MPI_Wait
9: leave BufferedCommunicator::sendRecv
9: communicator.hh::BufferedCommunicator::forward, left sendRecv
9 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), after communicator.free();
9 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
9: communicator.hh::BufferedCommunicator::forward, enter sendRecv
9: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 7
9: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 7
9: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 7
9: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 7
9: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 7
9: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 7
9: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 7
52: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 14 26
52: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 15 26
52: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 16 26
52: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 17 26
52: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 18 26
52: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 19 26
52: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 20 26
52: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 21 26
52: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 22 26
52: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 23 26
52: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 24 26
52: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 25 26
52: communicator.hh::sendRecv, left MPI_Wait
52: leave BufferedCommunicator::sendRecv
52: communicator.hh::BufferedCommunicator::forward, left sendRecv
52 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), after communicator.free();
52 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
52: communicator.hh::BufferedCommunicator::forward, enter sendRecv
52: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 26
52: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 26
52: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 26
52: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 26
52: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 26
52: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 26
52: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 26
52: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 26
52: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 26
52: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 26
52: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 10 26
52: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 11 26
52: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 12 26
52: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 13 26
52: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 14 26
52: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 15 26
52: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 16 26
52: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 17 26
52: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), rec41: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 13 20
41: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 14 20
41: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 15 20
41: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 16 20
41: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 17 20
41: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 18 20
41: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 19 20
41: communicator.hh::sendRecv, left MPI_Wait
41: leave BufferedCommunicator::sendRecv
41: communicator.hh::BufferedCommunicator::forward, left sendRecv
41 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), after communicator.free();
41 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
41: communicator.hh::BufferedCommunicator::forward, enter sendRecv
41: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 20
41: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 20
41: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 20
41: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 20
41: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 20
41: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 20
41: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 20
41: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 20
41: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 20
41: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 20
41: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 10 20
41: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 11 20
41: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 12 20
41: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 13 20
41: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 14 20
41: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 15 20
41: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 16 20
41: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 17 20
41: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 18 20
41: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 19 20
41: communicator.hh::sendRecv, left MPI_Wait
36: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 14
36: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 14
36: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 14
36: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 14
36: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 10 14
36: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 11 14
36: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 12 14
36: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 13 14
36: communicator.hh::sendRecv, left MPI_Wait
36: leave BufferedCommunicator::sendRecv
36: communicator.hh::BufferedCommunicator::forward, left sendRecv
36 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), after communicator.free();
36 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
36: communicator.hh::BufferedCommunicator::forward, enter sendRecv
36: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 14
36: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 14
36: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 14
36: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 14
36: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 14
36: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 14
36: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 14
36: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 14
36: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 14
36: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 14
36: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 10 14
36: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 11 14
36: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 12 14
36: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 13 14
36: communicator.hh::sendRecv, left MPI_Wait
40: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 15
40: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 15
40: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 15
40: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 10 15
40: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 11 15
40: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 12 15
40: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 13 15
40: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 14 15
40: communicator.hh::sendRecv, left MPI_Wait
40: leave BufferedCommunicator::sendRecv
40: communicator.hh::BufferedCommunicator::forward, left sendRecv
40 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), after communicator.free();
40 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
40: communicator.hh::BufferedCommunicator::forward, enter sendRecv
40: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 15
40: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 15
40: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 15
40: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 15
40: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 15
40: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 15
40: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 15
40: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 15
40: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 15
40: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 15
40: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 10 15
40: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 11 15
40: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 12 15
40: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 13 15
40: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 14 15
40: communicator.hh::sendRecv, left MPI_Wait
26: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 19 26
26: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 20 26
26: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 21 26
26: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 22 26
26: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 23 26
26: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 24 26
26: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 25 26
26: communicator.hh::sendRecv, left MPI_Wait
26: leave BufferedCommunicator::sendRecv
26: communicator.hh::BufferedCommunicator::forward, left sendRecv
26 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), after communicator.free();
26 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
26: communicator.hh::BufferedCommunicator::forward, enter sendRecv
26: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 26
26: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 26
26: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 26
26: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 26
26: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 26
26: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 26
26: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 26
26: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 26
26: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 26
26: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 26
26: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 10 26
26: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 11 26
26: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 12 26
26: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 13 26
26: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 14 26
26: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 15 26
26: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 16 26
26: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 17 26
26: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 18 26
26: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 19 26
26: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 20 26
26: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 21 26
26: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 22 26
26: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), rec23: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 11
23: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 11
23: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 10 11
23: communicator.hh::sendRecv, left MPI_Wait
23: leave BufferedCommunicator::sendRecv
23: communicator.hh::BufferedCommunicator::forward, left sendRecv
23 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), after communicator.free();
23 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
23: communicator.hh::BufferedCommunicator::forward, enter sendRecv
23: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 11
23: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 11
23: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 11
23: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 11
23: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 11
23: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 11
23: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 11
23: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 11
23: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 11
23: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 11
23: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 10 11
23: communicator.hh::sendRecv, left MPI_Wait
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 11
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 11
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 11
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 11
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 11
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 11
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 11
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 10 11
5: communicator.hh::sendRecv, left MPI_Wait
5: leave BufferedCommunicator::sendRecv
5: communicator.hh::BufferedCommunicator::forward, left sendRecv
5 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), after communicator.free();
5 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
5: communicator.hh::BufferedCommunicator::forward, enter sendRecv
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 11
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 11
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 11
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 11
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 11
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 11
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 11
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 11
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 11
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 11
51: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 10
51: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 10
51: communicator.hh::sendRecv, left MPI_Wait
51: leave BufferedCommunicator::sendRecv
51: communicator.hh::BufferedCommunicator::forward, left sendRecv
51 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), after communicator.free();
51 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
51: communicator.hh::BufferedCommunicator::forward, enter sendRecv
51: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 10
51: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 10
51: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 10
51: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 10
51: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 10
51: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 10
51: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 10
51: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 10
51: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 10
51: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 10
51: communicator.hh::sendRecv, left MPI_Wait
13: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 11
13: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 11
13: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 11
13: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 11
13: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 10 11
13: communicator.hh::sendRecv, left MPI_Wait
13: leave BufferedCommunicator::sendRecv
13: communicator.hh::BufferedCommunicator::forward, left sendRecv
13 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), after communicator.free();
13 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
13: communicator.hh::BufferedCommunicator::forward, enter sendRecv
13: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 11
13: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 11
13: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 11
13: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 11
13: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 11
13: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 11
13: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 11
13: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 11
13: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 11
13: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 11
13: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 10 11
46: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 12
46: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 12
46: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 12
46: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 10 12
46: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 11 12
46: communicator.hh::sendRecv, left MPI_Wait
46: leave BufferedCommunicator::sendRecv
46: communicator.hh::BufferedCommunicator::forward, left sendRecv
46 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), after communicator.free();
46 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
46: communicator.hh::BufferedCommunicator::forward, enter sendRecv
46: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 12
46: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 12
46: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 12
46: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 12
46: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 12
46: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 12
46: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 12
46: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 12
46: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 12
46: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 12
46: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 10 12
46: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 11 12
46: communicator.hh::sendRecv, left MPI_Wait
46: leave BufferedCommunicator::sendRecv
46: communicator.hh::BufferedCommunicator::forward, left sendRecv
46 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), after communicator.free();
63: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 9
63: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 9
63: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 9
63: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 9
63: communicator.hh::sendRecv, left MPI_Wait
63: leave BufferedCommunicator::sendRecv
63: communicator.hh::BufferedCommunicator::forward, left sendRecv
63 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), after communicator.free();
63 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
63: communicator.hh::BufferedCommunicator::forward, enter sendRecv
63: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 9
63: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 9
63: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 9
63: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 9
63: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 9
63: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 9
63: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 9
63: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 9
63: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 9
63: communicator.hh::sendRecv, left MPI_Wait
15: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 8
15: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 8
15: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 8
15: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 8
15: communicator.hh::sendRecv, left MPI_Wait
15: leave BufferedCommunicator::sendRecv
15: communicator.hh::BufferedCommunicator::forward, left sendRecv
15 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), after communicator.free();
15 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
15: communicator.hh::BufferedCommunicator::forward, enter sendRecv
15: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 8
15: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 8
15: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 8
15: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 8
15: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 8
15: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 8
15: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 8
15: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 8
15: communicator.hh::sendRecv, left MPI_Wait
106: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 11 16
106: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 12 16
106: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 13 16
106: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 14 16
106: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 15 16
106: communicator.hh::sendRecv, left MPI_Wait
106: leave BufferedCommunicator::sendRecv
106: communicator.hh::BufferedCommunicator::forward, left sendRecv
106 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), after communicator.free();
106 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
106: communicator.hh::BufferedCommunicator::forward, enter sendRecv
106: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 16
106: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 16
106: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 16
106: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 16
106: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 16
106: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 16
106: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 16
106: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 16
106: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 16
106: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 16
106: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 10 16
106: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 11 16
106: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 12 16
106: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 13 16
106: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 14 16
106: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 15 16
106: communicator.hh::sendRecv, left MPI_Wait
106: leave BufferedCommunicator::sendRecv
106: communicator.hh::BufferedCommunicator::forward, left sendRecv
106 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), after communicator.free();
54: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 18
54: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 10 18
54: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 11 18
54: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 12 18
54: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 13 18
54: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 14 18
54: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 15 18
54: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 16 18
54: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 17 18
54: communicator.hh::sendRecv, left MPI_Wait
54: leave BufferedCommunicator::sendRecv
54: communicator.hh::BufferedCommunicator::forward, left sendRecv
54 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), after communicator.free();
54 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
54: communicator.hh::BufferedCommunicator::forward, enter sendRecv
54: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 18
54: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 18
54: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 18
54: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 18
54: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 18
54: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 18
54: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 18
54: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 18
54: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 18
54: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 18
54: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 10 18
54: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 11 18
54: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 12 18
54: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 13 18
54: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 14 18
54: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 15 18
54: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 16 18
54: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 17 18
54: communicator.hh::sendRecv, left MPI_Wait
54: leave BufferedCommunicator::sendRecv
54: communicator.hh::BufferedCommunicator::forward, left sendRecv
54 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), after communicator.free();
81: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 11
81: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 11
81: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 10 11
81: communicator.hh::sendRecv, left MPI_Wait
81: leave BufferedCommunicator::sendRecv
81: communicator.hh::BufferedCommunicator::forward, left sendRecv
81 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), after communicator.free();
81 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
81: communicator.hh::BufferedCommunicator::forward, enter sendRecv
81: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 11
81: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 11
81: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 11
81: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 11
81: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 11
81: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 11
81: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 11
81: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 11
81: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 11
81: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 11
81: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 10 11
81: communicator.hh::sendRecv, left MPI_Wait
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 9
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 9
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 9
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 9
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 9
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 9
3: communicator.hh::sendRecv, left MPI_Wait
93: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 13
93: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 10 13
93: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 11 13
93: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 12 13
93: communicator.hh::sendRecv, left MPI_Wait
93: leave BufferedCommunicator::sendRecv
93: communicator.hh::BufferedCommunicator::forward, left sendRecv
93 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), after communicator.free();
93 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
93: communicator.hh::BufferedCommunicator::forward, enter sendRecv
93: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 13
93: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 13
93: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 13
93: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 13
93: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 13
93: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 13
93: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 13
93: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 13
93: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 13
93: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 13
93: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 10 13
93: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 11 13
93: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 12 13
93: communicator.hh::sendRecv, left MPI_Wait
45: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 12 17
45: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 13 17
45: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 14 17
45: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 15 17
45: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 16 17
45: communicator.hh::sendRecv, left MPI_Wait
45: leave BufferedCommunicator::sendRecv
45: communicator.hh::BufferedCommunicator::forward, left sendRecv
45 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), after communicator.free();
45 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
45: communicator.hh::BufferedCommunicator::forward, enter sendRecv
45: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 17
45: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 17
45: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 17
45: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 17
45: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 17
45: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 17
45: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 17
45: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 17
45: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 17
45: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 17
45: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 10 17
45: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 11 17
45: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 12 17
45: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 13 17
45: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 14 17
45: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 15 17
45: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 16 17
45: communicator.hh::sendRecv, left MPI_Wait
110: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 11
110: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 10 11
110: communicator.hh::sendRecv, left MPI_Wait
110: leave BufferedCommunicator::sendRecv
110: communicator.hh::BufferedCommunicator::forward, left sendRecv
110 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), after communicator.free();
110 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
110: communicator.hh::BufferedCommunicator::forward, enter sendRecv
110: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 11
110: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 11
110: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 11
110: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 11
110: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 11
110: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 11
110: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 11
110: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 11
110: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 11
110: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 11
110: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 10 11
110: communicator.hh::sendRecv, left MPI_Wait
110: leave BufferedCommunicator::sendRecv
110: communicator.hh::BufferedCommunicator::forward, left sendRecv
110 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), after communicator.free();
68: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 16 22
68: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 17 22
68: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 18 22
68: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 19 22
68: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 20 22
68: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 21 22
68: communicator.hh::sendRecv, left MPI_Wait
68: leave BufferedCommunicator::sendRecv
68: communicator.hh::BufferedCommunicator::forward, left sendRecv
68 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), after communicator.free();
68 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
68: communicator.hh::BufferedCommunicator::forward, enter sendRecv
68: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 22
68: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 22
68: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 22
68: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 22
68: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 22
68: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 22
68: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 22
68: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 22
68: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 22
68: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 22
68: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 10 22
68: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 11 22
68: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 12 22
68: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 13 22
68: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 14 22
68: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 15 22
68: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 16 22
68: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 17 22
68: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 18 22
68: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 19 22
68: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 20 22
68: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 21 22
68: communicator.hh::sendRecv, left MPI_Wait
64: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 10 14
64: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 11 14
64: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 12 14
64: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 13 14
64: communicator.hh::sendRecv, left MPI_Wait
64: leave BufferedCommunicator::sendRecv
64: communicator.hh::BufferedCommunicator::forward, left sendRecv
64 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), after communicator.free();
64 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
64: communicator.hh::BufferedCommunicator::forward, enter sendRecv
64: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 14
64: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 14
64: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 14
64: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 14
64: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 14
64: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 14
64: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 14
64: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 14
64: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 14
64: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 14
64: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 10 14
64: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 11 14
64: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 12 14
64: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 13 14
64: communicator.hh::sendRecv, left MPI_Wait
96: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 12
96: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 10 12
96: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 11 12
96: communicator.hh::sendRecv, left MPI_Wait
96: leave BufferedCommunicator::sendRecv
96: communicator.hh::BufferedCommunicator::forward, left sendRecv
96 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), after communicator.free();
96 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
96: communicator.hh::BufferedCommunicator::forward, enter sendRecv
96: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 12
96: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 12
96: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 12
96: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 12
96: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 12
96: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 12
96: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 12
96: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 12
96: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 12
96: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 12
96: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 10 12
96: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 11 12
96: communicator.hh::sendRecv, left MPI_Wait
57: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 14 19
57: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 15 19
57: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 16 19
57: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 17 19
57: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 18 19
57: communicator.hh::sendRecv, left MPI_Wait
57: leave BufferedCommunicator::sendRecv
57: communicator.hh::BufferedCommunicator::forward, left sendRecv
57 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), after communicator.free();
57 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
57: communicator.hh::BufferedCommunicator::forward, enter sendRecv
57: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 19
57: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 19
57: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 19
57: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 19
57: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 19
57: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 19
57: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 19
57: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 19
57: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 19
57: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 19
57: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 10 19
57: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 11 19
57: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 12 19
57: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 13 19
57: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 14 19
57: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 15 19
57: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 16 19
57: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 17 19
57: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 18 19
57: communicator.hh::sendRecv, left MPI_Wait
57: leave BufferedCommunicator::sendRecv
57: communicator.hh::BufferedCommunicator::forward, left sendRecv
57 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), after communicator.free();
65: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 9
65: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 9
65: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 9
65: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 9
65: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 9
65: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 9
65: communicator.hh::sendRecv, left MPI_Wait
65: leave BufferedCommunicator::sendRecv
65: communicator.hh::BufferedCommunicator::forward, left sendRecv
65 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), after communicator.free();
65 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
65: communicator.hh::BufferedCommunicator::forward, enter sendRecv
65: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 9
65: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 9
65: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 9
65: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 9
65: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 9
65: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 9
65: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 9
65: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 9
65: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 9
65: communicator.hh::sendRecv, left MPI_Wait
50: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 10
50: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 10
50: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 10
50: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 10
50: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 10
50: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 10
50: communicator.hh::sendRecv, left MPI_Wait
50: leave BufferedCommunicator::sendRecv
50: communicator.hh::BufferedCommunicator::forward, left sendRecv
50 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), after communicator.free();
50 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
50: communicator.hh::BufferedCommunicator::forward, enter sendRecv
50: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 10
50: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 10
50: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 10
50: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 10
50: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 10
50: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 10
50: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 10
50: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 10
50: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 10
50: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 10
50: communicator.hh::sendRecv, left MPI_Wait
50: leave BufferedCommunicator::sendRecv
50: communicator.hh::BufferedCommunicator::forward, left sendRecv
50 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), after communicator.free();
25: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 7
25: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 7
25: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 7
25: communicator.hh::sendRecv, left MPI_Wait
25: leave BufferedCommunicator::sendRecv
25: communicator.hh::BufferedCommunicator::forward, left sendRecv
25 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), after communicator.free();
25 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
25: communicator.hh::BufferedCommunicator::forward, enter sendRecv
25: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 7
25: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 7
25: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 7
25: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 7
25: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 7
25: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 7
25: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 7
25: communicator.hh::sendRecv, left MPI_Wait
25: leave BufferedCommunicator::sendRecv
25: communicator.hh::BufferedCommunicator::forward, left sendRecv
25 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), after communicator.free();
49: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 15
49: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 15
49: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 15
49: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 10 15
49: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 11 15
49: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 12 15
49: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 13 15
49: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 14 15
49: communicator.hh::sendRecv, left MPI_Wait
49: leave BufferedCommunicator::sendRecv
49: communicator.hh::BufferedCommunicator::forward, left sendRecv
49 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), after communicator.free();
49 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
49: communicator.hh::BufferedCommunicator::forward, enter sendRecv
49: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 15
49: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 15
49: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 15
49: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 15
49: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 15
49: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 15
49: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 15
49: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 15
49: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 15
49: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 15
49: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 10 15
49: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 11 15
49: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 12 15
49: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 13 15
49: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 14 15
49: communicator.hh::sendRecv, left MPI_Wait
92: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 12 16
92: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 13 16
92: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 14 16
92: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 15 16
92: communicator.hh::sendRecv, left MPI_Wait
92: leave BufferedCommunicator::sendRecv
92: communicator.hh::BufferedCommunicator::forward, left sendRecv
92 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), after communicator.free();
92 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
92: communicator.hh::BufferedCommunicator::forward, enter sendRecv
92: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 16
92: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 16
92: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 16
92: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 16
92: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 16
92: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 16
92: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 16
92: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 16
92: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 16
92: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 16
92: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 10 16
92: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 11 16
92: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 12 16
92: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 13 16
92: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 14 16
92: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 15 16
92: communicator.hh::sendRecv, left MPI_Wait
44: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 11
44: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 11
44: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 11
44: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 10 11
44: communicator.hh::sendRecv, left MPI_Wait
44: leave BufferedCommunicator::sendRecv
44: communicator.hh::BufferedCommunicator::forward, left sendRecv
44 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), after communicator.free();
44 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
44: communicator.hh::BufferedCommunicator::forward, enter sendRecv
44: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 11
44: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 11
44: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 11
44: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 11
44: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 11
44: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 11
44: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 11
44: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 11
44: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 11
44: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 11
44: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 10 11
44: communicator.hh::sendRecv, left MPI_Wait
95: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 7
95: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 7
95: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 7
95: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 7
95: communicator.hh::sendRecv, left MPI_Wait
95: leave BufferedCommunicator::sendRecv
95: communicator.hh::BufferedCommunicator::forward, left sendRecv
95 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), after communicator.free();
114: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 13
114: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 13
114: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 10 13
114: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 11 13
114: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 12 13
114: communicator.hh::sendRecv, left MPI_Wait
114: leave BufferedCommunicator::sendRecv
114: communicator.hh::BufferedCommunicator::forward, left sendRecv
114 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), after communicator.free();
62: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 15
62: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 15
62: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 15
62: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 15
62: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 15
62: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 10 15
62: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 11 15
62: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 12 15
62: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 13 15
62: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 14 15
62: communicator.hh::sendRecv, left MPI_Wait
62: leave BufferedCommunicator::sendRecv
62: communicator.hh::BufferedCommunicator::forward, left sendRecv
62 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), after communicator.free();
104: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 9
104: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 9
104: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 9
104: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 9
104: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 9
104: communicator.hh::sendRecv, left MPI_Wait
104: leave BufferedCommunicator::sendRecv
104: communicator.hh::BufferedCommunicator::forward, left sendRecv
104 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), after communicator.free();
79: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 16
79: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 16
79: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 16
79: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 16
79: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 10 16
79: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 11 16
79: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 12 16
79: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 13 16
79: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 14 16
79: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 15 16
79: communicator.hh::sendRecv, left MPI_Wait
79: leave BufferedCommunicator::sendRecv
79: communicator.hh::BufferedCommunicator::forward, left sendRecv
79 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), after communicator.free();
43: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 11
43: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 11
43: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 11
43: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 11
43: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 10 11
43: communicator.hh::sendRecv, left MPI_Wait
43: leave BufferedCommunicator::sendRecv
43: communicator.hh::BufferedCommunicator::forward, left sendRecv
43 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), after communicator.free();
48: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 15
48: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 15
48: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 15
48: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 10 15
48: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 11 15
48: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 12 15
48: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 13 15
48: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 14 15
48: communicator.hh::sendRecv, left MPI_Wait
48: leave BufferedCommunicator::sendRecv
48: communicator.hh::BufferedCommunicator::forward, left sendRecv
48 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), after communicator.free();
80: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 10
80: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 10
80: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 10
80: communicator.hh::sendRecv, left MPI_Wait
80: leave BufferedCommunicator::sendRecv
80: communicator.hh::BufferedCommunicator::forward, left sendRecv
80 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), after communicator.free();
17: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 3
17: communicator.hh::sendRecv, left MPI_Wait
17: leave BufferedCommunicator::sendRecv
17: communicator.hh::BufferedCommunicator::forward, left sendRecv
17 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), after communicator.free();
84: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 12 18
84: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 13 18
84: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 14 18
84: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 15 18
84: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 16 18
84: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 17 18
84: communicator.hh::sendRecv, left MPI_Wait
84: leave BufferedCommunicator::sendRecv
84: communicator.hh::BufferedCommunicator::forward, left sendRecv
84 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), after communicator.free();
60: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 6
60: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 6
60: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 6
60: communicator.hh::sendRecv, left MPI_Wait
60: leave BufferedCommunicator::sendRecv
60: communicator.hh::BufferedCommunicator::forward, left sendRecv
60 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), after communicator.free();
82: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 15 26
82: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 16 26
82: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 17 26
82: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 18 26
82: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 19 26
82: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 20 26
82: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 21 26
82: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 22 26
82: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 23 26
82: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 24 26
82: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 25 26
82: communicator.hh::sendRecv, left MPI_Wait
82: leave BufferedCommunicator::sendRecv
82: communicator.hh::BufferedCommunicator::forward, left sendRecv
82 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), after communicator.free();
109: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 10 20
109: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 11 20
109: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 12 20
109: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 13 20
109: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 14 20
109: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 15 20
109: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 16 20
109: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 17 20
109: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 18 20
109: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 19 20
109: communicator.hh::sendRecv, left MPI_Wait
109: leave BufferedCommunicator::sendRecv
109: communicator.hh::BufferedCommunicator::forward, left sendRecv
109 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), after communicator.free();
33: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 9
33: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 9
33: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 9
33: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 9
33: communicator.hh::sendRecv, left MPI_Wait
33: leave BufferedCommunicator::sendRecv
33: communicator.hh::BufferedCommunicator::forward, left sendRecv
33 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), after communicator.free();
29: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 11
29: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 10 11
29: communicator.hh::sendRecv, left MPI_Wait
29: leave BufferedCommunicator::sendRecv
29: communicator.hh::BufferedCommunicator::forward, left sendRecv
29 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), after communicator.free();
69: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 7
69: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 7
69: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 7
69: communicator.hh::sendRecv, left MPI_Wait
69: leave BufferedCommunicator::sendRecv
69: communicator.hh::BufferedCommunicator::forward, left sendRecv
69 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), after communicator.free();
31: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 10
31: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 10
31: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 10
31: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 10
31: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 10
31: communicator.hh::sendRecv, left MPI_Wait
31: leave BufferedCommunicator::sendRecv
31: communicator.hh::BufferedCommunicator::forward, left sendRecv
31 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), after communicator.free();
14: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 10 17
14: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 11 17
14: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 12 17
14: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 13 17
14: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 14 17
14: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 15 17
14: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 16 17
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 17
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 17
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 17
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 17
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 17
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 17
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 10 17
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 11 17
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 12 17
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 13 17
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 14 17
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 15 17
86: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 19 31
86: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 20 31
86: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 21 31
86: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 22 31
86: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 23 31
86: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 24 31
86: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 25 31
86: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 26 31
86: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 27 31
86: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 28 31
86: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 29 31
86: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 30 31
86: communicator.hh::sendRecv, left MPI_Wait
86: leave BufferedCommunicator::sendRecv
86: communicator.hh::BufferedCommunicator::forward, left sendRecv
86 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), after communicator.free();
55: communicator.hh::sendRecv, left MPI_Wait
55: leave BufferedCommunicator::sendRecv
55: communicator.hh::BufferedCommunicator::forward, left sendRecv
55 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), after communicator.free();
99: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 13
99: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 10 13
99: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 11 13
99: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 12 13
99: communicator.hh::sendRecv, left MPI_Wait
99: leave BufferedCommunicator::sendRecv
99: communicator.hh::BufferedCommunicator::forward, left sendRecv
99 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), after communicator.free();
61: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 9
61: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 9
61: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 9
61: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 9
61: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 9
61: communicator.hh::sendRecv, left MPI_Wait
61: leave BufferedCommunicator::sendRecv
61: communicator.hh::BufferedCommunicator::forward, left sendRecv
61 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), after communicator.free();
90: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 6
90: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 6
90: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 6
90: communicator.hh::sendRecv, left MPI_Wait
90: leave BufferedCommunicator::sendRecv
90: communicator.hh::BufferedCommunicator::forward, left sendRecv
90 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), after communicator.free();
102: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 15
102: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 15
102: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 10 15
102: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 11 15
102: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 12 15
102: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 13 15
102: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 14 15
102: communicator.hh::sendRecv, left MPI_Wait
102: leave BufferedCommunicator::sendRecv
102: communicator.hh::BufferedCommunicator::forward, left sendRecv
102 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), after communicator.free();
73: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 11
73: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 11
73: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 11
73: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 11
73: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 11
73: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 11
73: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 11
73: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 11
73: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 11
73: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 10 11
73: communicator.hh::sendRecv, left MPI_Wait
73: leave BufferedCommunicator::sendRecv
73: communicator.hh::BufferedCommunicator::forward, left sendRecv
73 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), after communicator.free();
16: communicator.hh::sendRecv, left MPI_Wait
16: leave BufferedCommunicator::sendRecv
16: communicator.hh::BufferedCommunicator::forward, left sendRecv
16 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), after communicator.free();
30: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 6
30: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 6
30: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 6
30: communicator.hh::sendRecv, left MPI_Wait
30: leave BufferedCommunicator::sendRecv
30: communicator.hh::BufferedCommunicator::forward, left sendRecv
30 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), after communicator.free();
37: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 12 15
37: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 13 15
37: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 14 15
37: communicator.hh::sendRecv, left MPI_Wait
37: leave BufferedCommunicator::sendRecv
37: communicator.hh::BufferedCommunicator::forward, left sendRecv
37 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), after communicator.free();
42: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 20
42: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 10 20
42: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 11 20
42: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 12 20
42: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 13 20
42: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 14 20
42: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 15 20
42: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 16 20
42: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 17 20
42: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 18 20
42: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 19 20
42: communicator.hh::sendRecv, left MPI_Wait
42: leave BufferedCommunicator::sendRecv
42: communicator.hh::BufferedCommunicator::forward, left sendRecv
42 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), after communicator.free();
44: leave BufferedCommunicator::sendRecv
44: communicator.hh::BufferedCommunicator::forward, left sendRecv
44 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), after communicator.free();
89: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 10 11
89: communicator.hh::sendRecv, left MPI_Wait
89: leave BufferedCommunicator::sendRecv
89: communicator.hh::BufferedCommunicator::forward, left sendRecv
89 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), after communicator.free();
97: leave BufferedCommunicator::sendRecv
97: communicator.hh::BufferedCommunicator::forward, left sendRecv
97 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), after communicator.free();
97 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
97: communicator.hh::BufferedCommunicator::forward, enter sendRecv
97: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 9
97: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 9
97: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 9
97: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 9
97: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 9
97: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 9
97: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 9
97: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 9
97: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 9
97: communicator.hh::sendRecv, left MPI_Wait
97: leave BufferedCommunicator::sendRecv
97: communicator.hh::BufferedCommunicator::forward, left sendRecv
97 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), after communicator.free();
cator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 20
22: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 20
22: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 10 20
22: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 11 20
22: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 12 20
22: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 13 20
22: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 14 20
22: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 15 20
22: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 16 20
22: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 17 20
22: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 18 20
22: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 19 20
22: communicator.hh::sendRecv, left MPI_Wait
22: leave BufferedCommunicator::sendRecv
22: communicator.hh::BufferedCommunicator::forward, left sendRecv
22 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), after communicator.free();
101: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 16 24
101: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 17 24
101: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 18 24
101: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 19 24
101: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 20 24
101: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 21 24
101: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 22 24
101: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 23 24
101: communicator.hh::sendRecv, left MPI_Wait
101: leave BufferedCommunicator::sendRecv
101: communicator.hh::BufferedCommunicator::forward, left sendRecv
101 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), after communicator.free();
103: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 11 16
103: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 12 16
103: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 13 16
103: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 14 16
103: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 15 16
103: communicator.hh::sendRecv, left MPI_Wait
103: leave BufferedCommunicator::sendRecv
103: communicator.hh::BufferedCommunicator::forward, left sendRecv
103 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), after communicator.free();
vRequests, &finished, &status); 22 26
12: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 23 26
12: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 24 26
75: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 11 17
75: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 12 17
75: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 13 17
75: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 14 17
75: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 15 17
75: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 16 17
75: communicator.hh::sendRecv, left MPI_Wait
75: leave BufferedCommunicator::sendRecv
75: communicator.hh::BufferedCommunicator::forward, left sendRecv
75 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), after communicator.free();
71: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 17 20
71: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 18 20
71: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 19 20
71: communicator.hh::sendRecv, left MPI_Wait
71: leave BufferedCommunicator::sendRecv
71: communicator.hh::BufferedCommunicator::forward, left sendRecv
71 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), after communicator.free();
ator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 37 117
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 38 117
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 39 117
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 40 117
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 41 117
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 42 117
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 43 117
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 44 117
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 45 117
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 46 117
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 47 117
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 48 117
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 49 117
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 50 117
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 51 117
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 52 117
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 53 117
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 54 117
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 55 117
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 56 117
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 57 117
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 58 117
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 59 117
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 60 117
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 61 117
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 62 117
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 63 117
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 64 117
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 65 117
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 66 117
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 67 117
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 68 117
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 69 117
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 70 117
0: communicator.hh::sendRecv, before MPI_Waitany(messageInfo111: leave BufferedCommunicator::sendRecv
111: communicator.hh::BufferedCommunicator::forward, left sendRecv
111 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), after communicator.free();
vRequests, &finished, &status); 11 31
56: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 12 31
56: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 13 31
56: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 14 31
56: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 15 31
56: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 16 31
56: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 17 31
56: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 18 31
56: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 19 31
56: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 20 31
56: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 21 31
56: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 22 31
56: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 23 31
56: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 24 31
56: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 25 31
56: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 26 31
56: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 27 31
56: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 28 31
56: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 29 31
56: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 30 31
56: communicator.hh::sendRecv, left MPI_Wait
56: leave BufferedCommunicator::sendRecv
56: communicator.hh::BufferedCommunicator::forward, left sendRecv
56 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), after communicator.free();
67: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 11 13
67: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 12 13
67: communicator.hh::sendRecv, left MPI_Wait
67: leave BufferedCommunicator::sendRecv
67: communicator.hh::BufferedCommunicator::forward, left sendRecv
67 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), after communicator.free();
32: communicator.hh::sendRecv, left MPI_Wait
32: leave BufferedCommunicator::sendRecv
32: communicator.hh::BufferedCommunicator::forward, left sendRecv
32 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), after communicator.free();
115: communicator.hh::sendRecv, left MPI_Wait
115: leave BufferedCommunicator::sendRecv
115: communicator.hh::BufferedCommunicator::forward, left sendRecv
115 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), after communicator.free();
53: leave BufferedCommunicator::sendRecv
53: communicator.hh::BufferedCommunicator::forward, left sendRecv
53 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), after communicator.free();
77: leave BufferedCommunicator::sendRecv
77: communicator.hh::BufferedCommunicator::forward, left sendRecv
77 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), after communicator.free();
Requests, &finished, &status); 18 21
88: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 19 21
88: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 20 21
88: communicator.hh::sendRecv, left MPI_Wait
88: leave BufferedCommunicator::sendRecv
88: communicator.hh::BufferedCommunicator::forward, left sendRecv
88 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), after communicator.free();
113: leave BufferedCommunicator::sendRecv
113: communicator.hh::BufferedCommunicator::forward, left sendRecv
113 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), after communicator.free();
112: leave BufferedCommunicator::sendRecv
112: communicator.hh::BufferedCommunicator::forward, left sendRecv
112 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), after communicator.free();
35: leave BufferedCommunicator::sendRecv
35: communicator.hh::BufferedCommunicator::forward, left sendRecv
35 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), after communicator.free();
87: leave BufferedCommunicator::sendRecv
87: communicator.hh::BufferedCommunicator::forward, left sendRecv
87 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), after communicator.free();
70: leave BufferedCommunicator::sendRecv
70: communicator.hh::BufferedCommunicator::forward, left sendRecv
70 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), after communicator.free();
vRequests, &finished, &status); 19 21
94: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 20 21
94: communicator.hh::sendRecv, left MPI_Wait
94: leave BufferedCommunicator::sendRecv
94: communicator.hh::BufferedCommunicator::forward, left sendRecv
94 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), after communicator.free();
119: leave BufferedCommunicator::sendRecv
119: communicator.hh::BufferedCommunicator::forward, left sendRecv
119 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), after communicator.free();
18: leave BufferedCommunicator::sendRecv
18: communicator.hh::BufferedCommunicator::forward, left sendRecv
18 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), after communicator.free();
vRequests, &finished, &status); 18 26
52: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 19 26
52: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 20 26
52: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 21 26
52: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 22 26
52: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 23 26
52: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 24 26
52: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 25 26
52: communicator.hh::sendRecv, left MPI_Wait
52: leave BufferedCommunicator::sendRecv
52: communicator.hh::BufferedCommunicator::forward, left sendRecv
52 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), after communicator.free();
41: leave BufferedCommunicator::sendRecv
41: communicator.hh::BufferedCommunicator::forward, left sendRecv
41 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), after communicator.free();
36: leave BufferedCommunicator::sendRecv
36: communicator.hh::BufferedCommunicator::forward, left sendRecv
36 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), after communicator.free();
40: leave BufferedCommunicator::sendRecv
40: communicator.hh::BufferedCommunicator::forward, left sendRecv
40 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), after communicator.free();
vRequests, &finished, &status); 23 26
26: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 24 26
26: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 25 26
26: communicator.hh::sendRecv, left MPI_Wait
26: leave BufferedCommunicator::sendRecv
26: communicator.hh::BufferedCommunicator::forward, left sendRecv
26 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), after communicator.free();
23: leave BufferedCommunicator::sendRecv
23: communicator.hh::BufferedCommunicator::forward, left sendRecv
23 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), after communicator.free();
51: leave BufferedCommunicator::sendRecv
51: communicator.hh::BufferedCommunicator::forward, left sendRecv
51 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), after communicator.free();
63: leave BufferedCommunicator::sendRecv
63: communicator.hh::BufferedCommunicator::forward, left sendRecv
63 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), after communicator.free();
15: leave BufferedCommunicator::sendRecv
15: communicator.hh::BufferedCommunicator::forward, left sendRecv
15 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), after communicator.free();
81: leave BufferedCommunicator::sendRecv
81: communicator.hh::BufferedCommunicator::forward, left sendRecv
81 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), after communicator.free();
93: leave BufferedCommunicator::sendRecv
93: communicator.hh::BufferedCommunicator::forward, left sendRecv
93 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), after communicator.free();
45: leave BufferedCommunicator::sendRecv
45: communicator.hh::BufferedCommunicator::forward, left sendRecv
45 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), after communicator.free();
68: leave BufferedCommunicator::sendRecv
68: communicator.hh::BufferedCommunicator::forward, left sendRecv
68 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), after communicator.free();
64: leave BufferedCommunicator::sendRecv
64: communicator.hh::BufferedCommunicator::forward, left sendRecv
64 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), after communicator.free();
96: leave BufferedCommunicator::sendRecv
96: communicator.hh::BufferedCommunicator::forward, left sendRecv
96 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), after communicator.free();
65: leave BufferedCommunicator::sendRecv
65: communicator.hh::BufferedCommunicator::forward, left sendRecv
65 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), after communicator.free();
49: leave BufferedCommunicator::sendRecv
49: communicator.hh::BufferedCommunicator::forward, left sendRecv
49 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), after communicator.free();
92: leave BufferedCommunicator::sendRecv
92: communicator.hh::BufferedCommunicator::forward, left sendRecv
92 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), after communicator.free();
100: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 6
100: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 6
100: communicator.hh::sendRecv, left MPI_Wait
100: leave BufferedCommunicator::sendRecv
100: communicator.hh::BufferedCommunicator::forward, left sendRecv
100 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), after communicator.free();
59: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 11
59: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 11
59: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 11
59: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 11
59: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 11
59: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 10 11
59: communicator.hh::sendRecv, left MPI_Wait
59: leave BufferedCommunicator::sendRecv
59: communicator.hh::BufferedCommunicator::forward, left sendRecv
59 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), after communicator.free();
117: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 14 22
117: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 15 22
117: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 16 22
117: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 17 22
117: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 18 22
117: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 19 22
117: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 20 22
117: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 21 22
117: communicator.hh::sendRecv, left MPI_Wait
117: leave BufferedCommunicator::sendRecv
117: communicator.hh::BufferedCommunicator::forward, left sendRecv
117 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), after communicator.free();
20: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 10
20: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 10
20: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 10
20: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 10
20: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 10
20: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 10
20: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 10
20: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 10
20: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 10
20: communicator.hh::sendRecv, left MPI_Wait
20: leave BufferedCommunicator::sendRecv
20: communicator.hh::BufferedCommunicator::forward, left sendRecv
20 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), after communicator.free();
19: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 17
19: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 17
19: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 17
19: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 17
19: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 10 17
19: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 11 17
19: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 12 17
19: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 13 17
19: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 14 17
19: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 15 17
19: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 16 17
19: communicator.hh::sendRecv, left MPI_Wait
19: leave BufferedCommunicator::sendRecv
19: communicator.hh::BufferedCommunicator::forward, left sendRecv
19 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), after communicator.free();
38: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 24
38: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 10 24
38: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 11 24
38: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 12 24
38: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 13 24
38: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 14 24
38: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 15 24
38: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 16 24
38: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 17 24
38: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 18 24
38: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 19 24
38: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 20 24
38: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 21 24
38: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 22 24
38: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 23 24
38: communicator.hh::sendRecv, left MPI_Wait
38: leave BufferedCommunicator::sendRecv
38: communicator.hh::BufferedCommunicator::forward, left sendRecv
38 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), after communicator.free();
120: communicator.hh::sendRecv, left MPI_Wait
120: leave BufferedCommunicator::sendRecv
120: communicator.hh::BufferedCommunicator::forward, left sendRecv
120 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), after communicator.free();
83: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 17
83: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 17
83: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 10 17
83: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 11 17
83: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 12 17
83: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 13 17
83: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 14 17
83: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 15 17
83: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 16 17
83: communicator.hh::sendRecv, left MPI_Wait
83: leave BufferedCommunicator::sendRecv
83: communicator.hh::BufferedCommunicator::forward, left sendRecv
83 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), after communicator.free();
21: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 12
21: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 12
21: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 10 12
21: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 11 12
21: communicator.hh::sendRecv, left MPI_Wait
21: leave BufferedCommunicator::sendRecv
21: communicator.hh::BufferedCommunicator::forward, left sendRecv
21 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), after communicator.free();
24: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 14
24: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 14
24: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 14
24: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 14
24: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 14
24: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 14
24: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 10 14
24: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 11 14
24: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 12 14
24: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 13 14
24: communicator.hh::sendRecv, left MPI_Wait
24: leave BufferedCommunicator::sendRecv
24: communicator.hh::BufferedCommunicator::forward, left sendRecv
24 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), after communicator.free();
85: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 11
85: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 11
85: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 11
85: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 11
85: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 10 11
85: communicator.hh::sendRecv, left MPI_Wait
85: leave BufferedCommunicator::sendRecv
85: communicator.hh::BufferedCommunicator::forward, left sendRecv
85 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), after communicator.free();
47: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 6
47: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 6
47: communicator.hh::sendRecv, left MPI_Wait
47: leave BufferedCommunicator::sendRecv
47: communicator.hh::BufferedCommunicator::forward, left sendRecv
47 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), after communicator.free();
72: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 20
72: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 20
72: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 20
72: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 20
72: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 20
72: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 20
72: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 10 20
72: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 11 20
72: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 12 20
72: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 13 20
72: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 14 20
72: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 15 20
72: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 16 20
72: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 17 20
72: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 18 20
72: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 19 20
72: communicator.hh::sendRecv, left MPI_Wait
72: leave BufferedCommunicator::sendRecv
72: communicator.hh::BufferedCommunicator::forward, left sendRecv
72 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), after communicator.free();
98: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 18
98: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 18
98: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 18
98: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 10 18
98: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 11 18
98: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 12 18
98: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 13 18
98: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 14 18
98: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 15 18
98: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 16 18
98: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 17 18
98: communicator.hh::sendRecv, left MPI_Wait
98: leave BufferedCommunicator::sendRecv
98: communicator.hh::BufferedCommunicator::forward, left sendRecv
98 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), after communicator.free();
28: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 13 18
28: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 14 18
28: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 15 18
28: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 16 18
28: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 17 18
28: communicator.hh::sendRecv, left MPI_Wait
28: leave BufferedCommunicator::sendRecv
28: communicator.hh::BufferedCommunicator::forward, left sendRecv
28 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), after communicator.free();
118: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 10 14
118: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 11 14
118: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 12 14
118: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 13 14
118: communicator.hh::sendRecv, left MPI_Wait
118: leave BufferedCommunicator::sendRecv
118: communicator.hh::BufferedCommunicator::forward, left sendRecv
118 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), after communicator.free();
27: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 17
27: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 17
27: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 10 17
27: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 11 17
27: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 12 17
27: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 13 17
27: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 14 17
27: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 15 17
27: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 16 17
27: communicator.hh::sendRecv, left MPI_Wait
27: leave BufferedCommunicator::sendRecv
27: communicator.hh::BufferedCommunicator::forward, left sendRecv
27 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), after communicator.free();
66: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 15
66: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 15
66: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 15
66: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 15
66: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 15
66: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 15
66: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 15
66: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 15
66: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 10 15
66: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 11 15
66: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 12 15
66: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 13 15
66: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 14 15
66: communicator.hh::sendRecv, left MPI_Wait
66: leave BufferedCommunicator::sendRecv
66: communicator.hh::BufferedCommunicator::forward, left sendRecv
66 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), after communicator.free();
58: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 11 21
58: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 12 21
58: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 13 21
58: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 14 21
58: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 15 21
58: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 16 21
58: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 17 21
58: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 18 21
58: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 19 21
58: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 20 21
58: communicator.hh::sendRecv, left MPI_Wait
58: leave BufferedCommunicator::sendRecv
58: communicator.hh::BufferedCommunicator::forward, left sendRecv
58 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), after communicator.free();
34: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 14
34: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 14
34: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 14
34: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 14
34: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 14
34: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 14
34: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 10 14
34: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 11 14
34: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 12 14
34: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 13 14
34: communicator.hh::sendRecv, left MPI_Wait
34: leave BufferedCommunicator::sendRecv
34: communicator.hh::BufferedCommunicator::forward, left sendRecv
34 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), after communicator.free();
91: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 9
91: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 9
91: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 9
91: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 9
91: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 9
91: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 9
91: communicator.hh::sendRecv, left MPI_Wait
91: leave BufferedCommunicator::sendRecv
91: communicator.hh::BufferedCommunicator::forward, left sendRecv
91 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), after communicator.free();
76: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 12
76: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 12
76: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 12
76: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 12
76: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 12
76: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 12
76: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 12
76: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 10 12
76: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 11 12
76: communicator.hh::sendRecv, left MPI_Wait
76: leave BufferedCommunicator::sendRecv
76: communicator.hh::BufferedCommunicator::forward, left sendRecv
76 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), after communicator.free();
74: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 11
74: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 11
74: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 11
74: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 11
74: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 11
74: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 11
74: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 10 11
74: communicator.hh::sendRecv, left MPI_Wait
74: leave BufferedCommunicator::sendRecv
74: communicator.hh::BufferedCommunicator::forward, left sendRecv
74 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), after communicator.free();
8: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 20
8: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 20
8: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 20
8: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 20
8: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 10 20
8: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 11 20
8: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 12 20
8: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 13 20
8: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 14 20
8: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 15 20
8: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 16 20
8: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 17 20
39: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 9
39: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 9
39: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 9
39: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 9
39: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 9
39: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 9
39: communicator.hh::sendRecv, left MPI_Wait
39: leave BufferedCommunicator::sendRecv
39: communicator.hh::BufferedCommunicator::forward, left sendRecv
39 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), after communicator.free();
108: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 9
108: communicator.hh::sendRecv, left MPI_Wait
108: leave BufferedCommunicator::sendRecv
108: communicator.hh::BufferedCommunicator::forward, left sendRecv
108 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), after communicator.free();
78: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 15
78: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 15
78: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 15
78: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 15
78: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 10 15
78: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 11 15
78: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 12 15
78: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 13 15
78: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 14 15
78: communicator.hh::sendRecv, left MPI_Wait
78: leave BufferedCommunicator::sendRecv
78: communicator.hh::BufferedCommunicator::forward, left sendRecv
78 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), after communicator.free();
rmation_.size(), recvRequests, &finished, &status); 71 117
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 72 117
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 73 117
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 74 117
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 75 117
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 76 117
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 77 117
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 78 117
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 79 117
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 80 117
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 81 117
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 82 117
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 83 117
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 84 117
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 85 117
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 86 117
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 87 117
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 88 117
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 89 117
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 90 117
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 91 117
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 92 117
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 93 117
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 94 117
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 95 117
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 96 117
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 97 117
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 98 117
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 99 117
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 100 117
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 101 117
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 102 117
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 103 117
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 104 117
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 105 117
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 106 117
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 107 117
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 108 117
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 109 117
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 110 117
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 111 117
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 112 117
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 113 117
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 114 117
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 115 117
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 116 117
0: communicator.hh::sendRecv, left MPI_Wait
0: leave BufferedCommunicator::sendRecv
0: communicator.hh::BufferedCommunicator::forward, left sendRecv
Using a direct coarse solver (SuperLU)
Building hierarchy of 2 levels (inclusive coarse solver) took 0.0312387 seconds.
0 121 before Dune::BiCGSTABSolver 
0 Dune::BiCGSTABSolver.apply(x, b, result_) 
0 BiCGSTABSolver::apply 
0 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
0: communicator.hh::BufferedCommunicator::forward, enter sendRecv
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 3
8: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 18 20
8: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 19 20
8: communicator.hh::sendRecv, left MPI_Wait
8: leave BufferedCommunicator::sendRecv
8: communicator.hh::BufferedCommunicator::forward, left sendRecv
8 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), after communicator.free();
8 === BiCGSTABSolver
 Iter          Defect            Rate
    0      1.17536e-06
8  (it = 0.5; it < _maxit; it+=.5) 0.5 250
8  before p = r 
8  before  y = W^-1 * p 
8  before _prec->apply(y,p); 0.5 250
8 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
8void AMG<M,X,S,PI,A>::mgc(LevelContext& levelContext), before presmooth(levelContext, preSteps_) 
void presmooth(LevelContext& levelContext, size_t steps), before SmootherApplier<typename LevelContext::SmootherType>::preSmooth, step: 0 of 2
void preSmooth(Smoother& smoother, Domain& v, const Range& d), before smoother.apply(v,d);
8 schwarz.hh:virtual void apply (X& v, const Y& d), before preconditioner.apply(v,d);; 
8 schwarz.hh:virtual void apply (X& v, const Y& d), before communication.copyOwnerToAll(v,v); 
8 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
14: communicator.hh::sendRecv, left MPI_Wait
14: leave BufferedCommunicator::sendRecv
14: communicator.hh::BufferedCommunicator::forward, left sendRecv
14 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), after communicator.free();
14 === BiCGSTABSolver
 Iter          Defect            Rate
    0      1.17536e-06
14  (it = 0.5; it < _maxit; it+=.5) 0.5 250
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 16 17
6: communicator.hh::sendRecv, left MPI_Wait
6: leave BufferedCommunicator::sendRecv
6: communicator.hh::BufferedCommunicator::forward, left sendRecv
6 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), after communicator.free();
6 === BiCGSTABSolver
 Iter          Defect            Rate
    0      1.17536e-06
6  (it = 0.5; it < _maxit; it+=.5) 0.5 250
6  before p = r 
6  before  y = W^-1 * p 
6  before _prec->apply(y,p); 0.5 250
6 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
6void AMG<M,X,S,PI,A>::mgc(LevelContext& levelContext), before presmooth(levelContext, preSteps_) 
void presmooth(LevelContext& levelContext, size_t steps), before SmootherApplier<typename LevelContext::SmootherType>::preSmooth, step: 0 of 2
void preSmooth(Smoother& smoother, Domain& v, const Range& d), before smoother.apply(v,d);
6 schwarz.hh:virtual void apply (X& v, const Y& d), before preconditioner.apply(v,d);; 
99 === BiCGSTABSolver
 Iter          Defect            Rate
    0      1.17536e-06
99  (it = 0.5; it < _maxit; it+=.5) 0.5 250
7: leave BufferedCommunicator::sendRecv
7: communicator.hh::BufferedCommunicator::forward, left sendRecv
7 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), after communicator.free();
7 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
7: communicator.hh::BufferedCommunicator::forward, enter sendRecv
7: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 13
7: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 13
7: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 13
7: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 13
7: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 13
7: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 13
7: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 13
7: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 13
7: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 13
7: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 13
7: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 10 13
7: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 11 13
7: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 12 13
7: communicator.hh::sendRecv, left MPI_Wait
7: leave BufferedCommunicator::sendRecv
7: communicator.hh::BufferedCommunicator::forward, left sendRecv
7 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), after communicator.free();
7 === BiCGSTABSolver
 Iter          Defect            Rate
    0      1.17536e-06
7  (it = 0.5; it < _maxit; it+=.5) 0.5 250
7  before p = r 
7  before  y = W^-1 * p 
7  before _prec->apply(y,p); 0.5 250
7 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
7void AMG<M,X,S,PI,A>::mgc(LevelContext& levelContext), before presmooth(levelContext, preSteps_) 
void presmooth(LevelContext& levelContext, size_t steps), before SmootherApplier<typename LevelContext::SmootherType>::preSmooth, step: 0 of 2
void preSmooth(Smoother& smoother, Domain& v, const Range& d), before smoother.apply(v,d);
7 schwarz.hh:virtual void apply (X& v, const Y& d), before preconditioner.apply(v,d);; 
7 schwarz.hh:virtual void apply (X& v, const Y& d), before communication.copyOwnerToAll(v,v); 
7 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
7: communicator.hh::BufferedCommunicator::forward, enter sendRecv
7: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 13
61 === BiCGSTABSolver
 Iter          Defect            Rate
    0      1.17536e-06
61  (it = 0.5; it < _maxit; it+=.5) 0.5 250
61  before p = r 
61  before  y = W^-1 * p 
61  before _prec->apply(y,p); 0.5 250
61 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
61void AMG<M,X,S,PI,A>::mgc(LevelContext& levelContext), before presmooth(levelContext, preSteps_) 
void presmooth(LevelContext& levelContext, size_t steps), before SmootherApplier<typename LevelContext::SmootherType>::preSmooth, step: 0 of 2
void preSmooth(Smoother& smoother, Domain& v, const Range& d), before smoother.apply(v,d);
61 schwarz.hh:virtual void apply (X& v, const Y& d), before preconditioner.apply(v,d);; 
61 schwarz.hh:virtual void apply (X& v, const Y& d), before communication.copyOwnerToAll(v,v); 
61 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
61: communicator.hh::BufferedCommunicator::forward, enter sendRecv
90 === BiCGSTABSolver
 Iter          Defect            Rate
    0      1.17536e-06
90  (it = 0.5; it < _maxit; it+=.5) 0.5 250
107 === BiCGSTABSolver
 Iter          Defect            Rate
    0      1.17536e-06
107  (it = 0.5; it < _maxit; it+=.5) 0.5 250
107  before p = r 
107  before  y = W^-1 * p 
107  before _prec->apply(y,p); 0.5 250
107 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
107void AMG<M,X,S,PI,A>::mgc(LevelContext& levelContext), before presmooth(levelContext, preSteps_) 
void presmooth(LevelContext& levelContext, size_t steps), before SmootherApplier<typename LevelContext::SmootherType>::preSmooth, step: 0 of 2
void preSmooth(Smoother& smoother, Domain& v, const Range& d), before smoother.apply(v,d);
107 schwarz.hh:virtual void apply (X& v, const Y& d), before preconditioner.apply(v,d);; 
107 schwarz.hh:virtual void apply (X& v, const Y& d), before communication.copyOwnerToAll(v,v); 
107 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
107: communicator.hh::BufferedCommunicator::forward, enter sendRecv
107: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 5
107: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 5
102 === BiCGSTABSolver
 Iter          Defect            Rate
    0      1.17536e-06
102  (it = 0.5; it < _maxit; it+=.5) 0.5 250
102  before p = r 
102  before  y = W^-1 * p 
73 === BiCGSTABSolver
 Iter          Defect            Rate
    0      1.17536e-06
73  (it = 0.5; it < _maxit; it+=.5) 0.5 250
73  before p = r 
73  before  y = W^-1 * p 
73  before _prec->apply(y,p); 0.5 250
73 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
73void AMG<M,X,S,PI,A>::mgc(LevelContext& levelContext), before presmooth(levelContext, preSteps_) 
void presmooth(LevelContext& levelContext, size_t steps), before SmootherApplier<typename LevelContext::SmootherType>::preSmooth, step: 0 of 2
void preSmooth(Smoother& smoother, Domain& v, const Range& d), before smoother.apply(v,d);
73 schwarz.hh:virtual void apply (X& v, const Y& d), before preconditioner.apply(v,d);; 
73 schwarz.hh:virtual void apply (X& v, const Y& d), before communication.copyOwnerToAll(v,v); 
73 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
73: communicator.hh::BufferedCommunicator::forward, enter sendRecv
16 === BiCGSTABSolver
 Iter          Defect            Rate
    0      1.17536e-06
16  (it = 0.5; it < _maxit; it+=.5) 0.5 250
1: leave BufferedCommunicator::sendRecv
1: communicator.hh::BufferedCommunicator::forward, left sendRecv
1 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), after communicator.free();
1 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
1: communicator.hh::BufferedCommunicator::forward, enter sendRecv
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 5
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 5
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 5
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 5
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 5
1: communicator.hh::sendRecv, left MPI_Wait
1: leave BufferedCommunicator::sendRecv
1: communicator.hh::BufferedCommunicator::forward, left sendRecv
1 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), after communicator.free();
1 === BiCGSTABSolver
 Iter          Defect            Rate
    0      1.17536e-06
1  (it = 0.5; it < _maxit; it+=.5) 0.5 250
1  before p = r 
1  before  y = W^-1 * p 
1  before _prec->apply(y,p); 0.5 250
1 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
1void AMG<M,X,S,PI,A>::mgc(LevelContext& levelContext), before presmooth(levelContext, preSteps_) 
void presmooth(LevelContext& levelContext, size_t steps), before SmootherApplier<typename LevelContext::SmootherType>::preSmooth, step: 0 of 2
void preSmooth(Smoother& smoother, Domain& v, const Range& d), before smoother.apply(v,d);
1 schwarz.hh:virtual void apply (X& v, const Y& d), before preconditioner.apply(v,d);; 
1 schwarz.hh:virtual void apply (X& v, const Y& d), before communication.copyOwnerToAll(v,v); 
1 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
1: communicator.hh::BufferedCommunicator::forward, enter sendRecv
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 5
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 5
30 === BiCGSTABSolver
 Iter          Defect            Rate
    0      1.17536e-06
30  (it = 0.5; it < _maxit; it+=.5) 0.5 250
30  before p = r 
30  before  y = W^-1 * p 
30  before _prec->apply(y,p); 0.5 250
30 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
30void AMG<M,X,S,PI,A>::mgc(LevelContext& levelContext), before presmooth(levelContext, preSteps_) 
void presmooth(LevelContext& levelContext, size_t steps), before SmootherApplier<typename LevelContext::SmootherType>::preSmooth, step: 0 of 2
void preSmooth(Smoother& smoother, Domain& v, const Range& d), before smoother.apply(v,d);
30 schwarz.hh:virtual void apply (X& v, const Y& d), before preconditioner.apply(v,d);; 
30 schwarz.hh:virtual void apply (X& v, const Y& d), before communication.copyOwnerToAll(v,v); 
30 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
30: communicator.hh::BufferedCommunicator::forward, enter sendRecv
30: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 6
30: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 6
37 === BiCGSTABSolver
 Iter          Defect            Rate
    0      1.17536e-06
37  (it = 0.5; it < _maxit; it+=.5) 0.5 250
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 3
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 3
0: communicator.hh::sendRecv, left MPI_Wait
0: leave BufferedCommunicator::sendRecv
0: communicator.hh::BufferedCommunicator::forward, left sendRecv
0 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), after communicator.free();
0 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
0: communicator.hh::BufferedCommunicator::forward, enter sendRecv
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 3
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 3
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 3
0: communicator.hh::sendRecv, left MPI_Wait
0: leave BufferedCommunicator::sendRecv
0: communicator.hh::BufferedCommunicator::forward, left sendRecv
0 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), after communicator.free();
0 === BiCGSTABSolver
 Iter          Defect            Rate
    0      1.17536e-06
0  (it = 0.5; it < _maxit; it+=.5) 0.5 250
0  before p = r 
0  before  y = W^-1 * p 
0  before _prec->apply(y,p); 0.5 250
0 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
0void AMG<M,X,S,PI,A>::mgc(LevelContext& levelContext), before presmooth(levelContext, preSteps_) 
void presmooth(LevelContext& levelContext, size_t steps), before SmootherApplier<typename LevelContext::SmootherType>::preSmooth, step: 0 of 2
12: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 25 26
12: communicator.hh::sendRecv, left MPI_Wait
12: leave BufferedCommunicator::sendRecv
12: communicator.hh::BufferedCommunicator::forward, left sendRecv
12 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), after communicator.free();
12 === BiCGSTABSolver
 Iter          Defect            Rate
    0      1.17536e-06
12  (it = 0.5; it < _maxit; it+=.5) 0.5 250
12  before p = r 
12  before  y = W^-1 * p 
12  before _prec->apply(y,p); 0.5 250
12 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
12void AMG<M,X,S,PI,A>::mgc(LevelContext& levelContext), before presmooth(levelContext, preSteps_) 
void presmooth(LevelContext& levelContext, size_t steps), before SmootherApplier<typename LevelContext::SmootherType>::preSmooth, step: 0 of 2
void preSmooth(Smoother& smoother, Domain& v, const Range& d), before smoother.apply(v,d);
12 schwarz.hh:virtual void apply (X& v, const Y& d), before preconditioner.apply(v,d);; 
10: communicator.hh::sendRecv, left MPI_Wait
10: leave BufferedCommunicator::sendRecv
10: communicator.hh::BufferedCommunicator::forward, left sendRecv
10 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), after communicator.free();
10 === BiCGSTABSolver
 Iter          Defect            Rate
    0      1.17536e-06
10  (it = 0.5; it < _maxit; it+=.5) 0.5 250
10  before p = r 
10  before  y = W^-1 * p 
10  before _prec->apply(y,p); 0.5 250
10 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
10void AMG<M,X,S,PI,A>::mgc(LevelContext& levelContext), before presmooth(levelContext, preSteps_) 
void presmooth(LevelContext& levelContext, size_t steps), before SmootherApplier<typename LevelContext::SmootherType>::preSmooth, step: 0 of 2
void preSmooth(Smoother& smoother, Domain& v, const Range& d), before smoother.apply(v,d);
10 schwarz.hh:virtual void apply (X& v, const Y& d), before preconditioner.apply(v,d);; 
10 schwarz.hh:virtual void apply (X& v, const Y& d), before communication.copyOwnerToAll(v,v); 
10 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
10: communicator.hh::BufferedCommunicator::forward, enter sendRecv
10: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 11
10: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 11
10: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 11
11: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 16 17
11: communicator.hh::sendRecv, left MPI_Wait
11: leave BufferedCommunicator::sendRecv
11: communicator.hh::BufferedCommunicator::forward, left sendRecv
11 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), after communicator.free();
11 === BiCGSTABSolver
 Iter          Defect            Rate
    0      1.17536e-06
11  (it = 0.5; it < _maxit; it+=.5) 0.5 250
11  before p = r 
11  before  y = W^-1 * p 
11  before _prec->apply(y,p); 0.5 250
11 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
11void AMG<M,X,S,PI,A>::mgc(LevelContext& levelContext), before presmooth(levelContext, preSteps_) 
void presmooth(LevelContext& levelContext, size_t steps), before SmootherApplier<typename LevelContext::SmootherType>::preSmooth, step: 0 of 2
void preSmooth(Smoother& smoother, Domain& v, const Range& d), before smoother.apply(v,d);
11 schwarz.hh:virtual void apply (X& v, const Y& d), before preconditioner.apply(v,d);; 
11 schwarz.hh:virtual void apply (X& v, const Y& d), before communication.copyOwnerToAll(v,v); 
11 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
11: communicator.hh::BufferedCommunicator::forward, enter sendRecv
11: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 17
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 12 14
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 13 14
4: communicator.hh::sendRecv, left MPI_Wait
4: leave BufferedCommunicator::sendRecv
4: communicator.hh::BufferedCommunicator::forward, left sendRecv
4 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), after communicator.free();
4 === BiCGSTABSolver
 Iter          Defect            Rate
    0      1.17536e-06
4  (it = 0.5; it < _maxit; it+=.5) 0.5 250
4  before p = r 
4  before  y = W^-1 * p 
4  before _prec->apply(y,p); 0.5 250
4 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
4void AMG<M,X,S,PI,A>::mgc(LevelContext& levelContext), before presmooth(levelContext, preSteps_) 
void presmooth(LevelContext& levelContext, size_t steps), before SmootherApplier<typename LevelContext::SmootherType>::preSmooth, step: 0 of 2
void preSmooth(Smoother& smoother, Domain& v, const Range& d), before smoother.apply(v,d);
4 schwarz.hh:virtual void apply (X& v, const Y& d), before preconditioner.apply(v,d);; 
4 schwarz.hh:virtual void apply (X& v, const Y& d), before communication.copyOwnerToAll(v,v); 
4 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
4: communicator.hh::BufferedCommunicator::forward, enter sendRecv
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 8
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 8
2: communicator.hh::sendRecv, left MPI_Wait
2: leave BufferedCommunicator::sendRecv
2: communicator.hh::BufferedCommunicator::forward, left sendRecv
2 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), after communicator.free();
2 === BiCGSTABSolver
 Iter          Defect            Rate
    0      1.17536e-06
2  (it = 0.5; it < _maxit; it+=.5) 0.5 250
2  before p = r 
2  before  y = W^-1 * p 
2  before _prec->apply(y,p); 0.5 250
2 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
2void AMG<M,X,S,PI,A>::mgc(LevelContext& levelContext), before presmooth(levelContext, preSteps_) 
void presmooth(LevelContext& levelContext, size_t steps), before SmootherApplier<typename LevelContext::SmootherType>::preSmooth, step: 0 of 2
void preSmooth(Smoother& smoother, Domain& v, const Range& d), before smoother.apply(v,d);
2 schwarz.hh:virtual void apply (X& v, const Y& d), before preconditioner.apply(v,d);; 
2 schwarz.hh:virtual void apply (X& v, const Y& d), before communication.copyOwnerToAll(v,v); 
2 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
2: communicator.hh::BufferedCommunicator::forward, enter sendRecv
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 8
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 8
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 8
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 8
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 8
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 8
9: communicator.hh::sendRecv, left MPI_Wait
9: leave BufferedCommunicator::sendRecv
9: communicator.hh::BufferedCommunicator::forward, left sendRecv
9 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), after communicator.free();
9 === BiCGSTABSolver
 Iter          Defect            Rate
    0      1.17536e-06
9  (it = 0.5; it < _maxit; it+=.5) 0.5 250
9  before p = r 
9  before  y = W^-1 * p 
9  before _prec->apply(y,p); 0.5 250
9 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
9void AMG<M,X,S,PI,A>::mgc(LevelContext& levelContext), before presmooth(levelContext, preSteps_) 
void presmooth(LevelContext& levelContext, size_t steps), before SmootherApplier<typename LevelContext::SmootherType>::preSmooth, step: 0 of 2
void preSmooth(Smoother& smoother, Domain& v, const Range& d), before smoother.apply(v,d);
9 schwarz.hh:virtual void apply (X& v, const Y& d), before preconditioner.apply(v,d);; 
9 schwarz.hh:virtual void apply (X& v, const Y& d), before communication.copyOwnerToAll(v,v); 
9 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
9: communicator.hh::BufferedCommunicator::forward, enter sendRecv
9: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 7
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 10 11
5: communicator.hh::sendRecv, left MPI_Wait
5: leave BufferedCommunicator::sendRecv
5: communicator.hh::BufferedCommunicator::forward, left sendRecv
5 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), after communicator.free();
5 === BiCGSTABSolver
 Iter          Defect            Rate
    0      1.17536e-06
5  (it = 0.5; it < _maxit; it+=.5) 0.5 250
5  before p = r 
5  before  y = W^-1 * p 
5  before _prec->apply(y,p); 0.5 250
5 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
5void AMG<M,X,S,PI,A>::mgc(LevelContext& levelContext), before presmooth(levelContext, preSteps_) 
void presmooth(LevelContext& levelContext, size_t steps), before SmootherApplier<typename LevelContext::SmootherType>::preSmooth, step: 0 of 2
void preSmooth(Smoother& smoother, Domain& v, const Range& d), before smoother.apply(v,d);
5 schwarz.hh:virtual void apply (X& v, const Y& d), before preconditioner.apply(v,d);; 
5 schwarz.hh:virtual void apply (X& v, const Y& d), before communication.copyOwnerToAll(v,v); 
5 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
5: communicator.hh::BufferedCommunicator::forward, enter sendRecv
13: communicator.hh::sendRecv, left MPI_Wait
13: leave BufferedCommunicator::sendRecv
13: communicator.hh::BufferedCommunicator::forward, left sendRecv
13 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), after communicator.free();
13 === BiCGSTABSolver
 Iter          Defect            Rate
    0      1.17536e-06
13  (it = 0.5; it < _maxit; it+=.5) 0.5 250
13  before p = r 
13  before  y = W^-1 * p 
13  before _prec->apply(y,p); 0.5 250
13 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
13void AMG<M,X,S,PI,A>::mgc(LevelContext& levelContext), before presmooth(levelContext, preSteps_) 
void presmooth(LevelContext& levelContext, size_t steps), before SmootherApplier<typename LevelContext::SmootherType>::preSmooth, step: 0 of 2
void preSmooth(Smoother& smoother, Domain& v, const Range& d), before smoother.apply(v,d);
13 schwarz.hh:virtual void apply (X& v, const Y& d), before preconditioner.apply(v,d);; 
13 schwarz.hh:virtual void apply (X& v, const Y& d), before communication.copyOwnerToAll(v,v); 
13 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
13: communicator.hh::BufferedCommunicator::forward, enter sendRecv
13: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 11
13: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 11
13: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 11
13: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 11
3: leave BufferedCommunicator::sendRecv
3: communicator.hh::BufferedCommunicator::forward, left sendRecv
3 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), after communicator.free();
3 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
3: communicator.hh::BufferedCommunicator::forward, enter sendRecv
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 9
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 9
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 9
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 9
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 9
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 9
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 9
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 9
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 9
3: communicator.hh::sendRecv, left MPI_Wait
3: leave BufferedCommunicator::sendRecv
3: communicator.hh::BufferedCommunicator::forward, left sendRecv
3 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), after communicator.free();
3 === BiCGSTABSolver
 Iter          Defect            Rate
    0      1.17536e-06
3  (it = 0.5; it < _maxit; it+=.5) 0.5 250
3  before p = r 
3  before  y = W^-1 * p 
3  before _prec->apply(y,p); 0.5 250
3 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
3void AMG<M,X,S,PI,A>::mgc(LevelContext& levelContext), before presmooth(levelContext, preSteps_) 
void presmooth(LevelContext& levelContext, size_t steps), before SmootherApplier<typename LevelContext::SmootherType>::preSmooth, step: 0 of 2
void preSmooth(Smoother& smoother, Domain& v, const Range& d), before smoother.apply(v,d);
3 schwarz.hh:virtual void apply (X& v, const Y& d), before preconditioner.apply(v,d);; 
3 schwarz.hh:virtual void apply (X& v, const Y& d), before communication.copyOwnerToAll(v,v); 
3 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
3: communicator.hh::BufferedCommunicator::forward, enter sendRecv
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 9
82 === BiCGSTABSolver
 Iter          Defect            Rate
    0      1.17536e-06
82  (it = 0.5; it < _maxit; it+=.5) 0.5 250
82  before p = r 
82  before  y = W^-1 * p 
82  before _prec->apply(y,p); 0.5 250
82 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
82void AMG<M,X,S,PI,A>::mgc(LevelContext& levelContext), before presmooth(levelContext, preSteps_) 
void presmooth(LevelContext& levelContext, size_t steps), before SmootherApplier<typename LevelContext::SmootherType>::preSmooth, step: 0 of 2
void preSmooth(Smoother& smoother, Domain& v, const Range& d), before smoother.apply(v,d);
82 schwarz.hh:virtual void apply (X& v, const Y& d), before preconditioner.apply(v,d);; 
109 === BiCGSTABSolver
 Iter          Defect            Rate
    0      1.17536e-06
109  (it = 0.5; it < _maxit; it+=.5) 0.5 250
109  before p = r 
109  before  y = W^-1 * p 
109  before _prec->apply(y,p); 0.5 250
109 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
109void AMG<M,X,S,PI,A>::mgc(LevelContext& levelContext), before presmooth(levelContext, preSteps_) 
void presmooth(LevelContext& levelContext, size_t steps), before SmootherApplier<typename LevelContext::SmootherType>::preSmooth, step: 0 of 2
void preSmooth(Smoother& smoother, Domain& v, const Range& d), before smoother.apply(v,d);
109 schwarz.hh:virtual void apply (X& v, const Y& d), before preconditioner.apply(v,d);; 
109 schwarz.hh:virtual void apply (X& v, const Y& d), before communication.copyOwnerToAll(v,v); 
109 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
109: communicator.hh::BufferedCommunicator::forward, enter sendRecv
109: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 20
109: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 20
109: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 20
109: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 20
109: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 20
109: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 20
109: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 20
109: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 20
109: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 20
109: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 20
109: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 10 20
109: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 11 20
109: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 12 20
109: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 13 20
109: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 14 20
109: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 15 20
109: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 16 20
109: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 17 20
109: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 18 20
109: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 19 20
33 === BiCGSTABSolver
 Iter          Defect            Rate
    0      1.17536e-06
33  (it = 0.5; it < _maxit; it+=.5) 0.5 250
33  before p = r 
33  before  y = W^-1 * p 
33  before _prec->apply(y,p); 0.5 250
33 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
33void AMG<M,X,S,PI,A>::mgc(LevelContext& levelContext), before presmooth(levelContext, preSteps_) 
void presmooth(LevelContext& levelContext, size_t steps), before SmootherApplier<typename LevelContext::SmootherType>::preSmooth, step: 0 of 2
void preSmooth(Smoother& smoother, Domain& v, const Range& d), before smoother.apply(v,d);
33 schwarz.hh:virtual void apply (X& v, const Y& d), before preconditioner.apply(v,d);; 
33 schwarz.hh:virtual void apply (X& v, const Y& d), before communication.copyOwnerToAll(v,v); 
33 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
33: communicator.hh::BufferedCommunicator::forward, enter sendRecv
33: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 9
33: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 9
33: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 9
33: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 9
33: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 9
33: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 9
33: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 9
33: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 9
33: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 9
29 === BiCGSTABSolver
 Iter          Defect            Rate
    0      1.17536e-06
29  (it = 0.5; it < _maxit; it+=.5) 0.5 250
29  before p = r 
29  before  y = W^-1 * p 
29  before _prec->apply(y,p); 0.5 250
29 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
29void AMG<M,X,S,PI,A>::mgc(LevelContext& levelContext), before presmooth(levelContext, preSteps_) 
void presmooth(LevelContext& levelContext, size_t steps), before SmootherApplier<typename LevelContext::SmootherType>::preSmooth, step: 0 of 2
void preSmooth(Smoother& smoother, Domain& v, const Range& d), before smoother.apply(v,d);
29 schwarz.hh:virtual void apply (X& v, const Y& d), before preconditioner.apply(v,d);; 
29 schwarz.hh:virtual void apply (X& v, const Y& d), before communication.copyOwnerToAll(v,v); 
29 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
29: communicator.hh::BufferedCommunicator::forward, enter sendRecv
29: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 11
29: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 11
29: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 11
29: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 11
29: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 11
29: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 11
29: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 11
29: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 11
29: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 11
29: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 11
29: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 10 11
29: communicator.hh::sendRecv, left MPI_Wait
69 === BiCGSTABSolver
 Iter          Defect            Rate
    0      1.17536e-06
69  (it = 0.5; it < _maxit; it+=.5) 0.5 250
69  before p = r 
69  before  y = W^-1 * p 
69  before _prec->apply(y,p); 0.5 250
69 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
69void AMG<M,X,S,PI,A>::mgc(LevelContext& levelContext), before presmooth(levelContext, preSteps_) 
void presmooth(LevelContext& levelContext, size_t steps), before SmootherApplier<typename LevelContext::SmootherType>::preSmooth, step: 0 of 2
void preSmooth(Smoother& smoother, Domain& v, const Range& d), before smoother.apply(v,d);
69 schwarz.hh:virtual void apply (X& v, const Y& d), before preconditioner.apply(v,d);; 
69 schwarz.hh:virtual void apply (X& v, const Y& d), before communication.copyOwnerToAll(v,v); 
69 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
69: communicator.hh::BufferedCommunicator::forward, enter sendRecv
69: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 7
69: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 7
69: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 7
69: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 7
69: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 7
31 === BiCGSTABSolver
 Iter          Defect            Rate
    0      1.17536e-06
31  (it = 0.5; it < _maxit; it+=.5) 0.5 250
31  before p = r 
31  before  y = W^-1 * p 
31  before _prec->apply(y,p); 0.5 250
31 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
31void AMG<M,X,S,PI,A>::mgc(LevelContext& levelContext), before presmooth(levelContext, preSteps_) 
void presmooth(LevelContext& levelContext, size_t steps), before SmootherApplier<typename LevelContext::SmootherType>::preSmooth, step: 0 of 2
void preSmooth(Smoother& smoother, Domain& v, const Range& d), before smoother.apply(v,d);
31 schwarz.hh:virtual void apply (X& v, const Y& d), before preconditioner.apply(v,d);; 
31 schwarz.hh:virtual void apply (X& v, const Y& d), before communication.copyOwnerToAll(v,v); 
31 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
31: communicator.hh::BufferedCommunicator::forward, enter sendRecv
31: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 10
31: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 10
31: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 10
31: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 10
31: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 10
31: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 10
31: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 10
31: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 10
31: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 10
31: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 10
31: communicator.hh::sendRecv, left MPI_Wait
8: communicator.hh::BufferedCommunicator::forward, enter sendRecv
8: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 20
8: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 20
8: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 20
8: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 20
8: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 20
8: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 20
8: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 20
8: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 20
8: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 20
8: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 20
8: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 10 20
8: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 11 20
8: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 12 20
8: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 13 20
8: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 14 20
8: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 15 20
8: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 16 20
8: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 17 20
8: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 18 20
14  before p = r 
14  before  y = W^-1 * p 
14  before _prec->apply(y,p); 0.5 250
14 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
14void AMG<M,X,S,PI,A>::mgc(LevelContext& levelContext), before presmooth(levelContext, preSteps_) 
void presmooth(LevelContext& levelContext, size_t steps), before SmootherApplier<typename LevelContext::SmootherType>::preSmooth, step: 0 of 2
void preSmooth(Smoother& smoother, Domain& v, const Range& d), before smoother.apply(v,d);
14 schwarz.hh:virtual void apply (X& v, const Y& d), before preconditioner.apply(v,d);; 
14 schwarz.hh:virtual void apply (X& v, const Y& d), before communication.copyOwnerToAll(v,v); 
14 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
14: communicator.hh::BufferedCommunicator::forward, enter sendRecv
14: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 17
14: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 17
14: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 17
14: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 17
14: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 17
14: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 17
14: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 17
14: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 17
14: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 17
14: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 17
14: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 10 17
14: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 11 17
14: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 12 17
14: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 13 17
14: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 14 17
14: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 15 17
14: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 16 17
14: communicator.hh::sendRecv, left MPI_Wait
86 === BiCGSTABSolver
 Iter          Defect            Rate
    0      1.17536e-06
86  (it = 0.5; it < _maxit; it+=.5) 0.5 250
86  before p = r 
86  before  y = W^-1 * p 
86  before _prec->apply(y,p); 0.5 250
86 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
86void AMG<M,X,S,PI,A>::mgc(LevelContext& levelContext), before presmooth(levelContext, preSteps_) 
void presmooth(LevelContext& levelContext, size_t steps), before SmootherApplier<typename LevelContext::SmootherType>::preSmooth, step: 0 of 2
void preSmooth(Smoother& smoother, Domain& v, const Range& d), before smoother.apply(v,d);
86 schwarz.hh:virtual void apply (X& v, const Y& d), before preconditioner.apply(v,d);; 
86 schwarz.hh:virtual void apply (X& v, const Y& d), before communication.copyOwnerToAll(v,v); 
86 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
86: communicator.hh::BufferedCommunicator::forward, enter sendRecv
86: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 31
86: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 31
86: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 31
86: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 31
86: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 31
86: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 31
86: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 31
86: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 31
86: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 31
86: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 31
86: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 10 31
86: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 11 31
86: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 12 31
86: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 13 31
86: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 14 31
86: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 15 31
86: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 16 31
86: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 17 31
86: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 18 31
86: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 19 31
86: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 20 31
86: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 21 31
86: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 22 31
86: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 23 31
86: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 24 31
55 === BiCGSTABSolver
 Iter          Defect            Rate
    0      1.17536e-06
55  (it = 0.5; it < _maxit; it+=.5) 0.5 250
55  before p = r 
55  before  y = W^-1 * p 
55  before _prec->apply(y,p); 0.5 250
55 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
55void AMG<M,X,S,PI,A>::mgc(LevelContext& levelContext), before presmooth(levelContext, preSteps_) 
void presmooth(LevelContext& levelContext, size_t steps), before SmootherApplier<typename LevelContext::SmootherType>::preSmooth, step: 0 of 2
void preSmooth(Smoother& smoother, Domain& v, const Range& d), before smoother.apply(v,d);
55 schwarz.hh:virtual void apply (X& v, const Y& d), before preconditioner.apply(v,d);; 
55 schwarz.hh:virtual void apply (X& v, const Y& d), before communication.copyOwnerToAll(v,v); 
55 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
55: communicator.hh::BufferedCommunicator::forward, enter sendRecv
55: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 11
55: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 11
55: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 11
55: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 11
55: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 11
55: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 11
55: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 11
55: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 11
55: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 11
55: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 11
55: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 10 11
6 schwarz.hh:virtual void apply (X& v, const Y& d), before communication.copyOwnerToAll(v,v); 
6 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
6: communicator.hh::BufferedCommunicator::forward, enter sendRecv
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 17
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 17
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 17
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 17
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 17
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 17
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 17
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 17
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 17
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 17
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 10 17
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 11 17
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 12 17
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 13 17
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 14 17
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 15 17
99  before p = r 
99  before  y = W^-1 * p 
99  before _prec->apply(y,p); 0.5 250
99 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
99void AMG<M,X,S,PI,A>::mgc(LevelContext& levelContext), before presmooth(levelContext, preSteps_) 
void presmooth(LevelContext& levelContext, size_t steps), before SmootherApplier<typename LevelContext::SmootherType>::preSmooth, step: 0 of 2
void preSmooth(Smoother& smoother, Domain& v, const Range& d), before smoother.apply(v,d);
99 schwarz.hh:virtual void apply (X& v, const Y& d), before preconditioner.apply(v,d);; 
99 schwarz.hh:virtual void apply (X& v, const Y& d), before communication.copyOwnerToAll(v,v); 
99 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
99: communicator.hh::BufferedCommunicator::forward, enter sendRecv
99: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 13
99: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 13
99: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 13
99: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 13
99: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 13
99: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 13
99: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 13
99: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 13
99: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 13
99: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 13
99: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 10 13
99: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 11 13
99: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 12 13
99: communicator.hh::sendRecv, left MPI_Wait
7: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 13
7: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 13
7: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 13
7: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 13
7: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 13
7: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 13
7: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 13
7: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 13
7: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 13
7: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 10 13
7: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 11 13
7: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 12 13
7: communicator.hh::sendRecv, left MPI_Wait
7: leave BufferedCommunicator::sendRecv
7: communicator.hh::BufferedCommunicator::forward, left sendRecv
7 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), after communicator.free();
7 schwarz.hh:virtual void apply (X& v, const Y& d), AFTER communication.copyOwnerToAll(v,v); 
void preSmooth(Smoother& smoother, Domain& v, const Range& d), AFTER smoother.apply(v,d);
void presmooth(LevelContext& levelContext, size_t steps), AFTER SmootherApplier<typename LevelContext::SmootherType>::preSmooth, step: 0 of 2
void presmooth(LevelContext& levelContext, size_t steps), before update defec, step: 0 of 2
void presmooth(LevelContext& levelContext, size_t steps), before SmootherApplier<typename LevelContext::SmootherType>::preSmooth, step: 1 of 2
void preSmooth(Smoother& smoother, Domain& v, const Range& d), before smoother.apply(v,d);
7 schwarz.hh:virtual void apply (X& v, const Y& d), before preconditioner.apply(v,d);; 
61: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 9
61: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 9
61: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 9
61: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 9
61: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 9
61: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 9
61: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 9
90  before p = r 
90  before  y = W^-1 * p 
90  before _prec->apply(y,p); 0.5 250
90 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
90void AMG<M,X,S,PI,A>::mgc(LevelContext& levelContext), before presmooth(levelContext, preSteps_) 
void presmooth(LevelContext& levelContext, size_t steps), before SmootherApplier<typename LevelContext::SmootherType>::preSmooth, step: 0 of 2
void preSmooth(Smoother& smoother, Domain& v, const Range& d), before smoother.apply(v,d);
90 schwarz.hh:virtual void apply (X& v, const Y& d), before preconditioner.apply(v,d);; 
90 schwarz.hh:virtual void apply (X& v, const Y& d), before communication.copyOwnerToAll(v,v); 
90 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
90: communicator.hh::BufferedCommunicator::forward, enter sendRecv
90: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 6
90: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 6
90: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 6
90: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 6
90: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 6
90: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 6
90: communicator.hh::sendRecv, left MPI_Wait
90: leave BufferedCommunicator::sendRecv
90: communicator.hh::BufferedCommunicator::forward, left sendRecv
90 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), after communicator.free();
90 schwarz.hh:virtual void apply (X& v, const Y& d), AFTER communication.copyOwnerToAll(v,v); 
void preSmooth(Smoother& smoother, Domain& v, const Range& d), AFTER smoother.apply(v,d);
void presmooth(LevelContext& levelContext, size_t steps), AFTER SmootherApplier<typename LevelContext::SmootherType>::preSmooth, step: 0 of 2
void presmooth(LevelContext& levelContext, size_t steps), before update defec, step: 0 of 2
107: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 5
107: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 5
107: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 5
107: communicator.hh::sendRecv, left MPI_Wait
107: leave BufferedCommunicator::sendRecv
107: communicator.hh::BufferedCommunicator::forward, left sendRecv
107 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), after communicator.free();
107 schwarz.hh:virtual void apply (X& v, const Y& d), AFTER communication.copyOwnerToAll(v,v); 
void preSmooth(Smoother& smoother, Domain& v, const Range& d), AFTER smoother.apply(v,d);
void presmooth(LevelContext& levelContext, size_t steps), AFTER SmootherApplier<typename LevelContext::SmootherType>::preSmooth, step: 0 of 2
void presmooth(LevelContext& levelContext, size_t steps), before update defec, step: 0 of 2
void presmooth(LevelContext& levelContext, size_t steps), before SmootherApplier<typename LevelContext::SmootherType>::preSmooth, step: 1 of 2
void preSmooth(Smoother& smoother, Domain& v, const Range& d), before smoother.apply(v,d);
107 schwarz.hh:virtual void apply (X& v, const Y& d), before preconditioner.apply(v,d);; 
107 schwarz.hh:virtual void apply (X& v, const Y& d), before communication.copyOwnerToAll(v,v); 
107 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
107: communicator.hh::BufferedCommunicator::forward, enter sendRecv
107: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 5
102  before _prec->apply(y,p); 0.5 250
102 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
102void AMG<M,X,S,PI,A>::mgc(LevelContext& levelContext), before presmooth(levelContext, preSteps_) 
void presmooth(LevelContext& levelContext, size_t steps), before SmootherApplier<typename LevelContext::SmootherType>::preSmooth, step: 0 of 2
void preSmooth(Smoother& smoother, Domain& v, const Range& d), before smoother.apply(v,d);
102 schwarz.hh:virtual void apply (X& v, const Y& d), before preconditioner.apply(v,d);; 
102 schwarz.hh:virtual void apply (X& v, const Y& d), before communication.copyOwnerToAll(v,v); 
102 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
102: communicator.hh::BufferedCommunicator::forward, enter sendRecv
102: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 15
102: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 15
102: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 15
102: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 15
102: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 15
102: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 15
102: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 15
102: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 15
102: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 15
102: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 15
102: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 10 15
102: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 11 15
102: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 12 15
102: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 13 15
102: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 14 15
73: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 11
73: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 11
73: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 11
73: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 11
73: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 11
73: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 11
73: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 11
73: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 11
16  before p = r 
16  before  y = W^-1 * p 
16  before _prec->apply(y,p); 0.5 250
16 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
16void AMG<M,X,S,PI,A>::mgc(LevelContext& levelContext), before presmooth(levelContext, preSteps_) 
void presmooth(LevelContext& levelContext, size_t steps), before SmootherApplier<typename LevelContext::SmootherType>::preSmooth, step: 0 of 2
void preSmooth(Smoother& smoother, Domain& v, const Range& d), before smoother.apply(v,d);
16 schwarz.hh:virtual void apply (X& v, const Y& d), before preconditioner.apply(v,d);; 
16 schwarz.hh:virtual void apply (X& v, const Y& d), before communication.copyOwnerToAll(v,v); 
16 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
16: communicator.hh::BufferedCommunicator::forward, enter sendRecv
16: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 6
16: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 6
16: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 6
16: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 6
16: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 6
16: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 6
16: communicator.hh::sendRecv, left MPI_Wait
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 5
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 5
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 5
1: communicator.hh::sendRecv, left MPI_Wait
1: leave BufferedCommunicator::sendRecv
1: communicator.hh::BufferedCommunicator::forward, left sendRecv
1 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), after communicator.free();
1 schwarz.hh:virtual void apply (X& v, const Y& d), AFTER communication.copyOwnerToAll(v,v); 
void preSmooth(Smoother& smoother, Domain& v, const Range& d), AFTER smoother.apply(v,d);
void presmooth(LevelContext& levelContext, size_t steps), AFTER SmootherApplier<typename LevelContext::SmootherType>::preSmooth, step: 0 of 2
void presmooth(LevelContext& levelContext, size_t steps), before update defec, step: 0 of 2
void presmooth(LevelContext& levelContext, size_t steps), before SmootherApplier<typename LevelContext::SmootherType>::preSmooth, step: 1 of 2
void preSmooth(Smoother& smoother, Domain& v, const Range& d), before smoother.apply(v,d);
1 schwarz.hh:virtual void apply (X& v, const Y& d), before preconditioner.apply(v,d);; 
1 schwarz.hh:virtual void apply (X& v, const Y& d), before communication.copyOwnerToAll(v,v); 
1 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
1: communicator.hh::BufferedCommunicator::forward, enter sendRecv
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 5
30: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 6
30: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 6
30: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 6
30: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 6
30: communicator.hh::sendRecv, left MPI_Wait
30: leave BufferedCommunicator::sendRecv
30: communicator.hh::BufferedCommunicator::forward, left sendRecv
30 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), after communicator.free();
30 schwarz.hh:virtual void apply (X& v, const Y& d), AFTER communication.copyOwnerToAll(v,v); 
void preSmooth(Smoother& smoother, Domain& v, const Range& d), AFTER smoother.apply(v,d);
void presmooth(LevelContext& levelContext, size_t steps), AFTER SmootherApplier<typename LevelContext::SmootherType>::preSmooth, step: 0 of 2
void presmooth(LevelContext& levelContext, size_t steps), before update defec, step: 0 of 2
void presmooth(LevelContext& levelContext, size_t steps), before SmootherApplier<typename LevelContext::SmootherType>::preSmooth, step: 1 of 2
void preSmooth(Smoother& smoother, Domain& v, const Range& d), before smoother.apply(v,d);
30 schwarz.hh:virtual void apply (X& v, const Y& d), before preconditioner.apply(v,d);; 
30 schwarz.hh:virtual void apply (X& v, const Y& d), before communication.copyOwnerToAll(v,v); 
30 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
30: communicator.hh::BufferedCommunicator::forward, enter sendRecv
30: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 6
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 9
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 9
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 9
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 9
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 9
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 9
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 9
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 9
3: communicator.hh::sendRecv, left MPI_Wait
44 === BiCGSTABSolver
 Iter          Defect            Rate
    0      1.17536e-06
44  (it = 0.5; it < _maxit; it+=.5) 0.5 250
44  before p = r 
44  before  y = W^-1 * p 
44  before _prec->apply(y,p); 0.5 250
44 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
44void AMG<M,X,S,PI,A>::mgc(LevelContext& levelContext), before presmooth(levelContext, preSteps_) 
void presmooth(LevelContext& levelContext, size_t steps), before SmootherApplier<typename LevelContext::SmootherType>::preSmooth, step: 0 of 2
void preSmooth(Smoother& smoother, Domain& v, const Range& d), before smoother.apply(v,d);
44 schwarz.hh:virtual void apply (X& v, const Y& d), before preconditioner.apply(v,d);; 
89 === BiCGSTABSolver
 Iter          Defect            Rate
    0      1.17536e-06
89  (it = 0.5; it < _maxit; it+=.5) 0.5 250
89  before p = r 
89  before  y = W^-1 * p 
89  before _prec->apply(y,p); 0.5 250
89 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
89void AMG<M,X,S,PI,A>::mgc(LevelContext& levelContext), before presmooth(levelContext, preSteps_) 
void presmooth(LevelContext& levelContext, size_t steps), before SmootherApplier<typename LevelContext::SmootherType>::preSmooth, step: 0 of 2
void preSmooth(Smoother& smoother, Domain& v, const Range& d), before smoother.apply(v,d);
89 schwarz.hh:virtual void apply (X& v, const Y& d), before preconditioner.apply(v,d);; 
89 schwarz.hh:virtual void apply (X& v, const Y& d), before communication.copyOwnerToAll(v,v); 
89 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
89: communicator.hh::BufferedCommunicator::forward, enter sendRecv
89: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 11
89: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 11
89: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 11
89: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 11
89: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 11
89: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 11
89: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 11
89: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 11
89: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 11
97 === BiCGSTABSolver
 Iter          Defect            Rate
    0      1.17536e-06
97  (it = 0.5; it < _maxit; it+=.5) 0.5 250
97  before p = r 
97  before  y = W^-1 * p 
97  before _prec->apply(y,p); 0.5 250
97 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
97void AMG<M,X,S,PI,A>::mgc(LevelContext& levelContext), before presmooth(levelContext, preSteps_) 
void presmooth(LevelContext& levelContext, size_t steps), before SmootherApplier<typename LevelContext::SmootherType>::preSmooth, step: 0 of 2
void preSmooth(Smoother& smoother, Domain& v, const Range& d), before smoother.apply(v,d);
97 schwarz.hh:virtual void apply (X& v, const Y& d), before preconditioner.apply(v,d);; 
97 schwarz.hh:virtual void apply (X& v, const Y& d), before communication.copyOwnerToAll(v,v); 
97 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
97: communicator.hh::BufferedCommunicator::forward, enter sendRecv
97: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 9
97: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 9
97: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 9
97: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 9
97: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 9
97: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 9
97: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 9
97: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 9
97: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 9
97: communicator.hh::sendRecv, left MPI_Wait
116 === BiCGSTABSolver
 Iter          Defect            Rate
    0      1.17536e-06
116  (it = 0.5; it < _maxit; it+=.5) 0.5 250
116  before p = r 
116  before  y = W^-1 * p 
116  before _prec->apply(y,p); 0.5 250
116 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
116void AMG<M,X,S,PI,A>::mgc(LevelContext& levelContext), before presmooth(levelContext, preSteps_) 
void presmooth(LevelContext& levelContext, size_t steps), before SmootherApplier<typename LevelContext::SmootherType>::preSmooth, step: 0 of 2
void preSmooth(Smoother& smoother, Domain& v, const Range& d), before smoother.apply(v,d);
116 schwarz.hh:virtual void apply (X& v, const Y& d), before preconditioner.apply(v,d);; 
116 schwarz.hh:virtual void apply (X& v, const Y& d), before communication.copyOwnerToAll(v,v); 
116 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
116: communicator.hh::BufferedCommunicator::forward, enter sendRecv
116: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 7
116: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 7
116: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 7
116: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 7
116: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 7
116: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 7
116: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 7
116: communicator.hh::sendRecv, left MPI_Wait
116: leave BufferedCommunicator::sendRecv
116: communicator.hh::BufferedCommunicator::forward, left sendRecv
116 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), after communicator.free();
116 schwarz.hh:virtual void apply (X& v, const Y& d), AFTER communication.copyOwnerToAll(v,v); 
void preSmooth(Smoother& smoother, Domain& v, const Range& d), AFTER smoother.apply(v,d);
void presmooth(LevelContext& levelContext, size_t steps), AFTER SmootherApplier<typename LevelContext::SmootherType>::preSmooth, step: 0 of 2
void presmooth(LevelContext& levelContext, size_t steps), before update defec, step: 0 of 2
void presmooth(LevelContext& levelContext, size_t steps), before SmootherApplier<typename LevelContext::SmootherType>::preSmooth, step: 1 of 2
void preSmooth(Smoother& smoother, Domain& v, const Range& d), before smoother.apply(v,d);
116 schwarz.hh:virtual void apply (X& v, const Y& d), before preconditioner.apply(v,d);; 
116 schwarz.hh:virtual void apply (X& v, const Y& d), before communication.copyOwnerToAll(v,v); 
116 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
116: communicator.hh::BufferedCommunicator::forward, enter sendRecv
116: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 7
22 === BiCGSTABSolver
 Iter          Defect            Rate
    0      1.17536e-06
22  (it = 0.5; it < _maxit; it+=.5) 0.5 250
22  before p = r 
22  before  y = W^-1 * p 
22  before _prec->apply(y,p); 0.5 250
22 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
22void AMG<M,X,S,PI,A>::mgc(LevelContext& levelContext), before presmooth(levelContext, preSteps_) 
void presmooth(LevelContext& levelContext, size_t steps), before SmootherApplier<typename LevelContext::SmootherType>::preSmooth, step: 0 of 2
void preSmooth(Smoother& smoother, Domain& v, const Range& d), before smoother.apply(v,d);
22 schwarz.hh:virtual void apply (X& v, const Y& d), before preconditioner.apply(v,d);; 
22 schwarz.hh:virtual void apply (X& v, const Y& d), before communication.copyOwnerToAll(v,v); 
22 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
22: communicator.hh::BufferedCommunicator::forward, enter sendRecv
22: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 20
22: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 20
22: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 20
22: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 20
22: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 20
22: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 20
22: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 20
22: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 20
22: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 20
22: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 20
22: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 10 20
22: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 11 20
22: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 12 20
22: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 13 20
22: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 14 20
22: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 15 20
22: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 16 20
22: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 17 20
22: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 18 20
22: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 19 20
22: communicator.hh::sendRecv, left MPI_Wait
101 === BiCGSTABSolver
 Iter          Defect            Rate
    0      1.17536e-06
101  (it = 0.5; it < _maxit; it+=.5) 0.5 250
101  before p = r 
101  before  y = W^-1 * p 
101  before _prec->apply(y,p); 0.5 250
101 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
101void AMG<M,X,S,PI,A>::mgc(LevelContext& levelContext), before presmooth(levelContext, preSteps_) 
void presmooth(LevelContext& levelContext, size_t steps), before SmootherApplier<typename LevelContext::SmootherType>::preSmooth, step: 0 of 2
void preSmooth(Smoother& smoother, Domain& v, const Range& d), before smoother.apply(v,d);
101 schwarz.hh:virtual void apply (X& v, const Y& d), before preconditioner.apply(v,d);; 
101 schwarz.hh:virtual void apply (X& v, const Y& d), before communication.copyOwnerToAll(v,v); 
101 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
101: communicator.hh::BufferedCommunicator::forward, enter sendRecv
101: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 24
101: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 24
101: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 24
101: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 24
101: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 24
101: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 24
101: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 24
101: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 24
101: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 24
101: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 24
101: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 10 24
101: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 11 24
101: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 12 24
101: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 13 24
101: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 14 24
101: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 15 24
101: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 16 24
101: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 17 24
101: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 18 24
101: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 19 24
101: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 20 24
101: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 21 24
101: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 22 24
101: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 23 24
101: communicator.hh::sendRecv, left MPI_Wait
103 === BiCGSTABSolver
 Iter          Defect            Rate
    0      1.17536e-06
103  (it = 0.5; it < _maxit; it+=.5) 0.5 250
103  before p = r 
103  before  y = W^-1 * p 
103  before _prec->apply(y,p); 0.5 250
103 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
103void AMG<M,X,S,PI,A>::mgc(LevelContext& levelContext), before presmooth(levelContext, preSteps_) 
void presmooth(LevelContext& levelContext, size_t steps), before SmootherApplier<typename LevelContext::SmootherType>::preSmooth, step: 0 of 2
void preSmooth(Smoother& smoother, Domain& v, const Range& d), before smoother.apply(v,d);
103 schwarz.hh:virtual void apply (X& v, const Y& d), before preconditioner.apply(v,d);; 
103 schwarz.hh:virtual void apply (X& v, const Y& d), before communication.copyOwnerToAll(v,v); 
103 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
103: communicator.hh::BufferedCommunicator::forward, enter sendRecv
103: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 16
103: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 16
103: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 16
103: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 16
103: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 16
103: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 16
103: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 16
103: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 16
103: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 16
103: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 16
103: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 10 16
103: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 11 16
103: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 12 16
103: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 13 16
103: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 14 16
103: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 15 16
103: communicator.hh::sendRecv, left MPI_Wait
void preSmooth(Smoother& smoother, Domain& v, const Range& d), before smoother.apply(v,d);
0 schwarz.hh:virtual void apply (X& v, const Y& d), before preconditioner.apply(v,d);; 
0 schwarz.hh:virtual void apply (X& v, const Y& d), before communication.copyOwnerToAll(v,v); 
0 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
0: communicator.hh::BufferedCommunicator::forward, enter sendRecv
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 3
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 3
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 3
0: communicator.hh::sendRecv, left MPI_Wait
0: leave BufferedCommunicator::sendRecv
0: communicator.hh::BufferedCommunicator::forward, left sendRecv
0 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), after communicator.free();
0 schwarz.hh:virtual void apply (X& v, const Y& d), AFTER communication.copyOwnerToAll(v,v); 
void preSmooth(Smoother& smoother, Domain& v, const Range& d), AFTER smoother.apply(v,d);
void presmooth(LevelContext& levelContext, size_t steps), AFTER SmootherApplier<typename LevelContext::SmootherType>::preSmooth, step: 0 of 2
void presmooth(LevelContext& levelContext, size_t steps), before update defec, step: 0 of 2
void presmooth(LevelContext& levelContext, size_t steps), before SmootherApplier<typename LevelContext::SmootherType>::preSmooth, step: 1 of 2
void preSmooth(Smoother& smoother, Domain& v, const Range& d), before smoother.apply(v,d);
0 schwarz.hh:virtual void apply (X& v, const Y& d), before preconditioner.apply(v,d);; 
0 schwarz.hh:virtual void apply (X& v, const Y& d), before communication.copyOwnerToAll(v,v); 
0 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
0: communicator.hh::BufferedCommunicator::forward, enter sendRecv
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 3
75 === BiCGSTABSolver
 Iter          Defect            Rate
    0      1.17536e-06
75  (it = 0.5; it < _maxit; it+=.5) 0.5 250
75  before p = r 
75  before  y = W^-1 * p 
75  before _prec->apply(y,p); 0.5 250
75 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
75void AMG<M,X,S,PI,A>::mgc(LevelContext& levelContext), before presmooth(levelContext, preSteps_) 
void presmooth(LevelContext& levelContext, size_t steps), before SmootherApplier<typename LevelContext::SmootherType>::preSmooth, step: 0 of 2
void preSmooth(Smoother& smoother, Domain& v, const Range& d), before smoother.apply(v,d);
75 schwarz.hh:virtual void apply (X& v, const Y& d), before preconditioner.apply(v,d);; 
71 === BiCGSTABSolver
 Iter          Defect            Rate
    0      1.17536e-06
71  (it = 0.5; it < _maxit; it+=.5) 0.5 250
71  before p = r 
71  before  y = W^-1 * p 
71  before _prec->apply(y,p); 0.5 250
71 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
71void AMG<M,X,S,PI,A>::mgc(LevelContext& levelContext), before presmooth(levelContext, preSteps_) 
void presmooth(LevelContext& levelContext, size_t steps), before SmootherApplier<typename LevelContext::SmootherType>::preSmooth, step: 0 of 2
void preSmooth(Smoother& smoother, Domain& v, const Range& d), before smoother.apply(v,d);
71 schwarz.hh:virtual void apply (X& v, const Y& d), before preconditioner.apply(v,d);; 
71 schwarz.hh:virtual void apply (X& v, const Y& d), before communication.copyOwnerToAll(v,v); 
71 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
71: communicator.hh::BufferedCommunicator::forward, enter sendRecv
71: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 20
71: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 20
71: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 20
71: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 20
71: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 20
71: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 20
71: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 20
71: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 20
71: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 20
71: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 20
71: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 10 20
71: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 11 20
71: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 12 20
71: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 13 20
71: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 14 20
78 === BiCGSTABSolver
 Iter          Defect            Rate
    0      1.17536e-06
78  (it = 0.5; it < _maxit; it+=.5) 0.5 250
78  before p = r 
78  before  y = W^-1 * p 
78  before _prec->apply(y,p); 0.5 250
78 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
78void AMG<M,X,S,PI,A>::mgc(LevelContext& levelContext), before presmooth(levelContext, preSteps_) 
void presmooth(LevelContext& levelContext, size_t steps), before SmootherApplier<typename LevelContext::SmootherType>::preSmooth, step: 0 of 2
void preSmooth(Smoother& smoother, Domain& v, const Range& d), before smoother.apply(v,d);
78 schwarz.hh:virtual void apply (X& v, const Y& d), before preconditioner.apply(v,d);; 
78 schwarz.hh:virtual void apply (X& v, const Y& d), before communication.copyOwnerToAll(v,v); 
78 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
78: communicator.hh::BufferedCommunicator::forward, enter sendRecv
78: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 15
78: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 15
78: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 15
78: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 15
78: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 15
78: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 15
78: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 15
78: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 15
78: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 15
78: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 15
78: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 10 15
78: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 11 15
111 === BiCGSTABSolver
 Iter          Defect            Rate
    0      1.17536e-06
111  (it = 0.5; it < _maxit; it+=.5) 0.5 250
111  before p = r 
111  before  y = W^-1 * p 
111  before _prec->apply(y,p); 0.5 250
111 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
111void AMG<M,X,S,PI,A>::mgc(LevelContext& levelContext), before presmooth(levelContext, preSteps_) 
void presmooth(LevelContext& levelContext, size_t steps), before SmootherApplier<typename LevelContext::SmootherType>::preSmooth, step: 0 of 2
void preSmooth(Smoother& smoother, Domain& v, const Range& d), before smoother.apply(v,d);
111 schwarz.hh:virtual void apply (X& v, const Y& d), before preconditioner.apply(v,d);; 
111 schwarz.hh:virtual void apply (X& v, const Y& d), before communication.copyOwnerToAll(v,v); 
111 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
111: communicator.hh::BufferedCommunicator::forward, enter sendRecv
111: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 11
111: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 11
111: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 11
111: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 11
111: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 11
111: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 11
111: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 11
111: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 11
111: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 11
111: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 11
111: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 10 11
111: communicator.hh::sendRecv, left MPI_Wait
111: leave BufferedCommunicator::sendRecv
111: communicator.hh::BufferedCommunicator::forward, left sendRecv
111 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), after communicator.free();
111 schwarz.hh:virtual void apply (X& v, const Y& d), AFTER communication.copyOwnerToAll(v,v); 
void preSmooth(Smoother& smoother, Domain& v, const Range& d), AFTER smoother.apply(v,d);
void presmooth(LevelContext& levelContext, size_t steps), AFTER SmootherApplier<typename LevelContext::SmootherType>::preSmooth, step: 0 of 2
void presmooth(LevelContext& levelContext, size_t steps), before update defec, step: 0 of 2
void presmooth(LevelContext& levelContext, size_t steps), before SmootherApplier<typename LevelContext::SmootherType>::preSmooth, step: 1 of 2
void preSmooth(Smoother& smoother, Domain& v, const Range& d), before smoother.apply(v,d);
111 schwarz.hh:virtual void apply (X& v, const Y& d), before preconditioner.apply(v,d);; 
111 schwarz.hh:virtual void apply (X& v, const Y& d), before communication.copyOwnerToAll(v,v); 
111 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
56 === BiCGSTABSolver
 Iter          Defect            Rate
    0      1.17536e-06
56  (it = 0.5; it < _maxit; it+=.5) 0.5 250
56  before p = r 
56  before  y = W^-1 * p 
56  before _prec->apply(y,p); 0.5 250
56 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
56void AMG<M,X,S,PI,A>::mgc(LevelContext& levelContext), before presmooth(levelContext, preSteps_) 
void presmooth(LevelContext& levelContext, size_t steps), before SmootherApplier<typename LevelContext::SmootherType>::preSmooth, step: 0 of 2
void preSmooth(Smoother& smoother, Domain& v, const Range& d), before smoother.apply(v,d);
56 schwarz.hh:virtual void apply (X& v, const Y& d), before preconditioner.apply(v,d);; 
56 schwarz.hh:virtual void apply (X& v, const Y& d), before communication.copyOwnerToAll(v,v); 
56 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
56: communicator.hh::BufferedCommunicator::forward, enter sendRecv
56: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 31
56: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 31
56: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 31
56: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 31
56: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 31
56: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 31
56: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 31
56: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 31
56: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 31
56: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 31
56: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 10 31
56: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 11 31
56: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 12 31
56: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 13 31
56: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 14 31
56: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 15 31
56: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 16 31
56: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 17 31
56: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 18 31
56: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 19 31
56: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 20 31
56: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 21 31
67 === BiCGSTABSolver
 Iter          Defect            Rate
    0      1.17536e-06
67  (it = 0.5; it < _maxit; it+=.5) 0.5 250
67  before p = r 
67  before  y = W^-1 * p 
67  before _prec->apply(y,p); 0.5 250
67 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
67void AMG<M,X,S,PI,A>::mgc(LevelContext& levelContext), before presmooth(levelContext, preSteps_) 
void presmooth(LevelContext& levelContext, size_t steps), before SmootherApplier<typename LevelContext::SmootherType>::preSmooth, step: 0 of 2
void preSmooth(Smoother& smoother, Domain& v, const Range& d), before smoother.apply(v,d);
67 schwarz.hh:virtual void apply (X& v, const Y& d), before preconditioner.apply(v,d);; 
67 schwarz.hh:virtual void apply (X& v, const Y& d), before communication.copyOwnerToAll(v,v); 
67 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
67: communicator.hh::BufferedCommunicator::forward, enter sendRecv
67: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 13
67: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 13
67: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 13
67: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 13
67: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 13
67: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 13
67: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 13
67: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 13
67: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 13
32 === BiCGSTABSolver
 Iter          Defect            Rate
    0      1.17536e-06
32  (it = 0.5; it < _maxit; it+=.5) 0.5 250
32  before p = r 
32  before  y = W^-1 * p 
32  before _prec->apply(y,p); 0.5 250
32 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
32void AMG<M,X,S,PI,A>::mgc(LevelContext& levelContext), before presmooth(levelContext, preSteps_) 
void presmooth(LevelContext& levelContext, size_t steps), before SmootherApplier<typename LevelContext::SmootherType>::preSmooth, step: 0 of 2
void preSmooth(Smoother& smoother, Domain& v, const Range& d), before smoother.apply(v,d);
32 schwarz.hh:virtual void apply (X& v, const Y& d), before preconditioner.apply(v,d);; 
32 schwarz.hh:virtual void apply (X& v, const Y& d), before communication.copyOwnerToAll(v,v); 
32 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
32: communicator.hh::BufferedCommunicator::forward, enter sendRecv
32: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 16
32: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 16
32: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 16
32: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 16
32: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 16
32: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 16
32: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 16
32: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 16
32: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 16
32: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 16
32: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 10 16
32: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 11 16
32: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 12 16
32: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 13 16
32: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 14 16
32: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 15 16
32: communicator.hh::sendRecv, left MPI_Wait
12 schwarz.hh:virtual void apply (X& v, const Y& d), before communication.copyOwnerToAll(v,v); 
12 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
12: communicator.hh::BufferedCommunicator::forward, enter sendRecv
12: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 26
12: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 26
12: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 26
12: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 26
12: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 26
12: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 26
12: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 26
12: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 26
12: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 26
12: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 26
12: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 10 26
12: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 11 26
12: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 12 26
12: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 13 26
12: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 14 26
12: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 15 26
12: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 16 26
12: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 17 26
12: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 18 26
12: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 19 26
12: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 20 26
12: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 21 26
12: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 22 26
12: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 23 26
12: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 24 26
12: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 25 26
12: communicator.hh::sendRecv, left MPI_Wait
105 === BiCGSTABSolver
 Iter          Defect            Rate
    0      1.17536e-06
105  (it = 0.5; it < _maxit; it+=.5) 0.5 250
105  before p = r 
105  before  y = W^-1 * p 
105  before _prec->apply(y,p); 0.5 250
105 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
105void AMG<M,X,S,PI,A>::mgc(LevelContext& levelContext), before presmooth(levelContext, preSteps_) 
void presmooth(LevelContext& levelContext, size_t steps), before SmootherApplier<typename LevelContext::SmootherType>::preSmooth, step: 0 of 2
void preSmooth(Smoother& smoother, Domain& v, const Range& d), before smoother.apply(v,d);
105 schwarz.hh:virtual void apply (X& v, const Y& d), before preconditioner.apply(v,d);; 
105 schwarz.hh:virtual void apply (X& v, const Y& d), before communication.copyOwnerToAll(v,v); 
105 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
105: communicator.hh::BufferedCommunicator::forward, enter sendRecv
105: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 9
105: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 9
105: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 9
105: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 9
105: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 9
105: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 9
105: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 9
105: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 9
105: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 9
105: communicator.hh::sendRecv, left MPI_Wait
105: leave BufferedCommunicator::sendRecv
105: communicator.hh::BufferedCommunicator::forward, left sendRecv
105 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), after communicator.free();
105 schwarz.hh:virtual void apply (X& v, const Y& d), AFTER communication.copyOwnerToAll(v,v); 
void preSmooth(Smoother& smoother, Domain& v, const Range& d), AFTER smoother.apply(v,d);
void presmooth(LevelContext& levelContext, size_t steps), AFTER SmootherApplier<typename LevelContext::SmootherType>::preSmooth, step: 0 of 2
void presmooth(LevelContext& levelContext, size_t steps), before update defec, step: 0 of 2
void presmooth(LevelContext& levelContext, size_t steps), before SmootherApplier<typename LevelContext::SmootherType>::preSmooth, step: 1 of 2
void preSmooth(Smoother& smoother, Domain& v, const Range& d), before smoother.apply(v,d);
105 schwarz.hh:virtual void apply (X& v, const Y& d), before preconditioner.apply(v,d);; 
105 schwarz.hh:virtual void apply (X& v, const Y& d), before communication.copyOwnerToAll(v,v); 
105 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
10: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 11
10: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 11
10: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 11
10: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 11
10: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 11
10: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 11
10: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 11
10: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 10 11
10: communicator.hh::sendRecv, left MPI_Wait
10: leave BufferedCommunicator::sendRecv
10: communicator.hh::BufferedCommunicator::forward, left sendRecv
10 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), after communicator.free();
10 schwarz.hh:virtual void apply (X& v, const Y& d), AFTER communication.copyOwnerToAll(v,v); 
void preSmooth(Smoother& smoother, Domain& v, const Range& d), AFTER smoother.apply(v,d);
void presmooth(LevelContext& levelContext, size_t steps), AFTER SmootherApplier<typename LevelContext::SmootherType>::preSmooth, step: 0 of 2
void presmooth(LevelContext& levelContext, size_t steps), before update defec, step: 0 of 2
void presmooth(LevelContext& levelContext, size_t steps), before SmootherApplier<typename LevelContext::SmootherType>::preSmooth, step: 1 of 2
void preSmooth(Smoother& smoother, Domain& v, const Range& d), before smoother.apply(v,d);
115 === BiCGSTABSolver
 Iter          Defect            Rate
    0      1.17536e-06
115  (it = 0.5; it < _maxit; it+=.5) 0.5 250
115  before p = r 
115  before  y = W^-1 * p 
115  before _prec->apply(y,p); 0.5 250
115 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
115void AMG<M,X,S,PI,A>::mgc(LevelContext& levelContext), before presmooth(levelContext, preSteps_) 
void presmooth(LevelContext& levelContext, size_t steps), before SmootherApplier<typename LevelContext::SmootherType>::preSmooth, step: 0 of 2
void preSmooth(Smoother& smoother, Domain& v, const Range& d), before smoother.apply(v,d);
115 schwarz.hh:virtual void apply (X& v, const Y& d), before preconditioner.apply(v,d);; 
115 schwarz.hh:virtual void apply (X& v, const Y& d), before communication.copyOwnerToAll(v,v); 
115 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
115: communicator.hh::BufferedCommunicator::forward, enter sendRecv
115: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 11
115: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 11
115: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 11
115: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 11
115: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 11
115: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 11
115: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 11
115: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 11
115: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 11
115: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 11
115: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 10 11
115: communicator.hh::sendRecv, left MPI_Wait
115: leave BufferedCommunicator::sendRecv
115: communicator.hh::BufferedCommunicator::forward, left sendRecv
115 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), after communicator.free();
115 schwarz.hh:virtual void apply (X& v, const Y& d), AFTER communication.copyOwnerToAll(v,v); 
void preSmooth(Smoother& smoother, Domain& v, const Range& d), AFTER smoother.apply(v,d);
void presmooth(LevelContext& levelContext, size_t steps), AFTER SmootherApplier<typename LevelContext::SmootherType>::preSmooth, step: 0 of 2
void presmooth(LevelContext& levelContext, size_t steps), before update defec, step: 0 of 2
void presmooth(LevelContext& levelContext, size_t steps), before SmootherApplier<typename LevelContext::SmootherType>::preSmooth, step: 1 of 2
void preSmooth(Smoother& smoother, Domain& v, const Range& d), before smoother.apply(v,d);
115 schwarz.hh:virtual void apply (X& v, const Y& d), before preconditioner.apply(v,d);; 
53 === BiCGSTABSolver
 Iter          Defect            Rate
    0      1.17536e-06
53  (it = 0.5; it < _maxit; it+=.5) 0.5 250
53  before p = r 
53  before  y = W^-1 * p 
53  before _prec->apply(y,p); 0.5 250
53 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
53void AMG<M,X,S,PI,A>::mgc(LevelContext& levelContext), before presmooth(levelContext, preSteps_) 
void presmooth(LevelContext& levelContext, size_t steps), before SmootherApplier<typename LevelContext::SmootherType>::preSmooth, step: 0 of 2
void preSmooth(Smoother& smoother, Domain& v, const Range& d), before smoother.apply(v,d);
53 schwarz.hh:virtual void apply (X& v, const Y& d), before preconditioner.apply(v,d);; 
53 schwarz.hh:virtual void apply (X& v, const Y& d), before communication.copyOwnerToAll(v,v); 
53 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
53: communicator.hh::BufferedCommunicator::forward, enter sendRecv
53: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 17
53: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 17
53: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 17
53: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 17
53: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 17
53: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 17
53: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 17
53: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 17
53: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 17
53: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 17
53: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 10 17
53: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 11 17
53: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 12 17
53: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 13 17
53: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 14 17
53: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 15 17
53: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 16 17
77 === BiCGSTABSolver
 Iter          Defect            Rate
    0      1.17536e-06
77  (it = 0.5; it < _maxit; it+=.5) 0.5 250
77  before p = r 
77  before  y = W^-1 * p 
77  before _prec->apply(y,p); 0.5 250
77 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
77void AMG<M,X,S,PI,A>::mgc(LevelContext& levelContext), before presmooth(levelContext, preSteps_) 
void presmooth(LevelContext& levelContext, size_t steps), before SmootherApplier<typename LevelContext::SmootherType>::preSmooth, step: 0 of 2
void preSmooth(Smoother& smoother, Domain& v, const Range& d), before smoother.apply(v,d);
77 schwarz.hh:virtual void apply (X& v, const Y& d), before preconditioner.apply(v,d);; 
77 schwarz.hh:virtual void apply (X& v, const Y& d), before communication.copyOwnerToAll(v,v); 
77 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
77: communicator.hh::BufferedCommunicator::forward, enter sendRecv
77: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 6
77: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 6
77: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 6
77: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 6
77: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 6
77: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 6
77: communicator.hh::sendRecv, left MPI_Wait
77: leave BufferedCommunicator::sendRecv
77: communicator.hh::BufferedCommunicator::forward, left sendRecv
77 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), after communicator.free();
77 schwarz.hh:virtual void apply (X& v, const Y& d), AFTER communication.copyOwnerToAll(v,v); 
void preSmooth(Smoother& smoother, Domain& v, const Range& d), AFTER smoother.apply(v,d);
void presmooth(LevelContext& levelContext, size_t steps), AFTER SmootherApplier<typename LevelContext::SmootherType>::preSmooth, step: 0 of 2
void presmooth(LevelContext& levelContext, size_t steps), before update defec, step: 0 of 2
88 === BiCGSTABSolver
 Iter          Defect            Rate
    0      1.17536e-06
88  (it = 0.5; it < _maxit; it+=.5) 0.5 250
88  before p = r 
88  before  y = W^-1 * p 
88  before _prec->apply(y,p); 0.5 250
88 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
88void AMG<M,X,S,PI,A>::mgc(LevelContext& levelContext), before presmooth(levelContext, preSteps_) 
void presmooth(LevelContext& levelContext, size_t steps), before SmootherApplier<typename LevelContext::SmootherType>::preSmooth, step: 0 of 2
void preSmooth(Smoother& smoother, Domain& v, const Range& d), before smoother.apply(v,d);
88 schwarz.hh:virtual void apply (X& v, const Y& d), before preconditioner.apply(v,d);; 
88 schwarz.hh:virtual void apply (X& v, const Y& d), before communication.copyOwnerToAll(v,v); 
88 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
88: communicator.hh::BufferedCommunicator::forward, enter sendRecv
88: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 21
88: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 21
88: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 21
88: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 21
88: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 21
88: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 21
88: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 21
88: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 21
88: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 21
88: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 21
88: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 10 21
88: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 11 21
88: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 12 21
88: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 13 21
88: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 14 21
88: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 15 21
88: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 16 21
88: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 17 21
11: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 17
11: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 17
11: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 17
11: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 17
11: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 17
11: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 17
11: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 17
11: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 17
11: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 17
11: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 10 17
11: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 11 17
11: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 12 17
11: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 13 17
11: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 14 17
11: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 15 17
11: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 16 17
11: communicator.hh::sendRecv, left MPI_Wait
11: leave BufferedCommunicator::sendRecv
11: communicator.hh::BufferedCommunicator::forward, left sendRecv
11 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), after communicator.free();
11 schwarz.hh:virtual void apply (X& v, const Y& d), AFTER communication.copyOwnerToAll(v,v); 
void preSmooth(Smoother& smoother, Domain& v, const Range& d), AFTER smoother.apply(v,d);
void presmooth(LevelContext& levelContext, size_t steps), AFTER SmootherApplier<typename LevelContext::SmootherType>::preSmooth, step: 0 of 2
void presmooth(LevelContext& levelContext, size_t steps), before update defec, step: 0 of 2
void presmooth(LevelContext& levelContext, size_t steps), before SmootherApplier<typename LevelContext::SmootherType>::preSmooth, step: 1 of 2
void preSmooth(Smoother& smoother, Domain& v, const Range& d), before smoother.apply(v,d);
11 schwarz.hh:virtual void apply (X& v, const Y& d), before preconditioner.apply(v,d);; 
113 === BiCGSTABSolver
 Iter          Defect            Rate
    0      1.17536e-06
113  (it = 0.5; it < _maxit; it+=.5) 0.5 250
113  before p = r 
113  before  y = W^-1 * p 
113  before _prec->apply(y,p); 0.5 250
113 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
113void AMG<M,X,S,PI,A>::mgc(LevelContext& levelContext), before presmooth(levelContext, preSteps_) 
void presmooth(LevelContext& levelContext, size_t steps), before SmootherApplier<typename LevelContext::SmootherType>::preSmooth, step: 0 of 2
void preSmooth(Smoother& smoother, Domain& v, const Range& d), before smoother.apply(v,d);
113 schwarz.hh:virtual void apply (X& v, const Y& d), before preconditioner.apply(v,d);; 
113 schwarz.hh:virtual void apply (X& v, const Y& d), before communication.copyOwnerToAll(v,v); 
113 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
113: communicator.hh::BufferedCommunicator::forward, enter sendRecv
113: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 18
113: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 18
113: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 18
113: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 18
113: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 18
113: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 18
113: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 18
113: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 18
113: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 18
113: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 18
113: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 10 18
113: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 11 18
113: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 12 18
113: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 13 18
113: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 14 18
113: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 15 18
113: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 16 18
113: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 17 18
113: communicator.hh::sendRecv, left MPI_Wait
112 === BiCGSTABSolver
 Iter          Defect            Rate
    0      1.17536e-06
112  (it = 0.5; it < _maxit; it+=.5) 0.5 250
112  before p = r 
112  before  y = W^-1 * p 
112  before _prec->apply(y,p); 0.5 250
112 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
112void AMG<M,X,S,PI,A>::mgc(LevelContext& levelContext), before presmooth(levelContext, preSteps_) 
void presmooth(LevelContext& levelContext, size_t steps), before SmootherApplier<typename LevelContext::SmootherType>::preSmooth, step: 0 of 2
void preSmooth(Smoother& smoother, Domain& v, const Range& d), before smoother.apply(v,d);
112 schwarz.hh:virtual void apply (X& v, const Y& d), before preconditioner.apply(v,d);; 
112 schwarz.hh:virtual void apply (X& v, const Y& d), before communication.copyOwnerToAll(v,v); 
112 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
112: communicator.hh::BufferedCommunicator::forward, enter sendRecv
112: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 5
112: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 5
112: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 5
112: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 5
112: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 5
112: communicator.hh::sendRecv, left MPI_Wait
112: leave BufferedCommunicator::sendRecv
112: communicator.hh::BufferedCommunicator::forward, left sendRecv
112 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), after communicator.free();
112 schwarz.hh:virtual void apply (X& v, const Y& d), AFTER communication.copyOwnerToAll(v,v); 
void preSmooth(Smoother& smoother, Domain& v, const Range& d), AFTER smoother.apply(v,d);
void presmooth(LevelContext& levelContext, size_t steps), AFTER SmootherApplier<typename LevelContext::SmootherType>::preSmooth, step: 0 of 2
void presmooth(LevelContext& levelContext, size_t steps), before update defec, step: 0 of 2
35 === BiCGSTABSolver
 Iter          Defect            Rate
    0      1.17536e-06
35  (it = 0.5; it < _maxit; it+=.5) 0.5 250
35  before p = r 
35  before  y = W^-1 * p 
35  before _prec->apply(y,p); 0.5 250
35 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
35void AMG<M,X,S,PI,A>::mgc(LevelContext& levelContext), before presmooth(levelContext, preSteps_) 
void presmooth(LevelContext& levelContext, size_t steps), before SmootherApplier<typename LevelContext::SmootherType>::preSmooth, step: 0 of 2
void preSmooth(Smoother& smoother, Domain& v, const Range& d), before smoother.apply(v,d);
35 schwarz.hh:virtual void apply (X& v, const Y& d), before preconditioner.apply(v,d);; 
35 schwarz.hh:virtual void apply (X& v, const Y& d), before communication.copyOwnerToAll(v,v); 
35 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
35: communicator.hh::BufferedCommunicator::forward, enter sendRecv
35: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 9
35: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 9
35: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 9
35: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 9
35: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 9
35: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 9
35: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 9
87 === BiCGSTABSolver
 Iter          Defect            Rate
    0      1.17536e-06
87  (it = 0.5; it < _maxit; it+=.5) 0.5 250
87  before p = r 
87  before  y = W^-1 * p 
87  before _prec->apply(y,p); 0.5 250
87 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
87void AMG<M,X,S,PI,A>::mgc(LevelContext& levelContext), before presmooth(levelContext, preSteps_) 
void presmooth(LevelContext& levelContext, size_t steps), before SmootherApplier<typename LevelContext::SmootherType>::preSmooth, step: 0 of 2
void preSmooth(Smoother& smoother, Domain& v, const Range& d), before smoother.apply(v,d);
87 schwarz.hh:virtual void apply (X& v, const Y& d), before preconditioner.apply(v,d);; 
87 schwarz.hh:virtual void apply (X& v, const Y& d), before communication.copyOwnerToAll(v,v); 
87 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
87: communicator.hh::BufferedCommunicator::forward, enter sendRecv
87: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 19
87: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 19
87: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 19
87: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 19
87: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 19
87: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 19
87: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 19
87: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 19
87: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 19
87: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 19
87: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 10 19
87: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 11 19
87: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 12 19
87: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 13 19
87: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 14 19
70 === BiCGSTABSolver
 Iter          Defect            Rate
    0      1.17536e-06
70  (it = 0.5; it < _maxit; it+=.5) 0.5 250
70  before p = r 
70  before  y = W^-1 * p 
70  before _prec->apply(y,p); 0.5 250
70 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
70void AMG<M,X,S,PI,A>::mgc(LevelContext& levelContext), before presmooth(levelContext, preSteps_) 
void presmooth(LevelContext& levelContext, size_t steps), before SmootherApplier<typename LevelContext::SmootherType>::preSmooth, step: 0 of 2
void preSmooth(Smoother& smoother, Domain& v, const Range& d), before smoother.apply(v,d);
70 schwarz.hh:virtual void apply (X& v, const Y& d), before preconditioner.apply(v,d);; 
94 === BiCGSTABSolver
 Iter          Defect            Rate
    0      1.17536e-06
94  (it = 0.5; it < _maxit; it+=.5) 0.5 250
94  before p = r 
94  before  y = W^-1 * p 
94  before _prec->apply(y,p); 0.5 250
94 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
94void AMG<M,X,S,PI,A>::mgc(LevelContext& levelContext), before presmooth(levelContext, preSteps_) 
void presmooth(LevelContext& levelContext, size_t steps), before SmootherApplier<typename LevelContext::SmootherType>::preSmooth, step: 0 of 2
void preSmooth(Smoother& smoother, Domain& v, const Range& d), before smoother.apply(v,d);
94 schwarz.hh:virtual void apply (X& v, const Y& d), before preconditioner.apply(v,d);; 
94 schwarz.hh:virtual void apply (X& v, const Y& d), before communication.copyOwnerToAll(v,v); 
94 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
94: communicator.hh::BufferedCommunicator::forward, enter sendRecv
94: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 21
94: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 21
94: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 21
94: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 21
94: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 21
94: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 21
94: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 21
94: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 21
94: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 21
94: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 21
94: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 10 21
94: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 11 21
94: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 12 21
94: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 13 21
94: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 14 21
94: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 15 21
94: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 16 21
94: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 17 21
94: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 18 21
94: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 19 21
94: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 20 21
94: communicator.hh::sendRecv, left MPI_Wait
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 14
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 14
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 14
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 14
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 14
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 14
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 14
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 14
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 14
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 14
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 10 14
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 11 14
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 12 14
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 13 14
4: communicator.hh::sendRecv, left MPI_Wait
119 === BiCGSTABSolver
 Iter          Defect            Rate
    0      1.17536e-06
119  (it = 0.5; it < _maxit; it+=.5) 0.5 250
119  before p = r 
119  before  y = W^-1 * p 
119  before _prec->apply(y,p); 0.5 250
119 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
119void AMG<M,X,S,PI,A>::mgc(LevelContext& levelContext), before presmooth(levelContext, preSteps_) 
void presmooth(LevelContext& levelContext, size_t steps), before SmootherApplier<typename LevelContext::SmootherType>::preSmooth, step: 0 of 2
void preSmooth(Smoother& smoother, Domain& v, const Range& d), before smoother.apply(v,d);
119 schwarz.hh:virtual void apply (X& v, const Y& d), before preconditioner.apply(v,d);; 
119 schwarz.hh:virtual void apply (X& v, const Y& d), before communication.copyOwnerToAll(v,v); 
119 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
119: communicator.hh::BufferedCommunicator::forward, enter sendRecv
119: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 13
119: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 13
119: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 13
119: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 13
119: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 13
119: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 13
119: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 13
119: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 13
119: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 13
119: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 13
119: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 10 13
119: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 11 13
119: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 12 13
119: communicator.hh::sendRecv, left MPI_Wait
119: leave BufferedCommunicator::sendRecv
119: communicator.hh::BufferedCommunicator::forward, left sendRecv
119 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), after communicator.free();
119 schwarz.hh:virtual void apply (X& v, const Y& d), AFTER communication.copyOwnerToAll(v,v); 
void preSmooth(Smoother& smoother, Domain& v, const Range& d), AFTER smoother.apply(v,d);
void presmooth(LevelContext& levelContext, size_t steps), AFTER SmootherApplier<typename LevelContext::SmootherType>::preSmooth, step: 0 of 2
void presmooth(LevelContext& levelContext, size_t steps), before update defec, step: 0 of 2
void presmooth(LevelContext& levelContext, size_t steps), before SmootherApplier<typename LevelContext::SmootherType>::preSmooth, step: 1 of 2
void preSmooth(Smoother& smoother, Domain& v, const Range& d), before smoother.apply(v,d);
119 schwarz.hh:virtual void apply (X& v, const Y& d), before preconditioner.apply(v,d);; 
119 schwarz.hh:virtual void apply (X& v, const Y& d), before communication.copyOwnerToAll(v,v); 
119 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
119: communicator.hh::BufferedCommunicator::forward, enter sendRecv
119: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 13
119: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, 2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 8
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 8
2: communicator.hh::sendRecv, left MPI_Wait
18 === BiCGSTABSolver
 Iter          Defect            Rate
    0      1.17536e-06
18  (it = 0.5; it < _maxit; it+=.5) 0.5 250
18  before p = r 
18  before  y = W^-1 * p 
18  before _prec->apply(y,p); 0.5 250
18 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
18void AMG<M,X,S,PI,A>::mgc(LevelContext& levelContext), before presmooth(levelContext, preSteps_) 
void presmooth(LevelContext& levelContext, size_t steps), before SmootherApplier<typename LevelContext::SmootherType>::preSmooth, step: 0 of 2
void preSmooth(Smoother& smoother, Domain& v, const Range& d), before smoother.apply(v,d);
18 schwarz.hh:virtual void apply (X& v, const Y& d), before preconditioner.apply(v,d);; 
18 schwarz.hh:virtual void apply (X& v, const Y& d), before communication.copyOwnerToAll(v,v); 
18 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
18: communicator.hh::BufferedCommunicator::forward, enter sendRecv
18: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 14
18: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 14
18: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 14
18: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 14
18: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 14
18: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 14
18: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 14
18: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 14
18: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 14
18: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 14
18: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 10 14
18: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 11 14
18: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 12 14
18: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 13 14
18: communicator.hh::sendRecv, left MPI_Wait
52 === BiCGSTABSolver
 Iter          Defect            Rate
    0      1.17536e-06
52  (it = 0.5; it < _maxit; it+=.5) 0.5 250
52  before p = r 
52  before  y = W^-1 * p 
52  before _prec->apply(y,p); 0.5 250
52 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
52void AMG<M,X,S,PI,A>::mgc(LevelContext& levelContext), before presmooth(levelContext, preSteps_) 
void presmooth(LevelContext& levelContext, size_t steps), before SmootherApplier<typename LevelContext::SmootherType>::preSmooth, step: 0 of 2
void preSmooth(Smoother& smoother, Domain& v, const Range& d), before smoother.apply(v,d);
52 schwarz.hh:virtual void apply (X& v, const Y& d), before preconditioner.apply(v,d);; 
52 schwarz.hh:virtual void apply (X& v, const Y& d), before communication.copyOwnerToAll(v,v); 
52 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
52: communicator.hh::BufferedCommunicator::forward, enter sendRecv
52: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 26
52: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 26
52: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 26
52: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 26
52: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 26
52: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 26
52: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 26
52: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 26
52: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 26
52: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 26
52: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 10 26
52: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 11 26
52: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 12 26
52: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 13 26
52: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 14 26
52: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 15 26
52: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 16 26
52: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 17 26
52: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 18 26
52: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 19 26
52: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 20 26
52: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 21 26
52: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 22 26
52: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 23 26
52: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 24 26
52: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &statu41 === BiCGSTABSolver
 Iter          Defect            Rate
    0      1.17536e-06
41  (it = 0.5; it < _maxit; it+=.5) 0.5 250
41  before p = r 
41  before  y = W^-1 * p 
41  before _prec->apply(y,p); 0.5 250
41 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
41void AMG<M,X,S,PI,A>::mgc(LevelContext& levelContext), before presmooth(levelContext, preSteps_) 
void presmooth(LevelContext& levelContext, size_t steps), before SmootherApplier<typename LevelContext::SmootherType>::preSmooth, step: 0 of 2
void preSmooth(Smoother& smoother, Domain& v, const Range& d), before smoother.apply(v,d);
41 schwarz.hh:virtual void apply (X& v, const Y& d), before preconditioner.apply(v,d);; 
41 schwarz.hh:virtual void apply (X& v, const Y& d), before communication.copyOwnerToAll(v,v); 
41 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
41: communicator.hh::BufferedCommunicator::forward, enter sendRecv
41: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 20
41: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 20
41: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 20
41: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 20
41: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 20
41: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 20
41: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 20
41: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 20
41: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 20
41: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 20
41: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 10 20
41: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 11 20
41: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 12 20
41: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 13 20
41: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 14 20
41: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 15 20
41: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 16 20
41: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 17 20
36 === BiCGSTABSolver
 Iter          Defect            Rate
    0      1.17536e-06
36  (it = 0.5; it < _maxit; it+=.5) 0.5 250
36  before p = r 
36  before  y = W^-1 * p 
36  before _prec->apply(y,p); 0.5 250
36 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
36void AMG<M,X,S,PI,A>::mgc(LevelContext& levelContext), before presmooth(levelContext, preSteps_) 
void presmooth(LevelContext& levelContext, size_t steps), before SmootherApplier<typename LevelContext::SmootherType>::preSmooth, step: 0 of 2
void preSmooth(Smoother& smoother, Domain& v, const Range& d), before smoother.apply(v,d);
36 schwarz.hh:virtual void apply (X& v, const Y& d), before preconditioner.apply(v,d);; 
40 === BiCGSTABSolver
 Iter          Defect            Rate
    0      1.17536e-06
40  (it = 0.5; it < _maxit; it+=.5) 0.5 250
40  before p = r 
40  before  y = W^-1 * p 
40  before _prec->apply(y,p); 0.5 250
40 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
40void AMG<M,X,S,PI,A>::mgc(LevelContext& levelContext), before presmooth(levelContext, preSteps_) 
void presmooth(LevelContext& levelContext, size_t steps), before SmootherApplier<typename LevelContext::SmootherType>::preSmooth, step: 0 of 2
void preSmooth(Smoother& smoother, Domain& v, const Range& d), before smoother.apply(v,d);
40 schwarz.hh:virtual void apply (X& v, const Y& d), before preconditioner.apply(v,d);; 
40 schwarz.hh:virtual void apply (X& v, const Y& d), before communication.copyOwnerToAll(v,v); 
40 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
40: communicator.hh::BufferedCommunicator::forward, enter sendRecv
40: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 15
40: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 15
40: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 15
40: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 15
40: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 15
40: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 15
40: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 15
40: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 15
40: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 15
40: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 15
40: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 10 15
40: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 11 15
40: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 12 15
40: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 13 15
40: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 14 15
26 === BiCGSTABSolver
 Iter          Defect            Rate
    0      1.17536e-06
26  (it = 0.5; it < _maxit; it+=.5) 0.5 250
26  before p = r 
26  before  y = W^-1 * p 
26  before _prec->apply(y,p); 0.5 250
26 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
26void AMG<M,X,S,PI,A>::mgc(LevelContext& levelContext), before presmooth(levelContext, preSteps_) 
void presmooth(LevelContext& levelContext, size_t steps), before SmootherApplier<typename LevelContext::SmootherType>::preSmooth, step: 0 of 2
void preSmooth(Smoother& smoother, Domain& v, const Range& d), before smoother.apply(v,d);
26 schwarz.hh:virtual void apply (X& v, const Y& d), before preconditioner.apply(v,d);; 
26 schwarz.hh:virtual void apply (X& v, const Y& d), before communication.copyOwnerToAll(v,v); 
26 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
26: communicator.hh::BufferedCommunicator::forward, enter sendRecv
26: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 26
26: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 26
26: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 26
26: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 26
26: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 26
26: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 26
26: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 26
26: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 26
26: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 26
26: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 26
26: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 10 26
26: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 11 26
26: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 12 26
26: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 13 26
26: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 14 26
26: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 15 26
26: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 16 26
26: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 17 26
26: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 18 26
26: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 19 26
26: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 20 26
26: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 21 26
26: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 22 26
26: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 23 26
26: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 24 26
26: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &statu9: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 7
9: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 7
9: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 7
9: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 7
9: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 7
9: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 7
9: communicator.hh::sendRecv, left MPI_Wait
9: leave BufferedCommunicator::sendRecv
9: communicator.hh::BufferedCommunicator::forward, left sendRecv
9 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), after communicator.free();
9 schwarz.hh:virtual void apply (X& v, const Y& d), AFTER communication.copyOwnerToAll(v,v); 
void preSmooth(Smoother& smoother, Domain& v, const Range& d), AFTER smoother.apply(v,d);
void presmooth(LevelContext& levelContext, size_t steps), AFTER SmootherApplier<typename LevelContext::SmootherType>::preSmooth, step: 0 of 2
void presmooth(LevelContext& levelContext, size_t steps), before update defec, step: 0 of 2
void presmooth(LevelContext& levelContext, size_t steps), before SmootherApplier<typename LevelContext::SmootherType>::preSmooth, step: 1 of 2
void preSmooth(Smoother& smoother, Domain& v, const Range& d), before smoother.apply(v,d);
9 schwarz.hh:virtual void apply (X& v, const Y& d), before preconditioner.apply(v,d);; 
9 schwarz.hh:virtual void apply (X& v, const Y& d), before communication.copyOwnerToAll(v,v); 
9 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
9: communicator.hh::BufferedCommunicator::forward, enter sendRecv
9: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 7
23 === BiCGSTABSolver
 Iter          Defect            Rate
    0      1.17536e-06
23  (it = 0.5; it < _maxit; it+=.5) 0.5 250
23  before p = r 
23  before  y = W^-1 * p 
23  before _prec->apply(y,p); 0.5 250
23 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
23void AMG<M,X,S,PI,A>::mgc(LevelContext& levelContext), before presmooth(levelContext, preSteps_) 
void presmooth(LevelContext& levelContext, size_t steps), before SmootherApplier<typename LevelContext::SmootherType>::preSmooth, step: 0 of 2
void preSmooth(Smoother& smoother, Domain& v, const Range& d), before smoother.apply(v,d);
23 schwarz.hh:virtual void apply (X& v, const Y& d), before preconditioner.apply(v,d);; 
23 schwarz.hh:virtual void apply (X& v, const Y& d), before communication.copyOwnerToAll(v,v); 
23 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
23: communicator.hh::BufferedCommunicator::forward, enter sendRecv
23: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 11
23: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 11
23: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 11
23: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 11
23: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 11
23: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 11
23: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 11
23: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 11
23: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 11
23: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 11
23: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 10 11
23: communicator.hh::sendRecv, left MPI_Wait
23: leave BufferedCommunicator::sendRecv
23: communicator.hh::BufferedCommunicator::forward, left sendRecv
23 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), after communicator.free();
23 schwarz.hh:virtual void apply (X& v, const Y& d), AFTER communication.copyOwnerToAll(v,v); 
void preSmooth(Smoother& smoother, Domain& v, const Range& d), AFTER smoother.apply(v,d);
void presmooth(LevelContext& levelContext, size_t steps), AFTER SmootherApplier<typename LevelContext::SmootherType>::preSmooth, step: 0 of 2
void presmooth(LevelContext& levelContext, size_t steps), before update defec, step: 0 of 2
void presmooth(LevelContext& levelContext, size_t steps), before SmootherApplier<typename LevelContext::SmootherType>::preSmooth, step: 1 of 2
void preSmooth(Smoother& smoother, Domain& v, const Range& d), before smoother.apply(v,d);
23 schwarz.hh:virtual void apply (X& v, const Y& d), before preconditioner.apply(v,d);; 
23 schwarz.hh:virtual void apply (X& v, const Y& d), before communication.copyOwnerToAll(v,v); 
23 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
23: communicator.hh::BufferedCommunicator::forward, enter sendRecv
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 11
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 11
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 11
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 11
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 11
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 11
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 11
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 11
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 11
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 11
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 10 11
5: communicator.hh::sendRecv, left MPI_Wait
5: leave BufferedCommunicator::sendRecv
5: communicator.hh::BufferedCommunicator::forward, left sendRecv
5 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), after communicator.free();
5 schwarz.hh:virtual void apply (X& v, const Y& d), AFTER communication.copyOwnerToAll(v,v); 
void preSmooth(Smoother& smoother, Domain& v, const Range& d), AFTER smoother.apply(v,d);
void presmooth(LevelContext& levelContext, size_t steps), AFTER SmootherApplier<typename LevelContext::SmootherType>::preSmooth, step: 0 of 2
void presmooth(LevelContext& levelContext, size_t steps), before update defec, step: 0 of 2
void presmooth(LevelContext& levelContext, size_t steps), before SmootherApplier<typename LevelContext::SmootherType>::preSmooth, step: 1 of 2
void preSmooth(Smoother& smoother, Domain& v, const Range& d), before smoother.apply(v,d);
5 schwarz.hh:virtual void apply (X& v, const Y& d), before preconditioner.apply(v,d);; 
46 === BiCGSTABSolver
 Iter          Defect            Rate
    0      1.17536e-06
46  (it = 0.5; it < _maxit; it+=.5) 0.5 250
46  before p = r 
46  before  y = W^-1 * p 
46  before _prec->apply(y,p); 0.5 250
46 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
46void AMG<M,X,S,PI,A>::mgc(LevelContext& levelContext), before presmooth(levelContext, preSteps_) 
void presmooth(LevelContext& levelContext, size_t steps), before SmootherApplier<typename LevelContext::SmootherType>::preSmooth, step: 0 of 2
void preSmooth(Smoother& smoother, Domain& v, const Range& d), before smoother.apply(v,d);
46 schwarz.hh:virtual void apply (X& v, const Y& d), before preconditioner.apply(v,d);; 
46 schwarz.hh:virtual void apply (X& v, const Y& d), before communication.copyOwnerToAll(v,v); 
46 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
46: communicator.hh::BufferedCommunicator::forward, enter sendRecv
46: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 12
46: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 12
46: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 12
46: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 12
46: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 12
46: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 12
46: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 12
46: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 12
46: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 12
46: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 12
46: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 10 12
46: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 11 12
46: communicator.hh::sendRecv, left MPI_Wait
46: leave BufferedCommunicator::sendRecv
46: communicator.hh::BufferedCommunicator::forward, left sendRecv
46 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), after communicator.free();
46 schwarz.hh:virtual void apply (X& v, const Y& d), AFTER communication.copyOwnerToAll(v,v); 
void preSmooth(Smoother& smoother, Domain& v, const Range& d), AFTER smoother.apply(v,d);
void presmooth(LevelContext& levelContext, size_t steps), AFTER SmootherApplier<typename LevelContext::SmootherType>::preSmooth, step: 0 of 2
void presmooth(LevelContext& levelContext, size_t steps), before update defec, step: 0 of 2
void presmooth(LevelContext& levelContext, size_t steps), before SmootherApplier<typename LevelContext::SmootherType>::preSmooth, step: 1 of 2
void preSmooth(Smoother& smoother, Domain& v, const Range& d), before smoother.apply(v,d);
46 schwarz.hh:virtual void apply (X& v, const Y& d), before preconditioner.apply(v,d);; 
51 === BiCGSTABSolver
 Iter          Defect            Rate
    0      1.17536e-06
51  (it = 0.5; it < _maxit; it+=.5) 0.5 250
51  before p = r 
51  before  y = W^-1 * p 
51  before _prec->apply(y,p); 0.5 250
51 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
51void AMG<M,X,S,PI,A>::mgc(LevelContext& levelContext), before presmooth(levelContext, preSteps_) 
void presmooth(LevelContext& levelContext, size_t steps), before SmootherApplier<typename LevelContext::SmootherType>::preSmooth, step: 0 of 2
void preSmooth(Smoother& smoother, Domain& v, const Range& d), before smoother.apply(v,d);
51 schwarz.hh:virtual void apply (X& v, const Y& d), before preconditioner.apply(v,d);; 
51 schwarz.hh:virtual void apply (X& v, const Y& d), before communication.copyOwnerToAll(v,v); 
51 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
51: communicator.hh::BufferedCommunicator::forward, enter sendRecv
51: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 10
51: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 10
51: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 10
51: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 10
51: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 10
51: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 10
51: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 10
51: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 10
51: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 10
63 === BiCGSTABSolver
 Iter          Defect            Rate
    0      1.17536e-06
63  (it = 0.5; it < _maxit; it+=.5) 0.5 250
63  before p = r 
63  before  y = W^-1 * p 
63  before _prec->apply(y,p); 0.5 250
63 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
63void AMG<M,X,S,PI,A>::mgc(LevelContext& levelContext), before presmooth(levelContext, preSteps_) 
void presmooth(LevelContext& levelContext, size_t steps), before SmootherApplier<typename LevelContext::SmootherType>::preSmooth, step: 0 of 2
void preSmooth(Smoother& smoother, Domain& v, const Range& d), before smoother.apply(v,d);
63 schwarz.hh:virtual void apply (X& v, const Y& d), before preconditioner.apply(v,d);; 
63 schwarz.hh:virtual void apply (X& v, const Y& d), before communication.copyOwnerToAll(v,v); 
63 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
63: communicator.hh::BufferedCommunicator::forward, enter sendRecv
63: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 9
63: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 9
63: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 9
63: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 9
63: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 9
63: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 9
63: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 9
63: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 9
106 === BiCGSTABSolver
 Iter          Defect            Rate
    0      1.17536e-06
106  (it = 0.5; it < _maxit; it+=.5) 0.5 250
106  before p = r 
106  before  y = W^-1 * p 
106  before _prec->apply(y,p); 0.5 250
106 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
106void AMG<M,X,S,PI,A>::mgc(LevelContext& levelContext), before presmooth(levelContext, preSteps_) 
void presmooth(LevelContext& levelContext, size_t steps), before SmootherApplier<typename LevelContext::SmootherType>::preSmooth, step: 0 of 2
void preSmooth(Smoother& smoother, Domain& v, const Range& d), before smoother.apply(v,d);
106 schwarz.hh:virtual void apply (X& v, const Y& d), before preconditioner.apply(v,d);; 
106 schwarz.hh:virtual void apply (X& v, const Y& d), before communication.copyOwnerToAll(v,v); 
106 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
106: communicator.hh::BufferedCommunicator::forward, enter sendRecv
106: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 16
106: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 16
106: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 16
106: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 16
106: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 16
106: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 16
106: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 16
106: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 16
106: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 16
106: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 16
106: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 10 16
106: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 11 16
106: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 12 16
106: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 13 16
106: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 14 16
106: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 15 16
106: communicator.hh::sendRecv, left MPI_Wait
106: leave BufferedCommunicator::sendRecv
106: communicator.hh::BufferedCommunicator::forward, left sendRecv
106 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), after communicator.free();
106 schwarz.hh:virtual void apply (X& v, const Y& d), AFTER communication.copyOwnerToAll(v,v); 
void preSmooth(Smoother& smoother, Domain& v, const Range& d), AFTER smoother.apply(v,d);
void presmooth(LevelContext& levelContext, size_t steps), AFTER SmootherApplier<typename LevelContext::SmootherType>::preSmooth, step: 0 of 2
void presmooth(LevelContext& levelContext, size_t steps), before update defec, step: 0 of 2
void presmooth(LevelContext& levelContext, size_t steps), before SmootherApplier<typename LevelContext::SmootherType>::preSmooth, step: 1 of 2
void preSmooth(Smoother& smoother, Domain& v, const Range& d), before smoother.apply(v,d);
106 schwarz.hh:virtual void apply (X& v, const Y& d), before preconditioner.apply(v,d);; 
54 === BiCGSTABSolver
 Iter          Defect            Rate
    0      1.17536e-06
54  (it = 0.5; it < _maxit; it+=.5) 0.5 250
54  before p = r 
54  before  y = W^-1 * p 
54  before _prec->apply(y,p); 0.5 250
54 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
54void AMG<M,X,S,PI,A>::mgc(LevelContext& levelContext), before presmooth(levelContext, preSteps_) 
void presmooth(LevelContext& levelContext, size_t steps), before SmootherApplier<typename LevelContext::SmootherType>::preSmooth, step: 0 of 2
void preSmooth(Smoother& smoother, Domain& v, const Range& d), before smoother.apply(v,d);
54 schwarz.hh:virtual void apply (X& v, const Y& d), before preconditioner.apply(v,d);; 
54 schwarz.hh:virtual void apply (X& v, const Y& d), before communication.copyOwnerToAll(v,v); 
54 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
54: communicator.hh::BufferedCommunicator::forward, enter sendRecv
54: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 18
54: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 18
54: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 18
54: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 18
54: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 18
54: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 18
54: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 18
54: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 18
54: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 18
54: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 18
54: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 10 18
54: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 11 18
54: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 12 18
54: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 13 18
54: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 14 18
54: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 15 18
54: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 16 18
54: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 17 18
15 === BiCGSTABSolver
 Iter          Defect            Rate
    0      1.17536e-06
15  (it = 0.5; it < _maxit; it+=.5) 0.5 250
15  before p = r 
15  before  y = W^-1 * p 
15  before _prec->apply(y,p); 0.5 250
15 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
15void AMG<M,X,S,PI,A>::mgc(LevelContext& levelContext), before presmooth(levelContext, preSteps_) 
void presmooth(LevelContext& levelContext, size_t steps), before SmootherApplier<typename LevelContext::SmootherType>::preSmooth, step: 0 of 2
void preSmooth(Smoother& smoother, Domain& v, const Range& d), before smoother.apply(v,d);
15 schwarz.hh:virtual void apply (X& v, const Y& d), before preconditioner.apply(v,d);; 
15 schwarz.hh:virtual void apply (X& v, const Y& d), before communication.copyOwnerToAll(v,v); 
15 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
15: communicator.hh::BufferedCommunicator::forward, enter sendRecv
15: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 8
15: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 8
15: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 8
15: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 8
15: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 8
15: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 8
15: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 8
15: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 8
15: communicator.hh::sendRecv, left MPI_Wait
15: leave BufferedCommunicator::sendRecv
15: communicator.hh::BufferedCommunicator::forward, left sendRecv
15 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), after communicator.free();
15 schwarz.hh:virtual void apply (X& v, const Y& d), AFTER communication.copyOwnerToAll(v,v); 
void preSmooth(Smoother& smoother, Domain& v, const Range& d), AFTER smoother.apply(v,d);
void presmooth(LevelContext& levelContext, size_t steps), AFTER SmootherApplier<typename LevelContext::SmootherType>::preSmooth, step: 0 of 2
void presmooth(LevelContext& levelContext, size_t steps), before update defec, step: 0 of 2
void presmooth(LevelContext& levelContext, size_t steps), before SmootherApplier<typename LevelContext::SmootherType>::preSmooth, step: 1 of 2
void preSmooth(Smoother& smoother, Domain& v, const Range& d), before smoother.apply(v,d);
15 schwarz.hh:virtual void apply (X& v, const Y& d), before preconditioner.apply(v,d);; 
15 schwarz.hh:virtual void apply (X& v, const Y& d), before communication.copyOwnerToAll(v,v); 
15 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
15: communicator.hh::BufferedCommunicator::forward, enter sendRecv
15: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 8
13: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 11
13: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 11
13: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 11
13: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 11
13: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 11
13: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 11
13: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 10 11
13: communicator.hh::sendRecv, left MPI_Wait
13: leave BufferedCommunicator::sendRecv
13: communicator.hh::BufferedCommunicator::forward, left sendRecv
13 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), after communicator.free();
13 schwarz.hh:virtual void apply (X& v, const Y& d), AFTER communication.copyOwnerToAll(v,v); 
void preSmooth(Smoother& smoother, Domain& v, const Range& d), AFTER smoother.apply(v,d);
void presmooth(LevelContext& levelContext, size_t steps), AFTER SmootherApplier<typename LevelContext::SmootherType>::preSmooth, step: 0 of 2
void presmooth(LevelContext& levelContext, size_t steps), before update defec, step: 0 of 2
void presmooth(LevelContext& levelContext, size_t steps), before SmootherApplier<typename LevelContext::SmootherType>::preSmooth, step: 1 of 2
void preSmooth(Smoother& smoother, Domain& v, const Range& d), before smoother.apply(v,d);
13 schwarz.hh:virtual void apply (X& v, const Y& d), before preconditioner.apply(v,d);; 
81 === BiCGSTABSolver
 Iter          Defect            Rate
    0      1.17536e-06
81  (it = 0.5; it < _maxit; it+=.5) 0.5 250
81  before p = r 
81  before  y = W^-1 * p 
81  before _prec->apply(y,p); 0.5 250
81 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
81void AMG<M,X,S,PI,A>::mgc(LevelContext& levelContext), before presmooth(levelContext, preSteps_) 
void presmooth(LevelContext& levelContext, size_t steps), before SmootherApplier<typename LevelContext::SmootherType>::preSmooth, step: 0 of 2
void preSmooth(Smoother& smoother, Domain& v, const Range& d), before smoother.apply(v,d);
81 schwarz.hh:virtual void apply (X& v, const Y& d), before preconditioner.apply(v,d);; 
81 schwarz.hh:virtual void apply (X& v, const Y& d), before communication.copyOwnerToAll(v,v); 
81 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
81: communicator.hh::BufferedCommunicator::forward, enter sendRecv
81: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 11
81: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 11
81: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 11
81: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 11
81: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 11
81: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 11
81: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 11
81: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 11
81: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 11
81: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 11
81: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 10 11
81: communicator.hh::sendRecv, left MPI_Wait
81: leave BufferedCommunicator::sendRecv
81: communicator.hh::BufferedCommunicator::forward, left sendRecv
81 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), after communicator.free();
81 schwarz.hh:virtual void apply (X& v, const Y& d), AFTER communication.copyOwnerToAll(v,v); 
void preSmooth(Smoother& smoother, Domain& v, const Range& d), AFTER smoother.apply(v,d);
void presmooth(LevelContext& levelContext, size_t steps), AFTER SmootherApplier<typename LevelContext::SmootherType>::preSmooth, step: 0 of 2
void presmooth(LevelContext& levelContext, size_t steps), before update defec, step: 0 of 2
void presmooth(LevelContext& levelContext, size_t steps), before SmootherApplier<typename LevelContext::SmootherType>::preSmooth, step: 1 of 2
void preSmooth(Smoother& smoother, Domain& v, const Range& d), before smoother.apply(v,d);
81 schwarz.hh:virtual void apply (X& v, const Y& d), before preconditioner.apply(v,d);; 
81 schwarz.hh:virtual void apply (X& v, const Y& d), before communication.copyOwnerToAll(v,v); 
81 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
81: communicator.hh::BufferedCommunicator::forward, enter sendRecv
81: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 11
93 === BiCGSTABSolver
 Iter          Defect            Rate
    0      1.17536e-06
93  (it = 0.5; it < _maxit; it+=.5) 0.5 250
93  before p = r 
93  before  y = W^-1 * p 
93  before _prec->apply(y,p); 0.5 250
93 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
93void AMG<M,X,S,PI,A>::mgc(LevelContext& levelContext), before presmooth(levelContext, preSteps_) 
void presmooth(LevelContext& levelContext, size_t steps), before SmootherApplier<typename LevelContext::SmootherType>::preSmooth, step: 0 of 2
void preSmooth(Smoother& smoother, Domain& v, const Range& d), before smoother.apply(v,d);
93 schwarz.hh:virtual void apply (X& v, const Y& d), before preconditioner.apply(v,d);; 
93 schwarz.hh:virtual void apply (X& v, const Y& d), before communication.copyOwnerToAll(v,v); 
93 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
93: communicator.hh::BufferedCommunicator::forward, enter sendRecv
93: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 13
93: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 13
93: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 13
93: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 13
93: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 13
93: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 13
93: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 13
93: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 13
93: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 13
93: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 13
93: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 10 13
93: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 11 13
93: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 12 13
93: communicator.hh::sendRecv, left MPI_Wait
93: leave BufferedCommunicator::sendRecv
93: communicator.hh::BufferedCommunicator::forward, left sendRecv
93 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), after communicator.free();
93 schwarz.hh:virtual void apply (X& v, const Y& d), AFTER communication.copyOwnerToAll(v,v); 
void preSmooth(Smoother& smoother, Domain& v, const Range& d), AFTER smoother.apply(v,d);
void presmooth(LevelContext& levelContext, size_t steps), AFTER SmootherApplier<typename LevelContext::SmootherType>::preSmooth, step: 0 of 2
void presmooth(LevelContext& levelContext, size_t steps), before update defec, step: 0 of 2
void presmooth(LevelContext& levelContext, size_t steps), before SmootherApplier<typename LevelContext::SmootherType>::preSmooth, step: 1 of 2
void preSmooth(Smoother& smoother, Domain& v, const Range& d), before smoother.apply(v,d);
93 schwarz.hh:virtual void apply (X& v, const Y& d), before preconditioner.apply(v,d);; 
93 schwarz.hh:virtual void apply (X& v, const Y& d), before communication.copyOwnerToAll(v,v); 
93 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
93: communicator.hh::BufferedCommunicator::forward, enter sendRecv
93: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 13
93: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 13
93: commu110 === BiCGSTABSolver
 Iter          Defect            Rate
    0      1.17536e-06
110  (it = 0.5; it < _maxit; it+=.5) 0.5 250
110  before p = r 
110  before  y = W^-1 * p 
110  before _prec->apply(y,p); 0.5 250
110 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
110void AMG<M,X,S,PI,A>::mgc(LevelContext& levelContext), before presmooth(levelContext, preSteps_) 
void presmooth(LevelContext& levelContext, size_t steps), before SmootherApplier<typename LevelContext::SmootherType>::preSmooth, step: 0 of 2
void preSmooth(Smoother& smoother, Domain& v, const Range& d), before smoother.apply(v,d);
110 schwarz.hh:virtual void apply (X& v, const Y& d), before preconditioner.apply(v,d);; 
110 schwarz.hh:virtual void apply (X& v, const Y& d), before communication.copyOwnerToAll(v,v); 
110 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
110: communicator.hh::BufferedCommunicator::forward, enter sendRecv
110: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 11
110: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 11
110: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 11
110: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 11
110: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 11
110: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 11
110: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 11
110: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 11
110: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 11
110: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 11
110: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 10 11
110: communicator.hh::sendRecv, left MPI_Wait
110: leave BufferedCommunicator::sendRecv
110: communicator.hh::BufferedCommunicator::forward, left sendRecv
110 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), after communicator.free();
110 schwarz.hh:virtual void apply (X& v, const Y& d), AFTER communication.copyOwnerToAll(v,v); 
void preSmooth(Smoother& smoother, Domain& v, const Range& d), AFTER smoother.apply(v,d);
void presmooth(LevelContext& levelContext, size_t steps), AFTER SmootherApplier<typename LevelContext::SmootherType>::preSmooth, step: 0 of 2
void presmooth(LevelContext& levelContext, size_t steps), before update defec, step: 0 of 2
void presmooth(LevelContext& levelContext, size_t steps), before SmootherApplier<typename LevelContext::SmootherType>::preSmooth, step: 1 of 2
void preSmooth(Smoother& smoother, Domain& v, const Range& d), before smoother.apply(v,d);
110 schwarz.hh:virtual void apply (X& v, const Y& d), before preconditioner.apply(v,d);; 
110 schwarz.hh:virtual void apply (X& v, const Y& d), before communication.copyOwnerToAll(v,v); 
110 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
110: communicator.hh::BufferedCommunicator::forward, enter sendRecv
110: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 11
45 === BiCGSTABSolver
 Iter          Defect            Rate
    0      1.17536e-06
45  (it = 0.5; it < _maxit; it+=.5) 0.5 250
45  before p = r 
45  before  y = W^-1 * p 
45  before _prec->apply(y,p); 0.5 250
45 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
45void AMG<M,X,S,PI,A>::mgc(LevelContext& levelContext), before presmooth(levelContext, preSteps_) 
void presmooth(LevelContext& levelContext, size_t steps), before SmootherApplier<typename LevelContext::SmootherType>::preSmooth, step: 0 of 2
void preSmooth(Smoother& smoother, Domain& v, const Range& d), before smoother.apply(v,d);
45 schwarz.hh:virtual void apply (X& v, const Y& d), before preconditioner.apply(v,d);; 
45 schwarz.hh:virtual void apply (X& v, const Y& d), before communication.copyOwnerToAll(v,v); 
45 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
45: communicator.hh::BufferedCommunicator::forward, enter sendRecv
45: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 17
45: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 17
45: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 17
45: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 17
45: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 17
45: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 17
45: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 17
45: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 17
45: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 17
45: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 17
45: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 10 17
45: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 11 17
45: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 12 17
45: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 13 17
45: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 14 17
45: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 15 17
45: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 16 17
45: communicator.hh::sendRecv, left MPI_Wait
45: leave BufferedCommunicator::sendRecv
45: communicator.hh::BufferedCommunicator::forward, left sendRecv
45 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), after communicator.free();
45 schwarz.hh:virtual void apply (X& v, const Y& d), AFTER communication.copyOwnerToAll(v,v); 
void preSmooth(Smoother& smoother, Domain& v, const Range& d), AFTER smoother.apply(v,d);
void presmooth(LevelContext& levelContext, size_t steps), AFTER SmootherApplier<typename LevelContext::SmootherType>::preSmooth, step: 0 of 2
void presmooth(LevelContext& levelContext, size_t steps), before update defec, step: 0 of 2
void presmooth(LevelContext& levelContext, size_t steps), before SmootherApplier<typename LevelContext::SmootherType>::preSmooth, step: 1 of 2
void preSmooth(Smoother& smoother, Domain& v, const Range& d), before smoother.apply(v,d);
45 schwarz.hh:virtual void apply (X& v, const Y& d), before preconditioner.apply(v,d);; 
45 schwarz.hh:virtual void apply (X& v, const Y& d), before communication68 === BiCGSTABSolver
 Iter          Defect            Rate
    0      1.17536e-06
68  (it = 0.5; it < _maxit; it+=.5) 0.5 250
68  before p = r 
68  before  y = W^-1 * p 
68  before _prec->apply(y,p); 0.5 250
68 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
68void AMG<M,X,S,PI,A>::mgc(LevelContext& levelContext), before presmooth(levelContext, preSteps_) 
void presmooth(LevelContext& levelContext, size_t steps), before SmootherApplier<typename LevelContext::SmootherType>::preSmooth, step: 0 of 2
void preSmooth(Smoother& smoother, Domain& v, const Range& d), before smoother.apply(v,d);
68 schwarz.hh:virtual void apply (X& v, const Y& d), before preconditioner.apply(v,d);; 
64 === BiCGSTABSolver
 Iter          Defect            Rate
    0      1.17536e-06
64  (it = 0.5; it < _maxit; it+=.5) 0.5 250
64  before p = r 
64  before  y = W^-1 * p 
64  before _prec->apply(y,p); 0.5 250
64 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
64void AMG<M,X,S,PI,A>::mgc(LevelContext& levelContext), before presmooth(levelContext, preSteps_) 
void presmooth(LevelContext& levelContext, size_t steps), before SmootherApplier<typename LevelContext::SmootherType>::preSmooth, step: 0 of 2
void preSmooth(Smoother& smoother, Domain& v, const Range& d), before smoother.apply(v,d);
64 schwarz.hh:virtual void apply (X& v, const Y& d), before preconditioner.apply(v,d);; 
64 schwarz.hh:virtual void apply (X& v, const Y& d), before communication.copyOwnerToAll(v,v); 
64 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
64: communicator.hh::BufferedCommunicator::forward, enter sendRecv
64: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 14
64: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 14
64: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 14
64: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 14
64: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 14
64: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 14
64: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 14
64: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 14
64: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 14
64: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 14
64: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 10 14
57 === BiCGSTABSolver
 Iter          Defect            Rate
    0      1.17536e-06
57  (it = 0.5; it < _maxit; it+=.5) 0.5 250
57  before p = r 
57  before  y = W^-1 * p 
57  before _prec->apply(y,p); 0.5 250
57 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
57void AMG<M,X,S,PI,A>::mgc(LevelContext& levelContext), before presmooth(levelContext, preSteps_) 
void presmooth(LevelContext& levelContext, size_t steps), before SmootherApplier<typename LevelContext::SmootherType>::preSmooth, step: 0 of 2
void preSmooth(Smoother& smoother, Domain& v, const Range& d), before smoother.apply(v,d);
57 schwarz.hh:virtual void apply (X& v, const Y& d), before preconditioner.apply(v,d);; 
96 === BiCGSTABSolver
 Iter          Defect            Rate
    0      1.17536e-06
96  (it = 0.5; it < _maxit; it+=.5) 0.5 250
96  before p = r 
96  before  y = W^-1 * p 
96  before _prec->apply(y,p); 0.5 250
96 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
96void AMG<M,X,S,PI,A>::mgc(LevelContext& levelContext), before presmooth(levelContext, preSteps_) 
void presmooth(LevelContext& levelContext, size_t steps), before SmootherApplier<typename LevelContext::SmootherType>::preSmooth, step: 0 of 2
void preSmooth(Smoother& smoother, Domain& v, const Range& d), before smoother.apply(v,d);
96 schwarz.hh:virtual void apply (X& v, const Y& d), before preconditioner.apply(v,d);; 
96 schwarz.hh:virtual void apply (X& v, const Y& d), before communication.copyOwnerToAll(v,v); 
96 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
96: communicator.hh::BufferedCommunicator::forward, enter sendRecv
96: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 12
96: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 12
96: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 12
96: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 12
96: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 12
96: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 12
96: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 12
96: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 12
96: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 12
96: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 12
96: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 10 12
96: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 11 12
96: communicator.hh::sendRecv, left MPI_Wait
96: leave BufferedCommunicator::sendRecv
96: communicator.hh::BufferedCommunicator::forward, left sendRecv
96 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), after communicator.free();
96 schwarz.hh:virtual void apply (X& v, const Y& d), AFTER communication.copyOwnerToAll(v,v); 
void preSmooth(Smoother& smoother, Domain& v, const Range& d), AFTER smoother.apply(v,d);
void presmooth(LevelContext& levelContext, size_t steps), AFTER SmootherApplier<typename LevelContext::SmootherType>::preSmooth, step: 0 of 2
void presmooth(LevelContext& levelContext, size_t steps), before update defec, step: 0 of 2
void presmooth(LevelContext& levelContext, size_t steps), before SmootherApplier<typename LevelContext::SmootherType>::preSmooth, step: 1 of 2
void preSmooth(Smoother& smoother, Domain& v, const Range& d), before smoother.apply(v,d);
96 schwarz.hh:virtual void apply (X& v, const Y& d), before preconditioner.apply(v,d);; 
96 schwarz.hh:virtual void apply (X& v, const Y& d), before communication.copyOwnerToAll(v,v); 
96 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
96: communicator.hh::BufferedCommunicator::forward, enter sendRecv
96: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 12
96: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 12
50 === BiCGSTABSolver
 Iter          Defect            Rate
    0      1.17536e-06
50  (it = 0.5; it < _maxit; it+=.5) 0.5 250
50  before p = r 
50  before  y = W^-1 * p 
50  before _prec->apply(y,p); 0.5 250
50 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
50void AMG<M,X,S,PI,A>::mgc(LevelContext& levelContext), before presmooth(levelContext, preSteps_) 
void presmooth(LevelContext& levelContext, size_t steps), before SmootherApplier<typename LevelContext::SmootherType>::preSmooth, step: 0 of 2
void preSmooth(Smoother& smoother, Domain& v, const Range& d), before smoother.apply(v,d);
50 schwarz.hh:virtual void apply (X& v, const Y& d), before preconditioner.apply(v,d);; 
50 schwarz.hh:virtual void apply (X& v, const Y& d), before communication.copyOwnerToAll(v,v); 
50 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
50: communicator.hh::BufferedCommunicator::forward, enter sendRecv
50: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 10
50: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 10
50: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 10
50: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 10
50: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 10
50: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 10
50: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 10
50: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 10
50: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 10
50: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 10
50: communicator.hh::sendRecv, left MPI_Wait
50: leave BufferedCommunicator::sendRecv
50: communicator.hh::BufferedCommunicator::forward, left sendRecv
50 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), after communicator.free();
50 schwarz.hh:virtual void apply (X& v, const Y& d), AFTER communication.copyOwnerToAll(v,v); 
void preSmooth(Smoother& smoother, Domain& v, const Range& d), AFTER smoother.apply(v,d);
void presmooth(LevelContext& levelContext, size_t steps), AFTER SmootherApplier<typename LevelContext::SmootherType>::preSmooth, step: 0 of 2
void presmooth(LevelContext& levelContext, size_t steps), before update defec, step: 0 of 2
void presmooth(LevelContext& levelContext, size_t steps), before SmootherApplier<typename LevelContext::SmootherType>::preSmooth, step: 1 of 2
void preSmooth(Smoother& smoother, Domain& v, const Range& d), before smoother.apply(v,d);
50 schwarz.hh:virtual void apply (X& v, const Y& d), before preconditioner.apply(v,d);; 
50 schwarz.hh:virtual void apply (X& v, const Y& d), before communication.copyOwnerToAll(v,v); 
50 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
50: communicator.hh::BufferedCommunicator::forward, enter sendRecv
50: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 10
50: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 10
25 === BiCGSTABSolver
 Iter          Defect            Rate
    0      1.17536e-06
25  (it = 0.5; it < _maxit; it+=.5) 0.5 250
25  before p = r 
25  before  y = W^-1 * p 
25  before _prec->apply(y,p); 0.5 250
25 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
25void AMG<M,X,S,PI,A>::mgc(LevelContext& levelContext), before presmooth(levelContext, preSteps_) 
void presmooth(LevelContext& levelContext, size_t steps), before SmootherApplier<typename LevelContext::SmootherType>::preSmooth, step: 0 of 2
void preSmooth(Smoother& smoother, Domain& v, const Range& d), before smoother.apply(v,d);
25 schwarz.hh:virtual void apply (X& v, const Y& d), before preconditioner.apply(v,d);; 
25 schwarz.hh:virtual void apply (X& v, const Y& d), before communication.copyOwnerToAll(v,v); 
25 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
25: communicator.hh::BufferedCommunicator::forward, enter sendRecv
25: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 7
25: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 7
25: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 7
25: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 7
25: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 7
25: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 7
25: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 7
25: communicator.hh::sendRecv, left MPI_Wait
25: leave BufferedCommunicator::sendRecv
25: communicator.hh::BufferedCommunicator::forward, left sendRecv
25 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), after communicator.free();
25 schwarz.hh:virtual void apply (X& v, const Y& d), AFTER communication.copyOwnerToAll(v,v); 
void preSmooth(Smoother& smoother, Domain& v, const Range& d), AFTER smoother.apply(v,d);
void presmooth(LevelContext& levelContext, size_t steps), AFTER SmootherApplier<typename LevelContext::SmootherType>::preSmooth, step: 0 of 2
void presmooth(LevelContext& levelContext, size_t steps), before update defec, step: 0 of 2
void presmooth(LevelContext& levelContext, size_t steps), before SmootherApplier<typename LevelContext::SmootherType>::preSmooth, step: 1 of 2
void preSmooth(Smoother& smoother, Domain& v, const Range& d), before smoother.apply(v,d);
25 schwarz.hh:virtual void apply (X& v, const Y& d), before preconditioner.apply(v,d);; 
25 schwarz.hh:virtual void apply (X& v, const Y& d), before communication.copyOwnerToAll(v,v); 
25 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
25: communicator.hh::BufferedCommunicator::forward, enter sendRecv
25: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 7
25: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 7
25: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 7
25: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 7
25: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 7
65 === BiCGSTABSolver
 Iter          Defect            Rate
    0      1.17536e-06
65  (it = 0.5; it < _maxit; it+=.5) 0.5 250
65  before p = r 
65  before  y = W^-1 * p 
65  before _prec->apply(y,p); 0.5 250
65 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
65void AMG<M,X,S,PI,A>::mgc(LevelContext& levelContext), before presmooth(levelContext, preSteps_) 
void presmooth(LevelContext& levelContext, size_t steps), before SmootherApplier<typename LevelContext::SmootherType>::preSmooth, step: 0 of 2
void preSmooth(Smoother& smoother, Domain& v, const Range& d), before smoother.apply(v,d);
65 schwarz.hh:virtual void apply (X& v, const Y& d), before preconditioner.apply(v,d);; 
65 schwarz.hh:virtual void apply (X& v, const Y& d), before communication.copyOwnerToAll(v,v); 
65 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
65: communicator.hh::BufferedCommunicator::forward, enter sendRecv
65: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 9
65: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 9
65: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 9
65: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 9
65: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 9
65: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 9
65: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 9
65: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 9
65: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 9
65: communicator.hh::sendRecv, left MPI_Wait
65: leave BufferedCommunicator::sendRecv
65: communicator.hh::BufferedCommunicator::forward, left sendRecv
65 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), after communicator.free();
65 schwarz.hh:virtual void apply (X& v, const Y& d), AFTER communication.copyOwnerToAll(v,v); 
void preSmooth(Smoother& smoother, Domain& v, const Range& d), AFTER smoother.apply(v,d);
void presmooth(LevelContext& levelContext, size_t steps), AFTER SmootherApplier<typename LevelContext::SmootherType>::preSmooth, step: 0 of 2
void presmooth(LevelContext& levelContext, size_t steps), before update defec, step: 0 of 2
void presmooth(LevelContext& levelContext, size_t steps), before SmootherApplier<typename LevelContext::SmootherType>::preSmooth, step: 1 of 2
void preSmooth(Smoother& smoother, Domain& v, const Range& d), before smoother.apply(v,d);
65 schwarz.hh:virtual void apply (X& v, const Y& d), before preconditioner.apply(v,d);; 
65 schwarz.hh:virtual void apply (X& v, const Y& d), before communication.copyOwnerToAll(v,v); 
65 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
65: communicator.hh::BufferedCommunicator::forward, enter sendRecv
65: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 9
65: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 9
65: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 9
49 === BiCGSTABSolver
 Iter          Defect            Rate
    0      1.17536e-06
49  (it = 0.5; it < _maxit; it+=.5) 0.5 250
49  before p = r 
49  before  y = W^-1 * p 
49  before _prec->apply(y,p); 0.5 250
49 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
49void AMG<M,X,S,PI,A>::mgc(LevelContext& levelContext), before presmooth(levelContext, preSteps_) 
void presmooth(LevelContext& levelContext, size_t steps), before SmootherApplier<typename LevelContext::SmootherType>::preSmooth, step: 0 of 2
void preSmooth(Smoother& smoother, Domain& v, const Range& d), before smoother.apply(v,d);
49 schwarz.hh:virtual void apply (X& v, const Y& d), before preconditioner.apply(v,d);; 
49 schwarz.hh:virtual void apply (X& v, const Y& d), before communication.copyOwnerToAll(v,v); 
49 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
49: communicator.hh::BufferedCommunicator::forward, enter sendRecv
49: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 15
49: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 15
49: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 15
49: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 15
49: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 15
49: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 15
49: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 15
49: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 15
49: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 15
49: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 15
49: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 10 15
92 === BiCGSTABSolver
 Iter          Defect            Rate
    0      1.17536e-06
92  (it = 0.5; it < _maxit; it+=.5) 0.5 250
92  before p = r 
92  before  y = W^-1 * p 
92  before _prec->apply(y,p); 0.5 250
92 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
92void AMG<M,X,S,PI,A>::mgc(LevelContext& levelContext), before presmooth(levelContext, preSteps_) 
void presmooth(LevelContext& levelContext, size_t steps), before SmootherApplier<typename LevelContext::SmootherType>::preSmooth, step: 0 of 2
void preSmooth(Smoother& smoother, Domain& v, const Range& d), before smoother.apply(v,d);
92 schwarz.hh:virtual void apply (X& v, const Y& d), before preconditioner.apply(v,d);; 
92 schwarz.hh:virtual void apply (X& v, const Y& d), before communication.copyOwnerToAll(v,v); 
92 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
92: communicator.hh::BufferedCommunicator::forward, enter sendRecv
92: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 16
92: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 16
92: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 16
92: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 16
92: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 16
92: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 16
92: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 16
92: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 16
92: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 16
92: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 16
92: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 10 16
92: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 11 16
92: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 12 16
92: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 13 16
92: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 14 16
92: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 15 16
92: communicator.hh::sendRecv, left MPI_Wait
92: leave BufferedCommunicator::sendRecv
92: communicator.hh::BufferedCommunicator::forward, left sendRecv
92 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), after communicator.free();
92 schwarz.hh:virtual void apply (X& v, const Y& d), AFTER communication.copyOwnerToAll(v,v); 
void preSmooth(Smoother& smoother, Domain& v, const Range& d), AFTER smoother.apply(v,d);
void presmooth(LevelContext& levelContext, size_t steps), AFTER SmootherApplier<typename LevelContext::SmootherType>::preSmooth, step: 0 of 2
void presmooth(LevelContext& levelContext, size_t steps), before update defec, step: 0 of 2
void presmooth(LevelContext& levelContext, size_t steps), before SmootherApplier<typename LevelContext::SmootherType>::preSmooth, step: 1 of 2
void preSmooth(Smoother& smoother, Domain& v, const Range& d), before smoother.apply(v,d);
92 schwarz.hh:virtual void apply (X& v, const Y& d), before preconditioner.apply(v,d);; 
92 schwarz.hh:virtual void apply (X& v, const Y& d), before communication.copyOwnerToAll(v,v); 
92 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template 100 === BiCGSTABSolver
 Iter          Defect            Rate
    0      1.17536e-06
100  (it = 0.5; it < _maxit; it+=.5) 0.5 250
100  before p = r 
100  before  y = W^-1 * p 
100  before _prec->apply(y,p); 0.5 250
100 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
100void AMG<M,X,S,PI,A>::mgc(LevelContext& levelContext), before presmooth(levelContext, preSteps_) 
void presmooth(LevelContext& levelContext, size_t steps), before SmootherApplier<typename LevelContext::SmootherType>::preSmooth, step: 0 of 2
void preSmooth(Smoother& smoother, Domain& v, const Range& d), before smoother.apply(v,d);
100 schwarz.hh:virtual void apply (X& v, const Y& d), before preconditioner.apply(v,d);; 
100 schwarz.hh:virtual void apply (X& v, const Y& d), before communication.copyOwnerToAll(v,v); 
100 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
100: communicator.hh::BufferedCommunicator::forward, enter sendRecv
100: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 6
100: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 6
100: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 6
100: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 6
100: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 6
100: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 6
100: communicator.hh::sendRecv, left MPI_Wait
100: leave BufferedCommunicator::sendRecv
100: communicator.hh::BufferedCommunicator::forward, left sendRecv
100 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), after communicator.free();
100 schwarz.hh:virtual void apply (X& v, const Y& d), AFTER communication.copyOwnerToAll(v,v); 
void preSmooth(Smoother& smoother, Domain& v, const Range& d), AFTER smoother.apply(v,d);
void presmooth(LevelContext& levelContext, size_t steps), AFTER SmootherApplier<typename LevelContext::SmootherType>::preSmooth, step: 0 of 2
void presmooth(LevelContext& levelContext, size_t steps), before update defec, step: 0 of 2
void presmooth(LevelContext& levelContext, size_t steps), before SmootherApplier<typename LevelContext::SmootherType>::preSmooth, step: 1 of 2
void preSmooth(Smoother& smoother, Domain& v, const Range& d), before smoother.apply(v,d);
100 schwarz.hh:virtual void apply (X& v, const Y& d), before preconditioner.apply(v,d);; 
100 schwarz.hh:virtual void apply (X& v, const Y& d), before communication.copyOwnerToAll(v,v); 
100 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
100: communicator.hh::BufferedCommunicator::forward, enter sendRecv
100: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 6
100: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 6
100: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 6
100: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 6
59 === BiCGSTABSolver
 Iter          Defect            Rate
    0      1.17536e-06
59  (it = 0.5; it < _maxit; it+=.5) 0.5 250
59  before p = r 
59  before  y = W^-1 * p 
59  before _prec->apply(y,p); 0.5 250
59 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
59void AMG<M,X,S,PI,A>::mgc(LevelContext& levelContext), before presmooth(levelContext, preSteps_) 
void presmooth(LevelContext& levelContext, size_t steps), before SmootherApplier<typename LevelContext::SmootherType>::preSmooth, step: 0 of 2
void preSmooth(Smoother& smoother, Domain& v, const Range& d), before smoother.apply(v,d);
59 schwarz.hh:virtual void apply (X& v, const Y& d), before preconditioner.apply(v,d);; 
59 schwarz.hh:virtual void apply (X& v, const Y& d), before communication.copyOwnerToAll(v,v); 
59 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
59: communicator.hh::BufferedCommunicator::forward, enter sendRecv
59: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 11
59: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 11
59: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 11
59: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 11
59: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 11
59: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 11
59: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 11
59: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 11
117 === BiCGSTABSolver
 Iter          Defect            Rate
    0      1.17536e-06
117  (it = 0.5; it < _maxit; it+=.5) 0.5 250
117  before p = r 
117  before  y = W^-1 * p 
117  before _prec->apply(y,p); 0.5 250
117 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
117void AMG<M,X,S,PI,A>::mgc(LevelContext& levelContext), before presmooth(levelContext, preSteps_) 
void presmooth(LevelContext& levelContext, size_t steps), before SmootherApplier<typename LevelContext::SmootherType>::preSmooth, step: 0 of 2
void preSmooth(Smoother& smoother, Domain& v, const Range& d), before smoother.apply(v,d);
117 schwarz.hh:virtual void apply (X& v, const Y& d), before preconditioner.apply(v,d);; 
117 schwarz.hh:virtual void apply (X& v, const Y& d), before communication.copyOwnerToAll(v,v); 
117 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
117: communicator.hh::BufferedCommunicator::forward, enter sendRecv
117: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 22
117: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 22
117: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 22
117: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 22
117: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 22
117: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 22
117: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 22
117: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 22
117: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 22
117: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 22
117: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 10 22
117: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 11 22
117: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 12 22
117: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 13 22
117: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 14 22
117: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 15 22
117: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 16 22
117: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 17 22
117: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 18 22
117: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 19 22
117: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 20 22
117: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 21 22
117: communicator.hh::sendRecv, left MPI_Wait
117: leave BufferedCommunicator::sendRecv
117: communicator.hh::BufferedCommunicator::forward, left sendRecv
117 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), after communicator.free();
117 schwarz.hh:virtual void apply (X& v, const Y& d), AFTER communication.copyOwnerToAll(v,v); 
void preSmooth(Smoother& smoother, Domain& v, const Range& d), AFTER smoother.apply(v,d20 === BiCGSTABSolver
 Iter          Defect            Rate
    0      1.17536e-06
20  (it = 0.5; it < _maxit; it+=.5) 0.5 250
20  before p = r 
20  before  y = W^-1 * p 
20  before _prec->apply(y,p); 0.5 250
20 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
20void AMG<M,X,S,PI,A>::mgc(LevelContext& levelContext), before presmooth(levelContext, preSteps_) 
void presmooth(LevelContext& levelContext, size_t steps), before SmootherApplier<typename LevelContext::SmootherType>::preSmooth, step: 0 of 2
void preSmooth(Smoother& smoother, Domain& v, const Range& d), before smoother.apply(v,d);
20 schwarz.hh:virtual void apply (X& v, const Y& d), before preconditioner.apply(v,d);; 
20 schwarz.hh:virtual void apply (X& v, const Y& d), before communication.copyOwnerToAll(v,v); 
20 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
20: communicator.hh::BufferedCommunicator::forward, enter sendRecv
20: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 10
20: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 10
20: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 10
20: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 10
20: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 10
20: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 10
20: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 10
20: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 10
20: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 10
20: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 10
20: communicator.hh::sendRecv, left MPI_Wait
20: leave BufferedCommunicator::sendRecv
20: communicator.hh::BufferedCommunicator::forward, left sendRecv
20 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), after communicator.free();
20 schwarz.hh:virtual void apply (X& v, const Y& d), AFTER communication.copyOwnerToAll(v,v); 
void preSmooth(Smoother& smoother, Domain& v, const Range& d), AFTER smoother.apply(v,d);
void presmooth(LevelContext& levelContext, size_t steps), AFTER SmootherApplier<typename LevelContext::SmootherType>::preSmooth, step: 0 of 2
void presmooth(LevelContext& levelContext, size_t steps), before update defec, step: 0 of 2
void presmooth(LevelContext& levelContext, size_t steps), before SmootherApplier<typename LevelContext::SmootherType>::preSmooth, step: 1 of 2
void preSmooth(Smoother& smoother, Domain& v, const Range& d), before smoother.apply(v,d);
20 schwarz.hh:virtual void apply (X& v, const Y& d), before preconditioner.apply(v,d);; 
20 schwarz.hh:virtual void apply (X& v, const Y& d), before communication.copyOwnerToAll(v,v); 
20 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
20: communicator.hh::BufferedCommunicator::forward, enter sendRecv
20: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 10
20: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 10
20: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 10
20: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 10
20: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 10
20: communic19 === BiCGSTABSolver
 Iter          Defect            Rate
    0      1.17536e-06
19  (it = 0.5; it < _maxit; it+=.5) 0.5 250
19  before p = r 
19  before  y = W^-1 * p 
19  before _prec->apply(y,p); 0.5 250
19 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
19void AMG<M,X,S,PI,A>::mgc(LevelContext& levelContext), before presmooth(levelContext, preSteps_) 
void presmooth(LevelContext& levelContext, size_t steps), before SmootherApplier<typename LevelContext::SmootherType>::preSmooth, step: 0 of 2
void preSmooth(Smoother& smoother, Domain& v, const Range& d), before smoother.apply(v,d);
19 schwarz.hh:virtual void apply (X& v, const Y& d), before preconditioner.apply(v,d);; 
19 schwarz.hh:virtual void apply (X& v, const Y& d), before communication.copyOwnerToAll(v,v); 
19 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
19: communicator.hh::BufferedCommunicator::forward, enter sendRecv
19: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 17
19: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 17
19: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 17
19: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 17
19: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 17
19: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 17
19: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 17
19: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 17
19: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 17
19: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 17
19: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 10 17
19: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 11 17
19: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 12 17
19: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 13 17
19: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 14 17
19: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 15 17
19: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 16 17
19: communicator.hh::sendRecv, left MPI_Wait
19: leave BufferedCommunicator::sendRecv
19: communicator.hh::BufferedCommunicator::forward, left sendRecv
19 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), after communicator.free();
19 schwarz.hh:virtual void apply (X& v, const Y& d), AFTER communication.copyOwnerToAll(v,v); 
void preSmooth(Smoother& smoother, Domain& v, const Range& d), AFTER smoother.apply(v,d);
void presmooth(LevelContext& levelContext, size_t steps), AFTER SmootherApplier<typename LevelContext::SmootherType>::preSmooth, step: 0 of 2
void presmooth(LevelContext& levelContext, size_t steps), before update defec, step: 0 of 2
void presmooth(LevelContext& levelContext, size_t steps), before SmootherApplier<typename LevelContext::SmootherType>::preSmooth, step: 1 of 2
void preSmooth(Smoother& smoother, Domain& v, const Range& d), before smoother.apply(v,d);
19 schwarz.hh:virtual void apply (X& v, const Y& d), before preconditioner.apply(v,d);; 
19 schwarz.hh:virtual void apply (X& v, const Y& d), before communication38 === BiCGSTABSolver
 Iter          Defect            Rate
    0      1.17536e-06
38  (it = 0.5; it < _maxit; it+=.5) 0.5 250
38  before p = r 
38  before  y = W^-1 * p 
38  before _prec->apply(y,p); 0.5 250
38 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
38void AMG<M,X,S,PI,A>::mgc(LevelContext& levelContext), before presmooth(levelContext, preSteps_) 
void presmooth(LevelContext& levelContext, size_t steps), before SmootherApplier<typename LevelContext::SmootherType>::preSmooth, step: 0 of 2
void preSmooth(Smoother& smoother, Domain& v, const Range& d), before smoother.apply(v,d);
38 schwarz.hh:virtual void apply (X& v, const Y& d), before preconditioner.apply(v,d);; 
38 schwarz.hh:virtual void apply (X& v, const Y& d), before communication.copyOwnerToAll(v,v); 
38 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
38: communicator.hh::BufferedCommunicator::forward, enter sendRecv
38: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 24
38: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 24
38: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 24
38: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 24
38: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 24
38: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 24
38: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 24
38: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 24
38: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 24
38: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 24
38: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 10 24
38: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 11 24
38: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 12 24
38: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 13 24
38: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 14 24
38: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 15 24
38: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 16 24
38: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 17 24
38: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 18 24
38: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 19 24
38: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 20 24
38: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 21 24
38: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 22 24
38: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 23 24
120 === BiCGSTABSolver
 Iter          Defect            Rate
    0      1.17536e-06
120  (it = 0.5; it < _maxit; it+=.5) 0.5 250
120  before p = r 
120  before  y = W^-1 * p 
120  before _prec->apply(y,p); 0.5 250
120 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
120void AMG<M,X,S,PI,A>::mgc(LevelContext& levelContext), before presmooth(levelContext, preSteps_) 
void presmooth(LevelContext& levelContext, size_t steps), before SmootherApplier<typename LevelContext::SmootherType>::preSmooth, step: 0 of 2
void preSmooth(Smoother& smoother, Domain& v, const Range& d), before smoother.apply(v,d);
120 schwarz.hh:virtual void apply (X& v, const Y& d), before preconditioner.apply(v,d);; 
120 schwarz.hh:virtual void apply (X& v, const Y& d), before communication.copyOwnerToAll(v,v); 
120 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
120: communicator.hh::BufferedCommunicator::forward, enter sendRecv
120: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 7
120: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 7
120: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 7
120: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 7
120: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 7
120: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 7
120: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 7
120: communicator.hh::sendRecv, left MPI_Wait
120: leave BufferedCommunicator::sendRecv
120: communicator.hh::BufferedCommunicator::forward, left sendRecv
120 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), after communicator.free();
120 schwarz.hh:virtual void apply (X& v, const Y& d), AFTER communication.copyOwnerToAll(v,v); 
void preSmooth(Smoother& smoother, Domain& v, const Range& d), AFTER smoother.apply(v,d);
void presmooth(LevelContext& levelContext, size_t steps), AFTER SmootherApplier<typename LevelContext::SmootherType>::preSmooth, step: 0 of 2
void presmooth(LevelContext& levelContext, size_t steps), before update defec, step: 0 of 2
void presmooth(LevelContext& levelContext, size_t steps), before SmootherApplier<typename LevelContext::SmootherType>::preSmooth, step: 1 of 2
void preSmooth(Smoother& smoother, Domain& v, const Range& d), before smoother.apply(v,d);
120 schwarz.hh:virtual void apply (X& v, const Y& d), before preconditioner.apply(v,d);; 
120 schwarz.hh:virtual void apply (X& v, const Y& d), before communication.copyOwnerToAll(v,v); 
120 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
120: communicator.hh::BufferedCommunicator::forward, enter sendRecv
120: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 7
120: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 7
120: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 7
120: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 7
120: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 7
120: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 7
120: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 7
83 === BiCGSTABSolver
 Iter          Defect            Rate
    0      1.17536e-06
83  (it = 0.5; it < _maxit; it+=.5) 0.5 250
83  before p = r 
83  before  y = W^-1 * p 
83  before _prec->apply(y,p); 0.5 250
83 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
83void AMG<M,X,S,PI,A>::mgc(LevelContext& levelContext), before presmooth(levelContext, preSteps_) 
void presmooth(LevelContext& levelContext, size_t steps), before SmootherApplier<typename LevelContext::SmootherType>::preSmooth, step: 0 of 2
void preSmooth(Smoother& smoother, Domain& v, const Range& d), before smoother.apply(v,d);
83 schwarz.hh:virtual void apply (X& v, const Y& d), before preconditioner.apply(v,d);; 
21 === BiCGSTABSolver
 Iter          Defect            Rate
    0      1.17536e-06
21  (it = 0.5; it < _maxit; it+=.5) 0.5 250
21  before p = r 
21  before  y = W^-1 * p 
21  before _prec->apply(y,p); 0.5 250
21 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
21void AMG<M,X,S,PI,A>::mgc(LevelContext& levelContext), before presmooth(levelContext, preSteps_) 
void presmooth(LevelContext& levelContext, size_t steps), before SmootherApplier<typename LevelContext::SmootherType>::preSmooth, step: 0 of 2
void preSmooth(Smoother& smoother, Domain& v, const Range& d), before smoother.apply(v,d);
21 schwarz.hh:virtual void apply (X& v, const Y& d), before preconditioner.apply(v,d);; 
21 schwarz.hh:virtual void apply (X& v, const Y& d), before communication.copyOwnerToAll(v,v); 
21 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
21: communicator.hh::BufferedCommunicator::forward, enter sendRecv
21: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 12
21: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 12
21: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 12
21: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 12
21: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 12
21: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 12
21: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 12
21: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 12
21: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 12
21: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 12
21: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 10 12
21: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 11 12
21: communicator.hh::sendRecv, left MPI_Wait
21: leave BufferedCommunicator::sendRecv
21: communicator.hh::BufferedCommunicator::forward, left sendRecv
21 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), after communicator.free();
21 schwarz.hh:virtual void apply (X& v, const Y& d), AFTER communication.copyOwnerToAll(v,v); 
void preSmooth(Smoother& smoother, Domain& v, const Range& d), AFTER smoother.apply(v,d);
void presmooth(LevelContext& levelContext, size_t steps), AFTER SmootherApplier<typename LevelContext::SmootherType>::preSmooth, step: 0 of 2
void presmooth(LevelContext& levelContext, size_t steps), before update defec, step: 0 of 2
void presmooth(LevelContext& levelContext, size_t steps), before SmootherApplier<typename LevelContext::SmootherType>::preSmooth, step: 1 of 2
void preSmooth(Smoother& smoother, Domain& v, const Range& d), before smoother.apply(v,d);
21 schwarz.hh:virtual void apply (X& v, const Y& d), before preconditioner.apply(v,d);; 
21 schwarz.hh:virtual void apply (X& v, const Y& d), before communication.copyOwnerToAll(v,v); 
21 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
21: communicator.hh::BufferedCommunicator::forward, enter sendRecv
21: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 12
21: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 12
21: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 12
21: commun24 === BiCGSTABSolver
 Iter          Defect            Rate
    0      1.17536e-06
24  (it = 0.5; it < _maxit; it+=.5) 0.5 250
24  before p = r 
24  before  y = W^-1 * p 
24  before _prec->apply(y,p); 0.5 250
24 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
24void AMG<M,X,S,PI,A>::mgc(LevelContext& levelContext), before presmooth(levelContext, preSteps_) 
void presmooth(LevelContext& levelContext, size_t steps), before SmootherApplier<typename LevelContext::SmootherType>::preSmooth, step: 0 of 2
void preSmooth(Smoother& smoother, Domain& v, const Range& d), before smoother.apply(v,d);
24 schwarz.hh:virtual void apply (X& v, const Y& d), before preconditioner.apply(v,d);; 
24 schwarz.hh:virtual void apply (X& v, const Y& d), before communication.copyOwnerToAll(v,v); 
24 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
24: communicator.hh::BufferedCommunicator::forward, enter sendRecv
24: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 14
24: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 14
24: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 14
24: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 14
24: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 14
24: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 14
24: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 14
24: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 14
24: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 14
24: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 14
24: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 10 14
24: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 11 14
24: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 12 14
24: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 13 14
24: communicator.hh::sendRecv, left MPI_Wait
24: leave BufferedCommunicator::sendRecv
24: communicator.hh::BufferedCommunicator::forward, left sendRecv
24 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), after communicator.free();
24 schwarz.hh:virtual void apply (X& v, const Y& d), AFTER communication.copyOwnerToAll(v,v); 
void preSmooth(Smoother& smoother, Domain& v, const Range& d), AFTER smoother.apply(v,d);
void presmooth(LevelContext& levelContext, size_t steps), AFTER SmootherApplier<typename LevelContext::SmootherType>::preSmooth, step: 0 of 2
void presmooth(LevelContext& levelContext, size_t steps), before update defec, step: 0 of 2
void presmooth(LevelContext& levelContext, size_t steps), before SmootherApplier<typename LevelContext::SmootherType>::preSmooth, step: 1 of 2
void preSmooth(Smoother& smoother, Domain& v, const Range& d), before smoother.apply(v,d);
24 schwarz.hh:virtual void apply (X& v, const Y& d), before preconditioner.apply(v,d);; 
24 schwarz.hh:virtual void apply (X& v, const Y& d), before communication.copyOwnerToAll(v,v); 
24 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
24: communicator.hh::BufferedCommunicator::forward, enter sendRecv
24: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 14
24: comm85 === BiCGSTABSolver
 Iter          Defect            Rate
    0      1.17536e-06
85  (it = 0.5; it < _maxit; it+=.5) 0.5 250
85  before p = r 
85  before  y = W^-1 * p 
85  before _prec->apply(y,p); 0.5 250
85 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
85void AMG<M,X,S,PI,A>::mgc(LevelContext& levelContext), before presmooth(levelContext, preSteps_) 
void presmooth(LevelContext& levelContext, size_t steps), before SmootherApplier<typename LevelContext::SmootherType>::preSmooth, step: 0 of 2
void preSmooth(Smoother& smoother, Domain& v, const Range& d), before smoother.apply(v,d);
85 schwarz.hh:virtual void apply (X& v, const Y& d), before preconditioner.apply(v,d);; 
47 === BiCGSTABSolver
 Iter          Defect            Rate
    0      1.17536e-06
47  (it = 0.5; it < _maxit; it+=.5) 0.5 250
47  before p = r 
47  before  y = W^-1 * p 
47  before _prec->apply(y,p); 0.5 250
47 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
47void AMG<M,X,S,PI,A>::mgc(LevelContext& levelContext), before presmooth(levelContext, preSteps_) 
void presmooth(LevelContext& levelContext, size_t steps), before SmootherApplier<typename LevelContext::SmootherType>::preSmooth, step: 0 of 2
void preSmooth(Smoother& smoother, Domain& v, const Range& d), before smoother.apply(v,d);
47 schwarz.hh:virtual void apply (X& v, const Y& d), before preconditioner.apply(v,d);; 
47 schwarz.hh:virtual void apply (X& v, const Y& d), before communication.copyOwnerToAll(v,v); 
47 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
47: communicator.hh::BufferedCommunicator::forward, enter sendRecv
47: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 6
47: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 6
47: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 6
47: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 6
47: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 6
47: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 6
47: communicator.hh::sendRecv, left MPI_Wait
47: leave BufferedCommunicator::sendRecv
47: communicator.hh::BufferedCommunicator::forward, left sendRecv
47 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), after communicator.free();
47 schwarz.hh:virtual void apply (X& v, const Y& d), AFTER communication.copyOwnerToAll(v,v); 
void preSmooth(Smoother& smoother, Domain& v, const Range& d), AFTER smoother.apply(v,d);
void presmooth(LevelContext& levelContext, size_t steps), AFTER SmootherApplier<typename LevelContext::SmootherType>::preSmooth, step: 0 of 2
void presmooth(LevelContext& levelContext, size_t steps), before update defec, step: 0 of 2
void presmooth(LevelContext& levelContext, size_t steps), before SmootherApplier<typename LevelContext::SmootherType>::preSmooth, step: 1 of 2
void preSmooth(Smoother& smoother, Domain& v, const Range& d), before smoother.apply(v,d);
47 schwarz.hh:virtual void apply (X& v, const Y& d), before preconditioner.apply(v,d);; 
47 schwarz.hh:virtual void apply (X& v, const Y& d), before communication.copyOwnerToAll(v,v); 
47 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
47: communicator.hh::BufferedCommunicator::forward, enter sendRecv
47: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 6
47: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 6
47: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 6
47: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 6
47: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 6
72 === BiCGSTABSolver
 Iter          Defect            Rate
    0      1.17536e-06
72  (it = 0.5; it < _maxit; it+=.5) 0.5 250
72  before p = r 
72  before  y = W^-1 * p 
72  before _prec->apply(y,p); 0.5 250
72 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
72void AMG<M,X,S,PI,A>::mgc(LevelContext& levelContext), before presmooth(levelContext, preSteps_) 
void presmooth(LevelContext& levelContext, size_t steps), before SmootherApplier<typename LevelContext::SmootherType>::preSmooth, step: 0 of 2
void preSmooth(Smoother& smoother, Domain& v, const Range& d), before smoother.apply(v,d);
72 schwarz.hh:virtual void apply (X& v, const Y& d), before preconditioner.apply(v,d);; 
72 schwarz.hh:virtual void apply (X& v, const Y& d), before communication.copyOwnerToAll(v,v); 
72 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
72: communicator.hh::BufferedCommunicator::forward, enter sendRecv
72: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 20
72: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 20
72: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 20
72: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 20
72: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 20
72: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 20
72: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 20
72: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 20
72: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 20
72: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 20
72: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 10 20
72: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 11 20
72: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 12 20
72: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 13 20
72: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 14 20
72: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 15 20
72: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 16 20
72: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 17 20
72: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 18 20
72: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 19 20
72: communicator.hh::sendRecv, left MPI_Wait
72: leave BufferedCommunicator::sendRecv
72: communicator.hh::BufferedCommunicator::forward, left sendRecv
72 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), after communicator.free();
72 schwarz.hh:virtual void apply (X& v, const Y& d), AFTER communication.copyOwnerToAll(v,v); 
void preSmooth(Smoother& smoother, Domain& v, const Range& d), AFTER smoother.apply(v,d);
void presmooth(LevelContext& levelContext, size_t steps), AFTER SmootherApplier<typename LevelContext::SmootherType>::preSmooth, step: 0 of 2
void presmooth(LevelContext& levelContext, size_t steps), before update defec, step: 0 of 2
void presmooth(LevelContext& levelConte98 === BiCGSTABSolver
 Iter          Defect            Rate
    0      1.17536e-06
98  (it = 0.5; it < _maxit; it+=.5) 0.5 250
98  before p = r 
98  before  y = W^-1 * p 
98  before _prec->apply(y,p); 0.5 250
98 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
98void AMG<M,X,S,PI,A>::mgc(LevelContext& levelContext), before presmooth(levelContext, preSteps_) 
void presmooth(LevelContext& levelContext, size_t steps), before SmootherApplier<typename LevelContext::SmootherType>::preSmooth, step: 0 of 2
void preSmooth(Smoother& smoother, Domain& v, const Range& d), before smoother.apply(v,d);
98 schwarz.hh:virtual void apply (X& v, const Y& d), before preconditioner.apply(v,d);; 
98 schwarz.hh:virtual void apply (X& v, const Y& d), before communication.copyOwnerToAll(v,v); 
98 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
98: communicator.hh::BufferedCommunicator::forward, enter sendRecv
98: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 18
98: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 18
98: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 18
98: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 18
98: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 18
98: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 18
98: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 18
98: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 18
98: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 18
98: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 18
98: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 10 18
98: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 11 18
98: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 12 18
98: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 13 18
98: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 14 18
98: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 15 18
98: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 16 18
98: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 17 18
98: communicator.hh::sendRecv, left MPI_Wait
98: leave BufferedCommunicator::sendRecv
98: communicator.hh::BufferedCommunicator::forward, left sendRecv
98 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), after communicator.free();
98 schwarz.hh:virtual void apply (X& v, const Y& d), AFTER communication.copyOwnerToAll(v,v); 
void preSmooth(Smoother& smoother, Domain& v, const Range& d), AFTER smoother.apply(v,d);
void presmooth(LevelContext& levelContext, size_t steps), AFTER SmootherApplier<typename LevelContext::SmootherType>::preSmooth, step: 0 of 2
void presmooth(LevelContext& levelContext, size_t steps), before update defec, step: 0 of 2
void presmooth(LevelContext& levelContext, size_t steps), before SmootherApplier<typename LevelContext::SmootherType>::preSmooth, step: 1 of 2
void preSmooth(Smoother& smoother, Domain& v, const Range& d), before smoother.apply(v,d);
98 schwarz.hh:virtual void apply (X& v, con28 === BiCGSTABSolver
 Iter          Defect            Rate
    0      1.17536e-06
28  (it = 0.5; it < _maxit; it+=.5) 0.5 250
28  before p = r 
28  before  y = W^-1 * p 
28  before _prec->apply(y,p); 0.5 250
28 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
28void AMG<M,X,S,PI,A>::mgc(LevelContext& levelContext), before presmooth(levelContext, preSteps_) 
void presmooth(LevelContext& levelContext, size_t steps), before SmootherApplier<typename LevelContext::SmootherType>::preSmooth, step: 0 of 2
void preSmooth(Smoother& smoother, Domain& v, const Range& d), before smoother.apply(v,d);
28 schwarz.hh:virtual void apply (X& v, const Y& d), before preconditioner.apply(v,d);; 
28 schwarz.hh:virtual void apply (X& v, const Y& d), before communication.copyOwnerToAll(v,v); 
28 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
28: communicator.hh::BufferedCommunicator::forward, enter sendRecv
28: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 18
28: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 18
28: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 18
28: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 18
28: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 18
28: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 18
28: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 18
28: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 18
28: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 18
28: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 18
28: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 10 18
28: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 11 18
28: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 12 18
28: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 13 18
28: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 14 18
28: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 15 18
28: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 16 18
28: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 17 18
28: communicator.hh::sendRecv, left MPI_Wait
28: leave BufferedCommunicator::sendRecv
28: communicator.hh::BufferedCommunicator::forward, left sendRecv
28 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), after communicator.free();
28 schwarz.hh:virtual void apply (X& v, const Y& d), AFTER communication.copyOwnerToAll(v,v); 
void preSmooth(Smoother& smoother, Domain& v, const Range& d), AFTER smoother.apply(v,d);
void presmooth(LevelContext& levelContext, size_t steps), AFTER SmootherApplier<typename LevelContext::SmootherType>::preSmooth, step: 0 of 2
void presmooth(LevelContext& levelContext, size_t steps), before update defec, step: 0 of 2
void presmooth(LevelContext& levelContext, size_t steps), before SmootherApplier<typename LevelContext::SmootherType>::preSmooth, step: 1 of 2
void preSmooth(Smoother& smoother, Domain& v, const Range& d), before smoother.apply(v,d);
28 schwarz.hh:virtual void apply (X& v, con118 === BiCGSTABSolver
 Iter          Defect            Rate
    0      1.17536e-06
118  (it = 0.5; it < _maxit; it+=.5) 0.5 250
118  before p = r 
118  before  y = W^-1 * p 
118  before _prec->apply(y,p); 0.5 250
118 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
118void AMG<M,X,S,PI,A>::mgc(LevelContext& levelContext), before presmooth(levelContext, preSteps_) 
void presmooth(LevelContext& levelContext, size_t steps), before SmootherApplier<typename LevelContext::SmootherType>::preSmooth, step: 0 of 2
void preSmooth(Smoother& smoother, Domain& v, const Range& d), before smoother.apply(v,d);
118 schwarz.hh:virtual void apply (X& v, const Y& d), before preconditioner.apply(v,d);; 
118 schwarz.hh:virtual void apply (X& v, const Y& d), before communication.copyOwnerToAll(v,v); 
118 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
118: communicator.hh::BufferedCommunicator::forward, enter sendRecv
118: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 14
118: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 14
118: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 14
118: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 14
118: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 14
118: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 14
118: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 14
118: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 14
118: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 14
118: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 14
118: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 10 14
118: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 11 14
118: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 12 14
118: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 13 14
118: communicator.hh::sendRecv, left MPI_Wait
118: leave BufferedCommunicator::sendRecv
118: communicator.hh::BufferedCommunicator::forward, left sendRecv
118 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), after communicator.free();
118 schwarz.hh:virtual void apply (X& v, const Y& d), AFTER communication.copyOwnerToAll(v,v); 
void preSmooth(Smoother& smoother, Domain& v, const Range& d), AFTER smoother.apply(v,d);
void presmooth(LevelContext& levelContext, size_t steps), AFTER SmootherApplier<typename LevelContext::SmootherType>::preSmooth, step: 0 of 2
void presmooth(LevelContext& levelContext, size_t steps), before update defec, step: 0 of 2
void presmooth(LevelContext& levelContext, size_t steps), before SmootherApplier<typename LevelContext::SmootherType>::preSmooth, step: 1 of 2
void preSmooth(Smoother& smoother, Domain& v, const Range& d), before smoother.apply(v,d);
118 schwarz.hh:virtual void apply (X& v, const Y& d), before preconditioner.apply(v,d);; 
118 schwarz.hh:virtual void apply (X& v, const Y& d), before communication.copyOwnerToAll(v,v); 
118 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
118: communicator.hh::BufferedCommunicator::forward, enter sendRecv
118: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests,27 === BiCGSTABSolver
 Iter          Defect            Rate
    0      1.17536e-06
27  (it = 0.5; it < _maxit; it+=.5) 0.5 250
27  before p = r 
27  before  y = W^-1 * p 
27  before _prec->apply(y,p); 0.5 250
27 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
27void AMG<M,X,S,PI,A>::mgc(LevelContext& levelContext), before presmooth(levelContext, preSteps_) 
void presmooth(LevelContext& levelContext, size_t steps), before SmootherApplier<typename LevelContext::SmootherType>::preSmooth, step: 0 of 2
void preSmooth(Smoother& smoother, Domain& v, const Range& d), before smoother.apply(v,d);
27 schwarz.hh:virtual void apply (X& v, const Y& d), before preconditioner.apply(v,d);; 
27 schwarz.hh:virtual void apply (X& v, const Y& d), before communication.copyOwnerToAll(v,v); 
27 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
27: communicator.hh::BufferedCommunicator::forward, enter sendRecv
27: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 17
27: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 17
27: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 17
27: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 17
27: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 17
27: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 17
27: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 17
27: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 17
27: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 17
27: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 17
27: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 10 17
27: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 11 17
27: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 12 17
27: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 13 17
27: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 14 17
27: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 15 17
27: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 16 17
27: communicator.hh::sendRecv, left MPI_Wait
27: leave BufferedCommunicator::sendRecv
27: communicator.hh::BufferedCommunicator::forward, left sendRecv
27 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), after communicator.free();
27 schwarz.hh:virtual void apply (X& v, const Y& d), AFTER communication.copyOwnerToAll(v,v); 
void preSmooth(Smoother& smoother, Domain& v, const Range& d), AFTER smoother.apply(v,d);
void presmooth(LevelContext& levelContext, size_t steps), AFTER SmootherApplier<typename LevelContext::SmootherType>::preSmooth, step: 0 of 2
void presmooth(LevelContext& levelContext, size_t steps), before update defec, step: 0 of 2
void presmooth(LevelContext& levelContext, size_t steps), before SmootherApplier<typename LevelContext::SmootherType>::preSmooth, step: 1 of 2
void preSmooth(Smoother& smoother, Domain& v, const Range& d), before smoother.apply(v,d);
27 schwarz.hh:virtual void apply (X& v, const Y& d), before preconditioner.apply(v,d);; 
27 schwarz.hh:virtual void apply (X& v, const Y& d), before communication66 === BiCGSTABSolver
 Iter          Defect            Rate
    0      1.17536e-06
66  (it = 0.5; it < _maxit; it+=.5) 0.5 250
66  before p = r 
66  before  y = W^-1 * p 
66  before _prec->apply(y,p); 0.5 250
66 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
66void AMG<M,X,S,PI,A>::mgc(LevelContext& levelContext), before presmooth(levelContext, preSteps_) 
void presmooth(LevelContext& levelContext, size_t steps), before SmootherApplier<typename LevelContext::SmootherType>::preSmooth, step: 0 of 2
void preSmooth(Smoother& smoother, Domain& v, const Range& d), before smoother.apply(v,d);
66 schwarz.hh:virtual void apply (X& v, const Y& d), before preconditioner.apply(v,d);; 
66 schwarz.hh:virtual void apply (X& v, const Y& d), before communication.copyOwnerToAll(v,v); 
66 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
66: communicator.hh::BufferedCommunicator::forward, enter sendRecv
66: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 15
66: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 15
66: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 15
66: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 15
66: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 15
66: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 15
66: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 15
66: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 15
66: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 15
66: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 15
66: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 10 15
66: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 11 15
66: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 12 15
66: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 13 15
66: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 14 15
66: communicator.hh::sendRecv, left MPI_Wait
66: leave BufferedCommunicator::sendRecv
66: communicator.hh::BufferedCommunicator::forward, left sendRecv
66 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), after communicator.free();
66 schwarz.hh:virtual void apply (X& v, const Y& d), AFTER communication.copyOwnerToAll(v,v); 
void preSmooth(Smoother& smoother, Domain& v, const Range& d), AFTER smoother.apply(v,d);
void presmooth(LevelContext& levelContext, size_t steps), AFTER SmootherApplier<typename LevelContext::SmootherType>::preSmooth, step: 0 of 2
void presmooth(LevelContext& levelContext, size_t steps), before update defec, step: 0 of 2
void presmooth(LevelContext& levelContext, size_t steps), before SmootherApplier<typename LevelContext::SmootherType>::preSmooth, step: 1 of 2
void preSmooth(Smoother& smoother, Domain& v, const Range& d), before smoother.apply(v,d);
66 schwarz.hh:virtual void apply (X& v, const Y& d), before preconditioner.apply(v,d);; 
66 schwarz.hh:virtual void apply (X& v, const Y& d), before communication.copyOwnerToAll(v,v); 
66 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
66: communicator.hh::BufferedCommunicator::forward, enter sendRecv
66: com58 === BiCGSTABSolver
 Iter          Defect            Rate
    0      1.17536e-06
58  (it = 0.5; it < _maxit; it+=.5) 0.5 250
58  before p = r 
58  before  y = W^-1 * p 
58  before _prec->apply(y,p); 0.5 250
58 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
58void AMG<M,X,S,PI,A>::mgc(LevelContext& levelContext), before presmooth(levelContext, preSteps_) 
void presmooth(LevelContext& levelContext, size_t steps), before SmootherApplier<typename LevelContext::SmootherType>::preSmooth, step: 0 of 2
void preSmooth(Smoother& smoother, Domain& v, const Range& d), before smoother.apply(v,d);
58 schwarz.hh:virtual void apply (X& v, const Y& d), before preconditioner.apply(v,d);; 
58 schwarz.hh:virtual void apply (X& v, const Y& d), before communication.copyOwnerToAll(v,v); 
58 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
58: communicator.hh::BufferedCommunicator::forward, enter sendRecv
58: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 21
58: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 21
58: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 21
58: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 21
58: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 21
58: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 21
58: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 21
58: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 21
58: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 21
58: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 21
58: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 10 21
58: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 11 21
58: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 12 21
58: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 13 21
58: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 14 21
58: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 15 21
58: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 16 21
34 === BiCGSTABSolver
 Iter          Defect            Rate
    0      1.17536e-06
34  (it = 0.5; it < _maxit; it+=.5) 0.5 250
34  before p = r 
34  before  y = W^-1 * p 
34  before _prec->apply(y,p); 0.5 250
34 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
34void AMG<M,X,S,PI,A>::mgc(LevelContext& levelContext), before presmooth(levelContext, preSteps_) 
void presmooth(LevelContext& levelContext, size_t steps), before SmootherApplier<typename LevelContext::SmootherType>::preSmooth, step: 0 of 2
void preSmooth(Smoother& smoother, Domain& v, const Range& d), before smoother.apply(v,d);
34 schwarz.hh:virtual void apply (X& v, const Y& d), before preconditioner.apply(v,d);; 
34 schwarz.hh:virtual void apply (X& v, const Y& d), before communication.copyOwnerToAll(v,v); 
34 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
34: communicator.hh::BufferedCommunicator::forward, enter sendRecv
34: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 14
34: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 14
34: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 14
34: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 14
34: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 14
34: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 14
34: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 14
34: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 14
34: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 14
34: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 14
34: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 10 14
34: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 11 14
34: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 12 14
34: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 13 14
91 === BiCGSTABSolver
 Iter          Defect            Rate
    0      1.17536e-06
91  (it = 0.5; it < _maxit; it+=.5) 0.5 250
91  before p = r 
91  before  y = W^-1 * p 
91  before _prec->apply(y,p); 0.5 250
91 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
91void AMG<M,X,S,PI,A>::mgc(LevelContext& levelContext), before presmooth(levelContext, preSteps_) 
void presmooth(LevelContext& levelContext, size_t steps), before SmootherApplier<typename LevelContext::SmootherType>::preSmooth, step: 0 of 2
void preSmooth(Smoother& smoother, Domain& v, const Range& d), before smoother.apply(v,d);
91 schwarz.hh:virtual void apply (X& v, const Y& d), before preconditioner.apply(v,d);; 
91 schwarz.hh:virtual void apply (X& v, const Y& d), before communication.copyOwnerToAll(v,v); 
91 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
91: communicator.hh::BufferedCommunicator::forward, enter sendRecv
91: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 9
91: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 9
91: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 9
91: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 9
91: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 9
91: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 9
91: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 9
91: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 9
91: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 9
91: communicator.hh::sendRecv, left MPI_Wait
91: leave BufferedCommunicator::sendRecv
91: communicator.hh::BufferedCommunicator::forward, left sendRecv
91 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), after communicator.free();
91 schwarz.hh:virtual void apply (X& v, const Y& d), AFTER communication.copyOwnerToAll(v,v); 
void preSmooth(Smoother& smoother, Domain& v, const Range& d), AFTER smoother.apply(v,d);
void presmooth(LevelContext& levelContext, size_t steps), AFTER SmootherApplier<typename LevelContext::SmootherType>::preSmooth, step: 0 of 2
void presmooth(LevelContext& levelContext, size_t steps), before update defec, step: 0 of 2
void presmooth(LevelContext& levelContext, size_t steps), before SmootherApplier<typename LevelContext::SmootherType>::preSmooth, step: 1 of 2
void preSmooth(Smoother& smoother, Domain& v, const Range& d), before smoother.apply(v,d);
91 schwarz.hh:virtual void apply (X& v, const Y& d), before preconditioner.apply(v,d);; 
91 schwarz.hh:virtual void apply (X& v, const Y& d), before communication.copyOwnerToAll(v,v); 
91 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
91: communicator.hh::BufferedCommunicator::forward, enter sendRecv
91: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 9
91: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 9
91: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 9
91: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 9
91: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 9
91: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 9
91: communicator.hh::sendRe76 === BiCGSTABSolver
 Iter          Defect            Rate
    0      1.17536e-06
76  (it = 0.5; it < _maxit; it+=.5) 0.5 250
76  before p = r 
76  before  y = W^-1 * p 
76  before _prec->apply(y,p); 0.5 250
76 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
76void AMG<M,X,S,PI,A>::mgc(LevelContext& levelContext), before presmooth(levelContext, preSteps_) 
void presmooth(LevelContext& levelContext, size_t steps), before SmootherApplier<typename LevelContext::SmootherType>::preSmooth, step: 0 of 2
void preSmooth(Smoother& smoother, Domain& v, const Range& d), before smoother.apply(v,d);
76 schwarz.hh:virtual void apply (X& v, const Y& d), before preconditioner.apply(v,d);; 
76 schwarz.hh:virtual void apply (X& v, const Y& d), before communication.copyOwnerToAll(v,v); 
76 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
76: communicator.hh::BufferedCommunicator::forward, enter sendRecv
76: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 12
76: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 12
76: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 12
76: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 12
76: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 12
76: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 12
76: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 12
76: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 12
76: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 12
76: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 12
76: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 10 12
74 === BiCGSTABSolver
 Iter          Defect            Rate
    0      1.17536e-06
74  (it = 0.5; it < _maxit; it+=.5) 0.5 250
74  before p = r 
74  before  y = W^-1 * p 
74  before _prec->apply(y,p); 0.5 250
74 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
74void AMG<M,X,S,PI,A>::mgc(LevelContext& levelContext), before presmooth(levelContext, preSteps_) 
void presmooth(LevelContext& levelContext, size_t steps), before SmootherApplier<typename LevelContext::SmootherType>::preSmooth, step: 0 of 2
void preSmooth(Smoother& smoother, Domain& v, const Range& d), before smoother.apply(v,d);
74 schwarz.hh:virtual void apply (X& v, const Y& d), before preconditioner.apply(v,d);; 
74 schwarz.hh:virtual void apply (X& v, const Y& d), before communication.copyOwnerToAll(v,v); 
74 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
74: communicator.hh::BufferedCommunicator::forward, enter sendRecv
74: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 11
74: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 11
74: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 11
74: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 11
74: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 11
74: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 11
74: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 11
74: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 11
74: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 11
74: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 11
74: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 10 11
74: communicator.hh::sendRecv, left MPI_Wait
74: leave BufferedCommunicator::sendRecv
74: communicator.hh::BufferedCommunicator::forward, left sendRecv
74 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), after communicator.free();
74 schwarz.hh:virtual void apply (X& v, const Y& d), AFTER communication.copyOwnerToAll(v,v); 
void preSmooth(Smoother& smoother, Domain& v, const Range& d), AFTER smoother.apply(v,d);
void presmooth(LevelContext& levelContext, size_t steps), AFTER SmootherApplier<typename LevelContext::SmootherType>::preSmooth, step: 0 of 2
void presmooth(LevelContext& levelContext, size_t steps), before update defec, step: 0 of 2
void presmooth(LevelContext& levelContext, size_t steps), before SmootherApplier<typename LevelContext::SmootherType>::preSmooth, step: 1 of 2
void preSmooth(Smoother& smoother, Domain& v, const Range& d), before smoother.apply(v,d);
74 schwarz.hh:virtual void apply (X& v, const Y& d), before preconditioner.apply(v,d);; 
74 schwarz.hh:virtual void apply (X& v, const Y& d), before communication.copyOwnerToAll(v,v); 
74 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
74: communicator.hh::BufferedCommunicator::forward, enter sendRecv
74: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 11
74: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 11
74: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 11
74: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 11
74: communi37  before p = r 
37  before  y = W^-1 * p 
37  before _prec->apply(y,p); 0.5 250
37 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
37void AMG<M,X,S,PI,A>::mgc(LevelContext& levelContext), before presmooth(levelContext, preSteps_) 
void presmooth(LevelContext& levelContext, size_t steps), before SmootherApplier<typename LevelContext::SmootherType>::preSmooth, step: 0 of 2
void preSmooth(Smoother& smoother, Domain& v, const Range& d), before smoother.apply(v,d);
37 schwarz.hh:virtual void apply (X& v, const Y& d), before preconditioner.apply(v,d);; 
37 schwarz.hh:virtual void apply (X& v, const Y& d), before communication.copyOwnerToAll(v,v); 
37 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
37: communicator.hh::BufferedCommunicator::forward, enter sendRecv
37: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 15
37: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 15
37: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 15
37: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 15
37: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 15
37: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 15
37: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 15
37: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 15
37: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 15
37: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 15
37: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 10 15
37: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 11 15
37: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 12 15
37: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 13 15
37: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 14 15
37: communicator.hh::sendRecv, left MPI_Wait
37: leave BufferedCommunicator::sendRecv
37: communicator.hh::BufferedCommunicator::forward, left sendRecv
37 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), after communicator.free();
37 schwarz.hh:virtual void apply (X& v, const Y& d), AFTER communication.copyOwnerToAll(v,v); 
void preSmooth(Smoother& smoother, Domain& v, const Range& d), AFTER smoother.apply(v,d);
void presmooth(LevelContext& levelContext, size_t steps), AFTER SmootherApplier<typename LevelContext::SmootherType>::preSmooth, step: 0 of 2
void presmooth(LevelContext& levelContext, size_t steps), before update defec, step: 0 of 2
void presmooth(LevelContext& levelContext, size_t steps), before SmootherApplier<typename LevelContext::SmootherType>::preSmooth, step: 1 of 2
void preSmooth(Smoother& smoother, Domain& v, const Range& d), before smoother.apply(v,d);
37 schwarz.hh:virtual void apply (X& v, const Y& d), before preconditioner.apply(v,d);; 
37 schwarz.hh:virtual void apply (X& v, const Y& d), before communication.copyOwnerToAll(v,v); 
37 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
37: communicator.hh::BufferedCommunicator::forward, enter sendRecv
37: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 15
37: communicator39 === BiCGSTABSolver
 Iter          Defect            Rate
    0      1.17536e-06
39  (it = 0.5; it < _maxit; it+=.5) 0.5 250
39  before p = r 
39  before  y = W^-1 * p 
39  before _prec->apply(y,p); 0.5 250
39 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
39void AMG<M,X,S,PI,A>::mgc(LevelContext& levelContext), before presmooth(levelContext, preSteps_) 
void presmooth(LevelContext& levelContext, size_t steps), before SmootherApplier<typename LevelContext::SmootherType>::preSmooth, step: 0 of 2
void preSmooth(Smoother& smoother, Domain& v, const Range& d), before smoother.apply(v,d);
39 schwarz.hh:virtual void apply (X& v, const Y& d), before preconditioner.apply(v,d);; 
39 schwarz.hh:virtual void apply (X& v, const Y& d), before communication.copyOwnerToAll(v,v); 
39 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
39: communicator.hh::BufferedCommunicator::forward, enter sendRecv
39: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 9
39: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 9
39: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 9
39: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 9
39: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 9
39: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 9
39: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 9
39: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 9
39: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 9
39: communicator.hh::sendRecv, left MPI_Wait
39: leave BufferedCommunicator::sendRecv
39: communicator.hh::BufferedCommunicator::forward, left sendRecv
39 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), after communicator.free();
39 schwarz.hh:virtual void apply (X& v, const Y& d), AFTER communication.copyOwnerToAll(v,v); 
void preSmooth(Smoother& smoother, Domain& v, const Range& d), AFTER smoother.apply(v,d);
void presmooth(LevelContext& levelContext, size_t steps), AFTER SmootherApplier<typename LevelContext::SmootherType>::preSmooth, step: 0 of 2
void presmooth(LevelContext& levelContext, size_t steps), before update defec, step: 0 of 2
void presmooth(LevelContext& levelContext, size_t steps), before SmootherApplier<typename LevelContext::SmootherType>::preSmooth, step: 1 of 2
void preSmooth(Smoother& smoother, Domain& v, const Range& d), before smoother.apply(v,d);
39 schwarz.hh:virtual void apply (X& v, const Y& d), before preconditioner.apply(v,d);; 
39 schwarz.hh:virtual void apply (X& v, const Y& d), before communication.copyOwnerToAll(v,v); 
39 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
39: communicator.hh::BufferedCommunicator::forward, enter sendRecv
39: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 9
39: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 9
39: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 9
39: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 9
39: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 9
39: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 9
108 === BiCGSTABSolver
 Iter          Defect            Rate
    0      1.17536e-06
108  (it = 0.5; it < _maxit; it+=.5) 0.5 250
108  before p = r 
108  before  y = W^-1 * p 
108  before _prec->apply(y,p); 0.5 250
108 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
108void AMG<M,X,S,PI,A>::mgc(LevelContext& levelContext), before presmooth(levelContext, preSteps_) 
void presmooth(LevelContext& levelContext, size_t steps), before SmootherApplier<typename LevelContext::SmootherType>::preSmooth, step: 0 of 2
void preSmooth(Smoother& smoother, Domain& v, const Range& d), before smoother.apply(v,d);
108 schwarz.hh:virtual void apply (X& v, const Y& d), before preconditioner.apply(v,d);; 
108 schwarz.hh:virtual void apply (X& v, const Y& d), before communication.copyOwnerToAll(v,v); 
108 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
108: communicator.hh::BufferedCommunicator::forward, enter sendRecv
108: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 9
108: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 9
108: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 9
108: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 9
108: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 9
108: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 9
108: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 9
108: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 9
108: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 9
108: communicator.hh::sendRecv, left MPI_Wait
108: leave BufferedCommunicator::sendRecv
108: communicator.hh::BufferedCommunicator::forward, left sendRecv
108 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), after communicator.free();
108 schwarz.hh:virtual void apply (X& v, const Y& d), AFTER communication.copyOwnerToAll(v,v); 
void preSmooth(Smoother& smoother, Domain& v, const Range& d), AFTER smoother.apply(v,d);
void presmooth(LevelContext& levelContext, size_t steps), AFTER SmootherApplier<typename LevelContext::SmootherType>::preSmooth, step: 0 of 2
void presmooth(LevelContext& levelContext, size_t steps), before update defec, step: 0 of 2
void presmooth(LevelContext& levelContext, size_t steps), before SmootherApplier<typename LevelContext::SmootherType>::preSmooth, step: 1 of 2
void preSmooth(Smoother& smoother, Domain& v, const Range& d), before smoother.apply(v,d);
108 schwarz.hh:virtual void apply (X& v, const Y& d), before preconditioner.apply(v,d);; 
108 schwarz.hh:virtual void apply (X& v, const Y& d), before communication.copyOwnerToAll(v,v); 
108 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
108: communicator.hh::BufferedCommunicator::forward, enter sendRecv
108: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 9
108: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 9
108: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 9
108: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 9
108: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 9
108: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &statu42 === BiCGSTABSolver
 Iter          Defect            Rate
    0      1.17536e-06
42  (it = 0.5; it < _maxit; it+=.5) 0.5 250
42  before p = r 
42  before  y = W^-1 * p 
42  before _prec->apply(y,p); 0.5 250
42 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
42void AMG<M,X,S,PI,A>::mgc(LevelContext& levelContext), before presmooth(levelContext, preSteps_) 
void presmooth(LevelContext& levelContext, size_t steps), before SmootherApplier<typename LevelContext::SmootherType>::preSmooth, step: 0 of 2
void preSmooth(Smoother& smoother, Domain& v, const Range& d), before smoother.apply(v,d);
42 schwarz.hh:virtual void apply (X& v, const Y& d), before preconditioner.apply(v,d);; 
42 schwarz.hh:virtual void apply (X& v, const Y& d), before communication.copyOwnerToAll(v,v); 
42 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
42: communicator.hh::BufferedCommunicator::forward, enter sendRecv
42: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 20
42: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 20
42: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 20
42: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 20
42: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 20
42: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 20
42: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 20
42: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 20
42: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 20
42: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 20
42: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 10 20
42: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 11 20
95 === BiCGSTABSolver
 Iter          Defect            Rate
    0      1.17536e-06
95  (it = 0.5; it < _maxit; it+=.5) 0.5 250
95  before p = r 
95  before  y = W^-1 * p 
95  before _prec->apply(y,p); 0.5 250
95 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
95void AMG<M,X,S,PI,A>::mgc(LevelContext& levelContext), before presmooth(levelContext, preSteps_) 
void presmooth(LevelContext& levelContext, size_t steps), before SmootherApplier<typename LevelContext::SmootherType>::preSmooth, step: 0 of 2
void preSmooth(Smoother& smoother, Domain& v, const Range& d), before smoother.apply(v,d);
95 schwarz.hh:virtual void apply (X& v, const Y& d), before preconditioner.apply(v,d);; 
95 schwarz.hh:virtual void apply (X& v, const Y& d), before communication.copyOwnerToAll(v,v); 
95 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
95: communicator.hh::BufferedCommunicator::forward, enter sendRecv
95: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 7
95: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 7
95: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 7
95: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 7
95: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 7
95: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 7
95: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 7
95: communicator.hh::sendRecv, left MPI_Wait
95: leave BufferedCommunicator::sendRecv
95: communicator.hh::BufferedCommunicator::forward, left sendRecv
95 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), after communicator.free();
95 schwarz.hh:virtual void apply (X& v, const Y& d), AFTER communication.copyOwnerToAll(v,v); 
void preSmooth(Smoother& smoother, Domain& v, const Range& d), AFTER smoother.apply(v,d);
void presmooth(LevelContext& levelContext, size_t steps), AFTER SmootherApplier<typename LevelContext::SmootherType>::preSmooth, step: 0 of 2
void presmooth(LevelContext& levelContext, size_t steps), before update defec, step: 0 of 2
void presmooth(LevelContext& levelContext, size_t steps), before SmootherApplier<typename LevelContext::SmootherType>::preSmooth, step: 1 of 2
void preSmooth(Smoother& smoother, Domain& v, const Range& d), before smoother.apply(v,d);
95 schwarz.hh:virtual void apply (X& v, const Y& d), before preconditioner.apply(v,d);; 
95 schwarz.hh:virtual void apply (X& v, const Y& d), before communication.copyOwnerToAll(v,v); 
95 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
95: communicator.hh::BufferedCommunicator::forward, enter sendRecv
95: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 7
95: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 7
95: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 7
95: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 7
95: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 7
95: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 7
95: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 7
95: communicator.hh::sendRecv, left MPI_Wait
95: leave BufferedCommunicator::sendRecv
95: communicator.hh::BufferedCommunicator::forward, left s114 === BiCGSTABSolver
 Iter          Defect            Rate
    0      1.17536e-06
114  (it = 0.5; it < _maxit; it+=.5) 0.5 250
114  before p = r 
114  before  y = W^-1 * p 
114  before _prec->apply(y,p); 0.5 250
114 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
114void AMG<M,X,S,PI,A>::mgc(LevelContext& levelContext), before presmooth(levelContext, preSteps_) 
void presmooth(LevelContext& levelContext, size_t steps), before SmootherApplier<typename LevelContext::SmootherType>::preSmooth, step: 0 of 2
void preSmooth(Smoother& smoother, Domain& v, const Range& d), before smoother.apply(v,d);
114 schwarz.hh:virtual void apply (X& v, const Y& d), before preconditioner.apply(v,d);; 
114 schwarz.hh:virtual void apply (X& v, const Y& d), before communication.copyOwnerToAll(v,v); 
114 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
114: communicator.hh::BufferedCommunicator::forward, enter sendRecv
114: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 13
114: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 13
114: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 13
114: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 13
114: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 13
114: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 13
114: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 13
114: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 13
114: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 13
114: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 13
114: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 10 13
114: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 11 13
114: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 12 13
114: communicator.hh::sendRecv, left MPI_Wait
114: leave BufferedCommunicator::sendRecv
114: communicator.hh::BufferedCommunicator::forward, left sendRecv
114 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), after communicator.free();
114 schwarz.hh:virtual void apply (X& v, const Y& d), AFTER communication.copyOwnerToAll(v,v); 
void preSmooth(Smoother& smoother, Domain& v, const Range& d), AFTER smoother.apply(v,d);
void presmooth(LevelContext& levelContext, size_t steps), AFTER SmootherApplier<typename LevelContext::SmootherType>::preSmooth, step: 0 of 2
void presmooth(LevelContext& levelContext, size_t steps), before update defec, step: 0 of 2
void presmooth(LevelContext& levelContext, size_t steps), before SmootherApplier<typename LevelContext::SmootherType>::preSmooth, step: 1 of 2
void preSmooth(Smoother& smoother, Domain& v, const Range& d), before smoother.apply(v,d);
114 schwarz.hh:virtual void apply (X& v, const Y& d), before preconditioner.apply(v,d);; 
114 schwarz.hh:virtual void apply (X& v, const Y& d), before communication.copyOwnerToAll(v,v); 
114 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
114: communicator.hh::BufferedCommunicator::forward, enter sendRecv
114: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 13
114: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, 62 === BiCGSTABSolver
 Iter          Defect            Rate
    0      1.17536e-06
62  (it = 0.5; it < _maxit; it+=.5) 0.5 250
62  before p = r 
62  before  y = W^-1 * p 
62  before _prec->apply(y,p); 0.5 250
62 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
62void AMG<M,X,S,PI,A>::mgc(LevelContext& levelContext), before presmooth(levelContext, preSteps_) 
void presmooth(LevelContext& levelContext, size_t steps), before SmootherApplier<typename LevelContext::SmootherType>::preSmooth, step: 0 of 2
void preSmooth(Smoother& smoother, Domain& v, const Range& d), before smoother.apply(v,d);
62 schwarz.hh:virtual void apply (X& v, const Y& d), before preconditioner.apply(v,d);; 
104 === BiCGSTABSolver
 Iter          Defect            Rate
    0      1.17536e-06
104  (it = 0.5; it < _maxit; it+=.5) 0.5 250
104  before p = r 
104  before  y = W^-1 * p 
104  before _prec->apply(y,p); 0.5 250
104 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
104void AMG<M,X,S,PI,A>::mgc(LevelContext& levelContext), before presmooth(levelContext, preSteps_) 
void presmooth(LevelContext& levelContext, size_t steps), before SmootherApplier<typename LevelContext::SmootherType>::preSmooth, step: 0 of 2
void preSmooth(Smoother& smoother, Domain& v, const Range& d), before smoother.apply(v,d);
104 schwarz.hh:virtual void apply (X& v, const Y& d), before preconditioner.apply(v,d);; 
104 schwarz.hh:virtual void apply (X& v, const Y& d), before communication.copyOwnerToAll(v,v); 
104 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
104: communicator.hh::BufferedCommunicator::forward, enter sendRecv
104: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 9
104: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 9
104: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 9
104: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 9
104: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 9
104: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 9
104: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 9
104: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 9
104: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 9
104: communicator.hh::sendRecv, left MPI_Wait
104: leave BufferedCommunicator::sendRecv
104: communicator.hh::BufferedCommunicator::forward, left sendRecv
104 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), after communicator.free();
104 schwarz.hh:virtual void apply (X& v, const Y& d), AFTER communication.copyOwnerToAll(v,v); 
void preSmooth(Smoother& smoother, Domain& v, const Range& d), AFTER smoother.apply(v,d);
void presmooth(LevelContext& levelContext, size_t steps), AFTER SmootherApplier<typename LevelContext::SmootherType>::preSmooth, step: 0 of 2
void presmooth(LevelContext& levelContext, size_t steps), before update defec, step: 0 of 2
void presmooth(LevelContext& levelContext, size_t steps), before SmootherApplier<typename LevelContext::SmootherType>::preSmooth, step: 1 of 2
void preSmooth(Smoother& smoother, Domain& v, const Range& d), before smoother.apply(v,d);
104 schwarz.hh:virtual void apply (X& v, const Y& d), before preconditioner.apply(v,d);; 
104 schwarz.hh:virtual void apply (X& v, const Y& d), before communication.copyOwnerToAll(v,v); 
104 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
104: communicator.hh::BufferedCommunicator::forward, enter sendRecv
104: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 9
104: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 9
104: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 9
104: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 9
104: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 9
104: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &statu79 === BiCGSTABSolver
 Iter          Defect            Rate
    0      1.17536e-06
79  (it = 0.5; it < _maxit; it+=.5) 0.5 250
79  before p = r 
79  before  y = W^-1 * p 
79  before _prec->apply(y,p); 0.5 250
79 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
79void AMG<M,X,S,PI,A>::mgc(LevelContext& levelContext), before presmooth(levelContext, preSteps_) 
void presmooth(LevelContext& levelContext, size_t steps), before SmootherApplier<typename LevelContext::SmootherType>::preSmooth, step: 0 of 2
void preSmooth(Smoother& smoother, Domain& v, const Range& d), before smoother.apply(v,d);
79 schwarz.hh:virtual void apply (X& v, const Y& d), before preconditioner.apply(v,d);; 
79 schwarz.hh:virtual void apply (X& v, const Y& d), before communication.copyOwnerToAll(v,v); 
79 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
79: communicator.hh::BufferedCommunicator::forward, enter sendRecv
79: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 16
79: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 16
79: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 16
79: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 16
79: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 16
79: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 16
79: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 16
79: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 16
79: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 16
79: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 16
79: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 10 16
79: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 11 16
79: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 12 16
79: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 13 16
79: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 14 16
79: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 15 16
79: communicator.hh::sendRecv, left MPI_Wait
79: leave BufferedCommunicator::sendRecv
79: communicator.hh::BufferedCommunicator::forward, left sendRecv
79 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), after communicator.free();
79 schwarz.hh:virtual void apply (X& v, const Y& d), AFTER communication.copyOwnerToAll(v,v); 
void preSmooth(Smoother& smoother, Domain& v, const Range& d), AFTER smoother.apply(v,d);
void presmooth(LevelContext& levelContext, size_t steps), AFTER SmootherApplier<typename LevelContext::SmootherType>::preSmooth, step: 0 of 2
void presmooth(LevelContext& levelContext, size_t steps), before update defec, step: 0 of 2
void presmooth(LevelContext& levelContext, size_t steps), before SmootherApplier<typename LevelContext::SmootherType>::preSmooth, step: 1 of 2
void preSmooth(Smoother& smoother, Domain& v, const Range& d), before smoother.apply(v,d);
79 schwarz.hh:virtual void apply (X& v, const Y& d), before preconditioner.apply(v,d);; 
79 schwarz.hh:virtual void apply (X& v, const Y& d), before communication.copyOwnerToAll(v,v); 
79 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template 43 === BiCGSTABSolver
 Iter          Defect            Rate
    0      1.17536e-06
43  (it = 0.5; it < _maxit; it+=.5) 0.5 250
43  before p = r 
43  before  y = W^-1 * p 
43  before _prec->apply(y,p); 0.5 250
43 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
43void AMG<M,X,S,PI,A>::mgc(LevelContext& levelContext), before presmooth(levelContext, preSteps_) 
void presmooth(LevelContext& levelContext, size_t steps), before SmootherApplier<typename LevelContext::SmootherType>::preSmooth, step: 0 of 2
void preSmooth(Smoother& smoother, Domain& v, const Range& d), before smoother.apply(v,d);
43 schwarz.hh:virtual void apply (X& v, const Y& d), before preconditioner.apply(v,d);; 
43 schwarz.hh:virtual void apply (X& v, const Y& d), before communication.copyOwnerToAll(v,v); 
43 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
43: communicator.hh::BufferedCommunicator::forward, enter sendRecv
43: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 11
43: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 11
43: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 11
43: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 11
43: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 11
43: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 11
43: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 11
43: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 11
43: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 11
43: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 11
48 === BiCGSTABSolver
 Iter          Defect            Rate
    0      1.17536e-06
48  (it = 0.5; it < _maxit; it+=.5) 0.5 250
48  before p = r 
48  before  y = W^-1 * p 
48  before _prec->apply(y,p); 0.5 250
48 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
48void AMG<M,X,S,PI,A>::mgc(LevelContext& levelContext), before presmooth(levelContext, preSteps_) 
void presmooth(LevelContext& levelContext, size_t steps), before SmootherApplier<typename LevelContext::SmootherType>::preSmooth, step: 0 of 2
void preSmooth(Smoother& smoother, Domain& v, const Range& d), before smoother.apply(v,d);
48 schwarz.hh:virtual void apply (X& v, const Y& d), before preconditioner.apply(v,d);; 
48 schwarz.hh:virtual void apply (X& v, const Y& d), before communication.copyOwnerToAll(v,v); 
48 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
48: communicator.hh::BufferedCommunicator::forward, enter sendRecv
48: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 15
48: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 15
48: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 15
48: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 15
48: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 15
48: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 15
48: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 15
48: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 15
48: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 15
48: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 15
48: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 10 15
48: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 11 15
48: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 12 15
48: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 13 15
48: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 14 15
80 === BiCGSTABSolver
 Iter          Defect            Rate
    0      1.17536e-06
80  (it = 0.5; it < _maxit; it+=.5) 0.5 250
80  before p = r 
80  before  y = W^-1 * p 
80  before _prec->apply(y,p); 0.5 250
80 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
80void AMG<M,X,S,PI,A>::mgc(LevelContext& levelContext), before presmooth(levelContext, preSteps_) 
void presmooth(LevelContext& levelContext, size_t steps), before SmootherApplier<typename LevelContext::SmootherType>::preSmooth, step: 0 of 2
void preSmooth(Smoother& smoother, Domain& v, const Range& d), before smoother.apply(v,d);
80 schwarz.hh:virtual void apply (X& v, const Y& d), before preconditioner.apply(v,d);; 
80 schwarz.hh:virtual void apply (X& v, const Y& d), before communication.copyOwnerToAll(v,v); 
80 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
80: communicator.hh::BufferedCommunicator::forward, enter sendRecv
80: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 10
80: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 10
80: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 10
80: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 10
80: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 10
80: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 10
80: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 10
80: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 10
80: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 10
17 === BiCGSTABSolver
 Iter          Defect            Rate
    0      1.17536e-06
17  (it = 0.5; it < _maxit; it+=.5) 0.5 250
17  before p = r 
17  before  y = W^-1 * p 
17  before _prec->apply(y,p); 0.5 250
17 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
17void AMG<M,X,S,PI,A>::mgc(LevelContext& levelContext), before presmooth(levelContext, preSteps_) 
void presmooth(LevelContext& levelContext, size_t steps), before SmootherApplier<typename LevelContext::SmootherType>::preSmooth, step: 0 of 2
void preSmooth(Smoother& smoother, Domain& v, const Range& d), before smoother.apply(v,d);
17 schwarz.hh:virtual void apply (X& v, const Y& d), before preconditioner.apply(v,d);; 
17 schwarz.hh:virtual void apply (X& v, const Y& d), before communication.copyOwnerToAll(v,v); 
17 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
17: communicator.hh::BufferedCommunicator::forward, enter sendRecv
17: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 3
17: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 3
17: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 3
17: communicator.hh::sendRecv, left MPI_Wait
17: leave BufferedCommunicator::sendRecv
17: communicator.hh::BufferedCommunicator::forward, left sendRecv
17 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), after communicator.free();
17 schwarz.hh:virtual void apply (X& v, const Y& d), AFTER communication.copyOwnerToAll(v,v); 
void preSmooth(Smoother& smoother, Domain& v, const Range& d), AFTER smoother.apply(v,d);
void presmooth(LevelContext& levelContext, size_t steps), AFTER SmootherApplier<typename LevelContext::SmootherType>::preSmooth, step: 0 of 2
void presmooth(LevelContext& levelContext, size_t steps), before update defec, step: 0 of 2
void presmooth(LevelContext& levelContext, size_t steps), before SmootherApplier<typename LevelContext::SmootherType>::preSmooth, step: 1 of 2
void preSmooth(Smoother& smoother, Domain& v, const Range& d), before smoother.apply(v,d);
17 schwarz.hh:virtual void apply (X& v, const Y& d), before preconditioner.apply(v,d);; 
17 schwarz.hh:virtual void apply (X& v, const Y& d), before communication.copyOwnerToAll(v,v); 
17 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
17: communicator.hh::BufferedCommunicator::forward, enter sendRecv
17: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 3
17: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 3
17: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 3
17: communicator.hh::sendRecv, left MPI_Wait
17: leave BufferedCommunicator::sendRecv
17: communicator.hh::BufferedCommunicator::forward, left sendRecv
17 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), after communicator.free();
17 schwarz.hh:virtual void apply (X& v, const Y& d), AFTER communication.copyOwnerToAll(v,v); 
void preSmooth(Smoother& smoother, Domain& v, const Range& d), AFTER smoother.apply(v,d);
void presmooth(LevelContext& levelContext, size_t steps), AFTER SmootherApplier<typename LevelContext::SmootherType>::preSmooth, step: 1 of 2
void presmooth(LevelContext& levelContext, size_t steps), before update defec, step: 1 of 2
void presmooth(LevelContext& levelContext, size_t steps), finished 
17void AMG<M,X,S,PI,A>::mgc(LevelContext& levelContext), AFTER presmooth(levelContext, preSteps_) 
17: communicator.hh::BufferedCommunicator::forward, enter sendRecv
17: communicator.hh::sendRecv, left MPI_Wait
17: leave BufferedCommunicator::sendRecv
17: communicator.hh::BufferedCommunicator::forward, left sendRecv
17: communicator.hh::sendRecv84 === BiCGSTABSolver
 Iter          Defect            Rate
    0      1.17536e-06
84  (it = 0.5; it < _maxit; it+=.5) 0.5 250
84  before p = r 
84  before  y = W^-1 * p 
84  before _prec->apply(y,p); 0.5 250
84 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
84void AMG<M,X,S,PI,A>::mgc(LevelContext& levelContext), before presmooth(levelContext, preSteps_) 
void presmooth(LevelContext& levelContext, size_t steps), before SmootherApplier<typename LevelContext::SmootherType>::preSmooth, step: 0 of 2
void preSmooth(Smoother& smoother, Domain& v, const Range& d), before smoother.apply(v,d);
84 schwarz.hh:virtual void apply (X& v, const Y& d), before preconditioner.apply(v,d);; 
84 schwarz.hh:virtual void apply (X& v, const Y& d), before communication.copyOwnerToAll(v,v); 
84 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
84: communicator.hh::BufferedCommunicator::forward, enter sendRecv
84: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 18
84: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 18
84: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 18
84: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 18
84: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 18
84: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 18
84: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 18
84: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 18
84: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 18
84: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 18
84: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 10 18
84: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 11 18
84: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 12 18
84: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 13 18
60 === BiCGSTABSolver
 Iter          Defect            Rate
    0      1.17536e-06
60  (it = 0.5; it < _maxit; it+=.5) 0.5 250
60  before p = r 
60  before  y = W^-1 * p 
60  before _prec->apply(y,p); 0.5 250
60 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
60void AMG<M,X,S,PI,A>::mgc(LevelContext& levelContext), before presmooth(levelContext, preSteps_) 
void presmooth(LevelContext& levelContext, size_t steps), before SmootherApplier<typename LevelContext::SmootherType>::preSmooth, step: 0 of 2
void preSmooth(Smoother& smoother, Domain& v, const Range& d), before smoother.apply(v,d);
60 schwarz.hh:virtual void apply (X& v, const Y& d), before preconditioner.apply(v,d);; 
60 schwarz.hh:virtual void apply (X& v, const Y& d), before communication.copyOwnerToAll(v,v); 
60 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
60: communicator.hh::BufferedCommunicator::forward, enter sendRecv
60: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 6
60: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 6
60: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 6
60: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 6
60: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 6
60: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 6
60: communicator.hh::sendRecv, left MPI_Wait
60: leave BufferedCommunicator::sendRecv
60: communicator.hh::BufferedCommunicator::forward, left sendRecv
60 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), after communicator.free();
60 schwarz.hh:virtual void apply (X& v, const Y& d), AFTER communication.copyOwnerToAll(v,v); 
void preSmooth(Smoother& smoother, Domain& v, const Range& d), AFTER smoother.apply(v,d);
void presmooth(LevelContext& levelContext, size_t steps), AFTER SmootherApplier<typename LevelContext::SmootherType>::preSmooth, step: 0 of 2
void presmooth(LevelContext& levelContext, size_t steps), before update defec, step: 0 of 2
void presmooth(LevelContext& levelContext, size_t steps), before SmootherApplier<typename LevelContext::SmootherType>::preSmooth, step: 1 of 2
void preSmooth(Smoother& smoother, Domain& v, const Range& d), before smoother.apply(v,d);
60 schwarz.hh:virtual void apply (X& v, const Y& d), before preconditioner.apply(v,d);; 
60 schwarz.hh:virtual void apply (X& v, const Y& d), before communication.copyOwnerToAll(v,v); 
60 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
60: communicator.hh::BufferedCommunicator::forward, enter sendRecv
60: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 6
, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
30: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 6
30: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 6
30: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 6
30: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 6
Newton::solveLinearSystem : Caught exception from the linear solver when getting converged: "FMatrixError [luDecomposition:/home/m.giraud/DUMUXexud/dune-common/dune/common/densematrix.hh:909]: matrix is singular"
82 converged value: 0 121 1 let s go get convergedRemote
82 to  comm_.min(converged) 
109: communicator.hh::sendRecv, left MPI_Wait
109: leave BufferedCommunicator::sendRecv
109: communicator.hh::BufferedCommunicator::forward, left sendRecv
109 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), after communicator.free();
109 schwarz.hh:virtual void apply (X& v, const Y& d), AFTER communication.copyOwnerToAll(v,v); 
void preSmooth(Smoother& smoother, Domain& v, const Range& d), AFTER smoother.apply(v,d);
void presmooth(LevelContext& levelContext, size_t steps), AFTER SmootherApplier<typename LevelContext::SmootherType>::preSmooth, step: 0 of 2
void presmooth(LevelContext& levelContext, size_t steps), before update defec, step: 0 of 2
void presmooth(LevelContext& levelContext, size_t steps), before SmootherApplier<typename LevelContext::SmootherType>::preSmooth, step: 1 of 2
void preSmooth(Smoother& smoother, Domain& v, const Range& d), before smoother.apply(v,d);
109 schwarz.hh:virtual void apply (X& v, const Y& d), before preconditioner.apply(v,d);; 
109 schwarz.hh:virtual void apply (X& v, const Y& d), before communication.copyOwnerToAll(v,v); 
109 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
109: communicator.hh::BufferedCommunicator::forward, enter sendRecv
109: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 20
109: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 20
109: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 20
109: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 20
109: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 20
109: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 20
109: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 20
109: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 20
109: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 20
109: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 20
109: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 10 20
109: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 11 20
109: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 12 20
109: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 13 20
109: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 14 20
109: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 15 20
109: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 16 20
109: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 17 20
109: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 18 20
109: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 19 20
109: communicator.hh::sendRecv, left MPI_Wait
109: leave BufferedCommunicator::sendRecv
109: communicator.hh::BufferedCommunicator::forward, left sendRecv
109 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), after communicator.free();
109 schwarz.hh:virtual void apply (X& v, const Y& d), AFTER communication.copyOwnerToAll(v,v); 
void preSmooth(Smoother& smoother, Domain& v, const Range&29: leave BufferedCommunicator::sendRecv
29: communicator.hh::BufferedCommunicator::forward, left sendRecv
29 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), after communicator.free();
29 schwarz.hh:virtual void apply (X& v, const Y& d), AFTER communication.copyOwnerToAll(v,v); 
void preSmooth(Smoother& smoother, Domain& v, const Range& d), AFTER smoother.apply(v,d);
void presmooth(LevelContext& levelContext, size_t steps), AFTER SmootherApplier<typename LevelContext::SmootherType>::preSmooth, step: 0 of 2
void presmooth(LevelContext& levelContext, size_t steps), before update defec, step: 0 of 2
void presmooth(LevelContext& levelContext, size_t steps), before SmootherApplier<typename LevelContext::SmootherType>::preSmooth, step: 1 of 2
void preSmooth(Smoother& smoother, Domain& v, const Range& d), before smoother.apply(v,d);
29 schwarz.hh:virtual void apply (X& v, const Y& d), before preconditioner.apply(v,d);; 
29 schwarz.hh:virtual void apply (X& v, const Y& d), before communication.copyOwnerToAll(v,v); 
29 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
29: communicator.hh::BufferedCommunicator::forward, enter sendRecv
29: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 11
29: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 11
29: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 11
29: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 11
29: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 11
29: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 11
29: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 11
29: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 11
31: leave BufferedCommunicator::sendRecv
31: communicator.hh::BufferedCommunicator::forward, left sendRecv
31 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), after communicator.free();
31 schwarz.hh:virtual void apply (X& v, const Y& d), AFTER communication.copyOwnerToAll(v,v); 
void preSmooth(Smoother& smoother, Domain& v, const Range& d), AFTER smoother.apply(v,d);
void presmooth(LevelContext& levelContext, size_t steps), AFTER SmootherApplier<typename LevelContext::SmootherType>::preSmooth, step: 0 of 2
void presmooth(LevelContext& levelContext, size_t steps), before update defec, step: 0 of 2
void presmooth(LevelContext& levelContext, size_t steps), before SmootherApplier<typename LevelContext::SmootherType>::preSmooth, step: 1 of 2
void preSmooth(Smoother& smoother, Domain& v, const Range& d), before smoother.apply(v,d);
31 schwarz.hh:virtual void apply (X& v, const Y& d), before preconditioner.apply(v,d);; 
31 schwarz.hh:virtual void apply (X& v, const Y& d), before communication.copyOwnerToAll(v,v); 
31 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
31: communicator.hh::BufferedCommunicator::forward, enter sendRecv
31: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 10
31: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 10
31: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 10
31: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 10
31: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 10
31: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 10
31: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 10
8: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 19 20
8: communicator.hh::sendRecv, left MPI_Wait
8: leave BufferedCommunicator::sendRecv
8: communicator.hh::BufferedCommunicator::forward, left sendRecv
8 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), after communicator.free();
8 schwarz.hh:virtual void apply (X& v, const Y& d), AFTER communication.copyOwnerToAll(v,v); 
void preSmooth(Smoother& smoother, Domain& v, const Range& d), AFTER smoother.apply(v,d);
void presmooth(LevelContext& levelContext, size_t steps), AFTER SmootherApplier<typename LevelContext::SmootherType>::preSmooth, step: 0 of 2
void presmooth(LevelContext& levelContext, size_t steps), before update defec, step: 0 of 2
void presmooth(LevelContext& levelContext, size_t steps), before SmootherApplier<typename LevelContext::SmootherType>::preSmooth, step: 1 of 2
void preSmooth(Smoother& smoother, Domain& v, const Range& d), before smoother.apply(v,d);
8 schwarz.hh:virtual void apply (X& v, const Y& d), before preconditioner.apply(v,d);; 
8 schwarz.hh:virtual void apply (X& v, const Y& d), before communication.copyOwnerToAll(v,v); 
8 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
8: communicator.hh::BufferedCommunicator::forward, enter sendRecv
8: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 20
8: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 20
8: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 20
8: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 20
8: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 20
8: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 20
8: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 20
8: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 20
8: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 20
8: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 20
8: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 10 20
8: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 11 20
8: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 12 20
8: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 13 20
8: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 14 20
8: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 15 20
8: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 16 20
8: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 17 20
8: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 18 20
8: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 19 20
8: communicator.hh::sendRecv, left MPI_Wait
8: leave BufferedCommunicator::sendRecv
8: communicator.hh::BufferedCommunicator::forward, left sendRecv
8 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), after communicator.free();
8 schwarz.hh:virtual void apply (X& v, const Y& d), AFTER communication.copyOwnerToAll(v,v); 
void pre14: leave BufferedCommunicator::sendRecv
14: communicator.hh::BufferedCommunicator::forward, left sendRecv
14 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), after communicator.free();
14 schwarz.hh:virtual void apply (X& v, const Y& d), AFTER communication.copyOwnerToAll(v,v); 
void preSmooth(Smoother& smoother, Domain& v, const Range& d), AFTER smoother.apply(v,d);
void presmooth(LevelContext& levelContext, size_t steps), AFTER SmootherApplier<typename LevelContext::SmootherType>::preSmooth, step: 0 of 2
void presmooth(LevelContext& levelContext, size_t steps), before update defec, step: 0 of 2
void presmooth(LevelContext& levelContext, size_t steps), before SmootherApplier<typename LevelContext::SmootherType>::preSmooth, step: 1 of 2
void preSmooth(Smoother& smoother, Domain& v, const Range& d), before smoother.apply(v,d);
14 schwarz.hh:virtual void apply (X& v, const Y& d), before preconditioner.apply(v,d);; 
14 schwarz.hh:virtual void apply (X& v, const Y& d), before communication.copyOwnerToAll(v,v); 
14 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
14: communicator.hh::BufferedCommunicator::forward, enter sendRecv
14: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 17
14: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 17
14: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 17
14: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 17
14: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 17
14: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 17
14: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 17
14: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 17
14: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 17
14: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 17
14: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 10 17
14: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 11 17
14: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 12 17
14: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 13 17
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 16 17
6: communicator.hh::sendRecv, left MPI_Wait
6: leave BufferedCommunicator::sendRecv
6: communicator.hh::BufferedCommunicator::forward, left sendRecv
6 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), after communicator.free();
6 schwarz.hh:virtual void apply (X& v, const Y& d), AFTER communication.copyOwnerToAll(v,v); 
void preSmooth(Smoother& smoother, Domain& v, const Range& d), AFTER smoother.apply(v,d);
void presmooth(LevelContext& levelContext, size_t steps), AFTER SmootherApplier<typename LevelContext::SmootherType>::preSmooth, step: 0 of 2
void presmooth(LevelContext& levelContext, size_t steps), before update defec, step: 0 of 2
void presmooth(LevelContext& levelContext, size_t steps), before SmootherApplier<typename LevelContext::SmootherType>::preSmooth, step: 1 of 2
void preSmooth(Smoother& smoother, Domain& v, const Range& d), before smoother.apply(v,d);
6 schwarz.hh:virtual void apply (X& v, const Y& d), before preconditioner.apply(v,d);; 
6 schwarz.hh:virtual void apply (X& v, const Y& d), before communication.copyOwnerToAll(v,v); 
6 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
6: communicator.hh::BufferedCommunicator::forward, enter sendRecv
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 17
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 17
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 17
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 17
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 17
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 17
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 17
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 17
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 17
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 17
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 10 17
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 11 17
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 12 17
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 13 17
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 14 17
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 15 17
99: leave BufferedCommunicator::sendRecv
99: communicator.hh::BufferedCommunicator::forward, left sendRecv
99 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), after communicator.free();
99 schwarz.hh:virtual void apply (X& v, const Y& d), AFTER communication.copyOwnerToAll(v,v); 
void preSmooth(Smoother& smoother, Domain& v, const Range& d), AFTER smoother.apply(v,d);
void presmooth(LevelContext& levelContext, size_t steps), AFTER SmootherApplier<typename LevelContext::SmootherType>::preSmooth, step: 0 of 2
void presmooth(LevelContext& levelContext, size_t steps), before update defec, step: 0 of 2
void presmooth(LevelContext& levelContext, size_t steps), before SmootherApplier<typename LevelContext::SmootherType>::preSmooth, step: 1 of 2
void preSmooth(Smoother& smoother, Domain& v, const Range& d), before smoother.apply(v,d);
99 schwarz.hh:virtual void apply (X& v, const Y& d), before preconditioner.apply(v,d);; 
99 schwarz.hh:virtual void apply (X& v, const Y& d), before communication.copyOwnerToAll(v,v); 
99 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
99: communicator.hh::BufferedCommunicator::forward, enter sendRecv
99: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 13
99: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 13
99: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 13
99: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 13
99: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 13
99: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 13
99: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 13
99: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 13
99: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 13
99: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 13
99: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 10 13
99: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 11 13
7 schwarz.hh:virtual void apply (X& v, const Y& d), before communication.copyOwnerToAll(v,v); 
7 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
7: communicator.hh::BufferedCommunicator::forward, enter sendRecv
7: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 13
7: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 13
7: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 13
7: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 13
7: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 13
7: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 13
7: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 13
7: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 13
7: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 13
7: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 13
7: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 10 13
7: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 11 13
7: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 12 13
7: communicator.hh::sendRecv, left MPI_Wait
7: leave BufferedCommunicator::sendRecv
7: communicator.hh::BufferedCommunicator::forward, left sendRecv
7 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), after communicator.free();
7 schwarz.hh:virtual void apply (X& v, const Y& d), AFTER communication.copyOwnerToAll(v,v); 
void preSmooth(Smoother& smoother, Domain& v, const Range& d), AFTER smoother.apply(v,d);
void presmooth(LevelContext& levelContext, size_t steps), AFTER SmootherApplier<typename LevelContext::SmootherType>::preSmooth, step: 1 of 2
void presmooth(LevelContext& levelContext, size_t steps), before update defec, step: 1 of 2
void presmooth(LevelContext& levelContext, size_t steps), finished 
7void AMG<M,X,S,PI,A>::mgc(LevelContext& levelContext), AFTER presmooth(levelContext, preSteps_) 
7: communicator.hh::BufferedCommunicator::forward, enter sendRecv
7: communicator.hh::sendRecv, left MPI_Wait
7: leave BufferedCommunicator::sendRecv
7: communicator.hh::BufferedCommunicator::forward, left sendRecv
7: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
void presmooth(LevelContext& levelContext, size_t steps), before SmootherApplier<typename LevelContext::SmootherType>::preSmooth, step: 1 of 2
void preSmooth(Smoother& smoother, Domain& v, const Range& d), before smoother.apply(v,d);
90 schwarz.hh:virtual void apply (X& v, const Y& d), before preconditioner.apply(v,d);; 
90 schwarz.hh:virtual void apply (X& v, const Y& d), before communication.copyOwnerToAll(v,v); 
90 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
90: communicator.hh::BufferedCommunicator::forward, enter sendRecv
90: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 6
90: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 6
90: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 6
90: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 6
90: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 6
90: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 6
90: communicator.hh::sendRecv, left MPI_Wait
90: leave BufferedCommunicator::sendRecv
90: communicator.hh::BufferedCommunicator::forward, left sendRecv
90 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), after communicator.free();
90 schwarz.hh:virtual void apply (X& v, const Y& d), AFTER communication.copyOwnerToAll(v,v); 
void preSmooth(Smoother& smoother, Domain& v, const Range& d), AFTER smoother.apply(v,d);
void presmooth(LevelContext& levelContext, size_t steps), AFTER SmootherApplier<typename LevelContext::SmootherType>::preSmooth, step: 1 of 2
void presmooth(LevelContext& levelContext, size_t steps), before update defec, step: 1 of 2
void presmooth(LevelContext& levelContext, size_t steps), finished 
90void AMG<M,X,S,PI,A>::mgc(LevelContext& levelContext), AFTER presmooth(levelContext, preSteps_) 
90: communicator.hh::BufferedCommunicator::forward, enter sendRecv
90: communicator.hh::sendRecv, left MPI_Wait
90: leave BufferedCommunicator::sendRecv
90: communicator.hh::BufferedCommunicator::forward, left sendRecv
90: communicator.hh::sendRecv, left MPI_Wait
90: leave BufferedCommunicator::sendRecv
90 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
90: communicator.hh::BufferedCommunicator::forward, enter sendRecv
90: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 6
107: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 5
107: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 5
107: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 5
107: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 5
107: communicator.hh::sendRecv, left MPI_Wait
107: leave BufferedCommunicator::sendRecv
107: communicator.hh::BufferedCommunicator::forward, left sendRecv
107 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), after communicator.free();
107 schwarz.hh:virtual void apply (X& v, const Y& d), AFTER communication.copyOwnerToAll(v,v); 
void preSmooth(Smoother& smoother, Domain& v, const Range& d), AFTER smoother.apply(v,d);
void presmooth(LevelContext& levelContext, size_t steps), AFTER SmootherApplier<typename LevelContext::SmootherType>::preSmooth, step: 1 of 2
void presmooth(LevelContext& levelContext, size_t steps), before update defec, step: 1 of 2
void presmooth(LevelContext& levelContext, size_t steps), finished 
107void AMG<M,X,S,PI,A>::mgc(LevelContext& levelContext), AFTER presmooth(levelContext, preSteps_) 
107: communicator.hh::BufferedCommunicator::forward, enter sendRecv
107: communicator.hh::sendRecv, left MPI_Wait
107: leave BufferedCommunicator::sendRecv
107: communicator.hh::BufferedCommunicator::forward, left sendRecv
107: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
102: communicator.hh::sendRecv, left MPI_Wait
102: leave BufferedCommunicator::sendRecv
102: communicator.hh::BufferedCommunicator::forward, left sendRecv
102 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), after communicator.free();
102 schwarz.hh:virtual void apply (X& v, const Y& d), AFTER communication.copyOwnerToAll(v,v); 
void preSmooth(Smoother& smoother, Domain& v, const Range& d), AFTER smoother.apply(v,d);
void presmooth(LevelContext& levelContext, size_t steps), AFTER SmootherApplier<typename LevelContext::SmootherType>::preSmooth, step: 0 of 2
void presmooth(LevelContext& levelContext, size_t steps), before update defec, step: 0 of 2
void presmooth(LevelContext& levelContext, size_t steps), before SmootherApplier<typename LevelContext::SmootherType>::preSmooth, step: 1 of 2
void preSmooth(Smoother& smoother, Domain& v, const Range& d), before smoother.apply(v,d);
102 schwarz.hh:virtual void apply (X& v, const Y& d), before preconditioner.apply(v,d);; 
102 schwarz.hh:virtual void apply (X& v, const Y& d), before communication.copyOwnerToAll(v,v); 
102 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
102: communicator.hh::BufferedCommunicator::forward, enter sendRecv
102: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 15
102: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 15
102: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 15
102: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 15
102: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 15
102: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 15
102: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 15
102: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 15
102: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 15
102: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 15
102: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 10 15
102: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 11 15
16: leave BufferedCommunicator::sendRecv
16: communicator.hh::BufferedCommunicator::forward, left sendRecv
16 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), after communicator.free();
16 schwarz.hh:virtual void apply (X& v, const Y& d), AFTER communication.copyOwnerToAll(v,v); 
void preSmooth(Smoother& smoother, Domain& v, const Range& d), AFTER smoother.apply(v,d);
void presmooth(LevelContext& levelContext, size_t steps), AFTER SmootherApplier<typename LevelContext::SmootherType>::preSmooth, step: 0 of 2
void presmooth(LevelContext& levelContext, size_t steps), before update defec, step: 0 of 2
void presmooth(LevelContext& levelContext, size_t steps), before SmootherApplier<typename LevelContext::SmootherType>::preSmooth, step: 1 of 2
void preSmooth(Smoother& smoother, Domain& v, const Range& d), before smoother.apply(v,d);
16 schwarz.hh:virtual void apply (X& v, const Y& d), before preconditioner.apply(v,d);; 
16 schwarz.hh:virtual void apply (X& v, const Y& d), before communication.copyOwnerToAll(v,v); 
16 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
16: communicator.hh::BufferedCommunicator::forward, enter sendRecv
16: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 6
16: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 6
16: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 6
16: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 6
16: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 6
16: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 6
16: communicator.hh::sendRecv, left MPI_Wait
16: leave BufferedCommunicator::sendRecv
16: communicator.hh::BufferedCommunicator::forward, left sendRecv
16 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), after communicator.free();
16 schwarz.hh:virtual void apply (X& v, const Y& d), AFTER communication.copyOwnerToAll(v,v); 
void preSmooth(Smoother& smoother, Domain& v, const Range& d), AFTER smoother.apply(v,d);
void presmooth(LevelContext& levelContext, size_t steps), AFTER SmootherApplier<typename LevelContext::SmootherType>::preSmooth, step: 1 of 2
void presmooth(LevelContext& levelContext, size_t steps), before update defec, step: 1 of 2
void presmooth(LevelContext& levelContext, size_t steps), finished 
16void AMG<M,X,S,PI,A>::mgc(LevelContext& levelContext), AFTER presmooth(levelContext, preSteps_) 
16: communicator.hh::BufferedCommunicator::forward, enter sendRecv
16: communicator.hh::sendRecv, left MPI_Wait
16: leave BufferedCommunicator::sendRecv
16: communicator.hh::BufferedCommunicator::forward, left sendRecv
16: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 5
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 5
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 5
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 5
1: communicator.hh::sendRecv, left MPI_Wait
1: leave BufferedCommunicator::sendRecv
1: communicator.hh::BufferedCommunicator::forward, left sendRecv
1 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), after communicator.free();
1 schwarz.hh:virtual void apply (X& v, const Y& d), AFTER communication.copyOwnerToAll(v,v); 
void preSmooth(Smoother& smoother, Domain& v, const Range& d), AFTER smoother.apply(v,d);
void presmooth(LevelContext& levelContext, size_t steps), AFTER SmootherApplier<typename LevelContext::SmootherType>::preSmooth, step: 1 of 2
void presmooth(LevelContext& levelContext, size_t steps), before update defec, step: 1 of 2
void presmooth(LevelContext& levelContext, size_t steps), finished 
1void AMG<M,X,S,PI,A>::mgc(LevelContext& levelContext), AFTER presmooth(levelContext, preSteps_) 
1: communicator.hh::BufferedCommunicator::forward, enter sendRecv
1: communicator.hh::sendRecv, left MPI_Wait
1: leave BufferedCommunicator::sendRecv
1: communicator.hh::BufferedCommunicator::forward, left sendRecv
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
3: leave BufferedCommunicator::sendRecv
3: communicator.hh::BufferedCommunicator::forward, left sendRecv
3 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), after communicator.free();
3 schwarz.hh:virtual void apply (X& v, const Y& d), AFTER communication.copyOwnerToAll(v,v); 
void preSmooth(Smoother& smoother, Domain& v, const Range& d), AFTER smoother.apply(v,d);
void presmooth(LevelContext& levelContext, size_t steps), AFTER SmootherApplier<typename LevelContext::SmootherType>::preSmooth, step: 0 of 2
void presmooth(LevelContext& levelContext, size_t steps), before update defec, step: 0 of 2
void presmooth(LevelContext& levelContext, size_t steps), before SmootherApplier<typename LevelContext::SmootherType>::preSmooth, step: 1 of 2
void preSmooth(Smoother& smoother, Domain& v, const Range& d), before smoother.apply(v,d);
3 schwarz.hh:virtual void apply (X& v, const Y& d), before preconditioner.apply(v,d);; 
3 schwarz.hh:virtual void apply (X& v, const Y& d), before communication.copyOwnerToAll(v,v); 
3 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
3: communicator.hh::BufferedCommunicator::forward, enter sendRecv
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 9
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 9
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 9
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 9
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 9
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 9
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 9
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 9
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 9
3: communicator.hh::sendRecv, left MPI_Wait
3: leave BufferedCommunicator::sendRecv
3: communicator.hh::BufferedCommunicator::forward, left sendRecv
3 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), after communicator.free();
3 schwarz.hh:virtual void apply (X& v, const Y& d), AFTER communication.copyOwnerToAll(v,v); 
void preSmooth(Smoother& smoother, Domain& v, const Range& d), AFTER smoother.apply(v,d);
void presmooth(LevelContext& levelContext, size_t steps), AFTER SmootherApplier<typename LevelContext::SmootherType>::preSmooth, step: 1 of 2
void presmooth(LevelContext& levelContext, size_t steps), before update defec, step: 1 of 2
void presmooth(LevelContext& levelContext, size_t steps), finished 
3void AMG<M,X,S,PI,A>::mgc(LevelContext& levelContext), AFTER presmooth(levelContext, preSteps_) 
3: communicator.hh::BufferedCommunicator::forward, enter sendRecv
3: communicator.hh::sendRecv, left MPI_Wait
3: leave BufferedCommunicator::sendRecv
3: communicator.hh::BufferedCommunicator::forward, left sendRecv
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
Newton::solveLinearSystem : Caught exception from the linear solver when getting converged: "FMatrixError [luDecomposition:/home/m.giraud/DUMUXexud/dune-common/dune/common/densematrix.hh:909]: matrix is singular"
44 converged value: 0 121 1 let s go get convergedRemote
44 to  comm_.min(converged) 
97: leave BufferedCommunicator::sendRecv
97: communicator.hh::BufferedCommunicator::forward, left sendRecv
97 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), after communicator.free();
97 schwarz.hh:virtual void apply (X& v, const Y& d), AFTER communication.copyOwnerToAll(v,v); 
void preSmooth(Smoother& smoother, Domain& v, const Range& d), AFTER smoother.apply(v,d);
void presmooth(LevelContext& levelContext, size_t steps), AFTER SmootherApplier<typename LevelContext::SmootherType>::preSmooth, step: 0 of 2
void presmooth(LevelContext& levelContext, size_t steps), before update defec, step: 0 of 2
void presmooth(LevelContext& levelContext, size_t steps), before SmootherApplier<typename LevelContext::SmootherType>::preSmooth, step: 1 of 2
void preSmooth(Smoother& smoother, Domain& v, const Range& d), before smoother.apply(v,d);
97 schwarz.hh:virtual void apply (X& v, const Y& d), before preconditioner.apply(v,d);; 
97 schwarz.hh:virtual void apply (X& v, const Y& d), before communication.copyOwnerToAll(v,v); 
97 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
97: communicator.hh::BufferedCommunicator::forward, enter sendRecv
97: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 9
97: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 9
97: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 9
97: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 9
97: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 9
97: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 9
97: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 9
97: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 9
97: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 9
97: communicator.hh::sendRecv, left MPI_Wait
97: leave BufferedCommunicator::sendRecv
97: communicator.hh::BufferedCommunicator::forward, left sendRecv
97 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), after communicator.free();
97 schwarz.hh:virtual void apply (X& v, const Y& d), AFTER communication.copyOwnerToAll(v,v); 
void preSmooth(Smoother& smoother, Domain& v, const Range& d), AFTER smoother.apply(v,d);
void presmooth(LevelContext& levelContext, size_t steps), AFTER SmootherApplier<typename LevelContext::SmootherType>::preSmooth, step: 1 of 2
void presmooth(LevelContext& levelContext, size_t steps), before update defec, step: 1 of 2
void presmooth(LevelContext& levelContext, size_t steps), finished 
97void AMG<M,X,S,PI,A>::mgc(LevelContext& levelContext), AFTER presmooth(levelContext, preSteps_) 
97: communicator.hh::BufferedCommunicator::forward, enter sendRecv
97: communicator.hh::sendRecv, left MPI_Wait
97: leave BufferedCommunicator::sendRecv
97: communicator.hh::BufferedCommunicator::forward, left sendRecv
97: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
116: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 7
116: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 7
116: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 7
116: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 7
116: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 7
116: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 7
116: communicator.hh::sendRecv, left MPI_Wait
116: leave BufferedCommunicator::sendRecv
116: communicator.hh::BufferedCommunicator::forward, left sendRecv
116 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), after communicator.free();
116 schwarz.hh:virtual void apply (X& v, const Y& d), AFTER communication.copyOwnerToAll(v,v); 
void preSmooth(Smoother& smoother, Domain& v, const Range& d), AFTER smoother.apply(v,d);
void presmooth(LevelContext& levelContext, size_t steps), AFTER SmootherApplier<typename LevelContext::SmootherType>::preSmooth, step: 1 of 2
void presmooth(LevelContext& levelContext, size_t steps), before update defec, step: 1 of 2
void presmooth(LevelContext& levelContext, size_t steps), finished 
116void AMG<M,X,S,PI,A>::mgc(LevelContext& levelContext), AFTER presmooth(levelContext, preSteps_) 
116: communicator.hh::BufferedCommunicator::forward, enter sendRecv
116: communicator.hh::sendRecv, left MPI_Wait
116: leave BufferedCommunicator::sendRecv
116: communicator.hh::BufferedCommunicator::forward, left sendRecv
116: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
22: leave BufferedCommunicator::sendRecv
22: communicator.hh::BufferedCommunicator::forward, left sendRecv
22 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), after communicator.free();
22 schwarz.hh:virtual void apply (X& v, const Y& d), AFTER communication.copyOwnerToAll(v,v); 
void preSmooth(Smoother& smoother, Domain& v, const Range& d), AFTER smoother.apply(v,d);
void presmooth(LevelContext& levelContext, size_t steps), AFTER SmootherApplier<typename LevelContext::SmootherType>::preSmooth, step: 0 of 2
void presmooth(LevelContext& levelContext, size_t steps), before update defec, step: 0 of 2
void presmooth(LevelContext& levelContext, size_t steps), before SmootherApplier<typename LevelContext::SmootherType>::preSmooth, step: 1 of 2
void preSmooth(Smoother& smoother, Domain& v, const Range& d), before smoother.apply(v,d);
22 schwarz.hh:virtual void apply (X& v, const Y& d), before preconditioner.apply(v,d);; 
22 schwarz.hh:virtual void apply (X& v, const Y& d), before communication.copyOwnerToAll(v,v); 
22 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
22: communicator.hh::BufferedCommunicator::forward, enter sendRecv
22: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 20
22: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 20
22: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 20
22: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 20
22: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 20
22: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 20
22: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 20
22: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 20
22: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 20
22: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 20
22: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 10 20
22: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 11 20
22: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 12 20
22: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 13 20
22: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 14 20
22: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 15 20
22: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 16 20
22: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 17 20
22: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 18 20
22: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 19 20
22: communicator.hh::sendRecv, left MPI_Wait
22: leave BufferedCommunicator::sendRecv
22: communicator.hh::BufferedCommunicator::forward, left sendRecv
22 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), after communicator.free();
22 schwarz.hh:virtual void apply (X& v, const Y& d), AFTER communication.copyOwnerToAll(v,v); 
void preSmooth(Smoother& smoother, Domain& v, const Range& d), AFTER smoother.apply(v,d);
void presmooth(LevelContext& levelContext, size101: leave BufferedCommunicator::sendRecv
101: communicator.hh::BufferedCommunicator::forward, left sendRecv
101 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), after communicator.free();
101 schwarz.hh:virtual void apply (X& v, const Y& d), AFTER communication.copyOwnerToAll(v,v); 
void preSmooth(Smoother& smoother, Domain& v, const Range& d), AFTER smoother.apply(v,d);
void presmooth(LevelContext& levelContext, size_t steps), AFTER SmootherApplier<typename LevelContext::SmootherType>::preSmooth, step: 0 of 2
void presmooth(LevelContext& levelContext, size_t steps), before update defec, step: 0 of 2
void presmooth(LevelContext& levelContext, size_t steps), before SmootherApplier<typename LevelContext::SmootherType>::preSmooth, step: 1 of 2
void preSmooth(Smoother& smoother, Domain& v, const Range& d), before smoother.apply(v,d);
101 schwarz.hh:virtual void apply (X& v, const Y& d), before preconditioner.apply(v,d);; 
101 schwarz.hh:virtual void apply (X& v, const Y& d), before communication.copyOwnerToAll(v,v); 
101 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
101: communicator.hh::BufferedCommunicator::forward, enter sendRecv
101: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 24
101: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 24
101: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 24
101: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 24
101: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 24
101: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 24
101: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 24
101: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 24
101: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 24
101: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 24
101: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 10 24
101: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 11 24
101: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 12 24
101: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 13 24
101: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 14 24
101: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 15 24
101: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 16 24
101: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 17 24
101: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 18 24
101: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 19 24
101: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 20 24
103: leave BufferedCommunicator::sendRecv
103: communicator.hh::BufferedCommunicator::forward, left sendRecv
103 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), after communicator.free();
103 schwarz.hh:virtual void apply (X& v, const Y& d), AFTER communication.copyOwnerToAll(v,v); 
void preSmooth(Smoother& smoother, Domain& v, const Range& d), AFTER smoother.apply(v,d);
void presmooth(LevelContext& levelContext, size_t steps), AFTER SmootherApplier<typename LevelContext::SmootherType>::preSmooth, step: 0 of 2
void presmooth(LevelContext& levelContext, size_t steps), before update defec, step: 0 of 2
void presmooth(LevelContext& levelContext, size_t steps), before SmootherApplier<typename LevelContext::SmootherType>::preSmooth, step: 1 of 2
void preSmooth(Smoother& smoother, Domain& v, const Range& d), before smoother.apply(v,d);
103 schwarz.hh:virtual void apply (X& v, const Y& d), before preconditioner.apply(v,d);; 
103 schwarz.hh:virtual void apply (X& v, const Y& d), before communication.copyOwnerToAll(v,v); 
103 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
103: communicator.hh::BufferedCommunicator::forward, enter sendRecv
103: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 16
103: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 16
103: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 16
103: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 16
103: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 16
103: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 16
103: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 16
103: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 16
103: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 16
103: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 16
103: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 10 16
103: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 11 16
103: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 12 16
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 3
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 3
0: communicator.hh::sendRecv, left MPI_Wait
0: leave BufferedCommunicator::sendRecv
0: communicator.hh::BufferedCommunicator::forward, left sendRecv
0 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), after communicator.free();
0 schwarz.hh:virtual void apply (X& v, const Y& d), AFTER communication.copyOwnerToAll(v,v); 
void preSmooth(Smoother& smoother, Domain& v, const Range& d), AFTER smoother.apply(v,d);
void presmooth(LevelContext& levelContext, size_t steps), AFTER SmootherApplier<typename LevelContext::SmootherType>::preSmooth, step: 1 of 2
void presmooth(LevelContext& levelContext, size_t steps), before update defec, step: 1 of 2
void presmooth(LevelContext& levelContext, size_t steps), finished 
0void AMG<M,X,S,PI,A>::mgc(LevelContext& levelContext), AFTER presmooth(levelContext, preSteps_) 
0: communicator.hh::BufferedCommunicator::forward, enter sendRecv
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 117
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 117
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 117
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 117
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 117
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 117
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 117
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 117
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 117
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 117
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 10 117
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 11 117
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 12 117
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 13 117
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 14 117
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 15 117
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 16 117
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 17 117
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 18 117
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 19 117
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 20 117
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 21 117
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 22 117
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 23 117
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 24 117
0: comNewton::solveLinearSystem : Caught exception from the linear solver when getting converged: "FMatrixError [luDecomposition:/home/m.giraud/DUMUXexud/dune-common/dune/common/densematrix.hh:909]: matrix is singular"
75 converged value: 0 121 1 let s go get convergedRemote
75 to  comm_.min(converged) 
111: communicator.hh::BufferedCommunicator::forward, enter sendRecv
111: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 11
111: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 11
111: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 11
111: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 11
111: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 11
111: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 11
111: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 11
111: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 11
111: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 11
111: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 11
111: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 10 11
111: communicator.hh::sendRecv, left MPI_Wait
111: leave BufferedCommunicator::sendRecv
111: communicator.hh::BufferedCommunicator::forward, left sendRecv
111 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), after communicator.free();
111 schwarz.hh:virtual void apply (X& v, const Y& d), AFTER communication.copyOwnerToAll(v,v); 
void preSmooth(Smoother& smoother, Domain& v, const Range& d), AFTER smoother.apply(v,d);
void presmooth(LevelContext& levelContext, size_t steps), AFTER SmootherApplier<typename LevelContext::SmootherType>::preSmooth, step: 1 of 2
void presmooth(LevelContext& levelContext, size_t steps), before update defec, step: 1 of 2
void presmooth(LevelContext& levelContext, size_t steps), finished 
111void AMG<M,X,S,PI,A>::mgc(LevelContext& levelContext), AFTER presmooth(levelContext, preSteps_) 
111: communicator.hh::BufferedCommunicator::forward, enter sendRecv
111: communicator.hh::sendRecv, left MPI_Wait
111: leave BufferedCommunicator::sendRecv
111: communicator.hh::BufferedCommunicator::forward, left sendRecv
111: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
32: leave BufferedCommunicator::sendRecv
32: communicator.hh::BufferedCommunicator::forward, left sendRecv
32 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), after communicator.free();
32 schwarz.hh:virtual void apply (X& v, const Y& d), AFTER communication.copyOwnerToAll(v,v); 
void preSmooth(Smoother& smoother, Domain& v, const Range& d), AFTER smoother.apply(v,d);
void presmooth(LevelContext& levelContext, size_t steps), AFTER SmootherApplier<typename LevelContext::SmootherType>::preSmooth, step: 0 of 2
void presmooth(LevelContext& levelContext, size_t steps), before update defec, step: 0 of 2
void presmooth(LevelContext& levelContext, size_t steps), before SmootherApplier<typename LevelContext::SmootherType>::preSmooth, step: 1 of 2
void preSmooth(Smoother& smoother, Domain& v, const Range& d), before smoother.apply(v,d);
32 schwarz.hh:virtual void apply (X& v, const Y& d), before preconditioner.apply(v,d);; 
32 schwarz.hh:virtual void apply (X& v, const Y& d), before communication.copyOwnerToAll(v,v); 
32 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
32: communicator.hh::BufferedCommunicator::forward, enter sendRecv
32: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 16
32: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 16
32: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 16
32: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 16
32: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 16
32: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 16
32: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 16
32: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 16
32: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 16
32: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 16
12: leave BufferedCommunicator::sendRecv
12: communicator.hh::BufferedCommunicator::forward, left sendRecv
12 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), after communicator.free();
12 schwarz.hh:virtual void apply (X& v, const Y& d), AFTER communication.copyOwnerToAll(v,v); 
void preSmooth(Smoother& smoother, Domain& v, const Range& d), AFTER smoother.apply(v,d);
void presmooth(LevelContext& levelContext, size_t steps), AFTER SmootherApplier<typename LevelContext::SmootherType>::preSmooth, step: 0 of 2
void presmooth(LevelContext& levelContext, size_t steps), before update defec, step: 0 of 2
void presmooth(LevelContext& levelContext, size_t steps), before SmootherApplier<typename LevelContext::SmootherType>::preSmooth, step: 1 of 2
void preSmooth(Smoother& smoother, Domain& v, const Range& d), before smoother.apply(v,d);
12 schwarz.hh:virtual void apply (X& v, const Y& d), before preconditioner.apply(v,d);; 
12 schwarz.hh:virtual void apply (X& v, const Y& d), before communication.copyOwnerToAll(v,v); 
12 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
12: communicator.hh::BufferedCommunicator::forward, enter sendRecv
12: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 26
12: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 26
12: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 26
12: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 26
12: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 26
12: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 26
12: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 26
12: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 26
12: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 26
12: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 26
12: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 10 26
12: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 11 26
12: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 12 26
12: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 13 26
12: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 14 26
12: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 15 26
12: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 16 26
12: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 17 26
12: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 18 26
12: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 19 26
12: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 20 26
12: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 21 26
12: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 22 26
105: communicator.hh::BufferedCommunicator::forward, enter sendRecv
105: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 9
105: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 9
105: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 9
105: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 9
105: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 9
105: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 9
105: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 9
105: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 9
105: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 9
105: communicator.hh::sendRecv, left MPI_Wait
105: leave BufferedCommunicator::sendRecv
105: communicator.hh::BufferedCommunicator::forward, left sendRecv
105 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), after communicator.free();
105 schwarz.hh:virtual void apply (X& v, const Y& d), AFTER communication.copyOwnerToAll(v,v); 
void preSmooth(Smoother& smoother, Domain& v, const Range& d), AFTER smoother.apply(v,d);
void presmooth(LevelContext& levelContext, size_t steps), AFTER SmootherApplier<typename LevelContext::SmootherType>::preSmooth, step: 1 of 2
void presmooth(LevelContext& levelContext, size_t steps), before update defec, step: 1 of 2
void presmooth(LevelContext& levelContext, size_t steps), finished 
105void AMG<M,X,S,PI,A>::mgc(LevelContext& levelContext), AFTER presmooth(levelContext, preSteps_) 
105: communicator.hh::BufferedCommunicator::forward, enter sendRecv
105: communicator.hh::sendRecv, left MPI_Wait
105: leave BufferedCommunicator::sendRecv
105: communicator.hh::BufferedCommunicator::forward, left sendRecv
105: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
10 schwarz.hh:virtual void apply (X& v, const Y& d), before preconditioner.apply(v,d);; 
10 schwarz.hh:virtual void apply (X& v, const Y& d), before communication.copyOwnerToAll(v,v); 
10 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
10: communicator.hh::BufferedCommunicator::forward, enter sendRecv
10: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 11
10: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 11
10: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 11
10: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 11
10: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 11
10: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 11
10: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 11
10: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 11
10: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 11
10: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 11
10: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 10 11
10: communicator.hh::sendRecv, left MPI_Wait
10: leave BufferedCommunicator::sendRecv
10: communicator.hh::BufferedCommunicator::forward, left sendRecv
10 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), after communicator.free();
10 schwarz.hh:virtual void apply (X& v, const Y& d), AFTER communication.copyOwnerToAll(v,v); 
void preSmooth(Smoother& smoother, Domain& v, const Range& d), AFTER smoother.apply(v,d);
void presmooth(LevelContext& levelContext, size_t steps), AFTER SmootherApplier<typename LevelContext::SmootherType>::preSmooth, step: 1 of 2
void presmooth(LevelContext& levelContext, size_t steps), before update defec, step: 1 of 2
void presmooth(LevelContext& levelContext, size_t steps), finished 
10void AMG<M,X,S,PI,A>::mgc(LevelContext& levelContext), AFTER presmooth(levelContext, preSteps_) 
10: communicator.hh::BufferedCommunicator::forward, enter sendRecv
10: communicator.hh::sendRecv, left MPI_Wait
10: leave BufferedCommunicator::sendRecv
10: communicator.hh::BufferedCommunicator::forward, left sendRecv
10: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
115 schwarz.hh:virtual void apply (X& v, const Y& d), before communication.copyOwnerToAll(v,v); 
115 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
115: communicator.hh::BufferedCommunicator::forward, enter sendRecv
115: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 11
115: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 11
115: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 11
115: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 11
115: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 11
115: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 11
115: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 11
115: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 11
115: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 11
115: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 11
115: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 10 11
115: communicator.hh::sendRecv, left MPI_Wait
115: leave BufferedCommunicator::sendRecv
115: communicator.hh::BufferedCommunicator::forward, left sendRecv
115 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), after communicator.free();
115 schwarz.hh:virtual void apply (X& v, const Y& d), AFTER communication.copyOwnerToAll(v,v); 
void preSmooth(Smoother& smoother, Domain& v, const Range& d), AFTER smoother.apply(v,d);
void presmooth(LevelContext& levelContext, size_t steps), AFTER SmootherApplier<typename LevelContext::SmootherType>::preSmooth, step: 1 of 2
void presmooth(LevelContext& levelContext, size_t steps), before update defec, step: 1 of 2
void presmooth(LevelContext& levelContext, size_t steps), finished 
115void AMG<M,X,S,PI,A>::mgc(LevelContext& levelContext), AFTER presmooth(levelContext, preSteps_) 
115: communicator.hh::BufferedCommunicator::forward, enter sendRecv
115: communicator.hh::sendRecv, left MPI_Wait
115: leave BufferedCommunicator::sendRecv
115: communicator.hh::BufferedCommunicator::forward, left sendRecv
115: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
void presmooth(LevelContext& levelContext, size_t steps), before SmootherApplier<typename LevelContext::SmootherType>::preSmooth, step: 1 of 2
void preSmooth(Smoother& smoother, Domain& v, const Range& d), before smoother.apply(v,d);
77 schwarz.hh:virtual void apply (X& v, const Y& d), before preconditioner.apply(v,d);; 
77 schwarz.hh:virtual void apply (X& v, const Y& d), before communication.copyOwnerToAll(v,v); 
77 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
77: communicator.hh::BufferedCommunicator::forward, enter sendRecv
77: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 6
11 schwarz.hh:virtual void apply (X& v, const Y& d), before communication.copyOwnerToAll(v,v); 
11 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
11: communicator.hh::BufferedCommunicator::forward, enter sendRecv
11: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 17
11: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 17
11: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 17
11: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 17
11: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 17
11: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 17
11: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 17
11: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 17
11: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 17
11: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 17
11: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 10 17
11: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 11 17
11: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 12 17
11: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 13 17
11: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 14 17
11: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 15 17
113: leave BufferedCommunicator::sendRecv
113: communicator.hh::BufferedCommunicator::forward, left sendRecv
113 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), after communicator.free();
113 schwarz.hh:virtual void apply (X& v, const Y& d), AFTER communication.copyOwnerToAll(v,v); 
void preSmooth(Smoother& smoother, Domain& v, const Range& d), AFTER smoother.apply(v,d);
void presmooth(LevelContext& levelContext, size_t steps), AFTER SmootherApplier<typename LevelContext::SmootherType>::preSmooth, step: 0 of 2
void presmooth(LevelContext& levelContext, size_t steps), before update defec, step: 0 of 2
void presmooth(LevelContext& levelContext, size_t steps), before SmootherApplier<typename LevelContext::SmootherType>::preSmooth, step: 1 of 2
void preSmooth(Smoother& smoother, Domain& v, const Range& d), before smoother.apply(v,d);
113 schwarz.hh:virtual void apply (X& v, const Y& d), before preconditioner.apply(v,d);; 
113 schwarz.hh:virtual void apply (X& v, const Y& d), before communication.copyOwnerToAll(v,v); 
113 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
113: communicator.hh::BufferedCommunicator::forward, enter sendRecv
113: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 18
113: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 18
113: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 18
113: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 18
113: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 18
113: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 18
113: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 18
113: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 18
113: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 18
113: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 18
113: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 10 18
113: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 11 18
113: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 12 18
113: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 13 18
113: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 14 18
113: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 15 18
113: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 16 18
113: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 17 18
113: communicator.hh::sendRecv, left MPI_Wait
113: leave BufferedCommunicator::sendRecv
113: communicator.hh::BufferedCommunicator::forward, left sendRecv
113 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), after communicator.free();
113 schwarz.hh:virtual void apply (X& v, const Y& d), AFTER communication.copyOwnerToAll(v,v); 
void preSmooth(Smoother& smoother, Domain& v, const Range& d), AFTER smoother.apply(v,d);
void presmooth(LevelContext& levelContext, size_t steps), AFTER SmootherApplier<typename LevelContext::SmootherType>::preSmooth, step: 1 of 2
void presmooth(LevelContext& levelContext, size_t steps), before update defec, step: 1 of 2
void presmooth(Levelvoid presmooth(LevelContext& levelContext, size_t steps), before SmootherApplier<typename LevelContext::SmootherType>::preSmooth, step: 1 of 2
void preSmooth(Smoother& smoother, Domain& v, const Range& d), before smoother.apply(v,d);
112 schwarz.hh:virtual void apply (X& v, const Y& d), before preconditioner.apply(v,d);; 
112 schwarz.hh:virtual void apply (X& v, const Y& d), before communication.copyOwnerToAll(v,v); 
112 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
112: communicator.hh::BufferedCommunicator::forward, enter sendRecv
112: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 5
112: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 5
112: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 5
112: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 5
112: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 5
112: communicator.hh::sendRecv, left MPI_Wait
112: leave BufferedCommunicator::sendRecv
112: communicator.hh::BufferedCommunicator::forward, left sendRecv
112 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), after communicator.free();
112 schwarz.hh:virtual void apply (X& v, const Y& d), AFTER communication.copyOwnerToAll(v,v); 
void preSmooth(Smoother& smoother, Domain& v, const Range& d), AFTER smoother.apply(v,d);
void presmooth(LevelContext& levelContext, size_t steps), AFTER SmootherApplier<typename LevelContext::SmootherType>::preSmooth, step: 1 of 2
void presmooth(LevelContext& levelContext, size_t steps), before update defec, step: 1 of 2
void presmooth(LevelContext& levelContext, size_t steps), finished 
112void AMG<M,X,S,PI,A>::mgc(LevelContext& levelContext), AFTER presmooth(levelContext, preSteps_) 
112: communicator.hh::BufferedCommunicator::forward, enter sendRecv
112: communicator.hh::sendRecv, left MPI_Wait
112: leave BufferedCommunicator::sendRecv
112: communicator.hh::BufferedCommunicator::forward, left sendRecv
112: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
Newton::solveLinearSystem : Caught exception from the linear solver when getting converged: "FMatrixError [luDecomposition:/home/m.giraud/DUMUXexud/dune-common/dune/common/densematrix.hh:909]: matrix is singular"
70 converged value: 0 121 1 let s go get convergedRemote
70 to  comm_.min(converged) 
94: leave BufferedCommunicator::sendRecv
94: communicator.hh::BufferedCommunicator::forward, left sendRecv
94 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), after communicator.free();
94 schwarz.hh:virtual void apply (X& v, const Y& d), AFTER communication.copyOwnerToAll(v,v); 
void preSmooth(Smoother& smoother, Domain& v, const Range& d), AFTER smoother.apply(v,d);
void presmooth(LevelContext& levelContext, size_t steps), AFTER SmootherApplier<typename LevelContext::SmootherType>::preSmooth, step: 0 of 2
void presmooth(LevelContext& levelContext, size_t steps), before update defec, step: 0 of 2
void presmooth(LevelContext& levelContext, size_t steps), before SmootherApplier<typename LevelContext::SmootherType>::preSmooth, step: 1 of 2
void preSmooth(Smoother& smoother, Domain& v, const Range& d), before smoother.apply(v,d);
94 schwarz.hh:virtual void apply (X& v, const Y& d), before preconditioner.apply(v,d);; 
94 schwarz.hh:virtual void apply (X& v, const Y& d), before communication.copyOwnerToAll(v,v); 
94 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
94: communicator.hh::BufferedCommunicator::forward, enter sendRecv
94: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 21
94: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 21
94: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 21
94: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 21
94: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 21
94: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 21
94: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 21
94: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 21
94: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 21
94: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 21
94: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 10 21
94: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 11 21
94: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 12 21
94: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 13 21
94: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 14 21
94: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 15 21
94: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 16 21
94: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 17 21
94: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 18 21
94: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 19 21
4: leave BufferedCommunicator::sendRecv
4: communicator.hh::BufferedCommunicator::forward, left sendRecv
4 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), after communicator.free();
4 schwarz.hh:virtual void apply (X& v, const Y& d), AFTER communication.copyOwnerToAll(v,v); 
void preSmooth(Smoother& smoother, Domain& v, const Range& d), AFTER smoother.apply(v,d);
void presmooth(LevelContext& levelContext, size_t steps), AFTER SmootherApplier<typename LevelContext::SmootherType>::preSmooth, step: 0 of 2
void presmooth(LevelContext& levelContext, size_t steps), before update defec, step: 0 of 2
void presmooth(LevelContext& levelContext, size_t steps), before SmootherApplier<typename LevelContext::SmootherType>::preSmooth, step: 1 of 2
void preSmooth(Smoother& smoother, Domain& v, const Range& d), before smoother.apply(v,d);
4 schwarz.hh:virtual void apply (X& v, const Y& d), before preconditioner.apply(v,d);; 
4 schwarz.hh:virtual void apply (X& v, const Y& d), before communication.copyOwnerToAll(v,v); 
4 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
4: communicator.hh::BufferedCommunicator::forward, enter sendRecv
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 14
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 14
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 14
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 14
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 14
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 14
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 14
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 14
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 14
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 14
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 10 14
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 11 14
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 12 14
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 13 14
4: communicator.hh::sendRecv, left MPI_Wait
4: leave BufferedCommunicator::sendRecv
4: communicator.hh::BufferedCommunicator::forward, left sendRecv
4 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), after communicator.free();
4 schwarz.hh:virtual void apply (X& v, const Y& d), AFTER communication.copyOwnerToAll(v,v); 
void preSmooth(Smoother& smoother, Domain& v, const Range& d), AFTER smoother.apply(v,d);
void presmooth(LevelContext& levelContext, size_t steps), AFTER SmootherApplier<typename LevelContext::SmootherType>::preSmooth, step: 1 of 2
void presmooth(LevelContext& levelContext, size_t steps), before update defec, step: 1 of 2
void presmooth(LevelContext& levelContext, size_t steps), finished 
4void AMG<M,X,S,PI,A>::mgc(LevelContext& levelContext), AFTER presmooth(levelContext, preSteps_) 
4: communicator.hh::BufferedCommunicator::forward, enter sendRecv
4: communicator.hh::sendRecv, left MPI_Wait
4: leave BufferedCommunicator::sendRecv
4: communicator.hh::BufferedCommunicator::forward, left sendRecv
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
&finished, &status); 1 13
119: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 13
119: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 13
119: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 13
119: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 13
119: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 13
119: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 13
119: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 13
119: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 13
119: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 10 13
119: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 11 13
119: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 12 13
119: communicator.hh::sendRecv, left MPI_Wait
119: leave BufferedCommunicator::sendRecv
119: communicator.hh::BufferedCommunicator::forward, left sendRecv
119 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), after communicator.free();
119 schwarz.hh:virtual void apply (X& v, const Y& d), AFTER communication.copyOwnerToAll(v,v); 
void preSmooth(Smoother& smoother, Domain& v, const Range& d), AFTER smoother.apply(v,d);
void presmooth(LevelContext& levelContext, size_t steps), AFTER SmootherApplier<typename LevelContext::SmootherType>::preSmooth, step: 1 of 2
void presmooth(LevelContext& levelContext, size_t steps), before update defec, step: 1 of 2
void presmooth(LevelContext& levelContext, size_t steps), finished 
119void AMG<M,X,S,PI,A>::mgc(LevelContext& levelContext), AFTER presmooth(levelContext, preSteps_) 
119: communicator.hh::BufferedCommunicator::forward, enter sendRecv
119: communicator.hh::sendRecv, left MPI_Wait
119: leave BufferedCommunicator::sendRecv
119: communicator.hh::BufferedCommunicator::forward, left sendRecv
119: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
2: leave BufferedCommunicator::sendRecv
2: communicator.hh::BufferedCommunicator::forward, left sendRecv
2 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), after communicator.free();
2 schwarz.hh:virtual void apply (X& v, const Y& d), AFTER communication.copyOwnerToAll(v,v); 
void preSmooth(Smoother& smoother, Domain& v, const Range& d), AFTER smoother.apply(v,d);
void presmooth(LevelContext& levelContext, size_t steps), AFTER SmootherApplier<typename LevelContext::SmootherType>::preSmooth, step: 0 of 2
void presmooth(LevelContext& levelContext, size_t steps), before update defec, step: 0 of 2
void presmooth(LevelContext& levelContext, size_t steps), before SmootherApplier<typename LevelContext::SmootherType>::preSmooth, step: 1 of 2
void preSmooth(Smoother& smoother, Domain& v, const Range& d), before smoother.apply(v,d);
2 schwarz.hh:virtual void apply (X& v, const Y& d), before preconditioner.apply(v,d);; 
2 schwarz.hh:virtual void apply (X& v, const Y& d), before communication.copyOwnerToAll(v,v); 
2 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
2: communicator.hh::BufferedCommunicator::forward, enter sendRecv
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 8
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 8
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 8
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 8
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 8
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 8
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 8
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 8
2: communicator.hh::sendRecv, left MPI_Wait
2: leave BufferedCommunicator::sendRecv
2: communicator.hh::BufferedCommunicator::forward, left sendRecv
2 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), after communicator.free();
2 schwarz.hh:virtual void apply (X& v, const Y& d), AFTER communication.copyOwnerToAll(v,v); 
void preSmooth(Smoother& smoother, Domain& v, const Range& d), AFTER smoother.apply(v,d);
void presmooth(LevelContext& levelContext, size_t steps), AFTER SmootherApplier<typename LevelContext::SmootherType>::preSmooth, step: 1 of 2
void presmooth(LevelContext& levelContext, size_t steps), before update defec, step: 1 of 2
void presmooth(LevelContext& levelContext, size_t steps), finished 
2void AMG<M,X,S,PI,A>::mgc(LevelContext& levelContext), AFTER presmooth(levelContext, preSteps_) 
2: communicator.hh::BufferedCommunicator::forward, enter sendRecv
2: communicator.hh::sendRecv, left MPI_Wait
2: leave BufferedCommunicator::sendRecv
2: communicator.hh::BufferedCommunicator::forward, left sendRecv
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
18: leave BufferedCommunicator::sendRecv
18: communicator.hh::BufferedCommunicator::forward, left sendRecv
18 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), after communicator.free();
18 schwarz.hh:virtual void apply (X& v, const Y& d), AFTER communication.copyOwnerToAll(v,v); 
void preSmooth(Smoother& smoother, Domain& v, const Range& d), AFTER smoother.apply(v,d);
void presmooth(LevelContext& levelContext, size_t steps), AFTER SmootherApplier<typename LevelContext::SmootherType>::preSmooth, step: 0 of 2
void presmooth(LevelContext& levelContext, size_t steps), before update defec, step: 0 of 2
void presmooth(LevelContext& levelContext, size_t steps), before SmootherApplier<typename LevelContext::SmootherType>::preSmooth, step: 1 of 2
void preSmooth(Smoother& smoother, Domain& v, const Range& d), before smoother.apply(v,d);
18 schwarz.hh:virtual void apply (X& v, const Y& d), before preconditioner.apply(v,d);; 
18 schwarz.hh:virtual void apply (X& v, const Y& d), before communication.copyOwnerToAll(v,v); 
18 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
18: communicator.hh::BufferedCommunicator::forward, enter sendRecv
18: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 14
18: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 14
18: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 14
18: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 14
18: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 14
18: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 14
18: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 14
18: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 14
18: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 14
18: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 14
18: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 10 14
18: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 11 14
18: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 12 14
18: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 13 14
18: communicator.hh::sendRecv, left MPI_Wait
18: leave BufferedCommunicator::sendRecv
18: communicator.hh::BufferedCommunicator::forward, left sendRecv
18 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), after communicator.free();
18 schwarz.hh:virtual void apply (X& v, const Y& d), AFTER communication.copyOwnerToAll(v,v); 
void preSmooth(Smoother& smoother, Domain& v, const Range& d), AFTER smoother.apply(v,d);
void presmooth(LevelContext& levelContext, size_t steps), AFTER SmootherApplier<typename LevelContext::SmootherType>::preSmooth, step: 1 of 2
void presmooth(LevelContext& levelContext, size_t steps), before update defec, step: 1 of 2
void presmooth(LevelContext& levelContext, size_t steps), finished 
18void AMG<M,X,S,PI,A>::mgc(LevelContext& levelContext), AFTER presmooth(levelContext, preSteps_) 
18: communicator.hh::BufferedCommunicator::forward, enter sendRecv
18: communicator.hh::sendRecv, left MPI_Wait
18: leave BufferedCommunicator::sendRecv
18: communicator.hh::BufferedCommunicator::forward, left sendRecv
18: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
s); 25 26
Newton::solveLinearSystem : Caught exception from the linear solver when getting converged: "FMatrixError [luDecomposition:/home/m.giraud/DUMUXexud/dune-common/dune/common/densematrix.hh:909]: matrix is singular"
36 converged value: 0 121 1 let s go get convergedRemote
36 to  comm_.min(converged) 
s); 25 26
26: communicator.hh::sendRecv, left MPI_Wait
26: leave BufferedCommunicator::sendRecv
26: communicator.hh::BufferedCommunicator::forward, left sendRecv
26 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), after communicator.free();
26 schwarz.hh:virtual void apply (X& v, const Y& d), AFTER communication.copyOwnerToAll(v,v); 
void preSmooth(Smoother& smoother, Domain& v, const Range& d), AFTER smoother.apply(v,d);
void presmooth(LevelContext& levelContext, size_t steps), AFTER SmootherApplier<typename LevelContext::SmootherType>::preSmooth, step: 0 of 2
void presmooth(LevelContext& levelContext, size_t steps), before update defec, step: 0 of 2
void presmooth(LevelContext& levelContext, size_t steps), before SmootherApplier<typename LevelContext::SmootherType>::preSmooth, step: 1 of 2
void preSmooth(Smoother& smoother, Domain& v, const Range& d), before smoother.apply(v,d);
26 schwarz.hh:virtual void apply (X& v, const Y& d), before preconditioner.apply(v,d);; 
26 schwarz.hh:virtual void apply (X& v, const Y& d), before communication.copyOwnerToAll(v,v); 
26 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
26: communicator.hh::BufferedCommunicator::forward, enter sendRecv
26: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 26
26: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 26
26: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 26
26: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 26
26: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 26
26: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 26
26: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 26
26: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 26
26: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 26
26: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 26
26: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 10 26
26: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 11 26
26: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 12 26
26: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 13 26
26: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 14 26
26: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 15 26
26: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 16 26
26: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 17 26
26: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 18 26
26: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 19 26
26: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 20 26
9: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 7
9: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 7
9: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 7
9: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 7
9: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 7
9: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 7
9: communicator.hh::sendRecv, left MPI_Wait
9: leave BufferedCommunicator::sendRecv
9: communicator.hh::BufferedCommunicator::forward, left sendRecv
9 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), after communicator.free();
9 schwarz.hh:virtual void apply (X& v, const Y& d), AFTER communication.copyOwnerToAll(v,v); 
void preSmooth(Smoother& smoother, Domain& v, const Range& d), AFTER smoother.apply(v,d);
void presmooth(LevelContext& levelContext, size_t steps), AFTER SmootherApplier<typename LevelContext::SmootherType>::preSmooth, step: 1 of 2
void presmooth(LevelContext& levelContext, size_t steps), before update defec, step: 1 of 2
void presmooth(LevelContext& levelContext, size_t steps), finished 
9void AMG<M,X,S,PI,A>::mgc(LevelContext& levelContext), AFTER presmooth(levelContext, preSteps_) 
9: communicator.hh::BufferedCommunicator::forward, enter sendRecv
9: communicator.hh::sendRecv, left MPI_Wait
9: leave BufferedCommunicator::sendRecv
9: communicator.hh::BufferedCommunicator::forward, left sendRecv
9: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
23: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 11
23: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 11
23: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 11
23: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 11
23: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 11
23: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 11
23: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 11
23: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 11
23: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 11
23: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 11
23: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 10 11
23: communicator.hh::sendRecv, left MPI_Wait
23: leave BufferedCommunicator::sendRecv
23: communicator.hh::BufferedCommunicator::forward, left sendRecv
23 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), after communicator.free();
23 schwarz.hh:virtual void apply (X& v, const Y& d), AFTER communication.copyOwnerToAll(v,v); 
void preSmooth(Smoother& smoother, Domain& v, const Range& d), AFTER smoother.apply(v,d);
void presmooth(LevelContext& levelContext, size_t steps), AFTER SmootherApplier<typename LevelContext::SmootherType>::preSmooth, step: 1 of 2
void presmooth(LevelContext& levelContext, size_t steps), before update defec, step: 1 of 2
void presmooth(LevelContext& levelContext, size_t steps), finished 
23void AMG<M,X,S,PI,A>::mgc(LevelContext& levelContext), AFTER presmooth(levelContext, preSteps_) 
23: communicator.hh::BufferedCommunicator::forward, enter sendRecv
23: communicator.hh::sendRecv, left MPI_Wait
23: leave BufferedCommunicator::sendRecv
23: communicator.hh::BufferedCommunicator::forward, left sendRecv
23: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
5 schwarz.hh:virtual void apply (X& v, const Y& d), before communication.copyOwnerToAll(v,v); 
5 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
5: communicator.hh::BufferedCommunicator::forward, enter sendRecv
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 11
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 11
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 11
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 11
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 11
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 11
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 11
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 11
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 11
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 11
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 10 11
46 schwarz.hh:virtual void apply (X& v, const Y& d), before communication.copyOwnerToAll(v,v); 
46 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
46: communicator.hh::BufferedCommunicator::forward, enter sendRecv
46: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 12
46: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 12
46: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 12
46: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 12
46: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 12
46: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 12
46: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 12
46: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 12
106 schwarz.hh:virtual void apply (X& v, const Y& d), before communication.copyOwnerToAll(v,v); 
106 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
106: communicator.hh::BufferedCommunicator::forward, enter sendRecv
106: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 16
106: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 16
106: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 16
106: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 16
106: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 16
106: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 16
106: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 16
106: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 16
106: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 16
106: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 16
106: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 10 16
106: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 11 16
106: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 12 16
106: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 13 16
106: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 14 16
106: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 15 16
106: communicator.hh::sendRecv, left MPI_Wait
106: leave BufferedCommunicator::sendRecv
106: communicator.hh::BufferedCommunicator::forward, left sendRecv
106 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), after communicator.free();
106 schwarz.hh:virtual void apply (X& v, const Y& d), AFTER communication.copyOwnerToAll(v,v); 
void preSmooth(Smoother& smoother, Domain& v, const Range& d), AFTER smoother.apply(v,d);
void presmooth(LevelContext& levelContext, size_t steps), AFTER SmootherApplier<typename LevelContext::SmootherType>::preSmooth, step: 1 of 2
void presmooth(LevelContext& levelContext, size_t steps), before update defec, step: 1 of 2
void presmooth(LevelContext& levelContext, size_t steps), finished 
106void AMG<M,X,S,PI,A>::mgc(LevelContext& levelContext), AFTER presmooth(levelContext, preSteps_) 
106: communicator.hh::BufferedCommunicator::forward, enter sendRecv
106: communicator.hh::sendRecv, left MPI_Wait
106: leave BufferedCommunicator::sendRecv
106: communicator.hh::BufferedCommunicator::forward, left sendRecv
106: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
15: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 8
15: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 8
15: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 8
15: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 8
15: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 8
15: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 8
15: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 8
15: communicator.hh::sendRecv, left MPI_Wait
15: leave BufferedCommunicator::sendRecv
15: communicator.hh::BufferedCommunicator::forward, left sendRecv
15 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), after communicator.free();
15 schwarz.hh:virtual void apply (X& v, const Y& d), AFTER communication.copyOwnerToAll(v,v); 
void preSmooth(Smoother& smoother, Domain& v, const Range& d), AFTER smoother.apply(v,d);
void presmooth(LevelContext& levelContext, size_t steps), AFTER SmootherApplier<typename LevelContext::SmootherType>::preSmooth, step: 1 of 2
void presmooth(LevelContext& levelContext, size_t steps), before update defec, step: 1 of 2
void presmooth(LevelContext& levelContext, size_t steps), finished 
15void AMG<M,X,S,PI,A>::mgc(LevelContext& levelContext), AFTER presmooth(levelContext, preSteps_) 
15: communicator.hh::BufferedCommunicator::forward, enter sendRecv
15: communicator.hh::sendRecv, left MPI_Wait
15: leave BufferedCommunicator::sendRecv
15: communicator.hh::BufferedCommunicator::forward, left sendRecv
15: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
13 schwarz.hh:virtual void apply (X& v, const Y& d), before communication.copyOwnerToAll(v,v); 
13 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
13: communicator.hh::BufferedCommunicator::forward, enter sendRecv
13: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 11
13: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 11
13: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 11
13: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 11
13: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 11
13: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 11
13: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 11
13: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 11
13: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 11
13: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 11
81: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 11
81: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 11
81: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 11
81: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 11
81: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 11
81: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 11
81: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 11
nicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 13
93: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 13
93: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 13
93: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 13
93: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 13
93: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 13
93: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 13
93: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 13
93: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 10 13
93: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 11 13
93: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 12 13
93: communicator.hh::sendRecv, left MPI_Wait
93: leave BufferedCommunicator::sendRecv
93: communicator.hh::BufferedCommunicator::forward, left sendRecv
93 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), after communicator.free();
93 schwarz.hh:virtual void apply (X& v, const Y& d), AFTER communication.copyOwnerToAll(v,v); 
void preSmooth(Smoother& smoother, Domain& v, const Range& d), AFTER smoother.apply(v,d);
void presmooth(LevelContext& levelContext, size_t steps), AFTER SmootherApplier<typename LevelContext::SmootherType>::preSmooth, step: 1 of 2
void presmooth(LevelContext& levelContext, size_t steps), before update defec, step: 1 of 2
void presmooth(LevelContext& levelContext, size_t steps), finished 
93void AMG<M,X,S,PI,A>::mgc(LevelContext& levelContext), AFTER presmooth(levelContext, preSteps_) 
93: communicator.hh::BufferedCommunicator::forward, enter sendRecv
93: communicator.hh::sendRecv, left MPI_Wait
93: leave BufferedCommunicator::sendRecv
93: communicator.hh::BufferedCommunicator::forward, left sendRecv
93: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
110: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 11
110: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 11
110: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 11
110: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 11
110: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 11
110: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 11
110: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 11
110: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 11
110: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 11
110: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 10 11
110: communicator.hh::sendRecv, left MPI_Wait
110: leave BufferedCommunicator::sendRecv
110: communicator.hh::BufferedCommunicator::forward, left sendRecv
110 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), after communicator.free();
110 schwarz.hh:virtual void apply (X& v, const Y& d), AFTER communication.copyOwnerToAll(v,v); 
void preSmooth(Smoother& smoother, Domain& v, const Range& d), AFTER smoother.apply(v,d);
void presmooth(LevelContext& levelContext, size_t steps), AFTER SmootherApplier<typename LevelContext::SmootherType>::preSmooth, step: 1 of 2
void presmooth(LevelContext& levelContext, size_t steps), before update defec, step: 1 of 2
void presmooth(LevelContext& levelContext, size_t steps), finished 
110void AMG<M,X,S,PI,A>::mgc(LevelContext& levelContext), AFTER presmooth(levelContext, preSteps_) 
110: communicator.hh::BufferedCommunicator::forward, enter sendRecv
110: communicator.hh::sendRecv, left MPI_Wait
110: leave BufferedCommunicator::sendRecv
110: communicator.hh::BufferedCommunicator::forward, left sendRecv
110: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
.copyOwnerToAll(v,v); 
45 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
45: communicator.hh::BufferedCommunicator::forward, enter sendRecv
45: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 17
45: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 17
45: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 17
45: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 17
45: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 17
45: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 17
45: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 17
45: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 17
45: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 17
45: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 17
Newton::solveLinearSystem : Caught exception from the linear solver when getting converged: "FMatrixError [luDecomposition:/home/m.giraud/DUMUXexud/dune-common/dune/common/densematrix.hh:909]: matrix is singular"
68 converged value: 0 121 1 let s go get convergedRemote
68 to  comm_.min(converged) 
Newton::solveLinearSystem : Caught exception from the linear solver when getting converged: "FMatrixError [luDecomposition:/home/m.giraud/DUMUXexud/dune-common/dune/common/densematrix.hh:909]: matrix is singular"
57 converged value: 0 121 1 let s go get convergedRemote
57 to  comm_.min(converged) 
96: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 12
96: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 12
96: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 12
96: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 12
96: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 12
96: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 12
96: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 12
96: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 12
96: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 10 12
50: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 10
50: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 10
25: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 7
25: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 7
25: communicator.hh::sendRecv, left MPI_Wait
25: leave BufferedCommunicator::sendRecv
25: communicator.hh::BufferedCommunicator::forward, left sendRecv
25 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), after communicator.free();
25 schwarz.hh:virtual void apply (X& v, const Y& d), AFTER communication.copyOwnerToAll(v,v); 
void preSmooth(Smoother& smoother, Domain& v, const Range& d), AFTER smoother.apply(v,d);
void presmooth(LevelContext& levelContext, size_t steps), AFTER SmootherApplier<typename LevelContext::SmootherType>::preSmooth, step: 1 of 2
void presmooth(LevelContext& levelContext, size_t steps), before update defec, step: 1 of 2
void presmooth(LevelContext& levelContext, size_t steps), finished 
25void AMG<M,X,S,PI,A>::mgc(LevelContext& levelContext), AFTER presmooth(levelContext, preSteps_) 
25: communicator.hh::BufferedCommunicator::forward, enter sendRecv
25: communicator.hh::sendRecv, left MPI_Wait
25: leave BufferedCommunicator::sendRecv
25: communicator.hh::BufferedCommunicator::forward, left sendRecv
25: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
65: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 9
65: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 9
65: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 9
65: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 9
forward<CopyGatherScatter<T> >(source,dest);
92: communicator.hh::BufferedCommunicator::forward, enter sendRecv
92: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 16
92: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 16
92: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 16
92: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 16
92: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 16
92: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 16
92: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 16
92: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 16
92: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 16
92: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 16
92: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 10 16
92: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 11 16
92: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 12 16
92: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 13 16
92: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 14 16
92: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 15 16
100: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 6
100: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 6
);
void presmooth(LevelContext& levelContext, size_t steps), AFTER SmootherApplier<typename LevelContext::SmootherType>::preSmooth, step: 0 of 2
void presmooth(LevelContext& levelContext, size_t steps), before update defec, step: 0 of 2
void presmooth(LevelContext& levelContext, size_t steps), before SmootherApplier<typename LevelContext::SmootherType>::preSmooth, step: 1 of 2
void preSmooth(Smoother& smoother, Domain& v, const Range& d), before smoother.apply(v,d);
117 schwarz.hh:virtual void apply (X& v, const Y& d), before preconditioner.apply(v,d);; 
117 schwarz.hh:virtual void apply (X& v, const Y& d), before communication.copyOwnerToAll(v,v); 
117 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
117: communicator.hh::BufferedCommunicator::forward, enter sendRecv
117: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 22
117: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 22
117: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 22
117: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 22
117: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 22
117: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 22
117: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 22
117: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 22
117: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 22
117: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 22
117: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 10 22
117: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 11 22
117: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 12 22
117: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 13 22
117: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 14 22
117: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 15 22
117: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 16 22
117: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 17 22
117: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 18 22
117: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 19 22
117: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 20 22
117: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 21 22
117: communicator.hh::sendRecv, left MPI_Wait
117: leave BufferedCommunicator::sendRecv
117: communicator.hh::BufferedCommunicator::forward, left sendRecv
117 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), after communicator.free();
117 schwarz.hh:virtual void apply (X& v, const Y& d), AFTER communication.copyOwnerToAll(v,v); 
void preSmooth(Smoother& smoother, Domain& v, const Range& d), AFTER smoother.apply(v,d);
void presmooth(LevelContext& levelContext, size_t steps), AFTER SmootherApplier<typename LevelContext::SmootherType>::preSmooth, step: 1 of 2
void presmooth(Leveator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 10
20: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 10
20: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 10
20: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 10
20: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 10
20: communicator.hh::sendRecv, left MPI_Wait
20: leave BufferedCommunicator::sendRecv
20: communicator.hh::BufferedCommunicator::forward, left sendRecv
20 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), after communicator.free();
20 schwarz.hh:virtual void apply (X& v, const Y& d), AFTER communication.copyOwnerToAll(v,v); 
void preSmooth(Smoother& smoother, Domain& v, const Range& d), AFTER smoother.apply(v,d);
void presmooth(LevelContext& levelContext, size_t steps), AFTER SmootherApplier<typename LevelContext::SmootherType>::preSmooth, step: 1 of 2
void presmooth(LevelContext& levelContext, size_t steps), before update defec, step: 1 of 2
void presmooth(LevelContext& levelContext, size_t steps), finished 
20void AMG<M,X,S,PI,A>::mgc(LevelContext& levelContext), AFTER presmooth(levelContext, preSteps_) 
20: communicator.hh::BufferedCommunicator::forward, enter sendRecv
20: communicator.hh::sendRecv, left MPI_Wait
20: leave BufferedCommunicator::sendRecv
20: communicator.hh::BufferedCommunicator::forward, left sendRecv
20: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
.copyOwnerToAll(v,v); 
19 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
19: communicator.hh::BufferedCommunicator::forward, enter sendRecv
19: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 17
19: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 17
19: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 17
19: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 17
19: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 17
19: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 17
19: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 17
19: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 17
19: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 17
19: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 17
19: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 10 17
19: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 11 17
19: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 12 17
19: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 13 17
19: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 14 17
120: communicator.hh::sendRecv, left MPI_Wait
120: leave BufferedCommunicator::sendRecv
120: communicator.hh::BufferedCommunicator::forward, left sendRecv
120 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), after communicator.free();
120 schwarz.hh:virtual void apply (X& v, const Y& d), AFTER communication.copyOwnerToAll(v,v); 
void preSmooth(Smoother& smoother, Domain& v, const Range& d), AFTER smoother.apply(v,d);
void presmooth(LevelContext& levelContext, size_t steps), AFTER SmootherApplier<typename LevelContext::SmootherType>::preSmooth, step: 1 of 2
void presmooth(LevelContext& levelContext, size_t steps), before update defec, step: 1 of 2
void presmooth(LevelContext& levelContext, size_t steps), finished 
120void AMG<M,X,S,PI,A>::mgc(LevelContext& levelContext), AFTER presmooth(levelContext, preSteps_) 
120: communicator.hh::BufferedCommunicator::forward, enter sendRecv
120: communicator.hh::sendRecv, left MPI_Wait
120: leave BufferedCommunicator::sendRecv
120: communicator.hh::BufferedCommunicator::forward, left sendRecv
120: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
Newton::solveLinearSystem : Caught exception from the linear solver when getting converged: "FMatrixError [luDecomposition:/home/m.giraud/DUMUXexud/dune-common/dune/common/densematrix.hh:909]: matrix is singular"
83 converged value: 0 121 1 let s go get convergedRemote
83 to  comm_.min(converged) 
icator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 12
21: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 12
21: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 12
21: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 12
21: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 12
21: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 12
21: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 12
21: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 10 12
unicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 14
24: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 14
24: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 14
24: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 14
24: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 14
24: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 14
24: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 14
24: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 14
24: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 14
24: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 10 14
24: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 11 14
24: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 12 14
24: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 13 14
24: communicator.hh::sendRecv, left MPI_Wait
24: leave BufferedCommunicator::sendRecv
24: communicator.hh::BufferedCommunicator::forward, left sendRecv
24 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), after communicator.free();
24 schwarz.hh:virtual void apply (X& v, const Y& d), AFTER communication.copyOwnerToAll(v,v); 
void preSmooth(Smoother& smoother, Domain& v, const Range& d), AFTER smoother.apply(v,d);
void presmooth(LevelContext& levelContext, size_t steps), AFTER SmootherApplier<typename LevelContext::SmootherType>::preSmooth, step: 1 of 2
void presmooth(LevelContext& levelContext, size_t steps), before update defec, step: 1 of 2
void presmooth(LevelContext& levelContext, size_t steps), finished 
24void AMG<M,X,S,PI,A>::mgc(LevelContext& levelContext), AFTER presmooth(levelContext, preSteps_) 
24: communicator.hh::BufferedCommunicator::forward, enter sendRecv
24: communicator.hh::sendRecv, left MPI_Wait
24: leave BufferedCommunicator::sendRecv
24: communicator.hh::BufferedCommunicator::forward, left sendRecv
24: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
Newton::solveLinearSystem : Caught exception from the linear solver when getting converged: "FMatrixError [luDecomposition:/home/m.giraud/DUMUXexud/dune-common/dune/common/densematrix.hh:909]: matrix is singular"
85 converged value: 0 121 1 let s go get convergedRemote
85 to  comm_.min(converged) 
xt, size_t steps), before SmootherApplier<typename LevelContext::SmootherType>::preSmooth, step: 1 of 2
void preSmooth(Smoother& smoother, Domain& v, const Range& d), before smoother.apply(v,d);
72 schwarz.hh:virtual void apply (X& v, const Y& d), before preconditioner.apply(v,d);; 
72 schwarz.hh:virtual void apply (X& v, const Y& d), before communication.copyOwnerToAll(v,v); 
72 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
72: communicator.hh::BufferedCommunicator::forward, enter sendRecv
72: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 20
72: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 20
72: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 20
72: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 20
72: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 20
72: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 20
72: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 20
72: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 20
72: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 20
72: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 20
72: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 10 20
72: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 11 20
72: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 12 20
72: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 13 20
st Y& d), before preconditioner.apply(v,d);; 
98 schwarz.hh:virtual void apply (X& v, const Y& d), before communication.copyOwnerToAll(v,v); 
98 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
98: communicator.hh::BufferedCommunicator::forward, enter sendRecv
98: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 18
98: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 18
98: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 18
98: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 18
98: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 18
98: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 18
98: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 18
98: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 18
98: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 18
98: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 18
98: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 10 18
98: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 11 18
98: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 12 18
98: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 13 18
98: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 14 18
98: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 15 18
98: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 16 18
st Y& d), before preconditioner.apply(v,d);; 
28 schwarz.hh:virtual void apply (X& v, const Y& d), before communication.copyOwnerToAll(v,v); 
28 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
28: communicator.hh::BufferedCommunicator::forward, enter sendRecv
28: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 18
28: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 18
28: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 18
28: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 18
28: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 18
28: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 18
28: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 18
28: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 18
28: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 18
28: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 18
28: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 10 18
28: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 11 18
28: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 12 18
28: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 13 18
28: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 14 18
 &finished, &status); 0 14
118: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 14
118: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 14
118: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 14
118: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 14
118: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 14
118: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 14
118: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 14
118: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 14
118: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 14
118: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 10 14
118: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 11 14
118: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 12 14
118: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 13 14
118: communicator.hh::sendRecv, left MPI_Wait
118: leave BufferedCommunicator::sendRecv
118: communicator.hh::BufferedCommunicator::forward, left sendRecv
118 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), after communicator.free();
118 schwarz.hh:virtual void apply (X& v, const Y& d), AFTER communication.copyOwnerToAll(v,v); 
void preSmooth(Smoother& smoother, Domain& v, const Range& d), AFTER smoother.apply(v,d);
void presmooth(LevelContext& levelContext, size_t steps), AFTER SmootherApplier<typename LevelContext::SmootherType>::preSmooth, step: 1 of 2
void presmooth(LevelContext& levelContext, size_t steps), before update defec, step: 1 of 2
void presmooth(LevelContext& levelContext, size_t steps), finished 
118void AMG<M,X,S,PI,A>::mgc(LevelContext& levelContext), AFTER presmooth(levelContext, preSteps_) 
118: communicator.hh::BufferedCommunicator::forward, enter sendRecv
118: communicator.hh::sendRecv, left MPI_Wait
118: leave BufferedCommunicator::sendRecv
118: communicator.hh::BufferedCommunicator::forward, left sendRecv
118: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
.copyOwnerToAll(v,v); 
27 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
27: communicator.hh::BufferedCommunicator::forward, enter sendRecv
27: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 17
27: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 17
27: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 17
27: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 17
27: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 17
27: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 17
27: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 17
27: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 17
27: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 17
27: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 17
27: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 10 17
27: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 11 17
municator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 15
66: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 15
66: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 15
66: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 15
66: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 15
66: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 15
66: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 15
66: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 15
66: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 15
66: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 15
66: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 10 15
cv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 9
91: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 9
91: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 9
91: communicator.hh::sendRecv, left MPI_Wait
91: leave BufferedCommunicator::sendRecv
91: communicator.hh::BufferedCommunicator::forward, left sendRecv
91 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), after communicator.free();
91 schwarz.hh:virtual void apply (X& v, const Y& d), AFTER communication.copyOwnerToAll(v,v); 
void preSmooth(Smoother& smoother, Domain& v, const Range& d), AFTER smoother.apply(v,d);
void presmooth(LevelContext& levelContext, size_t steps), AFTER SmootherApplier<typename LevelContext::SmootherType>::preSmooth, step: 1 of 2
void presmooth(LevelContext& levelContext, size_t steps), before update defec, step: 1 of 2
void presmooth(LevelContext& levelContext, size_t steps), finished 
91void AMG<M,X,S,PI,A>::mgc(LevelContext& levelContext), AFTER presmooth(levelContext, preSteps_) 
91: communicator.hh::BufferedCommunicator::forward, enter sendRecv
91: communicator.hh::sendRecv, left MPI_Wait
91: leave BufferedCommunicator::sendRecv
91: communicator.hh::BufferedCommunicator::forward, left sendRecv
91: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
cator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 11
74: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 11
74: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 11
74: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 11
.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 15
37: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 15
37: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 15
37: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 15
37: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 15
37: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 15
37: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 15
37: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 15
37: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 15
s); 5 9
108: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 9
108: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 9
108: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 9
108: communicator.hh::sendRecv, left MPI_Wait
108: leave BufferedCommunicator::sendRecv
108: communicator.hh::BufferedCommunicator::forward, left sendRecv
108 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), after communicator.free();
108 schwarz.hh:virtual void apply (X& v, const Y& d), AFTER communication.copyOwnerToAll(v,v); 
void preSmooth(Smoother& smoother, Domain& v, const Range& d), AFTER smoother.apply(v,d);
void presmooth(LevelContext& levelContext, size_t steps), AFTER SmootherApplier<typename LevelContext::SmootherType>::preSmooth, step: 1 of 2
void presmooth(LevelContext& levelContext, size_t steps), before update defec, step: 1 of 2
void presmooth(LevelContext& levelContext, size_t steps), finished 
108void AMG<M,X,S,PI,A>::mgc(LevelContext& levelContext), AFTER presmooth(levelContext, preSteps_) 
108: communicator.hh::BufferedCommunicator::forward, enter sendRecv
108: communicator.hh::sendRecv, left MPI_Wait
108: leave BufferedCommunicator::sendRecv
108: communicator.hh::BufferedCommunicator::forward, left sendRecv
108: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
endRecv
95 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), after communicator.free();
95 schwarz.hh:virtual void apply (X& v, const Y& d), AFTER communication.copyOwnerToAll(v,v); 
void preSmooth(Smoother& smoother, Domain& v, const Range& d), AFTER smoother.apply(v,d);
void presmooth(LevelContext& levelContext, size_t steps), AFTER SmootherApplier<typename LevelContext::SmootherType>::preSmooth, step: 1 of 2
void presmooth(LevelContext& levelContext, size_t steps), before update defec, step: 1 of 2
void presmooth(LevelContext& levelContext, size_t steps), finished 
95void AMG<M,X,S,PI,A>::mgc(LevelContext& levelContext), AFTER presmooth(levelContext, preSteps_) 
95: communicator.hh::BufferedCommunicator::forward, enter sendRecv
95: communicator.hh::sendRecv, left MPI_Wait
95: leave BufferedCommunicator::sendRecv
95: communicator.hh::BufferedCommunicator::forward, left sendRecv
95: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
&finished, &status); 1 13
114: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 13
114: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 13
114: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 13
114: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 13
114: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 13
114: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 13
114: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 13
114: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 13
114: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 10 13
114: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 11 13
114: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 12 13
114: communicator.hh::sendRecv, left MPI_Wait
114: leave BufferedCommunicator::sendRecv
114: communicator.hh::BufferedCommunicator::forward, left sendRecv
114 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), after communicator.free();
114 schwarz.hh:virtual void apply (X& v, const Y& d), AFTER communication.copyOwnerToAll(v,v); 
void preSmooth(Smoother& smoother, Domain& v, const Range& d), AFTER smoother.apply(v,d);
void presmooth(LevelContext& levelContext, size_t steps), AFTER SmootherApplier<typename LevelContext::SmootherType>::preSmooth, step: 1 of 2
void presmooth(LevelContext& levelContext, size_t steps), before update defec, step: 1 of 2
void presmooth(LevelContext& levelContext, size_t steps), finished 
114void AMG<M,X,S,PI,A>::mgc(LevelContext& levelContext), AFTER presmooth(levelContext, preSteps_) 
114: communicator.hh::BufferedCommunicator::forward, enter sendRecv
114: communicator.hh::sendRecv, left MPI_Wait
114: leave BufferedCommunicator::sendRecv
114: communicator.hh::BufferedCommunicator::forward, left sendRecv
114: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
Newton::solveLinearSystem : Caught exception from the linear solver when getting converged: "FMatrixError [luDecomposition:/home/m.giraud/DUMUXexud/dune-common/dune/common/densematrix.hh:909]: matrix is singular"
62 converged value: 0 121 1 let s go get convergedRemote
62 to  comm_.min(converged) 
s); 5 9
forward<CopyGatherScatter<T> >(source,dest);
79: communicator.hh::BufferedCommunicator::forward, enter sendRecv
79: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 16
79: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 16
79: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 16
79: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 16
79: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 16
79: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 16
79: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 16
79: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 16
79: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 16
79: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 16
79: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 10 16
Context& levelContext, size_t steps), finished 
113void AMG<M,X,S,PI,A>::mgc(LevelContext& levelContext), AFTER presmooth(levelContext, preSteps_) 
113: communicator.hh::BufferedCommunicator::forward, enter sendRecv
113: communicator.hh::sendRecv, left MPI_Wait
113: leave BufferedCommunicator::sendRecv
113: communicator.hh::BufferedCommunicator::forward, left sendRecv
113: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
lContext& levelContext, size_t steps), before update defec, step: 1 of 2
void presmooth(LevelContext& levelContext, size_t steps), finished 
117void AMG<M,X,S,PI,A>::mgc(LevelContext& levelContext), AFTER presmooth(levelContext, preSteps_) 
117: communicator.hh::BufferedCommunicator::forward, enter sendRecv
117: communicator.hh::sendRecv, left MPI_Wait
117: leave BufferedCommunicator::sendRecv
117: communicator.hh::BufferedCommunicator::forward, left sendRecv
117: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
 d), AFTER smoother.apply(v,d);
void presmooth(LevelContext& levelContext, size_t steps), AFTER SmootherApplier<typename LevelContext::SmootherType>::preSmooth, step: 1 of 2
void presmooth(LevelContext& levelContext, size_t steps), before update defec, step: 1 of 2
void presmooth(LevelContext& levelContext, size_t steps), finished 
109void AMG<M,X,S,PI,A>::mgc(LevelContext& levelContext), AFTER presmooth(levelContext, preSteps_) 
109: communicator.hh::BufferedCommunicator::forward, enter sendRecv
109: communicator.hh::sendRecv, left MPI_Wait
109: leave BufferedCommunicator::sendRecv
109: communicator.hh::BufferedCommunicator::forward, left sendRecv
109: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
Smooth(Smoother& smoother, Domain& v, const Range& d), AFTER smoother.apply(v,d);
void presmooth(LevelContext& levelContext, size_t steps), AFTER SmootherApplier<typename LevelContext::SmootherType>::preSmooth, step: 1 of 2
void presmooth(LevelContext& levelContext, size_t steps), before update defec, step: 1 of 2
void presmooth(LevelContext& levelContext, size_t steps), finished 
8void AMG<M,X,S,PI,A>::mgc(LevelContext& levelContext), AFTER presmooth(levelContext, preSteps_) 
8: communicator.hh::BufferedCommunicator::forward, enter sendRecv
8: communicator.hh::sendRecv, left MPI_Wait
8: leave BufferedCommunicator::sendRecv
8: communicator.hh::BufferedCommunicator::forward, left sendRecv
8: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
_t steps), AFTER SmootherApplier<typename LevelContext::SmootherType>::preSmooth, step: 1 of 2
void presmooth(LevelContext& levelContext, size_t steps), before update defec, step: 1 of 2
void presmooth(LevelContext& levelContext, size_t steps), finished 
22void AMG<M,X,S,PI,A>::mgc(LevelContext& levelContext), AFTER presmooth(levelContext, preSteps_) 
22: communicator.hh::BufferedCommunicator::forward, enter sendRecv
22: communicator.hh::sendRecv, left MPI_Wait
22: leave BufferedCommunicator::sendRecv
22: communicator.hh::BufferedCommunicator::forward, left sendRecv
22: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
municator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 25 117
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 26 117
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 27 117
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 28 117
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 29 117
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 30 117
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 31 117
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 32 117
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 33 117
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 34 117
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 35 117
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 36 117
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 37 117
