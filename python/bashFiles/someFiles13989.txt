2  before _prec->apply(y,p); 0.5 250
2 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
5 === BiCGSTABSolver
 Iter          Defect            Rate
    0       1.3877e-11
5  (it = 0.5; it < _maxit; it+=.5) 0.5 250
5  before p = r 
5  before  y = W^-1 * p 
5  before _prec->apply(y,p); 0.5 250
5 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
3 === BiCGSTABSolver
 Iter          Defect            Rate
    0       1.3877e-11
3  (it = 0.5; it < _maxit; it+=.5) 0.5 250
3  before p = r 
3  before  y = W^-1 * p 
3  before _prec->apply(y,p); 0.5 250
3 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
1 === BiCGSTABSolver
 Iter          Defect            Rate
    0       1.3877e-11
1  (it = 0.5; it < _maxit; it+=.5) 0.5 250
1  before p = r 
1  before  y = W^-1 * p 
1  before _prec->apply(y,p); 0.5 250
1 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
6 === BiCGSTABSolver
 Iter          Defect            Rate
    0       1.3877e-11
6  (it = 0.5; it < _maxit; it+=.5) 0.5 250
6  before p = r 
6  before  y = W^-1 * p 
6  before _prec->apply(y,p); 0.5 250
6 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
0 === BiCGSTABSolver
 Iter          Defect            Rate
    0       1.3877e-11
0  (it = 0.5; it < _maxit; it+=.5) 0.5 250
0  before p = r 
0  before  y = W^-1 * p 
0  before _prec->apply(y,p); 0.5 250
0 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 7
0 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
0 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
6 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
6 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
6  before v = A * y 
4 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
4 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
4  before v = A * y 
2 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
2 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
5 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
5 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
5  before v = A * y 
3 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
3 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
3  before v = A * y 
1 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
1 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
0 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
6  before  alpha = rho_new / < rt, v > 
4  before  alpha = rho_new / < rt, v > 
2  before v = A * y 
2  before  alpha = rho_new / < rt, v > 
5  before  alpha = rho_new / < rt, v > 
3  before  alpha = rho_new / < rt, v > 
0  before v = A * y 
0  before  alpha = rho_new / < rt, v > 
0  before alpha = rho_new / h; 
0  before apply first correction to x 
0  before r = r - alpha*v 
0  before test stop criteria 
0 istlsolver::printOutput 
  0.5      1.64256e-27      1.18365e-16
0 === rate=1.40103e-32, T=0.0200638, TIT=0.0401276, IT=0.5
6  before alpha = rho_new / h; 
6  before apply first correction to x 
6  before r = r - alpha*v 
6  before test stop criteria 
6 istlsolver::printOutput 
  0.5      1.64256e-27      1.18365e-16
6 === rate=1.40103e-32, T=0.12275, TIT=0.245499, IT=0.5
6 finished solveLinearSystemImpl_ 1
6 converged value: 1 7 1 let s go get convergedRemote
6 to  comm_.min(converged) 
4  before alpha = rho_new / h; 
4  before apply first correction to x 
4  before r = r - alpha*v 
4  before test stop criteria 
4 istlsolver::printOutput 
  0.5      1.64256e-27      1.18365e-16
4 === rate=1.40103e-32, T=0.122752, TIT=0.245504, IT=0.5
4 finished solveLinearSystemImpl_ 1
4 converged value: 1 7 1 let s go get convergedRemote
4 to  comm_.min(converged) 
2  before alpha = rho_new / h; 
2  before apply first correction to x 
2  before r = r - alpha*v 
2  before test stop criteria 
2 istlsolver::printOutput 
  0.5      1.64256e-27      1.18365e-16
2 === rate=1.40103e-32, T=0.12275, TIT=0.245501, IT=0.5
2 finished solveLinearSystemImpl_ 1
2 converged value: 1 7 1 let s go get convergedRemote
2 to  comm_.min(converged) 
5  before alpha = rho_new / h; 
5  before apply first correction to x 
5  before r = r - alpha*v 
5  before test stop criteria 
5 istlsolver::printOutput 
  0.5      1.64256e-27      1.18365e-16
5 === rate=1.40103e-32, T=0.122691, TIT=0.245382, IT=0.5
5 finished solveLinearSystemImpl_ 1
5 converged value: 1 7 1 let s go get convergedRemote
5 to  comm_.min(converged) 
3  before alpha = rho_new / h; 
3  before apply first correction to x 
3  before r = r - alpha*v 
3  before test stop criteria 
3 istlsolver::printOutput 
  0.5      1.64256e-27      1.18365e-16
3 === rate=1.40103e-32, T=0.122691, TIT=0.245382, IT=0.5
3 finished solveLinearSystemImpl_ 1
3 converged value: 1 7 1 let s go get convergedRemote
3 to  comm_.min(converged) 
1  before v = A * y 
1  before  alpha = rho_new / < rt, v > 
1  before alpha = rho_new / h; 
1  before apply first correction to x 
1  before r = r - alpha*v 
1  before test stop criteria 
1 istlsolver::printOutput 
  0.5      1.64256e-27      1.18365e-16
1 === rate=1.40103e-32, T=0.122691, TIT=0.245382, IT=0.5
1 finished solveLinearSystemImpl_ 1
1 converged value: 1 7 1 let s go get convergedRemote
1 to  comm_.min(converged) 
4 did  comm_.min(converged) convergedRemote: 1
4 final convergedRemote: 1
4 Update: x^(k+1) = x^k - deltax^k 
4 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
4 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
4 Newton iteration  2 done4 , maximum relative shift = 2.334e-03
4 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
2 did  comm_.min(converged) convergedRemote: 1
2 final convergedRemote: 1
2 Update: x^(k+1) = x^k - deltax^k 
2 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
2 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
2 Newton iteration  2 done2 , maximum relative shift = 2.334e-03
2 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
1 did  comm_.min(converged) convergedRemote: 1
1 final convergedRemote: 1
1 Update: x^(k+1) = x^k - deltax^k 
1 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
1 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
1 Newton iteration  2 done1 , maximum relative shift = 2.334e-03
1 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
3 did  comm_.min(converged) convergedRemote: 1
3 final convergedRemote: 1
3 Update: x^(k+1) = x^k - deltax^k 
3 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
3 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
3 Newton iteration  2 done3 , maximum relative shift = 2.334e-03
3 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
5 did  comm_.min(converged) convergedRemote: 1
5 final convergedRemote: 1
5 Update: x^(k+1) = x^k - deltax^k 
5 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
5 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
5 Newton iteration  2 done5 , maximum relative shift = 2.334e-03
5 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
0 finished solveLinearSystemImpl_ 1
0 converged value: 1 7 1 let s go get convergedRemote
0 to  comm_.min(converged) 
0 did  comm_.min(converged) convergedRemote: 1
0 final convergedRemote: 1
0 Update: x^(k+1) = x^k - deltax^k 
0 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
0 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
0 Newton iteration  2 done0 , maximum relative shift = 2.334e-03
0 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
6 did  comm_.min(converged) convergedRemote: 1
6 final convergedRemote: 1
6 Update: x^(k+1) = x^k - deltax^k 
6 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
6 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
6 Newton iteration  2 done6 , maximum relative shift = 2.334e-03
6 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
1 Solve: M deltax^k = r 
1 Newton::solveLinearSystem : entering the try section 2
1 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
1 solveLinearSystemImpl_, before converged = ls.solve 
1 7 amgbackend::solve isParallel 1
3 Solve: M deltax^k = r 
3 Newton::solveLinearSystem : entering the try section 2
3 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
3 solveLinearSystemImpl_, before converged = ls.solve 
3 7 amgbackend::solve isParallel 1
5 Solve: M deltax^k = r 
5 Newton::solveLinearSystem : entering the try section 2
5 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
5 solveLinearSystemImpl_, before converged = ls.solve 
5 7 amgbackend::solve isParallel 1
0 Solve: M deltax^k = r 
0 Newton::solveLinearSystem : entering the try section 2
0 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
0 solveLinearSystemImpl_, before converged = ls.solve 
0 7 amgbackend::solve isParallel 1
2 Solve: M deltax^k = r 
2 Newton::solveLinearSystem : entering the try section 2
2 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
2 solveLinearSystemImpl_, before converged = ls.solve 
2 7 amgbackend::solve isParallel 1
4 Solve: M deltax^k = r 
4 Newton::solveLinearSystem : entering the try section 2
4 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
4 solveLinearSystemImpl_, before converged = ls.solve 
4 7 amgbackend::solve isParallel 1
6 Solve: M deltax^k = r 
6 Newton::solveLinearSystem : entering the try section 2
6 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
6 solveLinearSystemImpl_, before converged = ls.solve 
6 7 amgbackend::solve isParallel 1
5 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
5 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
0 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
0 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 7
2 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
2 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
6 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
6 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
4 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
4 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
1 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
1 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
3 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
3 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 7
6 7 before Dune::BiCGSTABSolver 
6 Dune::BiCGSTABSolver.apply(x, b, result_) 
6 BiCGSTABSolver::apply 
6 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
4 7 before Dune::BiCGSTABSolver 
4 Dune::BiCGSTABSolver.apply(x, b, result_) 
4 BiCGSTABSolver::apply 
4 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
1 7 before Dune::BiCGSTABSolver 
1 Dune::BiCGSTABSolver.apply(x, b, result_) 
1 BiCGSTABSolver::apply 
1 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
2 7 before Dune::BiCGSTABSolver 
2 Dune::BiCGSTABSolver.apply(x, b, result_) 
2 BiCGSTABSolver::apply 
2 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
5 7 before Dune::BiCGSTABSolver 
5 Dune::BiCGSTABSolver.apply(x, b, result_) 
5 BiCGSTABSolver::apply 
5 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
3 7 before Dune::BiCGSTABSolver 
3 Dune::BiCGSTABSolver.apply(x, b, result_) 
3 BiCGSTABSolver::apply 
3 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 7
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
Using a direct coarse solver (SuperLU)
Building hierarchy of 1 levels (inclusive coarse solver) took 0.110966 seconds.
0 7 before Dune::BiCGSTABSolver 
0 Dune::BiCGSTABSolver.apply(x, b, result_) 
0 BiCGSTABSolver::apply 
0 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
0 === BiCGSTABSolver
 Iter          Defect            Rate
    0      7.82903e-13
0  (it = 0.5; it < _maxit; it+=.5) 0.5 250
0  before p = r 
0  before  y = W^-1 * p 
0  before _prec->apply(y,p); 0.5 250
0 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 7
0 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
5 === BiCGSTABSolver
 Iter          Defect            Rate
    0      7.82903e-13
5  (it = 0.5; it < _maxit; it+=.5) 0.5 250
5  before p = r 
5  before  y = W^-1 * p 
5  before _prec->apply(y,p); 0.5 250
5 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
6 === BiCGSTABSolver
 Iter          Defect            Rate
    0      7.82903e-13
6  (it = 0.5; it < _maxit; it+=.5) 0.5 250
6  before p = r 
6  before  y = W^-1 * p 
6  before _prec->apply(y,p); 0.5 250
6 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
3 === BiCGSTABSolver
 Iter          Defect            Rate
    0      7.82903e-13
3  (it = 0.5; it < _maxit; it+=.5) 0.5 250
3  before p = r 
3  before  y = W^-1 * p 
3  before _prec->apply(y,p); 0.5 250
3 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
4 === BiCGSTABSolver
 Iter          Defect            Rate
    0      7.82903e-13
4  (it = 0.5; it < _maxit; it+=.5) 0.5 250
4  before p = r 
4  before  y = W^-1 * p 
4  before _prec->apply(y,p); 0.5 250
4 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
1 === BiCGSTABSolver
 Iter          Defect            Rate
    0      7.82903e-13
1  (it = 0.5; it < _maxit; it+=.5) 0.5 250
1  before p = r 
1  before  y = W^-1 * p 
1  before _prec->apply(y,p); 0.5 250
1 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
2 === BiCGSTABSolver
 Iter          Defect            Rate
    0      7.82903e-13
2  (it = 0.5; it < _maxit; it+=.5) 0.5 250
2  before p = r 
2  before  y = W^-1 * p 
2  before _prec->apply(y,p); 0.5 250
2 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
2 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
2 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
0 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
5 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
5 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
5  before v = A * y 
6 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
6 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
6  before v = A * y 
3 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
3 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
3  before v = A * y 
4 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
4 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
4  before v = A * y 
1 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
6  before  alpha = rho_new / < rt, v > 
3  before  alpha = rho_new / < rt, v > 
4  before  alpha = rho_new / < rt, v > 
1 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
0 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
0  before v = A * y 
5  before  alpha = rho_new / < rt, v > 
5  before alpha = rho_new / h; 
5  before apply first correction to x 
5  before r = r - alpha*v 
5  before test stop criteria 
5 istlsolver::printOutput 
  0.5      7.80144e-29      9.96477e-17
5 === rate=9.92966e-33, T=0.123287, TIT=0.246574, IT=0.5
5 finished solveLinearSystemImpl_ 1
5 converged value: 1 7 1 let s go get convergedRemote
5 to  comm_.min(converged) 
2  before v = A * y 
2  before  alpha = rho_new / < rt, v > 
2  before alpha = rho_new / h; 
2  before apply first correction to x 
2  before r = r - alpha*v 
2  before test stop criteria 
2 istlsolver::printOutput 
  0.5      7.80144e-29      9.96477e-17
2 === rate=9.92966e-33, T=0.123242, TIT=0.246483, IT=0.5
2 finished solveLinearSystemImpl_ 1
2 converged value: 1 7 1 let s go get convergedRemote
2 to  comm_.min(converged) 
1  before v = A * y 
1  before  alpha = rho_new / < rt, v > 
1  before alpha = rho_new / h; 
1  before apply first correction to x 
1  before r = r - alpha*v 
1  before test stop criteria 
1 istlsolver::printOutput 
  0.5      7.80144e-29      9.96477e-17
1 === rate=9.92966e-33, T=0.123289, TIT=0.246578, IT=0.5
1 finished solveLinearSystemImpl_ 1
1 converged value: 1 7 1 let s go get convergedRemote
1 to  comm_.min(converged) 
0  before  alpha = rho_new / < rt, v > 
0  before alpha = rho_new / h; 
0  before apply first correction to x 
0  before r = r - alpha*v 
0  before test stop criteria 
0 istlsolver::printOutput 
  0.5      7.80144e-29      9.96477e-17
0 === rate=9.92966e-33, T=0.0199799, TIT=0.0399598, IT=0.5
4  before alpha = rho_new / h; 
4  before apply first correction to x 
4  before r = r - alpha*v 
4  before test stop criteria 
4 istlsolver::printOutput 
  0.5      7.80144e-29      9.96477e-17
4 === rate=9.92966e-33, T=0.123241, TIT=0.246482, IT=0.5
4 finished solveLinearSystemImpl_ 1
4 converged value: 1 7 1 let s go get convergedRemote
4 to  comm_.min(converged) 
6  before alpha = rho_new / h; 
6  before apply first correction to x 
6  before r = r - alpha*v 
6  before test stop criteria 
6 istlsolver::printOutput 
  0.5      7.80144e-29      9.96477e-17
6 === rate=9.92966e-33, T=0.123241, TIT=0.246483, IT=0.5
6 finished solveLinearSystemImpl_ 1
6 converged value: 1 7 1 let s go get convergedRemote
6 to  comm_.min(converged) 
3  before alpha = rho_new / h; 
3  before apply first correction to x 
3  before r = r - alpha*v 
3  before test stop criteria 
3 istlsolver::printOutput 
  0.5      7.80144e-29      9.96477e-17
3 === rate=9.92966e-33, T=0.123288, TIT=0.246575, IT=0.5
3 finished solveLinearSystemImpl_ 1
3 converged value: 1 7 1 let s go get convergedRemote
3 to  comm_.min(converged) 
3 did  comm_.min(converged) convergedRemote: 1
3 final convergedRemote: 1
3 Update: x^(k+1) = x^k - deltax^k 
3 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
3 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
3 Newton iteration  3 done3 , maximum relative shift = 1.702e-04
3 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
5 did  comm_.min(converged) convergedRemote: 1
5 final convergedRemote: 1
5 Update: x^(k+1) = x^k - deltax^k 
5 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
5 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
5 Newton iteration  3 done5 , maximum relative shift = 1.702e-04
5 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
2 did  comm_.min(converged) convergedRemote: 1
2 final convergedRemote: 1
2 Update: x^(k+1) = x^k - deltax^k 
2 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
2 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
2 Newton iteration  3 done2 , maximum relative shift = 1.702e-04
2 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
1 did  comm_.min(converged) convergedRemote: 1
1 final convergedRemote: 1
1 Update: x^(k+1) = x^k - deltax^k 
1 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
1 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
1 Newton iteration  3 done1 , maximum relative shift = 1.702e-04
1 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
0 finished solveLinearSystemImpl_ 1
0 converged value: 1 7 1 let s go get convergedRemote
0 to  comm_.min(converged) 
0 did  comm_.min(converged) convergedRemote: 1
0 final convergedRemote: 1
0 Update: x^(k+1) = x^k - deltax^k 
0 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
0 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
0 Newton iteration  3 done0 , maximum relative shift = 1.702e-04
4 did  comm_.min(converged) convergedRemote: 1
4 final convergedRemote: 1
4 Update: x^(k+1) = x^k - deltax^k 
4 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
4 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
4 Newton iteration  3 done4 , maximum relative shift = 1.702e-04
4 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
6 did  comm_.min(converged) convergedRemote: 1
6 final convergedRemote: 1
6 Update: x^(k+1) = x^k - deltax^k 
6 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
6 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
6 Newton iteration  3 done6 , maximum relative shift = 1.702e-04
6 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
0 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
3 Solve: M deltax^k = r 
3 Newton::solveLinearSystem : entering the try section 3
3 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
3 solveLinearSystemImpl_, before converged = ls.solve 
3 7 amgbackend::solve isParallel 1
5 Solve: M deltax^k = r 
5 Newton::solveLinearSystem : entering the try section 3
5 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
5 solveLinearSystemImpl_, before converged = ls.solve 
5 7 amgbackend::solve isParallel 1
2 Solve: M deltax^k = r 
2 Newton::solveLinearSystem : entering the try section 3
2 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
2 solveLinearSystemImpl_, before converged = ls.solve 
2 7 amgbackend::solve isParallel 1
1 Solve: M deltax^k = r 
1 Newton::solveLinearSystem : entering the try section 3
1 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
1 solveLinearSystemImpl_, before converged = ls.solve 
1 7 amgbackend::solve isParallel 1
6 Solve: M deltax^k = r 
6 Newton::solveLinearSystem : entering the try section 3
6 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
6 solveLinearSystemImpl_, before converged = ls.solve 
6 7 amgbackend::solve isParallel 1
4 Solve: M deltax^k = r 
4 Newton::solveLinearSystem : entering the try section 3
4 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
4 solveLinearSystemImpl_, before converged = ls.solve 
4 7 amgbackend::solve isParallel 1
0 Solve: M deltax^k = r 
0 Newton::solveLinearSystem : entering the try section 3
0 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
0 solveLinearSystemImpl_, before converged = ls.solve 
0 7 amgbackend::solve isParallel 1
4 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
4 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
3 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
3 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
5 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
5 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
2 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
2 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
1 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
1 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
6 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
6 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
0 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
0 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 7
3 7 before Dune::BiCGSTABSolver 
3 Dune::BiCGSTABSolver.apply(x, b, result_) 
3 BiCGSTABSolver::apply 
3 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
5 7 before Dune::BiCGSTABSolver 
5 Dune::BiCGSTABSolver.apply(x, b, result_) 
5 BiCGSTABSolver::apply 
5 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
2 7 before Dune::BiCGSTABSolver 
2 Dune::BiCGSTABSolver.apply(x, b, result_) 
2 BiCGSTABSolver::apply 
2 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
1 7 before Dune::BiCGSTABSolver 
1 Dune::BiCGSTABSolver.apply(x, b, result_) 
1 BiCGSTABSolver::apply 
1 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 7
6 7 before Dune::BiCGSTABSolver 
6 Dune::BiCGSTABSolver.apply(x, b, result_) 
6 BiCGSTABSolver::apply 
6 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
4 7 before Dune::BiCGSTABSolver 
4 Dune::BiCGSTABSolver.apply(x, b, result_) 
4 BiCGSTABSolver::apply 
4 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
Using a direct coarse solver (SuperLU)
Building hierarchy of 1 levels (inclusive coarse solver) took 0.110138 seconds.
0 7 before Dune::BiCGSTABSolver 
0 Dune::BiCGSTABSolver.apply(x, b, result_) 
0 BiCGSTABSolver::apply 
0 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
0 === BiCGSTABSolver
 Iter          Defect            Rate
    0      5.50044e-14
0  (it = 0.5; it < _maxit; it+=.5) 0.5 250
3 === BiCGSTABSolver
 Iter          Defect            Rate
    0      5.50044e-14
3  (it = 0.5; it < _maxit; it+=.5) 0.5 250
3  before p = r 
3  before  y = W^-1 * p 
3  before _prec->apply(y,p); 0.5 250
3 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
5 === BiCGSTABSolver
 Iter          Defect            Rate
    0      5.50044e-14
5  (it = 0.5; it < _maxit; it+=.5) 0.5 250
5  before p = r 
5  before  y = W^-1 * p 
5  before _prec->apply(y,p); 0.5 250
5 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
2 === BiCGSTABSolver
 Iter          Defect            Rate
    0      5.50044e-14
2  (it = 0.5; it < _maxit; it+=.5) 0.5 250
2  before p = r 
2  before  y = W^-1 * p 
2  before _prec->apply(y,p); 0.5 250
2 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
0  before p = r 
0  before  y = W^-1 * p 
0  before _prec->apply(y,p); 0.5 250
0 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 7
0 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
4 === BiCGSTABSolver
 Iter          Defect            Rate
    0      5.50044e-14
4  (it = 0.5; it < _maxit; it+=.5) 0.5 250
4  before p = r 
4  before  y = W^-1 * p 
4  before _prec->apply(y,p); 0.5 250
4 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
6 === BiCGSTABSolver
 Iter          Defect            Rate
    0      5.50044e-14
6  (it = 0.5; it < _maxit; it+=.5) 0.5 250
6  before p = r 
6  before  y = W^-1 * p 
6  before _prec->apply(y,p); 0.5 250
6 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
1 === BiCGSTABSolver
 Iter          Defect            Rate
    0      5.50044e-14
1  (it = 0.5; it < _maxit; it+=.5) 0.5 250
1  before p = r 
1  before  y = W^-1 * p 
1  before _prec->apply(y,p); 0.5 250
1 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
1 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
0 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
4 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
4 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
4  before v = A * y 
6 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
6 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
6  before v = A * y 
6  before  alpha = rho_new / < rt, v > 
2 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
2 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
3 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
3 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
3  before v = A * y 
5 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
5 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
5  before v = A * y 
0 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
0  before v = A * y 
0  before  alpha = rho_new / < rt, v > 
4  before  alpha = rho_new / < rt, v > 
2  before v = A * y 
2  before  alpha = rho_new / < rt, v > 
3  before  alpha = rho_new / < rt, v > 
5  before  alpha = rho_new / < rt, v > 
1 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
1  before v = A * y 
1  before  alpha = rho_new / < rt, v > 
1  before alpha = rho_new / h; 
1  before apply first correction to x 
1  before r = r - alpha*v 
1  before test stop criteria 
5  before alpha = rho_new / h; 
5  before apply first correction to x 
5  before r = r - alpha*v 
5  before test stop criteria 
3  before alpha = rho_new / h; 
3  before apply first correction to x 
3  before r = r - alpha*v 
3  before test stop criteria 
6  before alpha = rho_new / h; 
6  before apply first correction to x 
6  before r = r - alpha*v 
6  before test stop criteria 
4  before alpha = rho_new / h; 
4  before apply first correction to x 
4  before r = r - alpha*v 
4  before test stop criteria 
2  before alpha = rho_new / h; 
2  before apply first correction to x 
2  before r = r - alpha*v 
2  before test stop criteria 
4 istlsolver::printOutput 
  0.5      5.76879e-30      1.04879e-16
4 === rate=1.09995e-32, T=0.125325, TIT=0.25065, IT=0.5
4 finished solveLinearSystemImpl_ 1
4 converged value: 1 7 1 let s go get convergedRemote
4 to  comm_.min(converged) 
2 istlsolver::printOutput 
  0.5      5.76879e-30      1.04879e-16
2 === rate=1.09995e-32, T=0.125325, TIT=0.25065, IT=0.5
2 finished solveLinearSystemImpl_ 1
2 converged value: 1 7 1 let s go get convergedRemote
2 to  comm_.min(converged) 
1 istlsolver::printOutput 
  0.5      5.76879e-30      1.04879e-16
1 === rate=1.09995e-32, T=0.125327, TIT=0.250654, IT=0.5
1 finished solveLinearSystemImpl_ 1
1 converged value: 1 7 1 let s go get convergedRemote
1 to  comm_.min(converged) 
5 istlsolver::printOutput 
  0.5      5.76879e-30      1.04879e-16
5 === rate=1.09995e-32, T=0.125328, TIT=0.250656, IT=0.5
5 finished solveLinearSystemImpl_ 1
5 converged value: 1 7 1 let s go get convergedRemote
5 to  comm_.min(converged) 
0  before alpha = rho_new / h; 
0  before apply first correction to x 
0  before r = r - alpha*v 
0  before test stop criteria 
0 istlsolver::printOutput 
  0.5      5.76879e-30      1.04879e-16
0 === rate=1.09995e-32, T=0.0229208, TIT=0.0458416, IT=0.5
3 istlsolver::printOutput 
  0.5      5.76879e-30      1.04879e-16
3 === rate=1.09995e-32, T=0.125329, TIT=0.250658, IT=0.5
3 finished solveLinearSystemImpl_ 1
3 converged value: 1 7 1 let s go get convergedRemote
3 to  comm_.min(converged) 
6 istlsolver::printOutput 
  0.5      5.76879e-30      1.04879e-16
6 === rate=1.09995e-32, T=0.125325, TIT=0.250651, IT=0.5
6 finished solveLinearSystemImpl_ 1
6 converged value: 1 7 1 let s go get convergedRemote
6 to  comm_.min(converged) 
3 did  comm_.min(converged) convergedRemote: 1
3 final convergedRemote: 1
3 Update: x^(k+1) = x^k - deltax^k 
3 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
3 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
3 Newton iteration  4 done3 , maximum relative shift = 1.828e-05
3 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
4 did  comm_.min(converged) convergedRemote: 1
4 final convergedRemote: 1
4 Update: x^(k+1) = x^k - deltax^k 
4 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
4 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
4 Newton iteration  4 done4 , maximum relative shift = 1.828e-05
4 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
6 did  comm_.min(converged) convergedRemote: 1
6 final convergedRemote: 1
6 Update: x^(k+1) = x^k - deltax^k 
6 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
6 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
6 Newton iteration  4 done6 , maximum relative shift = 1.828e-05
6 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
2 did  comm_.min(converged) convergedRemote: 1
2 final convergedRemote: 1
2 Update: x^(k+1) = x^k - deltax^k 
2 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
2 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
2 Newton iteration  4 done2 , maximum relative shift = 1.828e-05
2 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
1 did  comm_.min(converged) convergedRemote: 1
1 final convergedRemote: 1
1 Update: x^(k+1) = x^k - deltax^k 
1 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
1 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
1 Newton iteration  4 done1 , maximum relative shift = 1.828e-05
1 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
5 did  comm_.min(converged) convergedRemote: 1
5 final convergedRemote: 1
5 Update: x^(k+1) = x^k - deltax^k 
5 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
5 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
5 Newton iteration  4 done5 , maximum relative shift = 1.828e-05
5 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
0 finished solveLinearSystemImpl_ 1
0 converged value: 1 7 1 let s go get convergedRemote
0 to  comm_.min(converged) 
0 did  comm_.min(converged) convergedRemote: 1
0 final convergedRemote: 1
0 Update: x^(k+1) = x^k - deltax^k 
0 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
0 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
0 Newton iteration  4 done0 , maximum relative shift = 1.828e-05
0 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
0 Solve: M deltax^k = r 
0 Newton::solveLinearSystem : entering the try section 4
0 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
0 solveLinearSystemImpl_, before converged = ls.solve 
0 7 amgbackend::solve isParallel 1
6 Solve: M deltax^k = r 
6 Newton::solveLinearSystem : entering the try section 4
6 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
6 solveLinearSystemImpl_, before converged = ls.solve 
6 7 amgbackend::solve isParallel 1
2 Solve: M deltax^k = r 
2 Newton::solveLinearSystem : entering the try section 4
2 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
2 solveLinearSystemImpl_, before converged = ls.solve 
2 7 amgbackend::solve isParallel 1
1 Solve: M deltax^k = r 
1 Newton::solveLinearSystem : entering the try section 4
1 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
1 solveLinearSystemImpl_, before converged = ls.solve 
1 7 amgbackend::solve isParallel 1
5 Solve: M deltax^k = r 
5 Newton::solveLinearSystem : entering the try section 4
5 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
5 solveLinearSystemImpl_, before converged = ls.solve 
5 7 amgbackend::solve isParallel 1
4 Solve: M deltax^k = r 
4 Newton::solveLinearSystem : entering the try section 4
4 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
4 solveLinearSystemImpl_, before converged = ls.solve 
4 7 amgbackend::solve isParallel 1
3 Solve: M deltax^k = r 
3 Newton::solveLinearSystem : entering the try section 4
3 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
3 solveLinearSystemImpl_, before converged = ls.solve 
3 7 amgbackend::solve isParallel 1
2 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
2 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
1 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
1 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
5 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
5 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
4 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
4 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
3 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
3 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
0 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
0 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 7
6 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
6 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 7
3 7 before Dune::BiCGSTABSolver 
3 Dune::BiCGSTABSolver.apply(x, b, result_) 
3 BiCGSTABSolver::apply 
3 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
6 7 before Dune::BiCGSTABSolver 
6 Dune::BiCGSTABSolver.apply(x, b, result_) 
6 BiCGSTABSolver::apply 
6 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
4 7 before Dune::BiCGSTABSolver 
4 Dune::BiCGSTABSolver.apply(x, b, result_) 
4 BiCGSTABSolver::apply 
4 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
2 7 before Dune::BiCGSTABSolver 
2 Dune::BiCGSTABSolver.apply(x, b, result_) 
2 BiCGSTABSolver::apply 
2 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
1 7 before Dune::BiCGSTABSolver 
1 Dune::BiCGSTABSolver.apply(x, b, result_) 
1 BiCGSTABSolver::apply 
1 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
5 7 before Dune::BiCGSTABSolver 
5 Dune::BiCGSTABSolver.apply(x, b, result_) 
5 BiCGSTABSolver::apply 
5 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 7
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
Using a direct coarse solver (SuperLU)
Building hierarchy of 1 levels (inclusive coarse solver) took 0.110068 seconds.
0 7 before Dune::BiCGSTABSolver 
0 Dune::BiCGSTABSolver.apply(x, b, result_) 
0 BiCGSTABSolver::apply 
0 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
0 === BiCGSTABSolver
 Iter          Defect            Rate
    0      3.32857e-14
0  (it = 0.5; it < _maxit; it+=.5) 0.5 250
3 === BiCGSTABSolver
 Iter          Defect            Rate
    0      3.32857e-14
3  (it = 0.5; it < _maxit; it+=.5) 0.5 250
3  before p = r 
3  before  y = W^-1 * p 
3  before _prec->apply(y,p); 0.5 250
3 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
2 === BiCGSTABSolver
 Iter          Defect            Rate
    0      3.32857e-14
2  (it = 0.5; it < _maxit; it+=.5) 0.5 250
2  before p = r 
2  before  y = W^-1 * p 
2  before _prec->apply(y,p); 0.5 250
2 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
4 === BiCGSTABSolver
 Iter          Defect            Rate
    0      3.32857e-14
4  (it = 0.5; it < _maxit; it+=.5) 0.5 250
4  before p = r 
4  before  y = W^-1 * p 
4  before _prec->apply(y,p); 0.5 250
4 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
0  before p = r 
0  before  y = W^-1 * p 
0  before _prec->apply(y,p); 0.5 250
0 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 7
0 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
1 === BiCGSTABSolver
 Iter          Defect            Rate
    0      3.32857e-14
1  (it = 0.5; it < _maxit; it+=.5) 0.5 250
1  before p = r 
1  before  y = W^-1 * p 
1  before _prec->apply(y,p); 0.5 250
1 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
5 === BiCGSTABSolver
 Iter          Defect            Rate
    0      3.32857e-14
5  (it = 0.5; it < _maxit; it+=.5) 0.5 250
5  before p = r 
5  before  y = W^-1 * p 
5  before _prec->apply(y,p); 0.5 250
5 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
6 === BiCGSTABSolver
 Iter          Defect            Rate
    0      3.32857e-14
6  (it = 0.5; it < _maxit; it+=.5) 0.5 250
6  before p = r 
6  before  y = W^-1 * p 
6  before _prec->apply(y,p); 0.5 250
6 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
3 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
3 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
3  before v = A * y 
2 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
2 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
6 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
6 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
6  before v = A * y 
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
0 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
1 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
5 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
5 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
5  before v = A * y 
4 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
4 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
4  before v = A * y 
3  before  alpha = rho_new / < rt, v > 
4  before  alpha = rho_new / < rt, v > 
6  before  alpha = rho_new / < rt, v > 
0 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
0  before v = A * y 
1 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
5  before  alpha = rho_new / < rt, v > 
2  before v = A * y 
2  before  alpha = rho_new / < rt, v > 
4  before alpha = rho_new / h; 
4  before apply first correction to x 
4  before r = r - alpha*v 
4  before test stop criteria 
4 istlsolver::printOutput 
  0.5      3.11855e-33      9.36903e-20
4 === rate=8.77788e-39, T=0.120408, TIT=0.240816, IT=0.5
4 finished solveLinearSystemImpl_ 1
4 converged value: 1 7 1 let s go get convergedRemote
4 to  comm_.min(converged) 
6  before alpha = rho_new / h; 
6  before apply first correction to x 
6  before r = r - alpha*v 
6  before test stop criteria 
6 istlsolver::printOutput 
  0.5      3.11855e-33      9.36903e-20
6 === rate=8.77788e-39, T=0.120408, TIT=0.240816, IT=0.5
6 finished solveLinearSystemImpl_ 1
6 converged value: 1 7 1 let s go get convergedRemote
6 to  comm_.min(converged) 
0  before  alpha = rho_new / < rt, v > 
0  before alpha = rho_new / h; 
0  before apply first correction to x 
0  before r = r - alpha*v 
0  before test stop criteria 
0 istlsolver::printOutput 
  0.5      3.11855e-33      9.36903e-20
0 === rate=8.77788e-39, T=0.0182976, TIT=0.0365952, IT=0.5
1  before v = A * y 
1  before  alpha = rho_new / < rt, v > 
1  before alpha = rho_new / h; 
1  before apply first correction to x 
1  before r = r - alpha*v 
1  before test stop criteria 
1 istlsolver::printOutput 
  0.5      3.11855e-33      9.36903e-20
1 === rate=8.77788e-39, T=0.120502, TIT=0.241004, IT=0.5
1 finished solveLinearSystemImpl_ 1
1 converged value: 1 7 1 let s go get convergedRemote
1 to  comm_.min(converged) 
5  before alpha = rho_new / h; 
5  before apply first correction to x 
5  before r = r - alpha*v 
5  before test stop criteria 
5 istlsolver::printOutput 
  0.5      3.11855e-33      9.36903e-20
5 === rate=8.77788e-39, T=0.120499, TIT=0.240998, IT=0.5
5 finished solveLinearSystemImpl_ 1
5 converged value: 1 7 1 let s go get convergedRemote
5 to  comm_.min(converged) 
3  before alpha = rho_new / h; 
3  before apply first correction to x 
3  before r = r - alpha*v 
3  before test stop criteria 
3 istlsolver::printOutput 
  0.5      3.11855e-33      9.36903e-20
3 === rate=8.77788e-39, T=0.120499, TIT=0.240999, IT=0.5
3 finished solveLinearSystemImpl_ 1
3 converged value: 1 7 1 let s go get convergedRemote
3 to  comm_.min(converged) 
2  before alpha = rho_new / h; 
2  before apply first correction to x 
2  before r = r - alpha*v 
2  before test stop criteria 
2 istlsolver::printOutput 
  0.5      3.11855e-33      9.36903e-20
2 === rate=8.77788e-39, T=0.120409, TIT=0.240818, IT=0.5
2 finished solveLinearSystemImpl_ 1
2 converged value: 1 7 1 let s go get convergedRemote
2 to  comm_.min(converged) 
2 did  comm_.min(converged) convergedRemote: 1
2 final convergedRemote: 1
2 Update: x^(k+1) = x^k - deltax^k 
2 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
2 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
2 Newton iteration  5 done2 , maximum relative shift = 1.110e-05
2 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
3 did  comm_.min(converged) convergedRemote: 1
3 final convergedRemote: 1
3 Update: x^(k+1) = x^k - deltax^k 
3 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
3 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
3 Newton iteration  5 done3 , maximum relative shift = 1.110e-05
3 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
4 did  comm_.min(converged) convergedRemote: 1
4 final convergedRemote: 1
4 Update: x^(k+1) = x^k - deltax^k 
4 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
4 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
4 Newton iteration  5 done4 , maximum relative shift = 1.110e-05
4 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
6 did  comm_.min(converged) convergedRemote: 1
6 final convergedRemote: 1
6 Update: x^(k+1) = x^k - deltax^k 
6 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
6 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
6 Newton iteration  5 done6 , maximum relative shift = 1.110e-05
6 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
0 finished solveLinearSystemImpl_ 1
0 converged value: 1 7 1 let s go get convergedRemote
0 to  comm_.min(converged) 
0 did  comm_.min(converged) convergedRemote: 1
0 final convergedRemote: 1
0 Update: x^(k+1) = x^k - deltax^k 
0 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
0 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
0 Newton iteration  5 done0 , maximum relative shift = 1.110e-05
0 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
1 did  comm_.min(converged) convergedRemote: 1
1 final convergedRemote: 1
1 Update: x^(k+1) = x^k - deltax^k 
1 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
1 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
1 Newton iteration  5 done1 , maximum relative shift = 1.110e-05
1 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
5 did  comm_.min(converged) convergedRemote: 1
5 final convergedRemote: 1
5 Update: x^(k+1) = x^k - deltax^k 
5 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
5 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
5 Newton iteration  5 done5 , maximum relative shift = 1.110e-05
5 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
6 Solve: M deltax^k = r 
6 Newton::solveLinearSystem : entering the try section 5
6 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
6 solveLinearSystemImpl_, before converged = ls.solve 
6 7 amgbackend::solve isParallel 1
0 Solve: M deltax^k = r 
0 Newton::solveLinearSystem : entering the try section 5
0 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
0 solveLinearSystemImpl_, before converged = ls.solve 
0 7 amgbackend::solve isParallel 1
1 Solve: M deltax^k = r 
1 Newton::solveLinearSystem : entering the try section 5
1 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
1 solveLinearSystemImpl_, before converged = ls.solve 
1 7 amgbackend::solve isParallel 1
5 Solve: M deltax^k = r 
5 Newton::solveLinearSystem : entering the try section 5
5 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
5 solveLinearSystemImpl_, before converged = ls.solve 
5 7 amgbackend::solve isParallel 1
2 Solve: M deltax^k = r 
2 Newton::solveLinearSystem : entering the try section 5
2 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
2 solveLinearSystemImpl_, before converged = ls.solve 
2 7 amgbackend::solve isParallel 1
3 Solve: M deltax^k = r 
3 Newton::solveLinearSystem : entering the try section 5
3 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
3 solveLinearSystemImpl_, before converged = ls.solve 
3 7 amgbackend::solve isParallel 1
4 Solve: M deltax^k = r 
4 Newton::solveLinearSystem : entering the try section 5
4 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
4 solveLinearSystemImpl_, before converged = ls.solve 
4 7 amgbackend::solve isParallel 1
6 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
6 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
0 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
0 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 7
4 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
4 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
5 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
5 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
2 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
2 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
3 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
3 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
1 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
1 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 7
1 7 before Dune::BiCGSTABSolver 
1 Dune::BiCGSTABSolver.apply(x, b, result_) 
1 BiCGSTABSolver::apply 
1 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
6 7 before Dune::BiCGSTABSolver 
6 Dune::BiCGSTABSolver.apply(x, b, result_) 
6 BiCGSTABSolver::apply 
6 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 7
4 7 before Dune::BiCGSTABSolver 
4 Dune::BiCGSTABSolver.apply(x, b, result_) 
4 BiCGSTABSolver::apply 
4 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
5 7 before Dune::BiCGSTABSolver 
5 Dune::BiCGSTABSolver.apply(x, b, result_) 
5 BiCGSTABSolver::apply 
5 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
2 7 before Dune::BiCGSTABSolver 
2 Dune::BiCGSTABSolver.apply(x, b, result_) 
2 BiCGSTABSolver::apply 
2 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
3 7 before Dune::BiCGSTABSolver 
3 Dune::BiCGSTABSolver.apply(x, b, result_) 
3 BiCGSTABSolver::apply 
3 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
Using a direct coarse solver (SuperLU)
Building hierarchy of 1 levels (inclusive coarse solver) took 0.111037 seconds.
0 7 before Dune::BiCGSTABSolver 
0 Dune::BiCGSTABSolver.apply(x, b, result_) 
0 BiCGSTABSolver::apply 
0 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
3 === BiCGSTABSolver
 Iter          Defect            Rate
    0      2.02095e-14
3  (it = 0.5; it < _maxit; it+=.5) 0.5 250
3  before p = r 
3  before  y = W^-1 * p 
3  before _prec->apply(y,p); 0.5 250
3 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
4 === BiCGSTABSolver
 Iter          Defect            Rate
    0      2.02095e-14
4  (it = 0.5; it < _maxit; it+=.5) 0.5 250
4  before p = r 
4  before  y = W^-1 * p 
4  before _prec->apply(y,p); 0.5 250
4 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
5 === BiCGSTABSolver
 Iter          Defect            Rate
    0      2.02095e-14
5  (it = 0.5; it < _maxit; it+=.5) 0.5 250
5  before p = r 
5  before  y = W^-1 * p 
5  before _prec->apply(y,p); 0.5 250
5 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
2 === BiCGSTABSolver
 Iter          Defect            Rate
    0      2.02095e-14
2  (it = 0.5; it < _maxit; it+=.5) 0.5 250
2  before p = r 
2  before  y = W^-1 * p 
2  before _prec->apply(y,p); 0.5 250
2 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
6 === BiCGSTABSolver
 Iter          Defect            Rate
    0      2.02095e-14
6  (it = 0.5; it < _maxit; it+=.5) 0.5 250
6  before p = r 
6  before  y = W^-1 * p 
6  before _prec->apply(y,p); 0.5 250
6 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
1 === BiCGSTABSolver
 Iter          Defect            Rate
    0      2.02095e-14
1  (it = 0.5; it < _maxit; it+=.5) 0.5 250
1  before p = r 
1  before  y = W^-1 * p 
1  before _prec->apply(y,p); 0.5 250
1 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
0 === BiCGSTABSolver
 Iter          Defect            Rate
    0      2.02095e-14
0  (it = 0.5; it < _maxit; it+=.5) 0.5 250
0  before p = r 
0  before  y = W^-1 * p 
0  before _prec->apply(y,p); 0.5 250
0 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 7
0 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
2 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
2 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
6 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
6 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
6  before v = A * y 
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
0 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
1 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
3 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
3 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
3  before v = A * y 
4 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
4 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
4  before v = A * y 
5 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
5 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
5  before v = A * y 
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
0 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
0  before v = A * y 
1 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
1  before v = A * y 
3  before  alpha = rho_new / < rt, v > 
4  before  alpha = rho_new / < rt, v > 
6  before  alpha = rho_new / < rt, v > 
2  before v = A * y 
5  before  alpha = rho_new / < rt, v > 
6  before alpha = rho_new / h; 
6  before apply first correction to x 
6  before r = r - alpha*v 
6  before test stop criteria 
6 istlsolver::printOutput 
  0.5      1.33198e-30      6.59083e-17
6 === rate=4.34391e-33, T=0.124887, TIT=0.249773, IT=0.5
6 finished solveLinearSystemImpl_ 1
6 converged value: 1 7 1 let s go get convergedRemote
6 to  comm_.min(converged) 
5  before alpha = rho_new / h; 
5  before apply first correction to x 
5  before r = r - alpha*v 
5  before test stop criteria 
5 istlsolver::printOutput 
  0.5      1.33198e-30      6.59083e-17
5 === rate=4.34391e-33, T=0.12489, TIT=0.249781, IT=0.5
5 finished solveLinearSystemImpl_ 1
5 converged value: 1 7 1 let s go get convergedRemote
5 to  comm_.min(converged) 
2  before  alpha = rho_new / < rt, v > 
2  before alpha = rho_new / h; 
2  before apply first correction to x 
2  before r = r - alpha*v 
2  before test stop criteria 
2 istlsolver::printOutput 
  0.5      1.33198e-30      6.59083e-17
2 === rate=4.34391e-33, T=0.124888, TIT=0.249777, IT=0.5
2 finished solveLinearSystemImpl_ 1
2 converged value: 1 7 1 let s go get convergedRemote
2 to  comm_.min(converged) 
0  before  alpha = rho_new / < rt, v > 
0  before alpha = rho_new / h; 
0  before apply first correction to x 
0  before r = r - alpha*v 
0  before test stop criteria 
0 istlsolver::printOutput 
  0.5      1.33198e-30      6.59083e-17
0 === rate=4.34391e-33, T=0.0216034, TIT=0.0432069, IT=0.5
1  before  alpha = rho_new / < rt, v > 
1  before alpha = rho_new / h; 
1  before apply first correction to x 
1  before r = r - alpha*v 
1  before test stop criteria 
1 istlsolver::printOutput 
  0.5      1.33198e-30      6.59083e-17
1 === rate=4.34391e-33, T=0.124892, TIT=0.249785, IT=0.5
1 finished solveLinearSystemImpl_ 1
1 converged value: 1 7 1 let s go get convergedRemote
1 to  comm_.min(converged) 
3  before alpha = rho_new / h; 
3  before apply first correction to x 
3  before r = r - alpha*v 
3  before test stop criteria 
3 istlsolver::printOutput 
  0.5      1.33198e-30      6.59083e-17
3 === rate=4.34391e-33, T=0.124891, TIT=0.249782, IT=0.5
3 finished solveLinearSystemImpl_ 1
3 converged value: 1 7 1 let s go get convergedRemote
3 to  comm_.min(converged) 
4  before alpha = rho_new / h; 
4  before apply first correction to x 
4  before r = r - alpha*v 
4  before test stop criteria 
4 istlsolver::printOutput 
  0.5      1.33198e-30      6.59083e-17
4 === rate=4.34391e-33, T=0.124885, TIT=0.249769, IT=0.5
4 finished solveLinearSystemImpl_ 1
4 converged value: 1 7 1 let s go get convergedRemote
4 to  comm_.min(converged) 
0 finished solveLinearSystemImpl_ 1
0 converged value: 1 7 1 let s go get convergedRemote
0 to  comm_.min(converged) 
0 did  comm_.min(converged) convergedRemote: 1
0 final convergedRemote: 1
0 Update: x^(k+1) = x^k - deltax^k 
0 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
0 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
0 Newton iteration  6 done0 , maximum relative shift = 4.492e-06
0 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
1 did  comm_.min(converged) convergedRemote: 1
1 final convergedRemote: 1
1 Update: x^(k+1) = x^k - deltax^k 
1 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
1 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
1 Newton iteration  6 done1 , maximum relative shift = 4.492e-06
1 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
3 did  comm_.min(converged) convergedRemote: 1
3 final convergedRemote: 1
3 Update: x^(k+1) = x^k - deltax^k 
3 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
3 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
3 Newton iteration  6 done3 , maximum relative shift = 4.492e-06
3 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
6 did  comm_.min(converged) convergedRemote: 1
6 final convergedRemote: 1
6 Update: x^(k+1) = x^k - deltax^k 
6 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
6 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
6 Newton iteration  6 done6 , maximum relative shift = 4.492e-06
6 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
4 did  comm_.min(converged) convergedRemote: 1
4 final convergedRemote: 1
4 Update: x^(k+1) = x^k - deltax^k 
4 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
4 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
4 Newton iteration  6 done4 , maximum relative shift = 4.492e-06
4 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
5 did  comm_.min(converged) convergedRemote: 1
5 final convergedRemote: 1
5 Update: x^(k+1) = x^k - deltax^k 
5 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
5 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
5 Newton iteration  6 done5 , maximum relative shift = 4.492e-06
5 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
2 did  comm_.min(converged) convergedRemote: 1
2 final convergedRemote: 1
2 Update: x^(k+1) = x^k - deltax^k 
2 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
2 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
2 Newton iteration  6 done2 , maximum relative shift = 4.492e-06
2 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
2 Solve: M deltax^k = r 
2 Newton::solveLinearSystem : entering the try section 6
2 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
2 solveLinearSystemImpl_, before converged = ls.solve 
2 7 amgbackend::solve isParallel 1
4 Solve: M deltax^k = r 
4 Newton::solveLinearSystem : entering the try section 6
4 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
4 solveLinearSystemImpl_, before converged = ls.solve 
4 7 amgbackend::solve isParallel 1
5 Solve: M deltax^k = r 
5 Newton::solveLinearSystem : entering the try section 6
5 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
5 solveLinearSystemImpl_, before converged = ls.solve 
5 7 amgbackend::solve isParallel 1
6 Solve: M deltax^k = r 
6 Newton::solveLinearSystem : entering the try section 6
6 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
6 solveLinearSystemImpl_, before converged = ls.solve 
6 7 amgbackend::solve isParallel 1
0 Solve: M deltax^k = r 
0 Newton::solveLinearSystem : entering the try section 6
0 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
0 solveLinearSystemImpl_, before converged = ls.solve 
0 7 amgbackend::solve isParallel 1
1 Solve: M deltax^k = r 
1 Newton::solveLinearSystem : entering the try section 6
1 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
1 solveLinearSystemImpl_, before converged = ls.solve 
1 7 amgbackend::solve isParallel 1
3 Solve: M deltax^k = r 
3 Newton::solveLinearSystem : entering the try section 6
3 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
3 solveLinearSystemImpl_, before converged = ls.solve 
3 7 amgbackend::solve isParallel 1
3 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
3 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
2 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
2 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
4 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
4 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
5 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
5 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
6 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
6 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
0 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
0 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 7
1 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
1 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 7
3 7 before Dune::BiCGSTABSolver 
3 Dune::BiCGSTABSolver.apply(x, b, result_) 
3 BiCGSTABSolver::apply 
3 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
2 7 before Dune::BiCGSTABSolver 
2 Dune::BiCGSTABSolver.apply(x, b, result_) 
2 BiCGSTABSolver::apply 
2 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
4 7 before Dune::BiCGSTABSolver 
4 Dune::BiCGSTABSolver.apply(x, b, result_) 
4 BiCGSTABSolver::apply 
4 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
5 7 before Dune::BiCGSTABSolver 
5 Dune::BiCGSTABSolver.apply(x, b, result_) 
5 BiCGSTABSolver::apply 
5 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
6 7 before Dune::BiCGSTABSolver 
6 Dune::BiCGSTABSolver.apply(x, b, result_) 
6 BiCGSTABSolver::apply 
6 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
1 7 before Dune::BiCGSTABSolver 
1 Dune::BiCGSTABSolver.apply(x, b, result_) 
1 BiCGSTABSolver::apply 
1 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 7
Using a direct coarse solver (SuperLU)
Building hierarchy of 1 levels (inclusive coarse solver) took 0.111408 seconds.
0 7 before Dune::BiCGSTABSolver 
0 Dune::BiCGSTABSolver.apply(x, b, result_) 
0 BiCGSTABSolver::apply 
0 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
0 === BiCGSTABSolver
 Iter          Defect            Rate
    0      1.44888e-15
0  (it = 0.5; it < _maxit; it+=.5) 0.5 250
4 === BiCGSTABSolver
 Iter          Defect            Rate
    0      1.44888e-15
4  (it = 0.5; it < _maxit; it+=.5) 0.5 250
4  before p = r 
4  before  y = W^-1 * p 
4  before _prec->apply(y,p); 0.5 250
4 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
5 === BiCGSTABSolver
 Iter          Defect            Rate
    0      1.44888e-15
5  (it = 0.5; it < _maxit; it+=.5) 0.5 250
5  before p = r 
5  before  y = W^-1 * p 
5  before _prec->apply(y,p); 0.5 250
5 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
6 === BiCGSTABSolver
 Iter          Defect            Rate
    0      1.44888e-15
6  (it = 0.5; it < _maxit; it+=.5) 0.5 250
6  before p = r 
6  before  y = W^-1 * p 
6  before _prec->apply(y,p); 0.5 250
6 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
0  before p = r 
0  before  y = W^-1 * p 
0  before _prec->apply(y,p); 0.5 250
0 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 7
0 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
1 === BiCGSTABSolver
 Iter          Defect            Rate
    0      1.44888e-15
1  (it = 0.5; it < _maxit; it+=.5) 0.5 250
1  before p = r 
1  before  y = W^-1 * p 
1  before _prec->apply(y,p); 0.5 250
1 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
3 === BiCGSTABSolver
 Iter          Defect            Rate
    0      1.44888e-15
3  (it = 0.5; it < _maxit; it+=.5) 0.5 250
3  before p = r 
3  before  y = W^-1 * p 
3  before _prec->apply(y,p); 0.5 250
3 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
2 === BiCGSTABSolver
 Iter          Defect            Rate
    0      1.44888e-15
2  (it = 0.5; it < _maxit; it+=.5) 0.5 250
2  before p = r 
2  before  y = W^-1 * p 
2  before _prec->apply(y,p); 0.5 250
2 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
0 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
1 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
3 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
3 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
3  before v = A * y 
6 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
6 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
6  before v = A * y 
6  before  alpha = rho_new / < rt, v > 
4 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
4 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
4  before v = A * y 
4  before  alpha = rho_new / < rt, v > 
5 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
5 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
5  before v = A * y 
2 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
2 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
3  before  alpha = rho_new / < rt, v > 
2  before v = A * y 
2  before  alpha = rho_new / < rt, v > 
5  before  alpha = rho_new / < rt, v > 
0 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
0  before v = A * y 
1 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
2  before alpha = rho_new / h; 
2  before apply first correction to x 
2  before r = r - alpha*v 
2  before test stop criteria 
2 istlsolver::printOutput 
  0.5      1.24503e-31      8.59304e-17
2 === rate=7.38404e-33, T=0.121773, TIT=0.243547, IT=0.5
2 finished solveLinearSystemImpl_ 1
2 converged value: 1 7 1 let s go get convergedRemote
2 to  comm_.min(converged) 
2 did  comm_.min(converged) convergedRemote: 1
2 final convergedRemote: 1
2 Update: x^(k+1) = x^k - deltax^k 
2 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
2 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
2 Newton iteration  7 done2 , maximum relative shift = 3.221e-07
2 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
5  before alpha = rho_new / h; 
5  before apply first correction to x 
5  before r = r - alpha*v 
5  before test stop criteria 
5 istlsolver::printOutput 
  0.5      1.24503e-31      8.59304e-17
5 === rate=7.38404e-33, T=0.121738, TIT=0.243476, IT=0.5
5 finished solveLinearSystemImpl_ 1
5 converged value: 1 7 1 let s go get convergedRemote
5 to  comm_.min(converged) 
5 did  comm_.min(converged) convergedRemote: 1
5 final convergedRemote: 1
5 Update: x^(k+1) = x^k - deltax^k 
5 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
5 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
5 Newton iteration  7 done5 , maximum relative shift = 3.221e-07
5 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
0  before  alpha = rho_new / < rt, v > 
0  before alpha = rho_new / h; 
0  before apply first correction to x 
0  before r = r - alpha*v 
0  before test stop criteria 
0 istlsolver::printOutput 
  0.5      1.24503e-31      8.59304e-17
0 === rate=7.38404e-33, T=0.0183472, TIT=0.0366943, IT=0.5
0 finished solveLinearSystemImpl_ 1
0 converged value: 1 7 1 let s go get convergedRemote
0 to  comm_.min(converged) 
0 did  comm_.min(converged) convergedRemote: 1
0 final convergedRemote: 1
0 Update: x^(k+1) = x^k - deltax^k 
0 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
0 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
0 Newton iteration  7 done0 , maximum relative shift = 3.221e-07
0 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
3  before alpha = rho_new / h; 
3  before apply first correction to x 
3  before r = r - alpha*v 
3  before test stop criteria 
3 istlsolver::printOutput 
  0.5      1.24503e-31      8.59304e-17
3 === rate=7.38404e-33, T=0.12174, TIT=0.243481, IT=0.5
3 finished solveLinearSystemImpl_ 1
3 converged value: 1 7 1 let s go get convergedRemote
3 to  comm_.min(converged) 
3 did  comm_.min(converged) convergedRemote: 1
3 final convergedRemote: 1
3 Update: x^(k+1) = x^k - deltax^k 
3 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
3 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
3 Newton iteration  7 done3 , maximum relative shift = 3.221e-07
3 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
6  before alpha = rho_new / h; 
6  before apply first correction to x 
6  before r = r - alpha*v 
6  before test stop criteria 
6 istlsolver::printOutput 
  0.5      1.24503e-31      8.59304e-17
6 === rate=7.38404e-33, T=0.121775, TIT=0.243551, IT=0.5
6 finished solveLinearSystemImpl_ 1
6 converged value: 1 7 1 let s go get convergedRemote
6 to  comm_.min(converged) 
6 did  comm_.min(converged) convergedRemote: 1
6 final convergedRemote: 1
6 Update: x^(k+1) = x^k - deltax^k 
6 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
6 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
6 Newton iteration  7 done6 , maximum relative shift = 3.221e-07
6 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
4  before alpha = rho_new / h; 
4  before apply first correction to x 
4  before r = r - alpha*v 
4  before test stop criteria 
4 istlsolver::printOutput 
  0.5      1.24503e-31      8.59304e-17
4 === rate=7.38404e-33, T=0.121771, TIT=0.243542, IT=0.5
4 finished solveLinearSystemImpl_ 1
4 converged value: 1 7 1 let s go get convergedRemote
4 to  comm_.min(converged) 
4 did  comm_.min(converged) convergedRemote: 1
4 final convergedRemote: 1
4 Update: x^(k+1) = x^k - deltax^k 
4 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
4 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
4 Newton iteration  7 done4 , maximum relative shift = 3.221e-07
4 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
1  before v = A * y 
1  before  alpha = rho_new / < rt, v > 
1  before alpha = rho_new / h; 
1  before apply first correction to x 
1  before r = r - alpha*v 
1  before test stop criteria 
1 istlsolver::printOutput 
  0.5      1.24503e-31      8.59304e-17
1 === rate=7.38404e-33, T=0.12174, TIT=0.24348, IT=0.5
1 finished solveLinearSystemImpl_ 1
1 converged value: 1 7 1 let s go get convergedRemote
1 to  comm_.min(converged) 
1 did  comm_.min(converged) convergedRemote: 1
1 final convergedRemote: 1
1 Update: x^(k+1) = x^k - deltax^k 
1 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
1 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
1 Newton iteration  7 done1 , maximum relative shift = 3.221e-07
1 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
3 Solve: M deltax^k = r 
3 Newton::solveLinearSystem : entering the try section 7
3 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
3 solveLinearSystemImpl_, before converged = ls.solve 
3 7 amgbackend::solve isParallel 1
6 Solve: M deltax^k = r 
6 Newton::solveLinearSystem : entering the try section 7
6 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
6 solveLinearSystemImpl_, before converged = ls.solve 
6 7 amgbackend::solve isParallel 1
1 Solve: M deltax^k = r 
1 Newton::solveLinearSystem : entering the try section 7
1 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
1 solveLinearSystemImpl_, before converged = ls.solve 
1 7 amgbackend::solve isParallel 1
4 Solve: M deltax^k = r 
4 Newton::solveLinearSystem : entering the try section 7
4 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
4 solveLinearSystemImpl_, before converged = ls.solve 
4 7 amgbackend::solve isParallel 1
2 Solve: M deltax^k = r 
2 Newton::solveLinearSystem : entering the try section 7
2 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
2 solveLinearSystemImpl_, before converged = ls.solve 
2 7 amgbackend::solve isParallel 1
5 Solve: M deltax^k = r 
5 Newton::solveLinearSystem : entering the try section 7
5 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
5 solveLinearSystemImpl_, before converged = ls.solve 
5 7 amgbackend::solve isParallel 1
0 Solve: M deltax^k = r 
0 Newton::solveLinearSystem : entering the try section 7
0 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
0 solveLinearSystemImpl_, before converged = ls.solve 
0 7 amgbackend::solve isParallel 1
6 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
6 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
3 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
3 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
0 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
0 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 7
1 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
1 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
4 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
4 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
2 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
2 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
5 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
5 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 7
2 7 before Dune::BiCGSTABSolver 
2 Dune::BiCGSTABSolver.apply(x, b, result_) 
2 BiCGSTABSolver::apply 
3 7 before Dune::BiCGSTABSolver 
3 Dune::BiCGSTABSolver.apply(x, b, result_) 
3 BiCGSTABSolver::apply 
3 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
6 7 before Dune::BiCGSTABSolver 
6 Dune::BiCGSTABSolver.apply(x, b, result_) 
6 BiCGSTABSolver::apply 
6 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 7
5 7 before Dune::BiCGSTABSolver 
5 Dune::BiCGSTABSolver.apply(x, b, result_) 
5 BiCGSTABSolver::apply 
5 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
1 7 before Dune::BiCGSTABSolver 
1 Dune::BiCGSTABSolver.apply(x, b, result_) 
1 BiCGSTABSolver::apply 
1 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
4 7 before Dune::BiCGSTABSolver 
4 Dune::BiCGSTABSolver.apply(x, b, result_) 
4 BiCGSTABSolver::apply 
4 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
2 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
Using a direct coarse solver (SuperLU)
Building hierarchy of 1 levels (inclusive coarse solver) took 0.109752 seconds.
0 7 before Dune::BiCGSTABSolver 
0 Dune::BiCGSTABSolver.apply(x, b, result_) 
0 BiCGSTABSolver::apply 
0 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
0 === BiCGSTABSolver
 Iter          Defect            Rate
1 === BiCGSTABSolver
 Iter          Defect            Rate
    0      1.03928e-16
1  (it = 0.5; it < _maxit; it+=.5) 0.5 250
1  before p = r 
1  before  y = W^-1 * p 
1  before _prec->apply(y,p); 0.5 250
1 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
5 === BiCGSTABSolver
 Iter          Defect            Rate
    0      1.03928e-16
5  (it = 0.5; it < _maxit; it+=.5) 0.5 250
5  before p = r 
5  before  y = W^-1 * p 
5  before _prec->apply(y,p); 0.5 250
5 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
4 === BiCGSTABSolver
 Iter          Defect            Rate
    0      1.03928e-16
4  (it = 0.5; it < _maxit; it+=.5) 0.5 250
4  before p = r 
4  before  y = W^-1 * p 
4  before _prec->apply(y,p); 0.5 250
4 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
6 === BiCGSTABSolver
 Iter          Defect            Rate
    0      1.03928e-16
6  (it = 0.5; it < _maxit; it+=.5) 0.5 250
6  before p = r 
6  before  y = W^-1 * p 
6  before _prec->apply(y,p); 0.5 250
6 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
2 === BiCGSTABSolver
 Iter          Defect            Rate
    0      1.03928e-16
2  (it = 0.5; it < _maxit; it+=.5) 0.5 250
2  before p = r 
2  before  y = W^-1 * p 
2  before _prec->apply(y,p); 0.5 250
2 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
3 === BiCGSTABSolver
 Iter          Defect            Rate
    0      1.03928e-16
3  (it = 0.5; it < _maxit; it+=.5) 0.5 250
3  before p = r 
3  before  y = W^-1 * p 
3  before _prec->apply(y,p); 0.5 250
3 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
    0      1.03928e-16
0  (it = 0.5; it < _maxit; it+=.5) 0.5 250
0  before p = r 
0  before  y = W^-1 * p 
0  before _prec->apply(y,p); 0.5 250
0 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 7
0 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
3 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
3 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
3  before v = A * y 
1 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
5 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
5 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
5  before v = A * y 
4 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
4 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
4  before v = A * y 
4  before  alpha = rho_new / < rt, v > 
6 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
6 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
6  before v = A * y 
6  before  alpha = rho_new / < rt, v > 
2 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
2 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
0 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
1 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
5  before  alpha = rho_new / < rt, v > 
0 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
0  before v = A * y 
3  before  alpha = rho_new / < rt, v > 
3  before alpha = rho_new / h; 
3  before apply first correction to x 
3  before r = r - alpha*v 
3  before test stop criteria 
3 istlsolver::printOutput 
  0.5      4.13473e-34      3.97846e-18
3 === rate=1.58281e-35, T=0.120439, TIT=0.240878, IT=0.5
3 finished solveLinearSystemImpl_ 1
3 converged value: 1 7 1 let s go get convergedRemote
3 to  comm_.min(converged) 
2  before v = A * y 
2  before  alpha = rho_new / < rt, v > 
2  before alpha = rho_new / h; 
2  before apply first correction to x 
2  before r = r - alpha*v 
2  before test stop criteria 
2 istlsolver::printOutput 
  0.5      4.13473e-34      3.97846e-18
2 === rate=1.58281e-35, T=0.120408, TIT=0.240815, IT=0.5
2 finished solveLinearSystemImpl_ 1
2 converged value: 1 7 1 let s go get convergedRemote
2 to  comm_.min(converged) 
0  before  alpha = rho_new / < rt, v > 
0  before alpha = rho_new / h; 
0  before apply first correction to x 
0  before r = r - alpha*v 
0  before test stop criteria 
0 istlsolver::printOutput 
  0.5      4.13473e-34      3.97846e-18
0 === rate=1.58281e-35, T=0.0183635, TIT=0.0367271, IT=0.5
5  before alpha = rho_new / h; 
5  before apply first correction to x 
5  before r = r - alpha*v 
5  before test stop criteria 
5 istlsolver::printOutput 
  0.5      4.13473e-34      3.97846e-18
5 === rate=1.58281e-35, T=0.12044, TIT=0.240879, IT=0.5
5 finished solveLinearSystemImpl_ 1
5 converged value: 1 7 1 let s go get convergedRemote
5 to  comm_.min(converged) 
1  before v = A * y 
1  before  alpha = rho_new / < rt, v > 
1  before alpha = rho_new / h; 
1  before apply first correction to x 
1  before r = r - alpha*v 
1  before test stop criteria 
1 istlsolver::printOutput 
  0.5      4.13473e-34      3.97846e-18
1 === rate=1.58281e-35, T=0.120439, TIT=0.240879, IT=0.5
1 finished solveLinearSystemImpl_ 1
1 converged value: 1 7 1 let s go get convergedRemote
1 to  comm_.min(converged) 
4  before alpha = rho_new / h; 
4  before apply first correction to x 
4  before r = r - alpha*v 
4  before test stop criteria 
4 istlsolver::printOutput 
  0.5      4.13473e-34      3.97846e-18
4 === rate=1.58281e-35, T=0.120413, TIT=0.240827, IT=0.5
4 finished solveLinearSystemImpl_ 1
4 converged value: 1 7 1 let s go get convergedRemote
4 to  comm_.min(converged) 
6  before alpha = rho_new / h; 
6  before apply first correction to x 
6  before r = r - alpha*v 
6  before test stop criteria 
6 istlsolver::printOutput 
  0.5      4.13473e-34      3.97846e-18
6 === rate=1.58281e-35, T=0.120414, TIT=0.240829, IT=0.5
6 finished solveLinearSystemImpl_ 1
6 converged value: 1 7 1 let s go get convergedRemote
6 to  comm_.min(converged) 
2 did  comm_.min(converged) convergedRemote: 1
2 final convergedRemote: 1
2 Update: x^(k+1) = x^k - deltax^k 
2 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
2 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
2 Newton iteration  8 done2 , maximum relative shift = 2.309e-08
2 Assemble/solve/update time: 0.353157(25.1063%)/1.05334(74.8827%)/0.000155481(0.0110533%)
2 gridVariables->advanceTimeStep
2 before, nonLinearSolver->suggestTimeStepSize, current time: 590.099 simTime 57600
2 before assembler->setPreviousSolution, ddt: 7.60358
2 nonLinearSolver->solve
2 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
0 finished solveLinearSystemImpl_ 1
0 converged value: 1 7 1 let s go get convergedRemote
0 to  comm_.min(converged) 
0 did  comm_.min(converged) convergedRemote: 1
0 final convergedRemote: 1
0 Update: x^(k+1) = x^k - deltax^k 
0 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
0 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
0 Newton iteration  8 done0 , maximum relative shift = 2.309e-08
0 Assemble/solve/update time: 0.351894(25.0396%)/1.05328(74.9477%)/0.000178631(0.0127108%)
0 gridVariables->advanceTimeStep
0 before, nonLinearSolver->suggestTimeStepSize, current time: 590.099 simTime 57600
0 before assembler->setPreviousSolution, ddt: 7.60358
0 nonLinearSolver->solve
0 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
5 did  comm_.min(converged) convergedRemote: 1
5 final convergedRemote: 1
5 Update: x^(k+1) = x^k - deltax^k 
5 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
5 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
5 Newton iteration  8 done5 , maximum relative shift = 2.309e-08
5 Assemble/solve/update time: 0.353299(25.1144%)/1.05331(74.875%)/0.000149481(0.0106259%)
5 gridVariables->advanceTimeStep
5 before, nonLinearSolver->suggestTimeStepSize, current time: 590.099 simTime 57600
5 before assembler->setPreviousSolution, ddt: 7.60358
5 nonLinearSolver->solve
5 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
1 did  comm_.min(converged) convergedRemote: 1
1 final convergedRemote: 1
1 Update: x^(k+1) = x^k - deltax^k 
1 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
1 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
1 Newton iteration  8 done1 , maximum relative shift = 2.309e-08
1 Assemble/solve/update time: 0.353288(25.1138%)/1.05331(74.8756%)/0.000148611(0.0105642%)
1 gridVariables->advanceTimeStep
1 before, nonLinearSolver->suggestTimeStepSize, current time: 590.099 simTime 57600
1 before assembler->setPreviousSolution, ddt: 7.60358
1 nonLinearSolver->solve
1 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
4 did  comm_.min(converged) convergedRemote: 1
4 final convergedRemote: 1
4 Update: x^(k+1) = x^k - deltax^k 
4 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
4 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
4 Newton iteration  8 done4 , maximum relative shift = 2.309e-08
4 Assemble/solve/update time: 0.3532(25.1092%)/1.0533(74.8801%)/0.000150451(0.0106957%)
4 gridVariables->advanceTimeStep
4 before, nonLinearSolver->suggestTimeStepSize, current time: 590.099 simTime 57600
4 before assembler->setPreviousSolution, ddt: 7.60358
4 nonLinearSolver->solve
4 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
6 did  comm_.min(converged) convergedRemote: 1
6 final convergedRemote: 1
6 Update: x^(k+1) = x^k - deltax^k 
6 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
6 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
6 Newton iteration  8 done6 , maximum relative shift = 2.309e-08
6 Assemble/solve/update time: 0.353198(25.1093%)/1.05333(74.8824%)/0.000117131(0.00832698%)
6 gridVariables->advanceTimeStep
6 before, nonLinearSolver->suggestTimeStepSize, current time: 590.099 simTime 57600
6 before assembler->setPreviousSolution, ddt: 7.60358
6 nonLinearSolver->solve
6 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
3 did  comm_.min(converged) convergedRemote: 1
3 final convergedRemote: 1
3 Update: x^(k+1) = x^k - deltax^k 
3 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
3 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
3 Newton iteration  8 done3 , maximum relative shift = 2.309e-08
3 Assemble/solve/update time: 0.353259(25.1118%)/1.05334(74.8775%)/0.000151421(0.0107639%)
3 gridVariables->advanceTimeStep
3 before, nonLinearSolver->suggestTimeStepSize, current time: 590.099 simTime 57600
3 before assembler->setPreviousSolution, ddt: 7.60358
3 nonLinearSolver->solve
3 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
3 Solve: M deltax^k = r 
3 Newton::solveLinearSystem : entering the try section 0
3to  comm_.sum(norm2) 
3did  comm_.sum(norm2) 
3 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
3 solveLinearSystemImpl_, before converged = ls.solve 
3 7 amgbackend::solve isParallel 1
6 Solve: M deltax^k = r 
6 Newton::solveLinearSystem : entering the try section 0
6to  comm_.sum(norm2) 
6did  comm_.sum(norm2) 
6 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
6 solveLinearSystemImpl_, before converged = ls.solve 
6 7 amgbackend::solve isParallel 1
4 Solve: M deltax^k = r 
4 Newton::solveLinearSystem : entering the try section 0
4to  comm_.sum(norm2) 
4did  comm_.sum(norm2) 
4 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
4 solveLinearSystemImpl_, before converged = ls.solve 
4 7 amgbackend::solve isParallel 1
2 Solve: M deltax^k = r 
2 Newton::solveLinearSystem : entering the try section 0
2to  comm_.sum(norm2) 
2did  comm_.sum(norm2) 
2 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
2 solveLinearSystemImpl_, before converged = ls.solve 
2 7 amgbackend::solve isParallel 1
0 Solve: M deltax^k = r 
0 Newton::solveLinearSystem : entering the try section 0
0to  comm_.sum(norm2) 
0did  comm_.sum(norm2) 
0 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
0 solveLinearSystemImpl_, before converged = ls.solve 
0 7 amgbackend::solve isParallel 1
5 Solve: M deltax^k = r 
5 Newton::solveLinearSystem : entering the try section 0
5to  comm_.sum(norm2) 
5did  comm_.sum(norm2) 
5 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
5 solveLinearSystemImpl_, before converged = ls.solve 
5 7 amgbackend::solve isParallel 1
1 Solve: M deltax^k = r 
1 Newton::solveLinearSystem : entering the try section 0
1to  comm_.sum(norm2) 
1did  comm_.sum(norm2) 
1 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
1 solveLinearSystemImpl_, before converged = ls.solve 
1 7 amgbackend::solve isParallel 1
5 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
5 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
1 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
1 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
3 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
3 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
6 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
6 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
4 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
4 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
2 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
2 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
0 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
0 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 7
1 7 before Dune::BiCGSTABSolver 
1 Dune::BiCGSTABSolver.apply(x, b, result_) 
1 BiCGSTABSolver::apply 
1 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
3 7 before Dune::BiCGSTABSolver 
3 Dune::BiCGSTABSolver.apply(x, b, result_) 
3 BiCGSTABSolver::apply 
3 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
6 7 before Dune::BiCGSTABSolver 
6 Dune::BiCGSTABSolver.apply(x, b, result_) 
6 BiCGSTABSolver::apply 
6 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
4 7 before Dune::BiCGSTABSolver 
4 Dune::BiCGSTABSolver.apply(x, b, result_) 
4 BiCGSTABSolver::apply 
4 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
2 7 before Dune::BiCGSTABSolver 
2 Dune::BiCGSTABSolver.apply(x, b, result_) 
2 BiCGSTABSolver::apply 
5 7 before Dune::BiCGSTABSolver 
5 Dune::BiCGSTABSolver.apply(x, b, result_) 
5 BiCGSTABSolver::apply 
5 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 7
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
2 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
Using a direct coarse solver (SuperLU)
Building hierarchy of 1 levels (inclusive coarse solver) took 0.110843 seconds.
0 7 before Dune::BiCGSTABSolver 
0 Dune::BiCGSTABSolver.apply(x, b, result_) 
0 BiCGSTABSolver::apply 
0 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
0 === BiCGSTABSolver
 Iter          Defect            Rate
    0      3.16253e-09
0  (it = 0.5; it < _maxit; it+=.5) 0.5 250
1 === BiCGSTABSolver
 Iter          Defect            Rate
    0      3.16253e-09
1  (it = 0.5; it < _maxit; it+=.5) 0.5 250
1  before p = r 
1  before  y = W^-1 * p 
1  before _prec->apply(y,p); 0.5 250
1 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
4 === BiCGSTABSolver
 Iter          Defect            Rate
    0      3.16253e-09
4  (it = 0.5; it < _maxit; it+=.5) 0.5 250
4  before p = r 
4  before  y = W^-1 * p 
4  before _prec->apply(y,p); 0.5 250
4 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
0  before p = r 
0  before  y = W^-1 * p 
0  before _prec->apply(y,p); 0.5 250
0 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 7
0 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
5 === BiCGSTABSolver
 Iter          Defect            Rate
    0      3.16253e-09
5  (it = 0.5; it < _maxit; it+=.5) 0.5 250
5  before p = r 
5  before  y = W^-1 * p 
5  before _prec->apply(y,p); 0.5 250
5 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
2 === BiCGSTABSolver
 Iter          Defect            Rate
    0      3.16253e-09
2  (it = 0.5; it < _maxit; it+=.5) 0.5 250
2  before p = r 
2  before  y = W^-1 * p 
2  before _prec->apply(y,p); 0.5 250
2 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
3 === BiCGSTABSolver
 Iter          Defect            Rate
    0      3.16253e-09
3  (it = 0.5; it < _maxit; it+=.5) 0.5 250
3  before p = r 
3  before  y = W^-1 * p 
3  before _prec->apply(y,p); 0.5 250
3 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
6 === BiCGSTABSolver
 Iter          Defect            Rate
    0      3.16253e-09
6  (it = 0.5; it < _maxit; it+=.5) 0.5 250
6  before p = r 
6  before  y = W^-1 * p 
6  before _prec->apply(y,p); 0.5 250
6 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
3 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
3 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
3  before v = A * y 
4 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
4 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
4  before v = A * y 
1 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
6 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
6 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
6  before v = A * y 
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
0 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
5 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
5 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
5  before v = A * y 
2 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
2 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
2  before v = A * y 
6  before  alpha = rho_new / < rt, v > 
0 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
0  before v = A * y 
5  before  alpha = rho_new / < rt, v > 
1 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
3  before  alpha = rho_new / < rt, v > 
4  before  alpha = rho_new / < rt, v > 
5  before alpha = rho_new / h; 
5  before apply first correction to x 
5  before r = r - alpha*v 
5  before test stop criteria 
5 istlsolver::printOutput 
  0.5       5.1593e-25      1.63138e-16
5 === rate=2.66141e-32, T=0.121337, TIT=0.242673, IT=0.5
5 finished solveLinearSystemImpl_ 1
5 converged value: 1 7 1 let s go get convergedRemote
5 to  comm_.min(converged) 
1  before v = A * y 
1  before  alpha = rho_new / < rt, v > 
1  before alpha = rho_new / h; 
1  before apply first correction to x 
1  before r = r - alpha*v 
1  before test stop criteria 
1 istlsolver::printOutput 
  0.5       5.1593e-25      1.63138e-16
1 === rate=2.66141e-32, T=0.12134, TIT=0.242681, IT=0.5
1 finished solveLinearSystemImpl_ 1
1 converged value: 1 7 1 let s go get convergedRemote
1 to  comm_.min(converged) 
3  before alpha = rho_new / h; 
3  before apply first correction to x 
3  before r = r - alpha*v 
3  before test stop criteria 
3 istlsolver::printOutput 
  0.5       5.1593e-25      1.63138e-16
3 === rate=2.66141e-32, T=0.121337, TIT=0.242675, IT=0.5
3 finished solveLinearSystemImpl_ 1
3 converged value: 1 7 1 let s go get convergedRemote
3 to  comm_.min(converged) 
4  before alpha = rho_new / h; 
4  before apply first correction to x 
4  before r = r - alpha*v 
4  before test stop criteria 
4 istlsolver::printOutput 
  0.5       5.1593e-25      1.63138e-16
4 === rate=2.66141e-32, T=0.121308, TIT=0.242616, IT=0.5
4 finished solveLinearSystemImpl_ 1
4 converged value: 1 7 1 let s go get convergedRemote
4 to  comm_.min(converged) 
2  before  alpha = rho_new / < rt, v > 
2  before alpha = rho_new / h; 
2  before apply first correction to x 
2  before r = r - alpha*v 
2  before test stop criteria 
2 istlsolver::printOutput 
  0.5       5.1593e-25      1.63138e-16
2 === rate=2.66141e-32, T=0.121292, TIT=0.242584, IT=0.5
2 finished solveLinearSystemImpl_ 1
2 converged value: 1 7 1 let s go get convergedRemote
2 to  comm_.min(converged) 
6  before alpha = rho_new / h; 
6  before apply first correction to x 
6  before r = r - alpha*v 
6  before test stop criteria 
6 istlsolver::printOutput 
  0.5       5.1593e-25      1.63138e-16
6 === rate=2.66141e-32, T=0.121309, TIT=0.242618, IT=0.5
6 finished solveLinearSystemImpl_ 1
6 converged value: 1 7 1 let s go get convergedRemote
6 to  comm_.min(converged) 
0  before  alpha = rho_new / < rt, v > 
0  before alpha = rho_new / h; 
0  before apply first correction to x 
0  before r = r - alpha*v 
0  before test stop criteria 
0 istlsolver::printOutput 
  0.5       5.1593e-25      1.63138e-16
0 === rate=2.66141e-32, T=0.0183742, TIT=0.0367483, IT=0.5
6 did  comm_.min(converged) convergedRemote: 1
6 final convergedRemote: 1
6 Update: x^(k+1) = x^k - deltax^k 
6 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
6 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
6 Newton iteration  1 done6 , maximum relative shift = 6.551e-02
6 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
3 did  comm_.min(converged) convergedRemote: 1
3 final convergedRemote: 1
3 Update: x^(k+1) = x^k - deltax^k 
3 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
3 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
3 Newton iteration  1 done3 , maximum relative shift = 6.551e-02
3 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
5 did  comm_.min(converged) convergedRemote: 1
5 final convergedRemote: 1
5 Update: x^(k+1) = x^k - deltax^k 
5 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
5 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
5 Newton iteration  1 done5 , maximum relative shift = 6.551e-02
5 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
1 did  comm_.min(converged) convergedRemote: 1
1 final convergedRemote: 1
1 Update: x^(k+1) = x^k - deltax^k 
1 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
1 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
1 Newton iteration  1 done1 , maximum relative shift = 6.551e-02
1 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
0 finished solveLinearSystemImpl_ 1
0 converged value: 1 7 1 let s go get convergedRemote
0 to  comm_.min(converged) 
0 did  comm_.min(converged) convergedRemote: 1
0 final convergedRemote: 1
0 Update: x^(k+1) = x^k - deltax^k 
0 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
0 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
0 Newton iteration  1 done0 , maximum relative shift = 6.551e-02
0 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
4 did  comm_.min(converged) convergedRemote: 1
4 final convergedRemote: 1
4 Update: x^(k+1) = x^k - deltax^k 
4 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
4 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
4 Newton iteration  1 done4 , maximum relative shift = 6.551e-02
4 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
2 did  comm_.min(converged) convergedRemote: 1
2 final convergedRemote: 1
2 Update: x^(k+1) = x^k - deltax^k 
2 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
2 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
2 Newton iteration  1 done2 , maximum relative shift = 6.551e-02
2 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
6 Solve: M deltax^k = r 
6 Newton::solveLinearSystem : entering the try section 1
6 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
6 solveLinearSystemImpl_, before converged = ls.solve 
6 7 amgbackend::solve isParallel 1
3 Solve: M deltax^k = r 
3 Newton::solveLinearSystem : entering the try section 1
3 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
3 solveLinearSystemImpl_, before converged = ls.solve 
3 7 amgbackend::solve isParallel 1
5 Solve: M deltax^k = r 
5 Newton::solveLinearSystem : entering the try section 1
5 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
5 solveLinearSystemImpl_, before converged = ls.solve 
5 7 amgbackend::solve isParallel 1
2 Solve: M deltax^k = r 
2 Newton::solveLinearSystem : entering the try section 1
2 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
2 solveLinearSystemImpl_, before converged = ls.solve 
2 7 amgbackend::solve isParallel 1
0 Solve: M deltax^k = r 
0 Newton::solveLinearSystem : entering the try section 1
0 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
0 solveLinearSystemImpl_, before converged = ls.solve 
0 7 amgbackend::solve isParallel 1
4 Solve: M deltax^k = r 
4 Newton::solveLinearSystem : entering the try section 1
4 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
4 solveLinearSystemImpl_, before converged = ls.solve 
4 7 amgbackend::solve isParallel 1
1 Solve: M deltax^k = r 
1 Newton::solveLinearSystem : entering the try section 1
1 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
1 solveLinearSystemImpl_, before converged = ls.solve 
1 7 amgbackend::solve isParallel 1
1 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
1 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
2 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
2 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
0 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
0 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 7
4 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
4 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
5 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
5 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
6 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
6 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
3 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
3 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 7
5 7 before Dune::BiCGSTABSolver 
5 Dune::BiCGSTABSolver.apply(x, b, result_) 
5 BiCGSTABSolver::apply 
5 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
6 7 before Dune::BiCGSTABSolver 
6 Dune::BiCGSTABSolver.apply(x, b, result_) 
6 BiCGSTABSolver::apply 
6 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
1 7 before Dune::BiCGSTABSolver 
1 Dune::BiCGSTABSolver.apply(x, b, result_) 
1 BiCGSTABSolver::apply 
1 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
2 7 before Dune::BiCGSTABSolver 
2 Dune::BiCGSTABSolver.apply(x, b, result_) 
2 BiCGSTABSolver::apply 
2 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
3 7 before Dune::BiCGSTABSolver 
3 Dune::BiCGSTABSolver.apply(x, b, result_) 
3 BiCGSTABSolver::apply 
3 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
4 7 before Dune::BiCGSTABSolver 
4 Dune::BiCGSTABSolver.apply(x, b, result_) 
4 BiCGSTABSolver::apply 
4 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
Using a direct coarse solver (SuperLU)
Building hierarchy of 1 levels (inclusive coarse solver) took 0.109587 seconds.
0 7 before Dune::BiCGSTABSolver 
0 Dune::BiCGSTABSolver.apply(x, b, result_) 
0 BiCGSTABSolver::apply 
0 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
0 === BiCGSTABSolver
 Iter          Defect            Rate
    0      6.87832e-11
0  (it = 0.5; it < _maxit; it+=.5) 0.5 250
0  before p = r 
0  before  y = W^-1 * p 
0  before _prec->apply(y,p); 0.5 250
0 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 7
0 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
6 === BiCGSTABSolver
 Iter          Defect            Rate
    0      6.87832e-11
6  (it = 0.5; it < _maxit; it+=.5) 0.5 250
6  before p = r 
6  before  y = W^-1 * p 
6  before _prec->apply(y,p); 0.5 250
6 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
1 === BiCGSTABSolver
 Iter          Defect            Rate
    0      6.87832e-11
1  (it = 0.5; it < _maxit; it+=.5) 0.5 250
1  before p = r 
1  before  y = W^-1 * p 
1  before _prec->apply(y,p); 0.5 250
1 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
2 === BiCGSTABSolver
 Iter          Defect            Rate
    0      6.87832e-11
2  (it = 0.5; it < _maxit; it+=.5) 0.5 250
2  before p = r 
2  before  y = W^-1 * p 
2  before _prec->apply(y,p); 0.5 250
2 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
3 === BiCGSTABSolver
 Iter          Defect            Rate
    0      6.87832e-11
3  (it = 0.5; it < _maxit; it+=.5) 0.5 250
3  before p = r 
3  before  y = W^-1 * p 
3  before _prec->apply(y,p); 0.5 250
3 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
4 === BiCGSTABSolver
 Iter          Defect            Rate
    0      6.87832e-11
4  (it = 0.5; it < _maxit; it+=.5) 0.5 250
4  before p = r 
4  before  y = W^-1 * p 
4  before _prec->apply(y,p); 0.5 250
4 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
5 === BiCGSTABSolver
 Iter          Defect            Rate
    0      6.87832e-11
5  (it = 0.5; it < _maxit; it+=.5) 0.5 250
5  before p = r 
5  before  y = W^-1 * p 
5  before _prec->apply(y,p); 0.5 250
5 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
1 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
2 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
2 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
3 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
3 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
3  before v = A * y 
4 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
4 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
4  before v = A * y 
5 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
5 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
5  before v = A * y 
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
0 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
6 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
6 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
6  before v = A * y 
6  before  alpha = rho_new / < rt, v > 
5  before  alpha = rho_new / < rt, v > 
0 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
0  before v = A * y 
4  before  alpha = rho_new / < rt, v > 
1 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
2  before v = A * y 
3  before  alpha = rho_new / < rt, v > 
5  before alpha = rho_new / h; 
5  before apply first correction to x 
5  before r = r - alpha*v 
5  before test stop criteria 
5 istlsolver::printOutput 
  0.5      5.89647e-27      8.57254e-17
5 === rate=7.34885e-33, T=0.120199, TIT=0.240398, IT=0.5
5 finished solveLinearSystemImpl_ 1
5 converged value: 1 7 1 let s go get convergedRemote
5 to  comm_.min(converged) 
0  before  alpha = rho_new / < rt, v > 
0  before alpha = rho_new / h; 
0  before apply first correction to x 
0  before r = r - alpha*v 
0  before test stop criteria 
0 istlsolver::printOutput 
  0.5      5.89647e-27      8.57254e-17
0 === rate=7.34885e-33, T=0.0183736, TIT=0.0367472, IT=0.5
4  before alpha = rho_new / h; 
4  before apply first correction to x 
4  before r = r - alpha*v 
4  before test stop criteria 
4 istlsolver::printOutput 
  0.5      5.89647e-27      8.57254e-17
4 === rate=7.34885e-33, T=0.120144, TIT=0.240288, IT=0.5
4 finished solveLinearSystemImpl_ 1
4 converged value: 1 7 1 let s go get convergedRemote
4 to  comm_.min(converged) 
1  before v = A * y 
1  before  alpha = rho_new / < rt, v > 
1  before alpha = rho_new / h; 
1  before apply first correction to x 
1  before r = r - alpha*v 
1  before test stop criteria 
1 istlsolver::printOutput 
  0.5      5.89647e-27      8.57254e-17
1 === rate=7.34885e-33, T=0.120201, TIT=0.240401, IT=0.5
1 finished solveLinearSystemImpl_ 1
1 converged value: 1 7 1 let s go get convergedRemote
1 to  comm_.min(converged) 
2  before  alpha = rho_new / < rt, v > 
2  before alpha = rho_new / h; 
2  before apply first correction to x 
2  before r = r - alpha*v 
2  before test stop criteria 
2 istlsolver::printOutput 
  0.5      5.89647e-27      8.57254e-17
2 === rate=7.34885e-33, T=0.120143, TIT=0.240286, IT=0.5
2 finished solveLinearSystemImpl_ 1
2 converged value: 1 7 1 let s go get convergedRemote
2 to  comm_.min(converged) 
3  before alpha = rho_new / h; 
3  before apply first correction to x 
3  before r = r - alpha*v 
3  before test stop criteria 
3 istlsolver::printOutput 
  0.5      5.89647e-27      8.57254e-17
3 === rate=7.34885e-33, T=0.120198, TIT=0.240395, IT=0.5
3 finished solveLinearSystemImpl_ 1
3 converged value: 1 7 1 let s go get convergedRemote
3 to  comm_.min(converged) 
6  before alpha = rho_new / h; 
6  before apply first correction to x 
6  before r = r - alpha*v 
6  before test stop criteria 
6 istlsolver::printOutput 
  0.5      5.89647e-27      8.57254e-17
6 === rate=7.34885e-33, T=0.120144, TIT=0.240288, IT=0.5
6 finished solveLinearSystemImpl_ 1
6 converged value: 1 7 1 let s go get convergedRemote
6 to  comm_.min(converged) 
4 did  comm_.min(converged) convergedRemote: 1
4 final convergedRemote: 1
4 Update: x^(k+1) = x^k - deltax^k 
4 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
4 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
4 Newton iteration  2 done4 , maximum relative shift = 1.281e-02
4 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
1 did  comm_.min(converged) convergedRemote: 1
1 final convergedRemote: 1
1 Update: x^(k+1) = x^k - deltax^k 
1 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
1 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
1 Newton iteration  2 done1 , maximum relative shift = 1.281e-02
1 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
6 did  comm_.min(converged) convergedRemote: 1
6 final convergedRemote: 1
6 Update: x^(k+1) = x^k - deltax^k 
6 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
6 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
6 Newton iteration  2 done6 , maximum relative shift = 1.281e-02
6 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
3 did  comm_.min(converged) convergedRemote: 1
3 final convergedRemote: 1
3 Update: x^(k+1) = x^k - deltax^k 
3 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
3 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
3 Newton iteration  2 done3 , maximum relative shift = 1.281e-02
3 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
2 did  comm_.min(converged) convergedRemote: 1
2 final convergedRemote: 1
2 Update: x^(k+1) = x^k - deltax^k 
2 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
2 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
2 Newton iteration  2 done2 , maximum relative shift = 1.281e-02
2 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
5 did  comm_.min(converged) convergedRemote: 1
5 final convergedRemote: 1
5 Update: x^(k+1) = x^k - deltax^k 
5 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
5 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
5 Newton iteration  2 done5 , maximum relative shift = 1.281e-02
5 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
0 finished solveLinearSystemImpl_ 1
0 converged value: 1 7 1 let s go get convergedRemote
0 to  comm_.min(converged) 
0 did  comm_.min(converged) convergedRemote: 1
0 final convergedRemote: 1
0 Update: x^(k+1) = x^k - deltax^k 
0 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
0 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
0 Newton iteration  2 done0 , maximum relative shift = 1.281e-02
0 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
6 Solve: M deltax^k = r 
6 Newton::solveLinearSystem : entering the try section 2
6 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
6 solveLinearSystemImpl_, before converged = ls.solve 
6 7 amgbackend::solve isParallel 1
3 Solve: M deltax^k = r 
3 Newton::solveLinearSystem : entering the try section 2
3 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
3 solveLinearSystemImpl_, before converged = ls.solve 
3 7 amgbackend::solve isParallel 1
2 Solve: M deltax^k = r 
2 Newton::solveLinearSystem : entering the try section 2
2 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
2 solveLinearSystemImpl_, before converged = ls.solve 
2 7 amgbackend::solve isParallel 1
5 Solve: M deltax^k = r 
5 Newton::solveLinearSystem : entering the try section 2
5 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
5 solveLinearSystemImpl_, before converged = ls.solve 
5 7 amgbackend::solve isParallel 1
1 Solve: M deltax^k = r 
1 Newton::solveLinearSystem : entering the try section 2
1 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
1 solveLinearSystemImpl_, before converged = ls.solve 
1 7 amgbackend::solve isParallel 1
4 Solve: M deltax^k = r 
4 Newton::solveLinearSystem : entering the try section 2
4 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
4 solveLinearSystemImpl_, before converged = ls.solve 
4 7 amgbackend::solve isParallel 1
0 Solve: M deltax^k = r 
0 Newton::solveLinearSystem : entering the try section 2
0 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
0 solveLinearSystemImpl_, before converged = ls.solve 
0 7 amgbackend::solve isParallel 1
2 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
2 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
5 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
5 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
1 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
1 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
0 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
0 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 7
4 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
4 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
6 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
6 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
3 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
3 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 7
3 7 before Dune::BiCGSTABSolver 
3 Dune::BiCGSTABSolver.apply(x, b, result_) 
3 BiCGSTABSolver::apply 
3 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
4 7 before Dune::BiCGSTABSolver 
4 Dune::BiCGSTABSolver.apply(x, b, result_) 
4 BiCGSTABSolver::apply 
4 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
6 7 before Dune::BiCGSTABSolver 
6 Dune::BiCGSTABSolver.apply(x, b, result_) 
6 BiCGSTABSolver::apply 
6 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
1 7 before Dune::BiCGSTABSolver 
1 Dune::BiCGSTABSolver.apply(x, b, result_) 
1 BiCGSTABSolver::apply 
1 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
2 7 before Dune::BiCGSTABSolver 
2 Dune::BiCGSTABSolver.apply(x, b, result_) 
2 BiCGSTABSolver::apply 
2 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
5 7 before Dune::BiCGSTABSolver 
5 Dune::BiCGSTABSolver.apply(x, b, result_) 
5 BiCGSTABSolver::apply 
5 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 7
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
Using a direct coarse solver (SuperLU)
Building hierarchy of 1 levels (inclusive coarse solver) took 0.111521 seconds.
0 7 before Dune::BiCGSTABSolver 
0 Dune::BiCGSTABSolver.apply(x, b, result_) 
0 BiCGSTABSolver::apply 
0 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
4 === BiCGSTABSolver
 Iter          Defect            Rate
    0      1.14006e-11
4  (it = 0.5; it < _maxit; it+=.5) 0.5 250
4  before p = r 
4  before  y = W^-1 * p 
4  before _prec->apply(y,p); 0.5 250
4 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
2 === BiCGSTABSolver
 Iter          Defect            Rate
    0      1.14006e-11
2  (it = 0.5; it < _maxit; it+=.5) 0.5 250
2  before p = r 
2  before  y = W^-1 * p 
2  before _prec->apply(y,p); 0.5 250
2 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
0 === BiCGSTABSolver
 Iter          Defect            Rate
    0      1.14006e-11
0  (it = 0.5; it < _maxit; it+=.5) 0.5 250
0  before p = r 
0  before  y = W^-1 * p 
0  before _prec->apply(y,p); 0.5 250
0 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 7
0 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
1 === BiCGSTABSolver
 Iter          Defect            Rate
    0      1.14006e-11
1  (it = 0.5; it < _maxit; it+=.5) 0.5 250
1  before p = r 
1  before  y = W^-1 * p 
1  before _prec->apply(y,p); 0.5 250
1 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
5 === BiCGSTABSolver
 Iter          Defect            Rate
    0      1.14006e-11
5  (it = 0.5; it < _maxit; it+=.5) 0.5 250
5  before p = r 
5  before  y = W^-1 * p 
5  before _prec->apply(y,p); 0.5 250
5 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
3 === BiCGSTABSolver
 Iter          Defect            Rate
    0      1.14006e-11
3  (it = 0.5; it < _maxit; it+=.5) 0.5 250
3  before p = r 
3  before  y = W^-1 * p 
3  before _prec->apply(y,p); 0.5 250
3 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
6 === BiCGSTABSolver
 Iter          Defect            Rate
    0      1.14006e-11
6  (it = 0.5; it < _maxit; it+=.5) 0.5 250
6  before p = r 
6  before  y = W^-1 * p 
6  before _prec->apply(y,p); 0.5 250
6 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
2 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
2 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
4 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
4 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
4  before v = A * y 
6 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
6 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
6  before v = A * y 
6  before  alpha = rho_new / < rt, v > 
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
0 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
1 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
5 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
5 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
3 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
3 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
3  before v = A * y 
4  before  alpha = rho_new / < rt, v > 
2  before v = A * y 
2  before  alpha = rho_new / < rt, v > 
3  before  alpha = rho_new / < rt, v > 
0 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
0  before v = A * y 
0  before  alpha = rho_new / < rt, v > 
1 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
1  before v = A * y 
1  before  alpha = rho_new / < rt, v > 
5  before v = A * y 
5  before  alpha = rho_new / < rt, v > 
6  before alpha = rho_new / h; 
6  before apply first correction to x 
6  before r = r - alpha*v 
6  before test stop criteria 
6 istlsolver::printOutput 
  0.5      9.21345e-28      8.08154e-17
6 === rate=6.53112e-33, T=0.121987, TIT=0.243974, IT=0.5
6 finished solveLinearSystemImpl_ 1
6 converged value: 1 7 1 let s go get convergedRemote
6 to  comm_.min(converged) 
3  before alpha = rho_new / h; 
3  before apply first correction to x 
3  before r = r - alpha*v 
3  before test stop criteria 
3 istlsolver::printOutput 
  0.5      9.21345e-28      8.08154e-17
3 === rate=6.53112e-33, T=0.121938, TIT=0.243875, IT=0.5
3 finished solveLinearSystemImpl_ 1
3 converged value: 1 7 1 let s go get convergedRemote
3 to  comm_.min(converged) 
0  before alpha = rho_new / h; 
0  before apply first correction to x 
0  before r = r - alpha*v 
0  before test stop criteria 
0 istlsolver::printOutput 
  0.5      9.21345e-28      8.08154e-17
0 === rate=6.53112e-33, T=0.0185206, TIT=0.0370413, IT=0.5
1  before alpha = rho_new / h; 
1  before apply first correction to x 
1  before r = r - alpha*v 
1  before test stop criteria 
1 istlsolver::printOutput 
  0.5      9.21345e-28      8.08154e-17
1 === rate=6.53112e-33, T=0.121938, TIT=0.243876, IT=0.5
1 finished solveLinearSystemImpl_ 1
1 converged value: 1 7 1 let s go get convergedRemote
1 to  comm_.min(converged) 
2  before alpha = rho_new / h; 
2  before apply first correction to x 
2  before r = r - alpha*v 
2  before test stop criteria 
2 istlsolver::printOutput 
  0.5      9.21345e-28      8.08154e-17
2 === rate=6.53112e-33, T=0.121984, TIT=0.243968, IT=0.5
2 finished solveLinearSystemImpl_ 1
2 converged value: 1 7 1 let s go get convergedRemote
2 to  comm_.min(converged) 
4  before alpha = rho_new / h; 
4  before apply first correction to x 
4  before r = r - alpha*v 
4  before test stop criteria 
4 istlsolver::printOutput 
  0.5      9.21345e-28      8.08154e-17
4 === rate=6.53112e-33, T=0.121985, TIT=0.243971, IT=0.5
4 finished solveLinearSystemImpl_ 1
4 converged value: 1 7 1 let s go get convergedRemote
4 to  comm_.min(converged) 
5  before alpha = rho_new / h; 
5  before apply first correction to x 
5  before r = r - alpha*v 
5  before test stop criteria 
5 istlsolver::printOutput 
  0.5      9.21345e-28      8.08154e-17
5 === rate=6.53112e-33, T=0.121937, TIT=0.243874, IT=0.5
5 finished solveLinearSystemImpl_ 1
5 converged value: 1 7 1 let s go get convergedRemote
5 to  comm_.min(converged) 
5 did  comm_.min(converged) convergedRemote: 1
5 final convergedRemote: 1
5 Update: x^(k+1) = x^k - deltax^k 
5 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
5 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
5 Newton iteration  3 done5 , maximum relative shift = 2.137e-03
5 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
4 did  comm_.min(converged) convergedRemote: 1
4 final convergedRemote: 1
4 Update: x^(k+1) = x^k - deltax^k 
4 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
4 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
4 Newton iteration  3 done4 , maximum relative shift = 2.137e-03
4 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
6 did  comm_.min(converged) convergedRemote: 1
6 final convergedRemote: 1
6 Update: x^(k+1) = x^k - deltax^k 
6 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
6 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
6 Newton iteration  3 done6 , maximum relative shift = 2.137e-03
6 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
3 did  comm_.min(converged) convergedRemote: 1
3 final convergedRemote: 1
3 Update: x^(k+1) = x^k - deltax^k 
3 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
3 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
3 Newton iteration  3 done3 , maximum relative shift = 2.137e-03
3 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
0 finished solveLinearSystemImpl_ 1
0 converged value: 1 7 1 let s go get convergedRemote
0 to  comm_.min(converged) 
0 did  comm_.min(converged) convergedRemote: 1
0 final convergedRemote: 1
0 Update: x^(k+1) = x^k - deltax^k 
0 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
0 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
0 Newton iteration  3 done0 , maximum relative shift = 2.137e-03
0 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
1 did  comm_.min(converged) convergedRemote: 1
1 final convergedRemote: 1
1 Update: x^(k+1) = x^k - deltax^k 
1 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
1 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
1 Newton iteration  3 done1 , maximum relative shift = 2.137e-03
1 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
2 did  comm_.min(converged) convergedRemote: 1
2 final convergedRemote: 1
2 Update: x^(k+1) = x^k - deltax^k 
2 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
2 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
2 Newton iteration  3 done2 , maximum relative shift = 2.137e-03
2 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
2 Solve: M deltax^k = r 
2 Newton::solveLinearSystem : entering the try section 3
2 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
2 solveLinearSystemImpl_, before converged = ls.solve 
2 7 amgbackend::solve isParallel 1
5 Solve: M deltax^k = r 
5 Newton::solveLinearSystem : entering the try section 3
5 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
5 solveLinearSystemImpl_, before converged = ls.solve 
5 7 amgbackend::solve isParallel 1
4 Solve: M deltax^k = r 
4 Newton::solveLinearSystem : entering the try section 3
4 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
4 solveLinearSystemImpl_, before converged = ls.solve 
4 7 amgbackend::solve isParallel 1
6 Solve: M deltax^k = r 
6 Newton::solveLinearSystem : entering the try section 3
6 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
6 solveLinearSystemImpl_, before converged = ls.solve 
6 7 amgbackend::solve isParallel 1
3 Solve: M deltax^k = r 
3 Newton::solveLinearSystem : entering the try section 3
3 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
3 solveLinearSystemImpl_, before converged = ls.solve 
3 7 amgbackend::solve isParallel 1
0 Solve: M deltax^k = r 
0 Newton::solveLinearSystem : entering the try section 3
0 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
0 solveLinearSystemImpl_, before converged = ls.solve 
0 7 amgbackend::solve isParallel 1
1 Solve: M deltax^k = r 
1 Newton::solveLinearSystem : entering the try section 3
1 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
1 solveLinearSystemImpl_, before converged = ls.solve 
1 7 amgbackend::solve isParallel 1
4 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
4 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
6 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
6 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
3 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
3 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
0 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
0 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 7
1 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
1 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
2 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
2 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
5 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
5 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 7
1 7 before Dune::BiCGSTABSolver 
1 Dune::BiCGSTABSolver.apply(x, b, result_) 
1 BiCGSTABSolver::apply 
1 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
2 7 before Dune::BiCGSTABSolver 
2 Dune::BiCGSTABSolver.apply(x, b, result_) 
2 BiCGSTABSolver::apply 
2 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
5 7 before Dune::BiCGSTABSolver 
5 Dune::BiCGSTABSolver.apply(x, b, result_) 
5 BiCGSTABSolver::apply 
5 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
4 7 before Dune::BiCGSTABSolver 
4 Dune::BiCGSTABSolver.apply(x, b, result_) 
4 BiCGSTABSolver::apply 
4 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
6 7 before Dune::BiCGSTABSolver 
6 Dune::BiCGSTABSolver.apply(x, b, result_) 
6 BiCGSTABSolver::apply 
6 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
3 7 before Dune::BiCGSTABSolver 
3 Dune::BiCGSTABSolver.apply(x, b, result_) 
3 BiCGSTABSolver::apply 
3 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
Using a direct coarse solver (SuperLU)
Building hierarchy of 1 levels (inclusive coarse solver) took 0.110185 seconds.
0 7 before Dune::BiCGSTABSolver 
0 Dune::BiCGSTABSolver.apply(x, b, result_) 
0 BiCGSTABSolver::apply 
0 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
6 === BiCGSTABSolver
 Iter          Defect            Rate
    0      1.83461e-12
6  (it = 0.5; it < _maxit; it+=.5) 0.5 250
6  before p = r 
6  before  y = W^-1 * p 
6  before _prec->apply(y,p); 0.5 250
6 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
0 === BiCGSTABSolver
 Iter          Defect            Rate
    0      1.83461e-12
0  (it = 0.5; it < _maxit; it+=.5) 0.5 250
0  before p = r 
0  before  y = W^-1 * p 
0  before _prec->apply(y,p); 0.5 250
0 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 7
0 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
3 === BiCGSTABSolver
 Iter          Defect            Rate
    0      1.83461e-12
3  (it = 0.5; it < _maxit; it+=.5) 0.5 250
3  before p = r 
3  before  y = W^-1 * p 
3  before _prec->apply(y,p); 0.5 250
3 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
1 === BiCGSTABSolver
 Iter          Defect            Rate
    0      1.83461e-12
1  (it = 0.5; it < _maxit; it+=.5) 0.5 250
1  before p = r 
1  before  y = W^-1 * p 
1  before _prec->apply(y,p); 0.5 250
1 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
2 === BiCGSTABSolver
 Iter          Defect            Rate
    0      1.83461e-12
2  (it = 0.5; it < _maxit; it+=.5) 0.5 250
2  before p = r 
2  before  y = W^-1 * p 
2  before _prec->apply(y,p); 0.5 250
2 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
5 === BiCGSTABSolver
 Iter          Defect            Rate
    0      1.83461e-12
5  (it = 0.5; it < _maxit; it+=.5) 0.5 250
5  before p = r 
5  before  y = W^-1 * p 
5  before _prec->apply(y,p); 0.5 250
5 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
4 === BiCGSTABSolver
 Iter          Defect            Rate
    0      1.83461e-12
4  (it = 0.5; it < _maxit; it+=.5) 0.5 250
4  before p = r 
4  before  y = W^-1 * p 
4  before _prec->apply(y,p); 0.5 250
4 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
3 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
3 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
3  before v = A * y 
1 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
2 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
2 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
5 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
5 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
5  before v = A * y 
6 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
6 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
6  before v = A * y 
6  before  alpha = rho_new / < rt, v > 
4 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
4 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
4  before v = A * y 
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
0 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
1 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
2  before v = A * y 
5  before  alpha = rho_new / < rt, v > 
0 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
0  before v = A * y 
4  before  alpha = rho_new / < rt, v > 
3  before  alpha = rho_new / < rt, v > 
0  before  alpha = rho_new / < rt, v > 
0  before alpha = rho_new / h; 
0  before apply first correction to x 
0  before r = r - alpha*v 
0  before test stop criteria 
0 istlsolver::printOutput 
  0.5      3.90184e-30      2.12679e-18
0 === rate=4.52325e-36, T=0.0185912, TIT=0.0371824, IT=0.5
6  before alpha = rho_new / h; 
6  before apply first correction to x 
6  before r = r - alpha*v 
6  before test stop criteria 
6 istlsolver::printOutput 
  0.5      3.90184e-30      2.12679e-18
6 === rate=4.52325e-36, T=0.120768, TIT=0.241535, IT=0.5
6 finished solveLinearSystemImpl_ 1
6 converged value: 1 7 1 let s go get convergedRemote
6 to  comm_.min(converged) 
4  before alpha = rho_new / h; 
4  before apply first correction to x 
4  before r = r - alpha*v 
4  before test stop criteria 
4 istlsolver::printOutput 
  0.5      3.90184e-30      2.12679e-18
4 === rate=4.52325e-36, T=0.120767, TIT=0.241533, IT=0.5
4 finished solveLinearSystemImpl_ 1
4 converged value: 1 7 1 let s go get convergedRemote
4 to  comm_.min(converged) 
5  before alpha = rho_new / h; 
5  before apply first correction to x 
5  before r = r - alpha*v 
5  before test stop criteria 
5 istlsolver::printOutput 
  0.5      3.90184e-30      2.12679e-18
5 === rate=4.52325e-36, T=0.120774, TIT=0.241549, IT=0.5
5 finished solveLinearSystemImpl_ 1
5 converged value: 1 7 1 let s go get convergedRemote
5 to  comm_.min(converged) 
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
1  before v = A * y 
1  before  alpha = rho_new / < rt, v > 
1  before alpha = rho_new / h; 
1  before apply first correction to x 
1  before r = r - alpha*v 
1  before test stop criteria 
1 istlsolver::printOutput 
  0.5      3.90184e-30      2.12679e-18
1 === rate=4.52325e-36, T=0.120777, TIT=0.241555, IT=0.5
1 finished solveLinearSystemImpl_ 1
1 converged value: 1 7 1 let s go get convergedRemote
1 to  comm_.min(converged) 
2  before  alpha = rho_new / < rt, v > 
2  before alpha = rho_new / h; 
2  before apply first correction to x 
2  before r = r - alpha*v 
2  before test stop criteria 
2 istlsolver::printOutput 
  0.5      3.90184e-30      2.12679e-18
2 === rate=4.52325e-36, T=0.120768, TIT=0.241535, IT=0.5
2 finished solveLinearSystemImpl_ 1
2 converged value: 1 7 1 let s go get convergedRemote
2 to  comm_.min(converged) 
3  before alpha = rho_new / h; 
3  before apply first correction to x 
3  before r = r - alpha*v 
3  before test stop criteria 
3 istlsolver::printOutput 
  0.5      3.90184e-30      2.12679e-18
3 === rate=4.52325e-36, T=0.120779, TIT=0.241559, IT=0.5
3 finished solveLinearSystemImpl_ 1
3 converged value: 1 7 1 let s go get convergedRemote
3 to  comm_.min(converged) 
6 did  comm_.min(converged) convergedRemote: 1
6 final convergedRemote: 1
6 Update: x^(k+1) = x^k - deltax^k 
6 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
6 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
6 Newton iteration  4 done6 , maximum relative shift = 4.586e-04
6 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
4 did  comm_.min(converged) convergedRemote: 1
4 final convergedRemote: 1
4 Update: x^(k+1) = x^k - deltax^k 
4 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
4 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
4 Newton iteration  4 done4 , maximum relative shift = 4.586e-04
4 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
5 did  comm_.min(converged) convergedRemote: 1
5 final convergedRemote: 1
5 Update: x^(k+1) = x^k - deltax^k 
5 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
5 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
5 Newton iteration  4 done5 , maximum relative shift = 4.586e-04
5 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
1 did  comm_.min(converged) convergedRemote: 1
1 final convergedRemote: 1
1 Update: x^(k+1) = x^k - deltax^k 
1 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
1 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
1 Newton iteration  4 done1 , maximum relative shift = 4.586e-04
1 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
3 did  comm_.min(converged) convergedRemote: 1
3 final convergedRemote: 1
3 Update: x^(k+1) = x^k - deltax^k 
3 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
3 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
3 Newton iteration  4 done3 , maximum relative shift = 4.586e-04
3 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
2 did  comm_.min(converged) convergedRemote: 1
2 final convergedRemote: 1
2 Update: x^(k+1) = x^k - deltax^k 
2 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
2 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
2 Newton iteration  4 done2 , maximum relative shift = 4.586e-04
2 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
0 finished solveLinearSystemImpl_ 1
0 converged value: 1 7 1 let s go get convergedRemote
0 to  comm_.min(converged) 
0 did  comm_.min(converged) convergedRemote: 1
0 final convergedRemote: 1
0 Update: x^(k+1) = x^k - deltax^k 
0 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
0 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
0 Newton iteration  4 done0 , maximum relative shift = 4.586e-04
0 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
0 Solve: M deltax^k = r 
0 Newton::solveLinearSystem : entering the try section 4
0 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
0 solveLinearSystemImpl_, before converged = ls.solve 
0 7 amgbackend::solve isParallel 1
3 Solve: M deltax^k = r 
3 Newton::solveLinearSystem : entering the try section 4
3 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
3 solveLinearSystemImpl_, before converged = ls.solve 
3 7 amgbackend::solve isParallel 1
2 Solve: M deltax^k = r 
2 Newton::solveLinearSystem : entering the try section 4
2 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
2 solveLinearSystemImpl_, before converged = ls.solve 
2 7 amgbackend::solve isParallel 1
1 Solve: M deltax^k = r 
1 Newton::solveLinearSystem : entering the try section 4
1 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
1 solveLinearSystemImpl_, before converged = ls.solve 
1 7 amgbackend::solve isParallel 1
6 Solve: M deltax^k = r 
6 Newton::solveLinearSystem : entering the try section 4
6 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
6 solveLinearSystemImpl_, before converged = ls.solve 
6 7 amgbackend::solve isParallel 1
4 Solve: M deltax^k = r 
4 Newton::solveLinearSystem : entering the try section 4
4 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
4 solveLinearSystemImpl_, before converged = ls.solve 
4 7 amgbackend::solve isParallel 1
5 Solve: M deltax^k = r 
5 Newton::solveLinearSystem : entering the try section 4
5 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
5 solveLinearSystemImpl_, before converged = ls.solve 
5 7 amgbackend::solve isParallel 1
0 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
0 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 7
3 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
3 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
2 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
2 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
1 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
1 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
6 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
6 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
4 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
4 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
5 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
5 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 7
5 7 before Dune::BiCGSTABSolver 
5 Dune::BiCGSTABSolver.apply(x, b, result_) 
5 BiCGSTABSolver::apply 
5 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
3 7 before Dune::BiCGSTABSolver 
3 Dune::BiCGSTABSolver.apply(x, b, result_) 
3 BiCGSTABSolver::apply 
3 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
2 7 before Dune::BiCGSTABSolver 
2 Dune::BiCGSTABSolver.apply(x, b, result_) 
2 BiCGSTABSolver::apply 
2 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
1 7 before Dune::BiCGSTABSolver 
1 Dune::BiCGSTABSolver.apply(x, b, result_) 
1 BiCGSTABSolver::apply 
1 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
6 7 before Dune::BiCGSTABSolver 
6 Dune::BiCGSTABSolver.apply(x, b, result_) 
6 BiCGSTABSolver::apply 
6 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 7
4 7 before Dune::BiCGSTABSolver 
4 Dune::BiCGSTABSolver.apply(x, b, result_) 
4 BiCGSTABSolver::apply 
4 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
Using a direct coarse solver (SuperLU)
Building hierarchy of 1 levels (inclusive coarse solver) took 0.113414 seconds.
0 7 before Dune::BiCGSTABSolver 
0 Dune::BiCGSTABSolver.apply(x, b, result_) 
0 BiCGSTABSolver::apply 
0 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
2 === BiCGSTABSolver
 Iter          Defect            Rate
    0      2.19954e-13
2  (it = 0.5; it < _maxit; it+=.5) 0.5 250
2  before p = r 
2  before  y = W^-1 * p 
2  before _prec->apply(y,p); 0.5 250
2 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
3 === BiCGSTABSolver
 Iter          Defect            Rate
    0      2.19954e-13
3  (it = 0.5; it < _maxit; it+=.5) 0.5 250
3  before p = r 
3  before  y = W^-1 * p 
3  before _prec->apply(y,p); 0.5 250
3 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
0 === BiCGSTABSolver
 Iter          Defect            Rate
    0      2.19954e-13
0  (it = 0.5; it < _maxit; it+=.5) 0.5 250
0  before p = r 
0  before  y = W^-1 * p 
0  before _prec->apply(y,p); 0.5 250
0 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 7
0 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
1 === BiCGSTABSolver
 Iter          Defect            Rate
    0      2.19954e-13
1  (it = 0.5; it < _maxit; it+=.5) 0.5 250
1  before p = r 
1  before  y = W^-1 * p 
1  before _prec->apply(y,p); 0.5 250
1 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
6 === BiCGSTABSolver
 Iter          Defect            Rate
    0      2.19954e-13
6  (it = 0.5; it < _maxit; it+=.5) 0.5 250
6  before p = r 
6  before  y = W^-1 * p 
6  before _prec->apply(y,p); 0.5 250
6 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
4 === BiCGSTABSolver
 Iter          Defect            Rate
    0      2.19954e-13
4  (it = 0.5; it < _maxit; it+=.5) 0.5 250
4  before p = r 
4  before  y = W^-1 * p 
4  before _prec->apply(y,p); 0.5 250
4 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
5 === BiCGSTABSolver
 Iter          Defect            Rate
    0      2.19954e-13
5  (it = 0.5; it < _maxit; it+=.5) 0.5 250
5  before p = r 
5  before  y = W^-1 * p 
5  before _prec->apply(y,p); 0.5 250
5 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
5 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
5 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
5  before v = A * y 
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
0 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
1 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
6 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
6 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
6  before v = A * y 
6  before  alpha = rho_new / < rt, v > 
4 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
4 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
4  before v = A * y 
4  before  alpha = rho_new / < rt, v > 
3 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
3 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
3  before v = A * y 
2 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
2 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
1 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
3  before  alpha = rho_new / < rt, v > 
2  before v = A * y 
2  before  alpha = rho_new / < rt, v > 
5  before  alpha = rho_new / < rt, v > 
0 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
0  before v = A * y 
6  before alpha = rho_new / h; 
6  before apply first correction to x 
6  before r = r - alpha*v 
6  before test stop criteria 
6 istlsolver::printOutput 
  0.5      2.47145e-30      1.12362e-17
6 === rate=1.26253e-34, T=0.126838, TIT=0.253677, IT=0.5
6 finished solveLinearSystemImpl_ 1
6 converged value: 1 7 1 let s go get convergedRemote
6 to  comm_.min(converged) 
4  before alpha = rho_new / h; 
4  before apply first correction to x 
4  before r = r - alpha*v 
4  before test stop criteria 
4 istlsolver::printOutput 
  0.5      2.47145e-30      1.12362e-17
4 === rate=1.26253e-34, T=0.126839, TIT=0.253678, IT=0.5
4 finished solveLinearSystemImpl_ 1
4 converged value: 1 7 1 let s go get convergedRemote
4 to  comm_.min(converged) 
1  before v = A * y 
1  before  alpha = rho_new / < rt, v > 
1  before alpha = rho_new / h; 
1  before apply first correction to x 
1  before r = r - alpha*v 
1  before test stop criteria 
1 istlsolver::printOutput 
  0.5      2.47145e-30      1.12362e-17
1 === rate=1.26253e-34, T=0.126934, TIT=0.253869, IT=0.5
1 finished solveLinearSystemImpl_ 1
1 converged value: 1 7 1 let s go get convergedRemote
1 to  comm_.min(converged) 
0  before  alpha = rho_new / < rt, v > 
0  before alpha = rho_new / h; 
0  before apply first correction to x 
0  before r = r - alpha*v 
0  before test stop criteria 
0 istlsolver::printOutput 
  0.5      2.47145e-30      1.12362e-17
0 === rate=1.26253e-34, T=0.0214268, TIT=0.0428535, IT=0.5
2  before alpha = rho_new / h; 
2  before apply first correction to x 
2  before r = r - alpha*v 
2  before test stop criteria 
2 istlsolver::printOutput 
  0.5      2.47145e-30      1.12362e-17
2 === rate=1.26253e-34, T=0.12684, TIT=0.253679, IT=0.5
2 finished solveLinearSystemImpl_ 1
2 converged value: 1 7 1 let s go get convergedRemote
2 to  comm_.min(converged) 
5  before alpha = rho_new / h; 
5  before apply first correction to x 
5  before r = r - alpha*v 
5  before test stop criteria 
5 istlsolver::printOutput 
  0.5      2.47145e-30      1.12362e-17
5 === rate=1.26253e-34, T=0.126933, TIT=0.253866, IT=0.5
5 finished solveLinearSystemImpl_ 1
5 converged value: 1 7 1 let s go get convergedRemote
5 to  comm_.min(converged) 
3  before alpha = rho_new / h; 
3  before apply first correction to x 
3  before r = r - alpha*v 
3  before test stop criteria 
3 istlsolver::printOutput 
  0.5      2.47145e-30      1.12362e-17
3 === rate=1.26253e-34, T=0.126934, TIT=0.253868, IT=0.5
3 finished solveLinearSystemImpl_ 1
3 converged value: 1 7 1 let s go get convergedRemote
3 to  comm_.min(converged) 
0 finished solveLinearSystemImpl_ 1
0 converged value: 1 7 1 let s go get convergedRemote
0 to  comm_.min(converged) 
0 did  comm_.min(converged) convergedRemote: 1
0 final convergedRemote: 1
0 Update: x^(k+1) = x^k - deltax^k 
0 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
0 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
0 Newton iteration  5 done0 , maximum relative shift = 5.499e-05
0 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
2 did  comm_.min(converged) convergedRemote: 1
2 final convergedRemote: 1
2 Update: x^(k+1) = x^k - deltax^k 
2 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
2 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
2 Newton iteration  5 done2 , maximum relative shift = 5.499e-05
2 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
5 did  comm_.min(converged) convergedRemote: 1
5 final convergedRemote: 1
5 Update: x^(k+1) = x^k - deltax^k 
5 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
5 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
5 Newton iteration  5 done5 , maximum relative shift = 5.499e-05
5 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
1 did  comm_.min(converged) convergedRemote: 1
1 final convergedRemote: 1
1 Update: x^(k+1) = x^k - deltax^k 
1 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
1 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
1 Newton iteration  5 done1 , maximum relative shift = 5.499e-05
1 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
6 did  comm_.min(converged) convergedRemote: 1
6 final convergedRemote: 1
6 Update: x^(k+1) = x^k - deltax^k 
6 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
6 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
6 Newton iteration  5 done6 , maximum relative shift = 5.499e-05
6 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
4 did  comm_.min(converged) convergedRemote: 1
4 final convergedRemote: 1
4 Update: x^(k+1) = x^k - deltax^k 
4 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
4 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
4 Newton iteration  5 done4 , maximum relative shift = 5.499e-05
4 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
3 did  comm_.min(converged) convergedRemote: 1
3 final convergedRemote: 1
3 Update: x^(k+1) = x^k - deltax^k 
3 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
3 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
3 Newton iteration  5 done3 , maximum relative shift = 5.499e-05
3 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
4 Solve: M deltax^k = r 
4 Newton::solveLinearSystem : entering the try section 5
4 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
4 solveLinearSystemImpl_, before converged = ls.solve 
4 7 amgbackend::solve isParallel 1
0 Solve: M deltax^k = r 
0 Newton::solveLinearSystem : entering the try section 5
0 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
0 solveLinearSystemImpl_, before converged = ls.solve 
0 7 amgbackend::solve isParallel 1
2 Solve: M deltax^k = r 
2 Newton::solveLinearSystem : entering the try section 5
2 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
2 solveLinearSystemImpl_, before converged = ls.solve 
2 7 amgbackend::solve isParallel 1
5 Solve: M deltax^k = r 
5 Newton::solveLinearSystem : entering the try section 5
5 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
5 solveLinearSystemImpl_, before converged = ls.solve 
5 7 amgbackend::solve isParallel 1
1 Solve: M deltax^k = r 
1 Newton::solveLinearSystem : entering the try section 5
1 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
1 solveLinearSystemImpl_, before converged = ls.solve 
1 7 amgbackend::solve isParallel 1
6 Solve: M deltax^k = r 
6 Newton::solveLinearSystem : entering the try section 5
6 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
6 solveLinearSystemImpl_, before converged = ls.solve 
6 7 amgbackend::solve isParallel 1
3 Solve: M deltax^k = r 
3 Newton::solveLinearSystem : entering the try section 5
3 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
3 solveLinearSystemImpl_, before converged = ls.solve 
3 7 amgbackend::solve isParallel 1
4 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
4 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
0 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
0 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 7
2 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
2 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
5 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
5 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
1 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
1 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
3 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
3 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
6 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
6 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 7
2 7 before Dune::BiCGSTABSolver 
2 Dune::BiCGSTABSolver.apply(x, b, result_) 
2 BiCGSTABSolver::apply 
2 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
5 7 before Dune::BiCGSTABSolver 
5 Dune::BiCGSTABSolver.apply(x, b, result_) 
5 BiCGSTABSolver::apply 
5 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 7
3 7 before Dune::BiCGSTABSolver 
3 Dune::BiCGSTABSolver.apply(x, b, result_) 
3 BiCGSTABSolver::apply 
3 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
1 7 before Dune::BiCGSTABSolver 
1 Dune::BiCGSTABSolver.apply(x, b, result_) 
1 BiCGSTABSolver::apply 
1 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
4 7 before Dune::BiCGSTABSolver 
4 Dune::BiCGSTABSolver.apply(x, b, result_) 
4 BiCGSTABSolver::apply 
4 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
6 7 before Dune::BiCGSTABSolver 
6 Dune::BiCGSTABSolver.apply(x, b, result_) 
6 BiCGSTABSolver::apply 
6 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
Using a direct coarse solver (SuperLU)
Building hierarchy of 1 levels (inclusive coarse solver) took 0.112959 seconds.
0 7 before Dune::BiCGSTABSolver 
0 Dune::BiCGSTABSolver.apply(x, b, result_) 
0 BiCGSTABSolver::apply 
0 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
0 === BiCGSTABSolver
 Iter          Defect            Rate
    0      2.64337e-14
0  (it = 0.5; it < _maxit; it+=.5) 0.5 250
4 === BiCGSTABSolver
 Iter          Defect            Rate
    0      2.64337e-14
4  (it = 0.5; it < _maxit; it+=.5) 0.5 250
4  before p = r 
4  before  y = W^-1 * p 
4  before _prec->apply(y,p); 0.5 250
4 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
5 === BiCGSTABSolver
 Iter          Defect            Rate
    0      2.64337e-14
5  (it = 0.5; it < _maxit; it+=.5) 0.5 250
5  before p = r 
5  before  y = W^-1 * p 
5  before _prec->apply(y,p); 0.5 250
5 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
2 === BiCGSTABSolver
 Iter          Defect            Rate
    0      2.64337e-14
2  (it = 0.5; it < _maxit; it+=.5) 0.5 250
2  before p = r 
2  before  y = W^-1 * p 
2  before _prec->apply(y,p); 0.5 250
2 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
0  before p = r 
0  before  y = W^-1 * p 
0  before _prec->apply(y,p); 0.5 250
0 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 7
0 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
6 === BiCGSTABSolver
 Iter          Defect            Rate
    0      2.64337e-14
6  (it = 0.5; it < _maxit; it+=.5) 0.5 250
6  before p = r 
6  before  y = W^-1 * p 
6  before _prec->apply(y,p); 0.5 250
6 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
3 === BiCGSTABSolver
 Iter          Defect            Rate
    0      2.64337e-14
3  (it = 0.5; it < _maxit; it+=.5) 0.5 250
3  before p = r 
3  before  y = W^-1 * p 
3  before _prec->apply(y,p); 0.5 250
3 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
1 === BiCGSTABSolver
 Iter          Defect            Rate
    0      2.64337e-14
1  (it = 0.5; it < _maxit; it+=.5) 0.5 250
1  before p = r 
1  before  y = W^-1 * p 
1  before _prec->apply(y,p); 0.5 250
1 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
0 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
6 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
6 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
6  before v = A * y 
3 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
3 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
3  before v = A * y 
2 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
2 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
4 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
4 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
4  before v = A * y 
5 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
5 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
5  before v = A * y 
1 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
5  before  alpha = rho_new / < rt, v > 
0 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
0  before v = A * y 
6  before  alpha = rho_new / < rt, v > 
3  before  alpha = rho_new / < rt, v > 
2  before v = A * y 
4  before  alpha = rho_new / < rt, v > 
1 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
3  before alpha = rho_new / h; 
3  before apply first correction to x 
3  before r = r - alpha*v 
3  before test stop criteria 
3 istlsolver::printOutput 
  0.5      6.62131e-31      2.50487e-17
3 === rate=6.27438e-34, T=0.123559, TIT=0.247118, IT=0.5
3 finished solveLinearSystemImpl_ 1
3 converged value: 1 7 1 let s go get convergedRemote
3 to  comm_.min(converged) 
3 did  comm_.min(converged) convergedRemote: 1
3 final convergedRemote: 1
3 Update: x^(k+1) = x^k - deltax^k 
3 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
3 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
3 Newton iteration  6 done3 , maximum relative shift = 6.608e-06
2  before  alpha = rho_new / < rt, v > 
2  before alpha = rho_new / h; 
2  before apply first correction to x 
2  before r = r - alpha*v 
2  before test stop criteria 
2 istlsolver::printOutput 
  0.5      6.62131e-31      2.50487e-17
2 === rate=6.27438e-34, T=0.123605, TIT=0.247209, IT=0.5
2 finished solveLinearSystemImpl_ 1
2 converged value: 1 7 1 let s go get convergedRemote
2 to  comm_.min(converged) 
2 did  comm_.min(converged) convergedRemote: 1
2 final convergedRemote: 1
2 Update: x^(k+1) = x^k - deltax^k 
2 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
2 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
1  before v = A * y 
1  before  alpha = rho_new / < rt, v > 
1  before alpha = rho_new / h; 
1  before apply first correction to x 
1  before r = r - alpha*v 
1  before test stop criteria 
1 istlsolver::printOutput 
  0.5      6.62131e-31      2.50487e-17
1 === rate=6.27438e-34, T=0.123561, TIT=0.247122, IT=0.5
1 finished solveLinearSystemImpl_ 1
1 converged value: 1 7 1 let s go get convergedRemote
1 to  comm_.min(converged) 
1 did  comm_.min(converged) convergedRemote: 1
1 final convergedRemote: 1
1 Update: x^(k+1) = x^k - deltax^k 
1 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
1 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
1 Newton iteration  6 done1 , maximum relative shift = 6.608e-06
1 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
4  before alpha = rho_new / h; 
4  before apply first correction to x 
4  before r = r - alpha*v 
4  before test stop criteria 
4 istlsolver::printOutput 
  0.5      6.62131e-31      2.50487e-17
4 === rate=6.27438e-34, T=0.123604, TIT=0.247208, IT=0.5
4 finished solveLinearSystemImpl_ 1
4 converged value: 1 7 1 let s go get convergedRemote
4 to  comm_.min(converged) 
4 did  comm_.min(converged) convergedRemote: 1
4 final convergedRemote: 1
4 Update: x^(k+1) = x^k - deltax^k 
4 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
4 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
5  before alpha = rho_new / h; 
5  before apply first correction to x 
5  before r = r - alpha*v 
5  before test stop criteria 
5 istlsolver::printOutput 
  0.5      6.62131e-31      2.50487e-17
5 === rate=6.27438e-34, T=0.123558, TIT=0.247116, IT=0.5
5 finished solveLinearSystemImpl_ 1
5 converged value: 1 7 1 let s go get convergedRemote
5 to  comm_.min(converged) 
5 did  comm_.min(converged) convergedRemote: 1
5 final convergedRemote: 1
5 Update: x^(k+1) = x^k - deltax^k 
5 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
5 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
0  before  alpha = rho_new / < rt, v > 
0  before alpha = rho_new / h; 
0  before apply first correction to x 
0  before r = r - alpha*v 
0  before test stop criteria 
0 istlsolver::printOutput 
  0.5      6.62131e-31      2.50487e-17
0 === rate=6.27438e-34, T=0.0184674, TIT=0.0369348, IT=0.5
0 finished solveLinearSystemImpl_ 1
0 converged value: 1 7 1 let s go get convergedRemote
0 to  comm_.min(converged) 
0 did  comm_.min(converged) convergedRemote: 1
0 final convergedRemote: 1
0 Update: x^(k+1) = x^k - deltax^k 
0 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
0 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
6  before alpha = rho_new / h; 
6  before apply first correction to x 
6  before r = r - alpha*v 
6  before test stop criteria 
6 istlsolver::printOutput 
  0.5      6.62131e-31      2.50487e-17
6 === rate=6.27438e-34, T=0.123595, TIT=0.247189, IT=0.5
6 finished solveLinearSystemImpl_ 1
6 converged value: 1 7 1 let s go get convergedRemote
6 to  comm_.min(converged) 
6 did  comm_.min(converged) convergedRemote: 1
6 final convergedRemote: 1
6 Update: x^(k+1) = x^k - deltax^k 
6 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
6 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
6 Newton iteration  6 done6 , maximum relative shift = 6.608e-06
6 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
4 Newton iteration  6 done4 , maximum relative shift = 6.608e-06
4 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
5 Newton iteration  6 done5 , maximum relative shift = 6.608e-06
5 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
0 Newton iteration  6 done0 , maximum relative shift = 6.608e-06
0 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
2 Newton iteration  6 done2 , maximum relative shift = 6.608e-06
2 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
3 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
6 Solve: M deltax^k = r 
6 Newton::solveLinearSystem : entering the try section 6
6 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
6 solveLinearSystemImpl_, before converged = ls.solve 
6 7 amgbackend::solve isParallel 1
4 Solve: M deltax^k = r 
4 Newton::solveLinearSystem : entering the try section 6
4 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
4 solveLinearSystemImpl_, before converged = ls.solve 
4 7 amgbackend::solve isParallel 1
5 Solve: M deltax^k = r 
5 Newton::solveLinearSystem : entering the try section 6
5 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
5 solveLinearSystemImpl_, before converged = ls.solve 
5 7 amgbackend::solve isParallel 1
0 Solve: M deltax^k = r 
0 Newton::solveLinearSystem : entering the try section 6
0 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
0 solveLinearSystemImpl_, before converged = ls.solve 
0 7 amgbackend::solve isParallel 1
2 Solve: M deltax^k = r 
2 Newton::solveLinearSystem : entering the try section 6
2 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
2 solveLinearSystemImpl_, before converged = ls.solve 
2 7 amgbackend::solve isParallel 1
3 Solve: M deltax^k = r 
3 Newton::solveLinearSystem : entering the try section 6
3 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
3 solveLinearSystemImpl_, before converged = ls.solve 
3 7 amgbackend::solve isParallel 1
1 Solve: M deltax^k = r 
1 Newton::solveLinearSystem : entering the try section 6
1 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
1 solveLinearSystemImpl_, before converged = ls.solve 
1 7 amgbackend::solve isParallel 1
2 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
2 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
6 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
6 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
4 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
4 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
5 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
5 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
0 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
0 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 7
1 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
1 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
3 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
3 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 7
1 7 before Dune::BiCGSTABSolver 
1 Dune::BiCGSTABSolver.apply(x, b, result_) 
1 BiCGSTABSolver::apply 
1 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
3 7 before Dune::BiCGSTABSolver 
3 Dune::BiCGSTABSolver.apply(x, b, result_) 
3 BiCGSTABSolver::apply 
3 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
2 7 before Dune::BiCGSTABSolver 
2 Dune::BiCGSTABSolver.apply(x, b, result_) 
2 BiCGSTABSolver::apply 
2 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
6 7 before Dune::BiCGSTABSolver 
6 Dune::BiCGSTABSolver.apply(x, b, result_) 
6 BiCGSTABSolver::apply 
6 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
4 7 before Dune::BiCGSTABSolver 
4 Dune::BiCGSTABSolver.apply(x, b, result_) 
4 BiCGSTABSolver::apply 
4 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
5 7 before Dune::BiCGSTABSolver 
5 Dune::BiCGSTABSolver.apply(x, b, result_) 
5 BiCGSTABSolver::apply 
5 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
Using a direct coarse solver (SuperLU)
Building hierarchy of 1 levels (inclusive coarse solver) took 0.113148 seconds.
0 7 before Dune::BiCGSTABSolver 
0 Dune::BiCGSTABSolver.apply(x, b, result_) 
0 BiCGSTABSolver::apply 
0 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
6 === BiCGSTABSolver
 Iter          Defect            Rate
    0      3.17586e-15
6  (it = 0.5; it < _maxit; it+=.5) 0.5 250
6  before p = r 
6  before  y = W^-1 * p 
6  before _prec->apply(y,p); 0.5 250
6 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
4 === BiCGSTABSolver
 Iter          Defect            Rate
    0      3.17586e-15
4  (it = 0.5; it < _maxit; it+=.5) 0.5 250
4  before p = r 
4  before  y = W^-1 * p 
4  before _prec->apply(y,p); 0.5 250
4 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
0 === BiCGSTABSolver
 Iter          Defect            Rate
    0      3.17586e-15
0  (it = 0.5; it < _maxit; it+=.5) 0.5 250
0  before p = r 
0  before  y = W^-1 * p 
0  before _prec->apply(y,p); 0.5 250
0 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 7
0 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
5 === BiCGSTABSolver
 Iter          Defect            Rate
    0      3.17586e-15
5  (it = 0.5; it < _maxit; it+=.5) 0.5 250
5  before p = r 
5  before  y = W^-1 * p 
5  before _prec->apply(y,p); 0.5 250
5 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
3 === BiCGSTABSolver
 Iter          Defect            Rate
    0      3.17586e-15
3  (it = 0.5; it < _maxit; it+=.5) 0.5 250
3  before p = r 
3  before  y = W^-1 * p 
3  before _prec->apply(y,p); 0.5 250
3 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
1 === BiCGSTABSolver
 Iter          Defect            Rate
    0      3.17586e-15
1  (it = 0.5; it < _maxit; it+=.5) 0.5 250
1  before p = r 
1  before  y = W^-1 * p 
1  before _prec->apply(y,p); 0.5 250
1 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
2 === BiCGSTABSolver
 Iter          Defect            Rate
    0      3.17586e-15
2  (it = 0.5; it < _maxit; it+=.5) 0.5 250
2  before p = r 
2  before  y = W^-1 * p 
2  before _prec->apply(y,p); 0.5 250
2 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
6 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
6 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
6  before v = A * y 
2 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
2 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
0 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
5 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
5 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
5  before v = A * y 
3 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
3 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
3  before v = A * y 
1 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
4 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
4 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
4  before v = A * y 
3  before  alpha = rho_new / < rt, v > 
1 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
1  before v = A * y 
6  before  alpha = rho_new / < rt, v > 
4  before  alpha = rho_new / < rt, v > 
0 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
0  before v = A * y 
5  before  alpha = rho_new / < rt, v > 
6  before alpha = rho_new / h; 
6  before apply first correction to x 
6  before r = r - alpha*v 
6  before test stop criteria 
6 istlsolver::printOutput 
  0.5      3.10752e-31      9.78483e-17
6 === rate=9.57429e-33, T=0.123379, TIT=0.246759, IT=0.5
6 finished solveLinearSystemImpl_ 1
6 converged value: 1 7 1 let s go get convergedRemote
6 to  comm_.min(converged) 
3  before alpha = rho_new / h; 
3  before apply first correction to x 
3  before r = r - alpha*v 
3  before test stop criteria 
3 istlsolver::printOutput 
  0.5      3.10752e-31      9.78483e-17
3 === rate=9.57429e-33, T=0.123535, TIT=0.247069, IT=0.5
3 finished solveLinearSystemImpl_ 1
3 converged value: 1 7 1 let s go get convergedRemote
3 to  comm_.min(converged) 
1  before  alpha = rho_new / < rt, v > 
1  before alpha = rho_new / h; 
1  before apply first correction to x 
1  before r = r - alpha*v 
1  before test stop criteria 
1 istlsolver::printOutput 
  0.5      3.10752e-31      9.78483e-17
1 === rate=9.57429e-33, T=0.123537, TIT=0.247074, IT=0.5
1 finished solveLinearSystemImpl_ 1
1 converged value: 1 7 1 let s go get convergedRemote
1 to  comm_.min(converged) 
5  before alpha = rho_new / h; 
5  before apply first correction to x 
5  before r = r - alpha*v 
5  before test stop criteria 
5 istlsolver::printOutput 
  0.5      3.10752e-31      9.78483e-17
5 === rate=9.57429e-33, T=0.123532, TIT=0.247064, IT=0.5
5 finished solveLinearSystemImpl_ 1
5 converged value: 1 7 1 let s go get convergedRemote
5 to  comm_.min(converged) 
2  before v = A * y 
2  before  alpha = rho_new / < rt, v > 
2  before alpha = rho_new / h; 
2  before apply first correction to x 
2  before r = r - alpha*v 
2  before test stop criteria 
2 istlsolver::printOutput 
  0.5      3.10752e-31      9.78483e-17
2 === rate=9.57429e-33, T=0.12338, TIT=0.24676, IT=0.5
2 finished solveLinearSystemImpl_ 1
2 converged value: 1 7 1 let s go get convergedRemote
2 to  comm_.min(converged) 
4  before alpha = rho_new / h; 
4  before apply first correction to x 
4  before r = r - alpha*v 
4  before test stop criteria 
4 istlsolver::printOutput 
  0.5      3.10752e-31      9.78483e-17
4 === rate=9.57429e-33, T=0.12338, TIT=0.24676, IT=0.5
4 finished solveLinearSystemImpl_ 1
4 converged value: 1 7 1 let s go get convergedRemote
4 to  comm_.min(converged) 
0  before  alpha = rho_new / < rt, v > 
0  before alpha = rho_new / h; 
0  before apply first correction to x 
0  before r = r - alpha*v 
0  before test stop criteria 
0 istlsolver::printOutput 
  0.5      3.10752e-31      9.78483e-17
0 === rate=9.57429e-33, T=0.0181812, TIT=0.0363624, IT=0.5
5 did  comm_.min(converged) convergedRemote: 1
5 final convergedRemote: 1
5 Update: x^(k+1) = x^k - deltax^k 
5 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
5 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
5 Newton iteration  7 done5 , maximum relative shift = 7.939e-07
5 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
2 did  comm_.min(converged) convergedRemote: 1
2 final convergedRemote: 1
2 Update: x^(k+1) = x^k - deltax^k 
2 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
2 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
2 Newton iteration  7 done2 , maximum relative shift = 7.939e-07
2 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
4 did  comm_.min(converged) convergedRemote: 1
4 final convergedRemote: 1
4 Update: x^(k+1) = x^k - deltax^k 
4 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
4 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
4 Newton iteration  7 done4 , maximum relative shift = 7.939e-07
4 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
1 did  comm_.min(converged) convergedRemote: 1
1 final convergedRemote: 1
1 Update: x^(k+1) = x^k - deltax^k 
1 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
1 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
1 Newton iteration  7 done1 , maximum relative shift = 7.939e-07
1 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
6 did  comm_.min(converged) convergedRemote: 1
6 final convergedRemote: 1
6 Update: x^(k+1) = x^k - deltax^k 
6 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
6 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
6 Newton iteration  7 done6 , maximum relative shift = 7.939e-07
6 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
3 did  comm_.min(converged) convergedRemote: 1
3 final convergedRemote: 1
3 Update: x^(k+1) = x^k - deltax^k 
3 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
3 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
3 Newton iteration  7 done3 , maximum relative shift = 7.939e-07
3 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
0 finished solveLinearSystemImpl_ 1
0 converged value: 1 7 1 let s go get convergedRemote
0 to  comm_.min(converged) 
0 did  comm_.min(converged) convergedRemote: 1
0 final convergedRemote: 1
0 Update: x^(k+1) = x^k - deltax^k 
0 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
0 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
0 Newton iteration  7 done0 , maximum relative shift = 7.939e-07
0 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
0 Solve: M deltax^k = r 
0 Newton::solveLinearSystem : entering the try section 7
0 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
0 solveLinearSystemImpl_, before converged = ls.solve 
0 7 amgbackend::solve isParallel 1
3 Solve: M deltax^k = r 
3 Newton::solveLinearSystem : entering the try section 7
3 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
3 solveLinearSystemImpl_, before converged = ls.solve 
3 7 amgbackend::solve isParallel 1
5 Solve: M deltax^k = r 
5 Newton::solveLinearSystem : entering the try section 7
5 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
5 solveLinearSystemImpl_, before converged = ls.solve 
5 7 amgbackend::solve isParallel 1
2 Solve: M deltax^k = r 
2 Newton::solveLinearSystem : entering the try section 7
2 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
2 solveLinearSystemImpl_, before converged = ls.solve 
2 7 amgbackend::solve isParallel 1
4 Solve: M deltax^k = r 
4 Newton::solveLinearSystem : entering the try section 7
4 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
4 solveLinearSystemImpl_, before converged = ls.solve 
4 7 amgbackend::solve isParallel 1
1 Solve: M deltax^k = r 
1 Newton::solveLinearSystem : entering the try section 7
1 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
1 solveLinearSystemImpl_, before converged = ls.solve 
1 7 amgbackend::solve isParallel 1
6 Solve: M deltax^k = r 
6 Newton::solveLinearSystem : entering the try section 7
6 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
6 solveLinearSystemImpl_, before converged = ls.solve 
6 7 amgbackend::solve isParallel 1
5 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
5 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
2 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
2 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
4 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
4 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
1 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
1 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
6 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
6 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
0 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
0 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 7
3 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
3 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 7
6 7 before Dune::BiCGSTABSolver 
6 Dune::BiCGSTABSolver.apply(x, b, result_) 
6 BiCGSTABSolver::apply 
6 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
3 7 before Dune::BiCGSTABSolver 
3 Dune::BiCGSTABSolver.apply(x, b, result_) 
3 BiCGSTABSolver::apply 
3 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
1 7 before Dune::BiCGSTABSolver 
1 Dune::BiCGSTABSolver.apply(x, b, result_) 
1 BiCGSTABSolver::apply 
1 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
5 7 before Dune::BiCGSTABSolver 
5 Dune::BiCGSTABSolver.apply(x, b, result_) 
5 BiCGSTABSolver::apply 
5 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
2 7 before Dune::BiCGSTABSolver 
2 Dune::BiCGSTABSolver.apply(x, b, result_) 
2 BiCGSTABSolver::apply 
2 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
4 7 before Dune::BiCGSTABSolver 
4 Dune::BiCGSTABSolver.apply(x, b, result_) 
4 BiCGSTABSolver::apply 
4 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 7
Using a direct coarse solver (SuperLU)
Building hierarchy of 1 levels (inclusive coarse solver) took 0.111704 seconds.
0 7 before Dune::BiCGSTABSolver 
0 Dune::BiCGSTABSolver.apply(x, b, result_) 
0 BiCGSTABSolver::apply 
0 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
4 === BiCGSTABSolver
 Iter          Defect            Rate
    0      3.81386e-16
4  (it = 0.5; it < _maxit; it+=.5) 0.5 250
4  before p = r 
4  before  y = W^-1 * p 
4  before _prec->apply(y,p); 0.5 250
4 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
6 === BiCGSTABSolver
 Iter          Defect            Rate
    0      3.81386e-16
6  (it = 0.5; it < _maxit; it+=.5) 0.5 250
6  before p = r 
6  before  y = W^-1 * p 
6  before _prec->apply(y,p); 0.5 250
6 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
3 === BiCGSTABSolver
 Iter          Defect            Rate
    0      3.81386e-16
3  (it = 0.5; it < _maxit; it+=.5) 0.5 250
3  before p = r 
3  before  y = W^-1 * p 
3  before _prec->apply(y,p); 0.5 250
3 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
1 === BiCGSTABSolver
 Iter          Defect            Rate
    0      3.81386e-16
1  (it = 0.5; it < _maxit; it+=.5) 0.5 250
1  before p = r 
1  before  y = W^-1 * p 
1  before _prec->apply(y,p); 0.5 250
1 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
5 === BiCGSTABSolver
 Iter          Defect            Rate
    0      3.81386e-16
5  (it = 0.5; it < _maxit; it+=.5) 0.5 250
5  before p = r 
5  before  y = W^-1 * p 
5  before _prec->apply(y,p); 0.5 250
5 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
2 === BiCGSTABSolver
 Iter          Defect            Rate
    0      3.81386e-16
2  (it = 0.5; it < _maxit; it+=.5) 0.5 250
2  before p = r 
2  before  y = W^-1 * p 
2  before _prec->apply(y,p); 0.5 250
2 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
0 === BiCGSTABSolver
 Iter          Defect            Rate
    0      3.81386e-16
0  (it = 0.5; it < _maxit; it+=.5) 0.5 250
0  before p = r 
0  before  y = W^-1 * p 
0  before _prec->apply(y,p); 0.5 250
0 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 7
0 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
0 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
2 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
2 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
4 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
4 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
4  before v = A * y 
6 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
6 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
6  before v = A * y 
6  before  alpha = rho_new / < rt, v > 
3 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
3 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
3  before v = A * y 
1 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
5 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
5 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
5  before v = A * y 
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
0 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
0  before v = A * y 
0  before  alpha = rho_new / < rt, v > 
2  before v = A * y 
2  before  alpha = rho_new / < rt, v > 
4  before  alpha = rho_new / < rt, v > 
3  before  alpha = rho_new / < rt, v > 
1 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
1  before v = A * y 
1  before  alpha = rho_new / < rt, v > 
1  before alpha = rho_new / h; 
5  before  alpha = rho_new / < rt, v > 
1  before apply first correction to x 
1  before r = r - alpha*v 
1  before test stop criteria 
1 istlsolver::printOutput 
  0.5       3.5389e-32      9.27906e-17
1 === rate=8.6101e-33, T=0.122477, TIT=0.244954, IT=0.5
1 finished solveLinearSystemImpl_ 1
1 converged value: 1 7 1 let s go get convergedRemote
1 to  comm_.min(converged) 
1 did  comm_.min(converged) convergedRemote: 1
1 final convergedRemote: 1
1 Update: x^(k+1) = x^k - deltax^k 
1 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
1 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
1 Newton iteration  8 done1 , maximum relative shift = 7.151e-08
1 Assemble/solve/update time: 0.353621(25.1548%)/1.05201(74.8346%)/0.000150022(0.0106718%)
1 gridVariables->advanceTimeStep
1 before, nonLinearSolver->suggestTimeStepSize, current time: 597.703 simTime 57600
1 before assembler->setPreviousSolution, ddt: 8.87084
1 nonLinearSolver->solve
1 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
0  before alpha = rho_new / h; 
0  before apply first correction to x 
0  before r = r - alpha*v 
0  before test stop criteria 
0 istlsolver::printOutput 
  0.5       3.5389e-32      9.27906e-17
0 === rate=8.6101e-33, T=0.0184326, TIT=0.0368652, IT=0.5
0 finished solveLinearSystemImpl_ 1
0 converged value: 1 7 1 let s go get convergedRemote
0 to  comm_.min(converged) 
0 did  comm_.min(converged) convergedRemote: 1
0 final convergedRemote: 1
0 Update: x^(k+1) = x^k - deltax^k 
0 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
0 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
0 Newton iteration  8 done0 , maximum relative shift = 7.151e-08
0 Assemble/solve/update time: 0.352645(25.1048%)/1.05183(74.88%)/0.000214404(0.0152634%)
2  before alpha = rho_new / h; 
2  before apply first correction to x 
2  before r = r - alpha*v 
2  before test stop criteria 
2 istlsolver::printOutput 
  0.5       3.5389e-32      9.27906e-17
2 === rate=8.6101e-33, T=0.122381, TIT=0.244763, IT=0.5
2 finished solveLinearSystemImpl_ 1
2 converged value: 1 7 1 let s go get convergedRemote
2 to  comm_.min(converged) 
2 did  comm_.min(converged) convergedRemote: 1
2 final convergedRemote: 1
2 Update: x^(k+1) = x^k - deltax^k 
2 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
2 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
2 Newton iteration  8 done2 , maximum relative shift = 7.151e-08
2 Assemble/solve/update time: 0.353445(25.1446%)/1.05203(74.8432%)/0.000171593(0.0122074%)
2 gridVariables->advanceTimeStep
2 before, nonLinearSolver->suggestTimeStepSize, current time: 597.703 simTime 57600
2 before assembler->setPreviousSolution, ddt: 8.87084
2 nonLinearSolver->solve
6  before alpha = rho_new / h; 
6  before apply first correction to x 
6  before r = r - alpha*v 
6  before test stop criteria 
6 istlsolver::printOutput 
  0.5       3.5389e-32      9.27906e-17
6 === rate=8.6101e-33, T=0.12238, TIT=0.244759, IT=0.5
6 finished solveLinearSystemImpl_ 1
6 converged value: 1 7 1 let s go get convergedRemote
6 to  comm_.min(converged) 
6 did  comm_.min(converged) convergedRemote: 1
6 final convergedRemote: 1
6 Update: x^(k+1) = x^k - deltax^k 
6 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
6 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
6 Newton iteration  8 done6 , maximum relative shift = 7.151e-08
6 Assemble/solve/update time: 0.353705(25.1595%)/1.05203(74.8321%)/0.000118392(0.00842135%)
6 gridVariables->advanceTimeStep
6 before, nonLinearSolver->suggestTimeStepSize, current time: 597.703 simTime 57600
6 before assembler->setPreviousSolution, ddt: 8.87084
6 nonLinearSolver->solve
6 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
4  before alpha = rho_new / h; 
4  before apply first correction to x 
4  before r = r - alpha*v 
4  before test stop criteria 
4 istlsolver::printOutput 
  0.5       3.5389e-32      9.27906e-17
4 === rate=8.6101e-33, T=0.122378, TIT=0.244755, IT=0.5
4 finished solveLinearSystemImpl_ 1
4 converged value: 1 7 1 let s go get convergedRemote
4 to  comm_.min(converged) 
4 did  comm_.min(converged) convergedRemote: 1
4 final convergedRemote: 1
4 Update: x^(k+1) = x^k - deltax^k 
4 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
4 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
4 Newton iteration  8 done4 , maximum relative shift = 7.151e-08
4 Assemble/solve/update time: 0.353488(25.1476%)/1.05201(74.8411%)/0.000159313(0.0113337%)
4 gridVariables->advanceTimeStep
4 before, nonLinearSolver->suggestTimeStepSize, current time: 597.703 simTime 57600
4 before assembler->setPreviousSolution, ddt: 8.87084
4 nonLinearSolver->solve
4 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
3  before alpha = rho_new / h; 
3  before apply first correction to x 
3  before r = r - alpha*v 
3  before test stop criteria 
3 istlsolver::printOutput 
  0.5       3.5389e-32      9.27906e-17
3 === rate=8.6101e-33, T=0.122476, TIT=0.244952, IT=0.5
3 finished solveLinearSystemImpl_ 1
3 converged value: 1 7 1 let s go get convergedRemote
3 to  comm_.min(converged) 
3 did  comm_.min(converged) convergedRemote: 1
3 final convergedRemote: 1
3 Update: x^(k+1) = x^k - deltax^k 
3 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
3 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
3 Newton iteration  8 done3 , maximum relative shift = 7.151e-08
3 Assemble/solve/update time: 0.353584(25.1521%)/1.05204(74.8361%)/0.000167112(0.0118874%)
3 gridVariables->advanceTimeStep
3 before, nonLinearSolver->suggestTimeStepSize, current time: 597.703 simTime 57600
3 before assembler->setPreviousSolution, ddt: 8.87084
3 nonLinearSolver->solve
3 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
5  before alpha = rho_new / h; 
5  before apply first correction to x 
5  before r = r - alpha*v 
5  before test stop criteria 
5 istlsolver::printOutput 
  0.5       3.5389e-32      9.27906e-17
5 === rate=8.6101e-33, T=0.122475, TIT=0.244949, IT=0.5
5 finished solveLinearSystemImpl_ 1
5 converged value: 1 7 1 let s go get convergedRemote
5 to  comm_.min(converged) 
5 did  comm_.min(converged) convergedRemote: 1
5 final convergedRemote: 1
5 Update: x^(k+1) = x^k - deltax^k 
5 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
5 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
5 Newton iteration  8 done5 , maximum relative shift = 7.151e-08
5 Assemble/solve/update time: 0.353616(25.1545%)/1.05201(74.8351%)/0.000146332(0.0104093%)
5 gridVariables->advanceTimeStep
5 before, nonLinearSolver->suggestTimeStepSize, current time: 597.703 simTime 57600
5 before assembler->setPreviousSolution, ddt: 8.87084
5 nonLinearSolver->solve
5 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
0 gridVariables->advanceTimeStep
0 before, nonLinearSolver->suggestTimeStepSize, current time: 597.703 simTime 57600
0 before assembler->setPreviousSolution, ddt: 8.87084
0 nonLinearSolver->solve
0 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
2 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
3 Solve: M deltax^k = r 
3 Newton::solveLinearSystem : entering the try section 0
3to  comm_.sum(norm2) 
3did  comm_.sum(norm2) 
3 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
3 solveLinearSystemImpl_, before converged = ls.solve 
3 7 amgbackend::solve isParallel 1
1 Solve: M deltax^k = r 
1 Newton::solveLinearSystem : entering the try section 0
1to  comm_.sum(norm2) 
1did  comm_.sum(norm2) 
1 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
1 solveLinearSystemImpl_, before converged = ls.solve 
1 7 amgbackend::solve isParallel 1
5 Solve: M deltax^k = r 
5 Newton::solveLinearSystem : entering the try section 0
5to  comm_.sum(norm2) 
5did  comm_.sum(norm2) 
5 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
5 solveLinearSystemImpl_, before converged = ls.solve 
5 7 amgbackend::solve isParallel 1
0 Solve: M deltax^k = r 
0 Newton::solveLinearSystem : entering the try section 0
0to  comm_.sum(norm2) 
0did  comm_.sum(norm2) 
0 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
0 solveLinearSystemImpl_, before converged = ls.solve 
0 7 amgbackend::solve isParallel 1
6 Solve: M deltax^k = r 
6 Newton::solveLinearSystem : entering the try section 0
6to  comm_.sum(norm2) 
6did  comm_.sum(norm2) 
6 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
6 solveLinearSystemImpl_, before converged = ls.solve 
6 7 amgbackend::solve isParallel 1
4 Solve: M deltax^k = r 
4 Newton::solveLinearSystem : entering the try section 0
4to  comm_.sum(norm2) 
4did  comm_.sum(norm2) 
4 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
4 solveLinearSystemImpl_, before converged = ls.solve 
4 7 amgbackend::solve isParallel 1
2 Solve: M deltax^k = r 
2 Newton::solveLinearSystem : entering the try section 0
2to  comm_.sum(norm2) 
2did  comm_.sum(norm2) 
2 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
2 solveLinearSystemImpl_, before converged = ls.solve 
2 7 amgbackend::solve isParallel 1
1 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
1 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
5 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
5 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
0 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
0 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 7
6 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
6 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
2 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
2 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
4 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
4 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
3 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
3 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 7
3 7 before Dune::BiCGSTABSolver 
3 Dune::BiCGSTABSolver.apply(x, b, result_) 
3 BiCGSTABSolver::apply 
3 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 7
2 7 before Dune::BiCGSTABSolver 
2 Dune::BiCGSTABSolver.apply(x, b, result_) 
2 BiCGSTABSolver::apply 
2 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
4 7 before Dune::BiCGSTABSolver 
4 Dune::BiCGSTABSolver.apply(x, b, result_) 
4 BiCGSTABSolver::apply 
4 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
6 7 before Dune::BiCGSTABSolver 
6 Dune::BiCGSTABSolver.apply(x, b, result_) 
6 BiCGSTABSolver::apply 
6 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
1 7 before Dune::BiCGSTABSolver 
1 Dune::BiCGSTABSolver.apply(x, b, result_) 
1 BiCGSTABSolver::apply 
1 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
5 7 before Dune::BiCGSTABSolver 
5 Dune::BiCGSTABSolver.apply(x, b, result_) 
5 BiCGSTABSolver::apply 
5 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
Using a direct coarse solver (SuperLU)
Building hierarchy of 1 levels (inclusive coarse solver) took 0.11165 seconds.
0 7 before Dune::BiCGSTABSolver 
0 Dune::BiCGSTABSolver.apply(x, b, result_) 
0 BiCGSTABSolver::apply 
0 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
0 === BiCGSTABSolver
 Iter          Defect            Rate
    0      3.16247e-09
0  (it = 0.5; it < _maxit; it+=.5) 0.5 250
1 === BiCGSTABSolver
 Iter          Defect            Rate
    0      3.16247e-09
1  (it = 0.5; it < _maxit; it+=.5) 0.5 250
1  before p = r 
1  before  y = W^-1 * p 
1  before _prec->apply(y,p); 0.5 250
1 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
3 === BiCGSTABSolver
 Iter          Defect            Rate
    0      3.16247e-09
3  (it = 0.5; it < _maxit; it+=.5) 0.5 250
3  before p = r 
3  before  y = W^-1 * p 
3  before _prec->apply(y,p); 0.5 250
3 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
0  before p = r 
0  before  y = W^-1 * p 
0  before _prec->apply(y,p); 0.5 250
0 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 7
0 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
5 === BiCGSTABSolver
 Iter          Defect            Rate
    0      3.16247e-09
5  (it = 0.5; it < _maxit; it+=.5) 0.5 250
5  before p = r 
5  before  y = W^-1 * p 
5  before _prec->apply(y,p); 0.5 250
5 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
2 === BiCGSTABSolver
 Iter          Defect            Rate
    0      3.16247e-09
2  (it = 0.5; it < _maxit; it+=.5) 0.5 250
2  before p = r 
2  before  y = W^-1 * p 
2  before _prec->apply(y,p); 0.5 250
2 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
4 === BiCGSTABSolver
 Iter          Defect            Rate
    0      3.16247e-09
4  (it = 0.5; it < _maxit; it+=.5) 0.5 250
4  before p = r 
4  before  y = W^-1 * p 
4  before _prec->apply(y,p); 0.5 250
4 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
6 === BiCGSTABSolver
 Iter          Defect            Rate
    0      3.16247e-09
6  (it = 0.5; it < _maxit; it+=.5) 0.5 250
6  before p = r 
6  before  y = W^-1 * p 
6  before _prec->apply(y,p); 0.5 250
6 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
3 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
3 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
3  before v = A * y 
1 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
6 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
6 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
6  before v = A * y 
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
0 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
5 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
5 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
5  before v = A * y 
2 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
2 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
4 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
4 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
4  before v = A * y 
5  before  alpha = rho_new / < rt, v > 
1 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
3  before  alpha = rho_new / < rt, v > 
4  before  alpha = rho_new / < rt, v > 
6  before  alpha = rho_new / < rt, v > 
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
0 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
4  before alpha = rho_new / h; 
4  before apply first correction to x 
4  before r = r - alpha*v 
4  before test stop criteria 
4 istlsolver::printOutput 
  0.5       4.3008e-25      1.35995e-16
4 === rate=1.84947e-32, T=0.122077, TIT=0.244154, IT=0.5
4 finished solveLinearSystemImpl_ 1
4 converged value: 1 7 1 let s go get convergedRemote
4 to  comm_.min(converged) 
6  before alpha = rho_new / h; 
6  before apply first correction to x 
6  before r = r - alpha*v 
6  before test stop criteria 
6 istlsolver::printOutput 
  0.5       4.3008e-25      1.35995e-16
6 === rate=1.84947e-32, T=0.122076, TIT=0.244153, IT=0.5
6 finished solveLinearSystemImpl_ 1
6 converged value: 1 7 1 let s go get convergedRemote
6 to  comm_.min(converged) 
3  before alpha = rho_new / h; 
3  before apply first correction to x 
3  before r = r - alpha*v 
3  before test stop criteria 
3 istlsolver::printOutput 
  0.5       4.3008e-25      1.35995e-16
3 === rate=1.84947e-32, T=0.122172, TIT=0.244343, IT=0.5
3 finished solveLinearSystemImpl_ 1
3 converged value: 1 7 1 let s go get convergedRemote
3 to  comm_.min(converged) 
2  before v = A * y 
2  before  alpha = rho_new / < rt, v > 
2  before alpha = rho_new / h; 
2  before apply first correction to x 
2  before r = r - alpha*v 
2  before test stop criteria 
2 istlsolver::printOutput 
  0.5       4.3008e-25      1.35995e-16
2 === rate=1.84947e-32, T=0.122077, TIT=0.244154, IT=0.5
2 finished solveLinearSystemImpl_ 1
2 converged value: 1 7 1 let s go get convergedRemote
2 to  comm_.min(converged) 
5  before alpha = rho_new / h; 
5  before apply first correction to x 
5  before r = r - alpha*v 
5  before test stop criteria 
5 istlsolver::printOutput 
  0.5       4.3008e-25      1.35995e-16
5 === rate=1.84947e-32, T=0.12217, TIT=0.244341, IT=0.5
5 finished solveLinearSystemImpl_ 1
5 converged value: 1 7 1 let s go get convergedRemote
5 to  comm_.min(converged) 
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
1  before v = A * y 
1  before  alpha = rho_new / < rt, v > 
1  before alpha = rho_new / h; 
1  before apply first correction to x 
1  before r = r - alpha*v 
1  before test stop criteria 
1 istlsolver::printOutput 
  0.5       4.3008e-25      1.35995e-16
1 === rate=1.84947e-32, T=0.122171, TIT=0.244342, IT=0.5
1 finished solveLinearSystemImpl_ 1
1 converged value: 1 7 1 let s go get convergedRemote
1 to  comm_.min(converged) 
0  before v = A * y 
0  before  alpha = rho_new / < rt, v > 
0  before alpha = rho_new / h; 
0  before apply first correction to x 
0  before r = r - alpha*v 
0  before test stop criteria 
0 istlsolver::printOutput 
  0.5       4.3008e-25      1.35995e-16
0 === rate=1.84947e-32, T=0.0184284, TIT=0.0368568, IT=0.5
0 finished solveLinearSystemImpl_ 1
0 converged value: 1 7 1 let s go get convergedRemote
0 to  comm_.min(converged) 
0 did  comm_.min(converged) convergedRemote: 1
0 final convergedRemote: 1
0 Update: x^(k+1) = x^k - deltax^k 
0 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
0 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
0 Newton iteration  1 done0 , maximum relative shift = 9.665e-02
0 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
1 did  comm_.min(converged) convergedRemote: 1
1 final convergedRemote: 1
1 Update: x^(k+1) = x^k - deltax^k 
1 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
1 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
1 Newton iteration  1 done1 , maximum relative shift = 9.665e-02
1 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
4 did  comm_.min(converged) convergedRemote: 1
4 final convergedRemote: 1
4 Update: x^(k+1) = x^k - deltax^k 
4 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
4 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
4 Newton iteration  1 done4 , maximum relative shift = 9.665e-02
4 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
6 did  comm_.min(converged) convergedRemote: 1
6 final convergedRemote: 1
6 Update: x^(k+1) = x^k - deltax^k 
6 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
6 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
6 Newton iteration  1 done6 , maximum relative shift = 9.665e-02
6 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
3 did  comm_.min(converged) convergedRemote: 1
3 final convergedRemote: 1
3 Update: x^(k+1) = x^k - deltax^k 
3 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
3 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
3 Newton iteration  1 done3 , maximum relative shift = 9.665e-02
3 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
2 did  comm_.min(converged) convergedRemote: 1
2 final convergedRemote: 1
2 Update: x^(k+1) = x^k - deltax^k 
2 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
2 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
2 Newton iteration  1 done2 , maximum relative shift = 9.665e-02
2 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
5 did  comm_.min(converged) convergedRemote: 1
5 final convergedRemote: 1
5 Update: x^(k+1) = x^k - deltax^k 
5 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
5 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
5 Newton iteration  1 done5 , maximum relative shift = 9.665e-02
5 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
1 Solve: M deltax^k = r 
1 Newton::solveLinearSystem : entering the try section 1
1 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
1 solveLinearSystemImpl_, before converged = ls.solve 
1 7 amgbackend::solve isParallel 1
4 Solve: M deltax^k = r 
4 Newton::solveLinearSystem : entering the try section 1
4 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
4 solveLinearSystemImpl_, before converged = ls.solve 
4 7 amgbackend::solve isParallel 1
6 Solve: M deltax^k = r 
6 Newton::solveLinearSystem : entering the try section 1
6 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
6 solveLinearSystemImpl_, before converged = ls.solve 
6 7 amgbackend::solve isParallel 1
3 Solve: M deltax^k = r 
3 Newton::solveLinearSystem : entering the try section 1
3 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
3 solveLinearSystemImpl_, before converged = ls.solve 
3 7 amgbackend::solve isParallel 1
2 Solve: M deltax^k = r 
2 Newton::solveLinearSystem : entering the try section 1
2 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
2 solveLinearSystemImpl_, before converged = ls.solve 
2 7 amgbackend::solve isParallel 1
5 Solve: M deltax^k = r 
5 Newton::solveLinearSystem : entering the try section 1
5 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
5 solveLinearSystemImpl_, before converged = ls.solve 
5 7 amgbackend::solve isParallel 1
0 Solve: M deltax^k = r 
0 Newton::solveLinearSystem : entering the try section 1
0 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
0 solveLinearSystemImpl_, before converged = ls.solve 
0 7 amgbackend::solve isParallel 1
3 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
3 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
0 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
0 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 7
5 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
5 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
2 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
2 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
1 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
1 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
4 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
4 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
6 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
6 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 7
4 7 before Dune::BiCGSTABSolver 
4 Dune::BiCGSTABSolver.apply(x, b, result_) 
4 BiCGSTABSolver::apply 
4 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
3 7 before Dune::BiCGSTABSolver 
3 Dune::BiCGSTABSolver.apply(x, b, result_) 
3 BiCGSTABSolver::apply 
3 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 7
6 7 before Dune::BiCGSTABSolver 
6 Dune::BiCGSTABSolver.apply(x, b, result_) 
6 BiCGSTABSolver::apply 
6 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
5 7 before Dune::BiCGSTABSolver 
5 Dune::BiCGSTABSolver.apply(x, b, result_) 
5 BiCGSTABSolver::apply 
5 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
2 7 before Dune::BiCGSTABSolver 
2 Dune::BiCGSTABSolver.apply(x, b, result_) 
2 BiCGSTABSolver::apply 
2 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
1 7 before Dune::BiCGSTABSolver 
1 Dune::BiCGSTABSolver.apply(x, b, result_) 
1 BiCGSTABSolver::apply 
1 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
Using a direct coarse solver (SuperLU)
Building hierarchy of 1 levels (inclusive coarse solver) took 0.111603 seconds.
0 7 before Dune::BiCGSTABSolver 
0 Dune::BiCGSTABSolver.apply(x, b, result_) 
0 BiCGSTABSolver::apply 
0 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
1 === BiCGSTABSolver
 Iter          Defect            Rate
    0      4.27352e-11
1  (it = 0.5; it < _maxit; it+=.5) 0.5 250
1  before p = r 
1  before  y = W^-1 * p 
1  before _prec->apply(y,p); 0.5 250
1 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
6 === BiCGSTABSolver
 Iter          Defect            Rate
    0      4.27352e-11
6  (it = 0.5; it < _maxit; it+=.5) 0.5 250
6  before p = r 
6  before  y = W^-1 * p 
6  before _prec->apply(y,p); 0.5 250
6 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
5 === BiCGSTABSolver
 Iter          Defect            Rate
    0      4.27352e-11
5  (it = 0.5; it < _maxit; it+=.5) 0.5 250
5  before p = r 
5  before  y = W^-1 * p 
5  before _prec->apply(y,p); 0.5 250
5 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
2 === BiCGSTABSolver
 Iter          Defect            Rate
    0      4.27352e-11
2  (it = 0.5; it < _maxit; it+=.5) 0.5 250
2  before p = r 
2  before  y = W^-1 * p 
2  before _prec->apply(y,p); 0.5 250
2 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
3 === BiCGSTABSolver
 Iter          Defect            Rate
    0      4.27352e-11
3  (it = 0.5; it < _maxit; it+=.5) 0.5 250
3  before p = r 
3  before  y = W^-1 * p 
3  before _prec->apply(y,p); 0.5 250
3 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
4 === BiCGSTABSolver
 Iter          Defect            Rate
    0      4.27352e-11
4  (it = 0.5; it < _maxit; it+=.5) 0.5 250
4  before p = r 
4  before  y = W^-1 * p 
4  before _prec->apply(y,p); 0.5 250
4 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
0 === BiCGSTABSolver
 Iter          Defect            Rate
    0      4.27352e-11
0  (it = 0.5; it < _maxit; it+=.5) 0.5 250
0  before p = r 
0  before  y = W^-1 * p 
0  before _prec->apply(y,p); 0.5 250
0 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 7
0 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
2 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
2 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
3 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
3 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
3  before v = A * y 
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
0 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
4 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
4 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
4  before v = A * y 
1 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
6 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
6 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
6  before v = A * y 
5 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
5 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
5  before v = A * y 
0 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
0  before v = A * y 
0  before  alpha = rho_new / < rt, v > 
4  before  alpha = rho_new / < rt, v > 
1 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
1  before v = A * y 
1  before  alpha = rho_new / < rt, v > 
6  before  alpha = rho_new / < rt, v > 
3  before  alpha = rho_new / < rt, v > 
2  before v = A * y 
2  before  alpha = rho_new / < rt, v > 
5  before  alpha = rho_new / < rt, v > 
6  before alpha = rho_new / h; 
6  before apply first correction to x 
6  before r = r - alpha*v 
6  before test stop criteria 
6 istlsolver::printOutput 
  0.5      6.53163e-27       1.5284e-16
6 === rate=2.336e-32, T=0.122219, TIT=0.244437, IT=0.5
6 finished solveLinearSystemImpl_ 1
6 converged value: 1 7 1 let s go get convergedRemote
6 to  comm_.min(converged) 
3  before alpha = rho_new / h; 
3  before apply first correction to x 
3  before r = r - alpha*v 
3  before test stop criteria 
3 istlsolver::printOutput 
  0.5      6.53163e-27       1.5284e-16
3 === rate=2.336e-32, T=0.12214, TIT=0.24428, IT=0.5
3 finished solveLinearSystemImpl_ 1
3 converged value: 1 7 1 let s go get convergedRemote
3 to  comm_.min(converged) 
5  before alpha = rho_new / h; 
5  before apply first correction to x 
5  before r = r - alpha*v 
5  before test stop criteria 
5 istlsolver::printOutput 
  0.5      6.53163e-27       1.5284e-16
5 === rate=2.336e-32, T=0.122139, TIT=0.244277, IT=0.5
5 finished solveLinearSystemImpl_ 1
5 converged value: 1 7 1 let s go get convergedRemote
5 to  comm_.min(converged) 
2  before alpha = rho_new / h; 
2  before apply first correction to x 
2  before r = r - alpha*v 
2  before test stop criteria 
2 istlsolver::printOutput 
  0.5      6.53163e-27       1.5284e-16
2 === rate=2.336e-32, T=0.122217, TIT=0.244434, IT=0.5
2 finished solveLinearSystemImpl_ 1
2 converged value: 1 7 1 let s go get convergedRemote
2 to  comm_.min(converged) 
0  before alpha = rho_new / h; 
0  before apply first correction to x 
0  before r = r - alpha*v 
0  before test stop criteria 
0 istlsolver::printOutput 
  0.5      6.53163e-27       1.5284e-16
0 === rate=2.336e-32, T=0.01845, TIT=0.0369, IT=0.5
4  before alpha = rho_new / h; 
4  before apply first correction to x 
4  before r = r - alpha*v 
4  before test stop criteria 
4 istlsolver::printOutput 
  0.5      6.53163e-27       1.5284e-16
4 === rate=2.336e-32, T=0.122216, TIT=0.244433, IT=0.5
4 finished solveLinearSystemImpl_ 1
4 converged value: 1 7 1 let s go get convergedRemote
4 to  comm_.min(converged) 
1  before alpha = rho_new / h; 
1  before apply first correction to x 
1  before r = r - alpha*v 
1  before test stop criteria 
1 istlsolver::printOutput 
  0.5      6.53163e-27       1.5284e-16
1 === rate=2.336e-32, T=0.122138, TIT=0.244277, IT=0.5
1 finished solveLinearSystemImpl_ 1
1 converged value: 1 7 1 let s go get convergedRemote
1 to  comm_.min(converged) 
5 did  comm_.min(converged) convergedRemote: 1
5 final convergedRemote: 1
5 Update: x^(k+1) = x^k - deltax^k 
5 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
5 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
5 Newton iteration  2 done5 , maximum relative shift = 8.816e-03
5 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
2 did  comm_.min(converged) convergedRemote: 1
2 final convergedRemote: 1
2 Update: x^(k+1) = x^k - deltax^k 
2 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
2 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
2 Newton iteration  2 done2 , maximum relative shift = 8.816e-03
2 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
0 finished solveLinearSystemImpl_ 1
0 converged value: 1 7 1 let s go get convergedRemote
0 to  comm_.min(converged) 
0 did  comm_.min(converged) convergedRemote: 1
0 final convergedRemote: 1
0 Update: x^(k+1) = x^k - deltax^k 
0 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
0 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
0 Newton iteration  2 done0 , maximum relative shift = 8.816e-03
0 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
4 did  comm_.min(converged) convergedRemote: 1
4 final convergedRemote: 1
4 Update: x^(k+1) = x^k - deltax^k 
4 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
4 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
4 Newton iteration  2 done4 , maximum relative shift = 8.816e-03
4 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
3 did  comm_.min(converged) convergedRemote: 1
3 final convergedRemote: 1
3 Update: x^(k+1) = x^k - deltax^k 
3 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
3 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
3 Newton iteration  2 done3 , maximum relative shift = 8.816e-03
3 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
6 did  comm_.min(converged) convergedRemote: 1
6 final convergedRemote: 1
6 Update: x^(k+1) = x^k - deltax^k 
6 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
6 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
6 Newton iteration  2 done6 , maximum relative shift = 8.816e-03
6 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
1 did  comm_.min(converged) convergedRemote: 1
1 final convergedRemote: 1
1 Update: x^(k+1) = x^k - deltax^k 
1 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
1 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
1 Newton iteration  2 done1 , maximum relative shift = 8.816e-03
1 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
4 Solve: M deltax^k = r 
4 Newton::solveLinearSystem : entering the try section 2
4 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
4 solveLinearSystemImpl_, before converged = ls.solve 
4 7 amgbackend::solve isParallel 1
3 Solve: M deltax^k = r 
3 Newton::solveLinearSystem : entering the try section 2
3 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
3 solveLinearSystemImpl_, before converged = ls.solve 
3 7 amgbackend::solve isParallel 1
1 Solve: M deltax^k = r 
1 Newton::solveLinearSystem : entering the try section 2
1 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
1 solveLinearSystemImpl_, before converged = ls.solve 
1 7 amgbackend::solve isParallel 1
6 Solve: M deltax^k = r 
6 Newton::solveLinearSystem : entering the try section 2
6 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
6 solveLinearSystemImpl_, before converged = ls.solve 
6 7 amgbackend::solve isParallel 1
5 Solve: M deltax^k = r 
5 Newton::solveLinearSystem : entering the try section 2
5 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
5 solveLinearSystemImpl_, before converged = ls.solve 
5 7 amgbackend::solve isParallel 1
2 Solve: M deltax^k = r 
2 Newton::solveLinearSystem : entering the try section 2
2 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
2 solveLinearSystemImpl_, before converged = ls.solve 
2 7 amgbackend::solve isParallel 1
0 Solve: M deltax^k = r 
0 Newton::solveLinearSystem : entering the try section 2
0 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
0 solveLinearSystemImpl_, before converged = ls.solve 
0 7 amgbackend::solve isParallel 1
1 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
1 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
6 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
6 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
5 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
5 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
2 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
2 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
3 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
3 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
4 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
4 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
0 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
0 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 7
5 7 before Dune::BiCGSTABSolver 
5 Dune::BiCGSTABSolver.apply(x, b, result_) 
5 BiCGSTABSolver::apply 
5 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
2 7 before Dune::BiCGSTABSolver 
2 Dune::BiCGSTABSolver.apply(x, b, result_) 
2 BiCGSTABSolver::apply 
2 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
3 7 before Dune::BiCGSTABSolver 
3 Dune::BiCGSTABSolver.apply(x, b, result_) 
3 BiCGSTABSolver::apply 
3 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 7
4 7 before Dune::BiCGSTABSolver 
4 Dune::BiCGSTABSolver.apply(x, b, result_) 
4 BiCGSTABSolver::apply 
4 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
1 7 before Dune::BiCGSTABSolver 
1 Dune::BiCGSTABSolver.apply(x, b, result_) 
1 BiCGSTABSolver::apply 
1 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
6 7 before Dune::BiCGSTABSolver 
6 Dune::BiCGSTABSolver.apply(x, b, result_) 
6 BiCGSTABSolver::apply 
6 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
Using a direct coarse solver (SuperLU)
Building hierarchy of 1 levels (inclusive coarse solver) took 0.111198 seconds.
0 7 before Dune::BiCGSTABSolver 
0 Dune::BiCGSTABSolver.apply(x, b, result_) 
0 BiCGSTABSolver::apply 
0 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
0 === BiCGSTABSolver
 Iter          Defect            Rate
3 === BiCGSTABSolver
 Iter          Defect            Rate
    0      4.69874e-12
3  (it = 0.5; it < _maxit; it+=.5) 0.5 250
3  before p = r 
3  before  y = W^-1 * p 
3  before _prec->apply(y,p); 0.5 250
3 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
2 === BiCGSTABSolver
 Iter          Defect            Rate
    0      4.69874e-12
2  (it = 0.5; it < _maxit; it+=.5) 0.5 250
2  before p = r 
2  before  y = W^-1 * p 
2  before _prec->apply(y,p); 0.5 250
2 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
    0      4.69874e-12
0  (it = 0.5; it < _maxit; it+=.5) 0.5 250
0  before p = r 
0  before  y = W^-1 * p 
0  before _prec->apply(y,p); 0.5 250
0 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 7
0 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
5 === BiCGSTABSolver
 Iter          Defect            Rate
    0      4.69874e-12
5  (it = 0.5; it < _maxit; it+=.5) 0.5 250
5  before p = r 
5  before  y = W^-1 * p 
5  before _prec->apply(y,p); 0.5 250
5 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
4 === BiCGSTABSolver
 Iter          Defect            Rate
    0      4.69874e-12
4  (it = 0.5; it < _maxit; it+=.5) 0.5 250
4  before p = r 
4  before  y = W^-1 * p 
4  before _prec->apply(y,p); 0.5 250
4 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
6 === BiCGSTABSolver
 Iter          Defect            Rate
    0      4.69874e-12
6  (it = 0.5; it < _maxit; it+=.5) 0.5 250
6  before p = r 
6  before  y = W^-1 * p 
6  before _prec->apply(y,p); 0.5 250
6 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
1 === BiCGSTABSolver
 Iter          Defect            Rate
    0      4.69874e-12
1  (it = 0.5; it < _maxit; it+=.5) 0.5 250
1  before p = r 
1  before  y = W^-1 * p 
1  before _prec->apply(y,p); 0.5 250
1 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
4 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
4 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
4  before v = A * y 
6 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
6 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
6  before v = A * y 
2 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
2 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
3 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
3 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
3  before v = A * y 
1 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
5 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
5 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
5  before v = A * y 
3  before  alpha = rho_new / < rt, v > 
4  before  alpha = rho_new / < rt, v > 
6  before  alpha = rho_new / < rt, v > 
5  before  alpha = rho_new / < rt, v > 
0 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
0 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
0  before v = A * y 
0  before  alpha = rho_new / < rt, v > 
0  before alpha = rho_new / h; 
0  before apply first correction to x 
0  before r = r - alpha*v 
0  before test stop criteria 
0 istlsolver::printOutput 
  0.5      4.72203e-28      1.00496e-16
0 === rate=1.00994e-32, T=0.0184747, TIT=0.0369495, IT=0.5
1 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
1  before v = A * y 
1  before  alpha = rho_new / < rt, v > 
1  before alpha = rho_new / h; 
1  before apply first correction to x 
1  before r = r - alpha*v 
1  before test stop criteria 
1 istlsolver::printOutput 
  0.5      4.72203e-28      1.00496e-16
1 === rate=1.00994e-32, T=0.122107, TIT=0.244214, IT=0.5
1 finished solveLinearSystemImpl_ 1
1 converged value: 1 7 1 let s go get convergedRemote
1 to  comm_.min(converged) 
5  before alpha = rho_new / h; 
5  before apply first correction to x 
5  before r = r - alpha*v 
5  before test stop criteria 
5 istlsolver::printOutput 
  0.5      4.72203e-28      1.00496e-16
5 === rate=1.00994e-32, T=0.122106, TIT=0.244212, IT=0.5
5 finished solveLinearSystemImpl_ 1
5 converged value: 1 7 1 let s go get convergedRemote
5 to  comm_.min(converged) 
6  before alpha = rho_new / h; 
6  before apply first correction to x 
6  before r = r - alpha*v 
6  before test stop criteria 
6 istlsolver::printOutput 
  0.5      4.72203e-28      1.00496e-16
6 === rate=1.00994e-32, T=0.12206, TIT=0.24412, IT=0.5
6 finished solveLinearSystemImpl_ 1
6 converged value: 1 7 1 let s go get convergedRemote
6 to  comm_.min(converged) 
3  before alpha = rho_new / h; 
3  before apply first correction to x 
3  before r = r - alpha*v 
3  before test stop criteria 
3 istlsolver::printOutput 
  0.5      4.72203e-28      1.00496e-16
3 === rate=1.00994e-32, T=0.122107, TIT=0.244215, IT=0.5
3 finished solveLinearSystemImpl_ 1
3 converged value: 1 7 1 let s go get convergedRemote
3 to  comm_.min(converged) 
4  before alpha = rho_new / h; 
4  before apply first correction to x 
4  before r = r - alpha*v 
4  before test stop criteria 
4 istlsolver::printOutput 
  0.5      4.72203e-28      1.00496e-16
4 === rate=1.00994e-32, T=0.122057, TIT=0.244113, IT=0.5
4 finished solveLinearSystemImpl_ 1
4 converged value: 1 7 1 let s go get convergedRemote
4 to  comm_.min(converged) 
2  before v = A * y 
2  before  alpha = rho_new / < rt, v > 
2  before alpha = rho_new / h; 
2  before apply first correction to x 
2  before r = r - alpha*v 
2  before test stop criteria 
2 istlsolver::printOutput 
  0.5      4.72203e-28      1.00496e-16
2 === rate=1.00994e-32, T=0.122057, TIT=0.244114, IT=0.5
2 finished solveLinearSystemImpl_ 1
2 converged value: 1 7 1 let s go get convergedRemote
2 to  comm_.min(converged) 
0 finished solveLinearSystemImpl_ 1
0 converged value: 1 7 1 let s go get convergedRemote
0 to  comm_.min(converged) 
0 did  comm_.min(converged) convergedRemote: 1
0 final convergedRemote: 1
0 Update: x^(k+1) = x^k - deltax^k 
0 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
0 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
0 Newton iteration  3 done0 , maximum relative shift = 9.899e-04
0 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
1 did  comm_.min(converged) convergedRemote: 1
1 final convergedRemote: 1
1 Update: x^(k+1) = x^k - deltax^k 
1 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
1 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
1 Newton iteration  3 done1 , maximum relative shift = 9.899e-04
1 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
5 did  comm_.min(converged) convergedRemote: 1
5 final convergedRemote: 1
5 Update: x^(k+1) = x^k - deltax^k 
5 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
5 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
5 Newton iteration  3 done5 , maximum relative shift = 9.899e-04
5 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
6 did  comm_.min(converged) convergedRemote: 1
6 final convergedRemote: 1
6 Update: x^(k+1) = x^k - deltax^k 
6 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
6 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
6 Newton iteration  3 done6 , maximum relative shift = 9.899e-04
6 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
3 did  comm_.min(converged) convergedRemote: 1
3 final convergedRemote: 1
3 Update: x^(k+1) = x^k - deltax^k 
3 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
3 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
3 Newton iteration  3 done3 , maximum relative shift = 9.899e-04
3 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
4 did  comm_.min(converged) convergedRemote: 1
4 final convergedRemote: 1
4 Update: x^(k+1) = x^k - deltax^k 
4 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
4 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
4 Newton iteration  3 done4 , maximum relative shift = 9.899e-04
4 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
2 did  comm_.min(converged) convergedRemote: 1
2 final convergedRemote: 1
2 Update: x^(k+1) = x^k - deltax^k 
2 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
2 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
2 Newton iteration  3 done2 , maximum relative shift = 9.899e-04
2 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
5 Solve: M deltax^k = r 
5 Newton::solveLinearSystem : entering the try section 3
5 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
5 solveLinearSystemImpl_, before converged = ls.solve 
5 7 amgbackend::solve isParallel 1
6 Solve: M deltax^k = r 
6 Newton::solveLinearSystem : entering the try section 3
6 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
6 solveLinearSystemImpl_, before converged = ls.solve 
6 7 amgbackend::solve isParallel 1
3 Solve: M deltax^k = r 
3 Newton::solveLinearSystem : entering the try section 3
3 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
3 solveLinearSystemImpl_, before converged = ls.solve 
3 7 amgbackend::solve isParallel 1
2 Solve: M deltax^k = r 
2 Newton::solveLinearSystem : entering the try section 3
2 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
2 solveLinearSystemImpl_, before converged = ls.solve 
2 7 amgbackend::solve isParallel 1
4 Solve: M deltax^k = r 
4 Newton::solveLinearSystem : entering the try section 3
4 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
4 solveLinearSystemImpl_, before converged = ls.solve 
4 7 amgbackend::solve isParallel 1
0 Solve: M deltax^k = r 
0 Newton::solveLinearSystem : entering the try section 3
0 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
0 solveLinearSystemImpl_, before converged = ls.solve 
0 7 amgbackend::solve isParallel 1
1 Solve: M deltax^k = r 
1 Newton::solveLinearSystem : entering the try section 3
1 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
1 solveLinearSystemImpl_, before converged = ls.solve 
1 7 amgbackend::solve isParallel 1
3 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
3 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
5 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
5 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
6 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
6 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
1 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
1 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
2 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
2 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
4 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
4 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
0 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
0 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 7
5 7 before Dune::BiCGSTABSolver 
5 Dune::BiCGSTABSolver.apply(x, b, result_) 
5 BiCGSTABSolver::apply 
5 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
1 7 before Dune::BiCGSTABSolver 
1 Dune::BiCGSTABSolver.apply(x, b, result_) 
1 BiCGSTABSolver::apply 
1 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
2 7 before Dune::BiCGSTABSolver 
2 Dune::BiCGSTABSolver.apply(x, b, result_) 
2 BiCGSTABSolver::apply 
2 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
4 7 before Dune::BiCGSTABSolver 
4 Dune::BiCGSTABSolver.apply(x, b, result_) 
4 BiCGSTABSolver::apply 
4 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
6 7 before Dune::BiCGSTABSolver 
6 Dune::BiCGSTABSolver.apply(x, b, result_) 
6 BiCGSTABSolver::apply 
6 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
3 7 before Dune::BiCGSTABSolver 
3 Dune::BiCGSTABSolver.apply(x, b, result_) 
3 BiCGSTABSolver::apply 
3 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
Using a direct coarse solver (SuperLU)
Building hierarchy of 1 levels (inclusive coarse solver) took 0.111976 seconds.
0 7 before Dune::BiCGSTABSolver 
0 Dune::BiCGSTABSolver.apply(x, b, result_) 
0 BiCGSTABSolver::apply 
0 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
0 === BiCGSTABSolver
 Iter          Defect            Rate
    0      5.48612e-13
0  (it = 0.5; it < _maxit; it+=.5) 0.5 250
0  before p = r 
0  before  y = W^-1 * p 
0  before _prec->apply(y,p); 0.5 250
0 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
1 === BiCGSTABSolver
 Iter          Defect            Rate
    0      5.48612e-13
1  (it = 0.5; it < _maxit; it+=.5) 0.5 250
1  before p = r 
1  before  y = W^-1 * p 
1  before _prec->apply(y,p); 0.5 250
1 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
2 === BiCGSTABSolver
 Iter          Defect            Rate
    0      5.48612e-13
2  (it = 0.5; it < _maxit; it+=.5) 0.5 250
2  before p = r 
2  before  y = W^-1 * p 
2  before _prec->apply(y,p); 0.5 250
2 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
4 === BiCGSTABSolver
 Iter          Defect            Rate
    0      5.48612e-13
4  (it = 0.5; it < _maxit; it+=.5) 0.5 250
4  before p = r 
4  before  y = W^-1 * p 
4  before _prec->apply(y,p); 0.5 250
4 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
6 === BiCGSTABSolver
 Iter          Defect            Rate
    0      5.48612e-13
6  (it = 0.5; it < _maxit; it+=.5) 0.5 250
6  before p = r 
6  before  y = W^-1 * p 
6  before _prec->apply(y,p); 0.5 250
6 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
3 === BiCGSTABSolver
 Iter          Defect            Rate
    0      5.48612e-13
3  (it = 0.5; it < _maxit; it+=.5) 0.5 250
3  before p = r 
3  before  y = W^-1 * p 
3  before _prec->apply(y,p); 0.5 250
3 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 7
0 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
5 === BiCGSTABSolver
 Iter          Defect            Rate
    0      5.48612e-13
5  (it = 0.5; it < _maxit; it+=.5) 0.5 250
5  before p = r 
5  before  y = W^-1 * p 
5  before _prec->apply(y,p); 0.5 250
5 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
0 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
3 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
3 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
3  before v = A * y 
1 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
2 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
2 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
4 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
4 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
4  before v = A * y 
4  before  alpha = rho_new / < rt, v > 
6 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
6 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
6  before v = A * y 
6  before  alpha = rho_new / < rt, v > 
5 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
5 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
5  before v = A * y 
0 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
0  before v = A * y 
3  before  alpha = rho_new / < rt, v > 
1 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
1  before v = A * y 
2  before v = A * y 
2  before  alpha = rho_new / < rt, v > 
5  before  alpha = rho_new / < rt, v > 
5  before alpha = rho_new / h; 
5  before apply first correction to x 
5  before r = r - alpha*v 
5  before test stop criteria 
5 istlsolver::printOutput 
  0.5      4.00695e-29       7.3038e-17
5 === rate=5.33455e-33, T=0.122088, TIT=0.244176, IT=0.5
5 finished solveLinearSystemImpl_ 1
5 converged value: 1 7 1 let s go get convergedRemote
5 to  comm_.min(converged) 
6  before alpha = rho_new / h; 
6  before apply first correction to x 
6  before r = r - alpha*v 
6  before test stop criteria 
6 istlsolver::printOutput 
  0.5      4.00695e-29       7.3038e-17
6 === rate=5.33455e-33, T=0.122004, TIT=0.244009, IT=0.5
6 finished solveLinearSystemImpl_ 1
6 converged value: 1 7 1 let s go get convergedRemote
6 to  comm_.min(converged) 
2  before alpha = rho_new / h; 
2  before apply first correction to x 
2  before r = r - alpha*v 
2  before test stop criteria 
2 istlsolver::printOutput 
  0.5      4.00695e-29       7.3038e-17
2 === rate=5.33455e-33, T=0.121996, TIT=0.243992, IT=0.5
2 finished solveLinearSystemImpl_ 1
2 converged value: 1 7 1 let s go get convergedRemote
2 to  comm_.min(converged) 
0  before  alpha = rho_new / < rt, v > 
0  before alpha = rho_new / h; 
0  before apply first correction to x 
0  before r = r - alpha*v 
0  before test stop criteria 
0 istlsolver::printOutput 
  0.5      4.00695e-29       7.3038e-17
0 === rate=5.33455e-33, T=0.018028, TIT=0.036056, IT=0.5
3  before alpha = rho_new / h; 
3  before apply first correction to x 
3  before r = r - alpha*v 
3  before test stop criteria 
3 istlsolver::printOutput 
  0.5      4.00695e-29       7.3038e-17
3 === rate=5.33455e-33, T=0.12209, TIT=0.24418, IT=0.5
3 finished solveLinearSystemImpl_ 1
3 converged value: 1 7 1 let s go get convergedRemote
3 to  comm_.min(converged) 
1  before  alpha = rho_new / < rt, v > 
1  before alpha = rho_new / h; 
1  before apply first correction to x 
1  before r = r - alpha*v 
1  before test stop criteria 
1 istlsolver::printOutput 
  0.5      4.00695e-29       7.3038e-17
1 === rate=5.33455e-33, T=0.122075, TIT=0.24415, IT=0.5
1 finished solveLinearSystemImpl_ 1
1 converged value: 1 7 1 let s go get convergedRemote
1 to  comm_.min(converged) 
4  before alpha = rho_new / h; 
4  before apply first correction to x 
4  before r = r - alpha*v 
4  before test stop criteria 
4 istlsolver::printOutput 
  0.5      4.00695e-29       7.3038e-17
4 === rate=5.33455e-33, T=0.122003, TIT=0.244006, IT=0.5
4 finished solveLinearSystemImpl_ 1
4 converged value: 1 7 1 let s go get convergedRemote
4 to  comm_.min(converged) 
1 did  comm_.min(converged) convergedRemote: 1
1 final convergedRemote: 1
1 Update: x^(k+1) = x^k - deltax^k 
1 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
1 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
1 Newton iteration  4 done1 , maximum relative shift = 1.541e-04
1 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
4 did  comm_.min(converged) convergedRemote: 1
4 final convergedRemote: 1
4 Update: x^(k+1) = x^k - deltax^k 
4 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
4 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
4 Newton iteration  4 done4 , maximum relative shift = 1.541e-04
4 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
5 did  comm_.min(converged) convergedRemote: 1
5 final convergedRemote: 1
5 Update: x^(k+1) = x^k - deltax^k 
5 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
5 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
5 Newton iteration  4 done5 , maximum relative shift = 1.541e-04
5 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
6 did  comm_.min(converged) convergedRemote: 1
6 final convergedRemote: 1
6 Update: x^(k+1) = x^k - deltax^k 
6 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
6 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
6 Newton iteration  4 done6 , maximum relative shift = 1.541e-04
6 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
2 did  comm_.min(converged) convergedRemote: 1
2 final convergedRemote: 1
2 Update: x^(k+1) = x^k - deltax^k 
2 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
2 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
2 Newton iteration  4 done2 , maximum relative shift = 1.541e-04
2 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
0 finished solveLinearSystemImpl_ 1
0 converged value: 1 7 1 let s go get convergedRemote
0 to  comm_.min(converged) 
0 did  comm_.min(converged) convergedRemote: 1
0 final convergedRemote: 1
0 Update: x^(k+1) = x^k - deltax^k 
0 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
0 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
0 Newton iteration  4 done0 , maximum relative shift = 1.541e-04
0 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
3 did  comm_.min(converged) convergedRemote: 1
3 final convergedRemote: 1
3 Update: x^(k+1) = x^k - deltax^k 
3 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
3 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
3 Newton iteration  4 done3 , maximum relative shift = 1.541e-04
3 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
4 Solve: M deltax^k = r 
4 Newton::solveLinearSystem : entering the try section 4
4 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
4 solveLinearSystemImpl_, before converged = ls.solve 
4 7 amgbackend::solve isParallel 1
5 Solve: M deltax^k = r 
5 Newton::solveLinearSystem : entering the try section 4
5 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
5 solveLinearSystemImpl_, before converged = ls.solve 
5 7 amgbackend::solve isParallel 1
6 Solve: M deltax^k = r 
6 Newton::solveLinearSystem : entering the try section 4
6 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
6 solveLinearSystemImpl_, before converged = ls.solve 
6 7 amgbackend::solve isParallel 1
2 Solve: M deltax^k = r 
2 Newton::solveLinearSystem : entering the try section 4
2 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
2 solveLinearSystemImpl_, before converged = ls.solve 
2 7 amgbackend::solve isParallel 1
0 Solve: M deltax^k = r 
0 Newton::solveLinearSystem : entering the try section 4
0 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
0 solveLinearSystemImpl_, before converged = ls.solve 
0 7 amgbackend::solve isParallel 1
1 Solve: M deltax^k = r 
1 Newton::solveLinearSystem : entering the try section 4
1 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
1 solveLinearSystemImpl_, before converged = ls.solve 
1 7 amgbackend::solve isParallel 1
3 Solve: M deltax^k = r 
3 Newton::solveLinearSystem : entering the try section 4
3 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
3 solveLinearSystemImpl_, before converged = ls.solve 
3 7 amgbackend::solve isParallel 1
5 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
5 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
6 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
6 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
2 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
2 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
0 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
0 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 7
3 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
3 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
1 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
1 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
4 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
4 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 7
2 7 before Dune::BiCGSTABSolver 
2 Dune::BiCGSTABSolver.apply(x, b, result_) 
2 BiCGSTABSolver::apply 
2 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 7
3 7 before Dune::BiCGSTABSolver 
3 Dune::BiCGSTABSolver.apply(x, b, result_) 
3 BiCGSTABSolver::apply 
3 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
1 7 before Dune::BiCGSTABSolver 
1 Dune::BiCGSTABSolver.apply(x, b, result_) 
1 BiCGSTABSolver::apply 
1 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
4 7 before Dune::BiCGSTABSolver 
4 Dune::BiCGSTABSolver.apply(x, b, result_) 
4 BiCGSTABSolver::apply 
4 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
5 7 before Dune::BiCGSTABSolver 
5 Dune::BiCGSTABSolver.apply(x, b, result_) 
5 BiCGSTABSolver::apply 
5 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
6 7 before Dune::BiCGSTABSolver 
6 Dune::BiCGSTABSolver.apply(x, b, result_) 
6 BiCGSTABSolver::apply 
6 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
Using a direct coarse solver (SuperLU)
Building hierarchy of 1 levels (inclusive coarse solver) took 0.112039 seconds.
0 7 before Dune::BiCGSTABSolver 
0 Dune::BiCGSTABSolver.apply(x, b, result_) 
0 BiCGSTABSolver::apply 
0 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
0 === BiCGSTABSolver
 Iter          Defect            Rate
    0      9.69685e-14
0  (it = 0.5; it < _maxit; it+=.5) 0.5 250
0  before p = r 
0  before  y = W^-1 * p 
0  before _prec->apply(y,p); 0.5 250
6 === BiCGSTABSolver
 Iter          Defect            Rate
    0      9.69685e-14
6  (it = 0.5; it < _maxit; it+=.5) 0.5 250
6  before p = r 
6  before  y = W^-1 * p 
6  before _prec->apply(y,p); 0.5 250
6 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
3 === BiCGSTABSolver
 Iter          Defect            Rate
    0      9.69685e-14
3  (it = 0.5; it < _maxit; it+=.5) 0.5 250
3  before p = r 
3  before  y = W^-1 * p 
3  before _prec->apply(y,p); 0.5 250
3 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
1 === BiCGSTABSolver
 Iter          Defect            Rate
    0      9.69685e-14
1  (it = 0.5; it < _maxit; it+=.5) 0.5 250
1  before p = r 
1  before  y = W^-1 * p 
1  before _prec->apply(y,p); 0.5 250
1 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
4 === BiCGSTABSolver
 Iter          Defect            Rate
    0      9.69685e-14
4  (it = 0.5; it < _maxit; it+=.5) 0.5 250
4  before p = r 
4  before  y = W^-1 * p 
4  before _prec->apply(y,p); 0.5 250
4 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
5 === BiCGSTABSolver
 Iter          Defect            Rate
    0      9.69685e-14
5  (it = 0.5; it < _maxit; it+=.5) 0.5 250
5  before p = r 
5  before  y = W^-1 * p 
5  before _prec->apply(y,p); 0.5 250
5 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
2 === BiCGSTABSolver
 Iter          Defect            Rate
    0      9.69685e-14
2  (it = 0.5; it < _maxit; it+=.5) 0.5 250
2  before p = r 
2  before  y = W^-1 * p 
2  before _prec->apply(y,p); 0.5 250
2 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
0 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 7
0 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
2 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
2 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
6 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
6 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
6  before v = A * y 
3 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
3 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
3  before v = A * y 
1 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
4 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
4 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
4  before v = A * y 
5 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
5 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
5  before v = A * y 
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
0 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
5  before  alpha = rho_new / < rt, v > 
6  before  alpha = rho_new / < rt, v > 
3  before  alpha = rho_new / < rt, v > 
1 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
4  before  alpha = rho_new / < rt, v > 
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
0 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
4  before alpha = rho_new / h; 
4  before apply first correction to x 
4  before r = r - alpha*v 
4  before test stop criteria 
4 istlsolver::printOutput 
  0.5      7.79823e-33      8.04202e-20
4 === rate=6.46741e-39, T=0.122685, TIT=0.24537, IT=0.5
4 finished solveLinearSystemImpl_ 1
4 converged value: 1 7 1 let s go get convergedRemote
4 to  comm_.min(converged) 
2  before v = A * y 
2  before  alpha = rho_new / < rt, v > 
2  before alpha = rho_new / h; 
2  before apply first correction to x 
2  before r = r - alpha*v 
2  before test stop criteria 
2 istlsolver::printOutput 
  0.5      7.79823e-33      8.04202e-20
2 === rate=6.46741e-39, T=0.122684, TIT=0.245368, IT=0.5
2 finished solveLinearSystemImpl_ 1
2 converged value: 1 7 1 let s go get convergedRemote
2 to  comm_.min(converged) 
5  before alpha = rho_new / h; 
5  before apply first correction to x 
5  before r = r - alpha*v 
5  before test stop criteria 
5 istlsolver::printOutput 
  0.5      7.79823e-33      8.04202e-20
5 === rate=6.46741e-39, T=0.122709, TIT=0.245419, IT=0.5
5 finished solveLinearSystemImpl_ 1
5 converged value: 1 7 1 let s go get convergedRemote
5 to  comm_.min(converged) 
6  before alpha = rho_new / h; 
6  before apply first correction to x 
6  before r = r - alpha*v 
6  before test stop criteria 
6 istlsolver::printOutput 
  0.5      7.79823e-33      8.04202e-20
6 === rate=6.46741e-39, T=0.122686, TIT=0.245372, IT=0.5
6 finished solveLinearSystemImpl_ 1
6 converged value: 1 7 1 let s go get convergedRemote
6 to  comm_.min(converged) 
3  before alpha = rho_new / h; 
3  before apply first correction to x 
3  before r = r - alpha*v 
3  before test stop criteria 
3 istlsolver::printOutput 
  0.5      7.79823e-33      8.04202e-20
3 === rate=6.46741e-39, T=0.12271, TIT=0.245419, IT=0.5
3 finished solveLinearSystemImpl_ 1
3 converged value: 1 7 1 let s go get convergedRemote
3 to  comm_.min(converged) 
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
1  before v = A * y 
1  before  alpha = rho_new / < rt, v > 
1  before alpha = rho_new / h; 
1  before apply first correction to x 
1  before r = r - alpha*v 
1  before test stop criteria 
1 istlsolver::printOutput 
  0.5      7.79823e-33      8.04202e-20
1 === rate=6.46741e-39, T=0.122716, TIT=0.245432, IT=0.5
1 finished solveLinearSystemImpl_ 1
1 converged value: 1 7 1 let s go get convergedRemote
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
0  before v = A * y 
0  before  alpha = rho_new / < rt, v > 
0  before alpha = rho_new / h; 
0  before apply first correction to x 
0  before r = r - alpha*v 
0  before test stop criteria 
0 istlsolver::printOutput 
  0.5      7.79823e-33      8.04202e-20
0 === rate=6.46741e-39, T=0.0185868, TIT=0.0371737, IT=0.5
1 to  comm_.min(converged) 
1 did  comm_.min(converged) convergedRemote: 1
1 final convergedRemote: 1
1 Update: x^(k+1) = x^k - deltax^k 
1 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
1 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
1 Newton iteration  5 done1 , maximum relative shift = 4.084e-05
1 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
4 did  comm_.min(converged) convergedRemote: 1
4 final convergedRemote: 1
4 Update: x^(k+1) = x^k - deltax^k 
4 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
4 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
4 Newton iteration  5 done4 , maximum relative shift = 4.084e-05
4 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
2 did  comm_.min(converged) convergedRemote: 1
2 final convergedRemote: 1
2 Update: x^(k+1) = x^k - deltax^k 
2 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
2 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
2 Newton iteration  5 done2 , maximum relative shift = 4.084e-05
2 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
5 did  comm_.min(converged) convergedRemote: 1
5 final convergedRemote: 1
5 Update: x^(k+1) = x^k - deltax^k 
5 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
5 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
5 Newton iteration  5 done5 , maximum relative shift = 4.084e-05
5 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
6 did  comm_.min(converged) convergedRemote: 1
6 final convergedRemote: 1
6 Update: x^(k+1) = x^k - deltax^k 
6 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
6 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
6 Newton iteration  5 done6 , maximum relative shift = 4.084e-05
6 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
3 did  comm_.min(converged) convergedRemote: 1
3 final convergedRemote: 1
3 Update: x^(k+1) = x^k - deltax^k 
3 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
3 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
3 Newton iteration  5 done3 , maximum relative shift = 4.084e-05
3 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
0 finished solveLinearSystemImpl_ 1
0 converged value: 1 7 1 let s go get convergedRemote
0 to  comm_.min(converged) 
0 did  comm_.min(converged) convergedRemote: 1
0 final convergedRemote: 1
0 Update: x^(k+1) = x^k - deltax^k 
0 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
0 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
0 Newton iteration  5 done0 , maximum relative shift = 4.084e-05
0 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
2 Solve: M deltax^k = r 
2 Newton::solveLinearSystem : entering the try section 5
2 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
2 solveLinearSystemImpl_, before converged = ls.solve 
2 7 amgbackend::solve isParallel 1
5 Solve: M deltax^k = r 
5 Newton::solveLinearSystem : entering the try section 5
5 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
5 solveLinearSystemImpl_, before converged = ls.solve 
5 7 amgbackend::solve isParallel 1
6 Solve: M deltax^k = r 
6 Newton::solveLinearSystem : entering the try section 5
6 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
6 solveLinearSystemImpl_, before converged = ls.solve 
6 7 amgbackend::solve isParallel 1
0 Solve: M deltax^k = r 
0 Newton::solveLinearSystem : entering the try section 5
0 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
0 solveLinearSystemImpl_, before converged = ls.solve 
0 7 amgbackend::solve isParallel 1
3 Solve: M deltax^k = r 
3 Newton::solveLinearSystem : entering the try section 5
3 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
3 solveLinearSystemImpl_, before converged = ls.solve 
3 7 amgbackend::solve isParallel 1
1 Solve: M deltax^k = r 
1 Newton::solveLinearSystem : entering the try section 5
1 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
1 solveLinearSystemImpl_, before converged = ls.solve 
1 7 amgbackend::solve isParallel 1
4 Solve: M deltax^k = r 
4 Newton::solveLinearSystem : entering the try section 5
4 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
4 solveLinearSystemImpl_, before converged = ls.solve 
4 7 amgbackend::solve isParallel 1
1 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
1 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
6 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
6 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
2 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
2 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
5 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
5 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
4 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
4 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
0 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
0 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 7
3 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
3 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 7
4 7 before Dune::BiCGSTABSolver 
4 Dune::BiCGSTABSolver.apply(x, b, result_) 
4 BiCGSTABSolver::apply 
4 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
3 7 before Dune::BiCGSTABSolver 
3 Dune::BiCGSTABSolver.apply(x, b, result_) 
3 BiCGSTABSolver::apply 
3 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
5 7 before Dune::BiCGSTABSolver 
5 Dune::BiCGSTABSolver.apply(x, b, result_) 
5 BiCGSTABSolver::apply 
5 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
1 7 before Dune::BiCGSTABSolver 
1 Dune::BiCGSTABSolver.apply(x, b, result_) 
1 BiCGSTABSolver::apply 
1 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
6 7 before Dune::BiCGSTABSolver 
6 Dune::BiCGSTABSolver.apply(x, b, result_) 
6 BiCGSTABSolver::apply 
6 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
2 7 before Dune::BiCGSTABSolver 
2 Dune::BiCGSTABSolver.apply(x, b, result_) 
2 BiCGSTABSolver::apply 
2 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 7
Using a direct coarse solver (SuperLU)
Building hierarchy of 1 levels (inclusive coarse solver) took 0.112752 seconds.
0 7 before Dune::BiCGSTABSolver 
0 Dune::BiCGSTABSolver.apply(x, b, result_) 
0 BiCGSTABSolver::apply 
0 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
0 === BiCGSTABSolver
 Iter          Defect            Rate
    0      7.41186e-14
0  (it = 0.5; it < _maxit; it+=.5) 0.5 250
0  before p = r 
0  before  y = W^-1 * p 
0  before _prec->apply(y,p); 0.5 250
2 === BiCGSTABSolver
 Iter          Defect            Rate
    0      7.41186e-14
2  (it = 0.5; it < _maxit; it+=.5) 0.5 250
2  before p = r 
2  before  y = W^-1 * p 
2  before _prec->apply(y,p); 0.5 250
2 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
4 === BiCGSTABSolver
 Iter          Defect            Rate
    0      7.41186e-14
4  (it = 0.5; it < _maxit; it+=.5) 0.5 250
4  before p = r 
4  before  y = W^-1 * p 
4  before _prec->apply(y,p); 0.5 250
4 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
3 === BiCGSTABSolver
 Iter          Defect            Rate
    0      7.41186e-14
3  (it = 0.5; it < _maxit; it+=.5) 0.5 250
3  before p = r 
3  before  y = W^-1 * p 
3  before _prec->apply(y,p); 0.5 250
3 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
5 === BiCGSTABSolver
 Iter          Defect            Rate
    0      7.41186e-14
5  (it = 0.5; it < _maxit; it+=.5) 0.5 250
5  before p = r 
5  before  y = W^-1 * p 
5  before _prec->apply(y,p); 0.5 250
5 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
1 === BiCGSTABSolver
 Iter          Defect            Rate
    0      7.41186e-14
1  (it = 0.5; it < _maxit; it+=.5) 0.5 250
1  before p = r 
1  before  y = W^-1 * p 
1  before _prec->apply(y,p); 0.5 250
1 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
6 === BiCGSTABSolver
 Iter          Defect            Rate
    0      7.41186e-14
6  (it = 0.5; it < _maxit; it+=.5) 0.5 250
6  before p = r 
6  before  y = W^-1 * p 
6  before _prec->apply(y,p); 0.5 250
6 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
0 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 7
0 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
3 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
3 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
3  before v = A * y 
5 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
5 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
5  before v = A * y 
1 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
0 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
6 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
6 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
6  before v = A * y 
6  before  alpha = rho_new / < rt, v > 
2 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
2 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
4 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
4 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
4  before v = A * y 
0 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
0  before v = A * y 
0  before  alpha = rho_new / < rt, v > 
2  before v = A * y 
2  before  alpha = rho_new / < rt, v > 
1 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
1  before v = A * y 
3  before  alpha = rho_new / < rt, v > 
5  before  alpha = rho_new / < rt, v > 
4  before  alpha = rho_new / < rt, v > 
4  before alpha = rho_new / h; 
4  before apply first correction to x 
4  before r = r - alpha*v 
4  before test stop criteria 
4 istlsolver::printOutput 
  0.5      4.39297e-30      5.92695e-17
4 === rate=3.51287e-33, T=0.126764, TIT=0.253529, IT=0.5
4 finished solveLinearSystemImpl_ 1
4 converged value: 1 7 1 let s go get convergedRemote
4 to  comm_.min(converged) 
5  before alpha = rho_new / h; 
5  before apply first correction to x 
5  before r = r - alpha*v 
5  before test stop criteria 
5 istlsolver::printOutput 
  0.5      4.39297e-30      5.92695e-17
5 === rate=3.51287e-33, T=0.126851, TIT=0.253701, IT=0.5
5 finished solveLinearSystemImpl_ 1
5 converged value: 1 7 1 let s go get convergedRemote
5 to  comm_.min(converged) 
6  before alpha = rho_new / h; 
6  before apply first correction to x 
6  before r = r - alpha*v 
6  before test stop criteria 
6 istlsolver::printOutput 
  0.5      4.39297e-30      5.92695e-17
6 === rate=3.51287e-33, T=0.126763, TIT=0.253526, IT=0.5
6 finished solveLinearSystemImpl_ 1
6 converged value: 1 7 1 let s go get convergedRemote
6 to  comm_.min(converged) 
0  before alpha = rho_new / h; 
0  before apply first correction to x 
0  before r = r - alpha*v 
0  before test stop criteria 
0 istlsolver::printOutput 
  0.5      4.39297e-30      5.92695e-17
0 === rate=3.51287e-33, T=0.0218728, TIT=0.0437457, IT=0.5
2  before alpha = rho_new / h; 
2  before apply first correction to x 
2  before r = r - alpha*v 
2  before test stop criteria 
2 istlsolver::printOutput 
  0.5      4.39297e-30      5.92695e-17
2 === rate=3.51287e-33, T=0.126763, TIT=0.253526, IT=0.5
2 finished solveLinearSystemImpl_ 1
2 converged value: 1 7 1 let s go get convergedRemote
2 to  comm_.min(converged) 
1  before  alpha = rho_new / < rt, v > 
1  before alpha = rho_new / h; 
1  before apply first correction to x 
1  before r = r - alpha*v 
1  before test stop criteria 
1 istlsolver::printOutput 
  0.5      4.39297e-30      5.92695e-17
1 === rate=3.51287e-33, T=0.126853, TIT=0.253705, IT=0.5
1 finished solveLinearSystemImpl_ 1
1 converged value: 1 7 1 let s go get convergedRemote
1 to  comm_.min(converged) 
3  before alpha = rho_new / h; 
3  before apply first correction to x 
3  before r = r - alpha*v 
3  before test stop criteria 
3 istlsolver::printOutput 
  0.5      4.39297e-30      5.92695e-17
3 === rate=3.51287e-33, T=0.126849, TIT=0.253699, IT=0.5
3 finished solveLinearSystemImpl_ 1
3 converged value: 1 7 1 let s go get convergedRemote
3 to  comm_.min(converged) 
4 did  comm_.min(converged) convergedRemote: 1
4 final convergedRemote: 1
4 Update: x^(k+1) = x^k - deltax^k 
4 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
4 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
4 Newton iteration  6 done4 , maximum relative shift = 2.082e-05
4 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
5 did  comm_.min(converged) convergedRemote: 1
5 final convergedRemote: 1
5 Update: x^(k+1) = x^k - deltax^k 
5 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
5 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
5 Newton iteration  6 done5 , maximum relative shift = 2.082e-05
5 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
6 did  comm_.min(converged) convergedRemote: 1
6 final convergedRemote: 1
6 Update: x^(k+1) = x^k - deltax^k 
6 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
6 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
6 Newton iteration  6 done6 , maximum relative shift = 2.082e-05
6 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
0 finished solveLinearSystemImpl_ 1
0 converged value: 1 7 1 let s go get convergedRemote
0 to  comm_.min(converged) 
0 did  comm_.min(converged) convergedRemote: 1
0 final convergedRemote: 1
0 Update: x^(k+1) = x^k - deltax^k 
0 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
0 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
0 Newton iteration  6 done0 , maximum relative shift = 2.082e-05
0 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
2 did  comm_.min(converged) convergedRemote: 1
2 final convergedRemote: 1
2 Update: x^(k+1) = x^k - deltax^k 
2 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
2 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
2 Newton iteration  6 done2 , maximum relative shift = 2.082e-05
2 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
1 did  comm_.min(converged) convergedRemote: 1
1 final convergedRemote: 1
1 Update: x^(k+1) = x^k - deltax^k 
1 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
1 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
1 Newton iteration  6 done1 , maximum relative shift = 2.082e-05
1 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
3 did  comm_.min(converged) convergedRemote: 1
3 final convergedRemote: 1
3 Update: x^(k+1) = x^k - deltax^k 
3 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
3 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
3 Newton iteration  6 done3 , maximum relative shift = 2.082e-05
3 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
1 Solve: M deltax^k = r 
1 Newton::solveLinearSystem : entering the try section 6
1 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
1 solveLinearSystemImpl_, before converged = ls.solve 
1 7 amgbackend::solve isParallel 1
4 Solve: M deltax^k = r 
4 Newton::solveLinearSystem : entering the try section 6
4 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
4 solveLinearSystemImpl_, before converged = ls.solve 
4 7 amgbackend::solve isParallel 1
5 Solve: M deltax^k = r 
5 Newton::solveLinearSystem : entering the try section 6
5 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
5 solveLinearSystemImpl_, before converged = ls.solve 
5 7 amgbackend::solve isParallel 1
6 Solve: M deltax^k = r 
6 Newton::solveLinearSystem : entering the try section 6
6 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
6 solveLinearSystemImpl_, before converged = ls.solve 
6 7 amgbackend::solve isParallel 1
0 Solve: M deltax^k = r 
0 Newton::solveLinearSystem : entering the try section 6
0 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
0 solveLinearSystemImpl_, before converged = ls.solve 
0 7 amgbackend::solve isParallel 1
2 Solve: M deltax^k = r 
2 Newton::solveLinearSystem : entering the try section 6
2 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
2 solveLinearSystemImpl_, before converged = ls.solve 
2 7 amgbackend::solve isParallel 1
3 Solve: M deltax^k = r 
3 Newton::solveLinearSystem : entering the try section 6
3 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
3 solveLinearSystemImpl_, before converged = ls.solve 
3 7 amgbackend::solve isParallel 1
5 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
5 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
6 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
6 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
0 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
0 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 7
3 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
3 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
2 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
2 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
1 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
1 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
4 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
4 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 7
3 7 before Dune::BiCGSTABSolver 
3 Dune::BiCGSTABSolver.apply(x, b, result_) 
3 BiCGSTABSolver::apply 
3 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
2 7 before Dune::BiCGSTABSolver 
2 Dune::BiCGSTABSolver.apply(x, b, result_) 
2 BiCGSTABSolver::apply 
2 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
1 7 before Dune::BiCGSTABSolver 
1 Dune::BiCGSTABSolver.apply(x, b, result_) 
1 BiCGSTABSolver::apply 
1 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
4 7 before Dune::BiCGSTABSolver 
4 Dune::BiCGSTABSolver.apply(x, b, result_) 
4 BiCGSTABSolver::apply 
4 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
5 7 before Dune::BiCGSTABSolver 
5 Dune::BiCGSTABSolver.apply(x, b, result_) 
5 BiCGSTABSolver::apply 
5 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
6 7 before Dune::BiCGSTABSolver 
6 Dune::BiCGSTABSolver.apply(x, b, result_) 
6 BiCGSTABSolver::apply 
6 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
Using a direct coarse solver (SuperLU)
Building hierarchy of 1 levels (inclusive coarse solver) took 0.112498 seconds.
0 7 before Dune::BiCGSTABSolver 
0 Dune::BiCGSTABSolver.apply(x, b, result_) 
0 BiCGSTABSolver::apply 
0 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
0 === BiCGSTABSolver
 Iter          Defect            Rate
    0      1.30951e-14
0  (it = 0.5; it < _maxit; it+=.5) 0.5 250
0  before p = r 
0  before  y = W^-1 * p 
0  before _prec->apply(y,p); 0.5 250
0 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
6 === BiCGSTABSolver
 Iter          Defect            Rate
    0      1.30951e-14
6  (it = 0.5; it < _maxit; it+=.5) 0.5 250
6  before p = r 
6  before  y = W^-1 * p 
6  before _prec->apply(y,p); 0.5 250
6 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
3 === BiCGSTABSolver
 Iter          Defect            Rate
    0      1.30951e-14
3  (it = 0.5; it < _maxit; it+=.5) 0.5 250
3  before p = r 
3  before  y = W^-1 * p 
3  before _prec->apply(y,p); 0.5 250
3 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
2 === BiCGSTABSolver
 Iter          Defect            Rate
    0      1.30951e-14
2  (it = 0.5; it < _maxit; it+=.5) 0.5 250
2  before p = r 
2  before  y = W^-1 * p 
2  before _prec->apply(y,p); 0.5 250
2 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
1 === BiCGSTABSolver
 Iter          Defect            Rate
    0      1.30951e-14
1  (it = 0.5; it < _maxit; it+=.5) 0.5 250
1  before p = r 
1  before  y = W^-1 * p 
1  before _prec->apply(y,p); 0.5 250
1 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
4 === BiCGSTABSolver
 Iter          Defect            Rate
    0      1.30951e-14
4  (it = 0.5; it < _maxit; it+=.5) 0.5 250
4  before p = r 
4  before  y = W^-1 * p 
4  before _prec->apply(y,p); 0.5 250
4 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
5 === BiCGSTABSolver
 Iter          Defect            Rate
    0      1.30951e-14
5  (it = 0.5; it < _maxit; it+=.5) 0.5 250
5  before p = r 
5  before  y = W^-1 * p 
5  before _prec->apply(y,p); 0.5 250
5 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 7
0 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
1 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
4 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
4 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
4  before v = A * y 
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
0 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
5 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
5 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
5  before v = A * y 
6 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
6 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
6  before v = A * y 
3 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
3 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
3  before v = A * y 
2 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
2 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
1 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
1  before v = A * y 
1  before  alpha = rho_new / < rt, v > 
2  before v = A * y 
0 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
0  before v = A * y 
5  before  alpha = rho_new / < rt, v > 
6  before  alpha = rho_new / < rt, v > 
3  before  alpha = rho_new / < rt, v > 
4  before  alpha = rho_new / < rt, v > 
2  before  alpha = rho_new / < rt, v > 
2  before alpha = rho_new / h; 
2  before apply first correction to x 
2  before r = r - alpha*v 
2  before test stop criteria 
2 istlsolver::printOutput 
  0.5      5.31645e-34      4.05988e-20
2 === rate=1.64827e-39, T=0.126057, TIT=0.252113, IT=0.5
2 finished solveLinearSystemImpl_ 1
2 converged value: 1 7 1 let s go get convergedRemote
2 to  comm_.min(converged) 
0  before  alpha = rho_new / < rt, v > 
0  before alpha = rho_new / h; 
0  before apply first correction to x 
0  before r = r - alpha*v 
0  before test stop criteria 
0 istlsolver::printOutput 
  0.5      5.31645e-34      4.05988e-20
0 === rate=1.64827e-39, T=0.021692, TIT=0.0433839, IT=0.5
5  before alpha = rho_new / h; 
5  before apply first correction to x 
5  before r = r - alpha*v 
5  before test stop criteria 
5 istlsolver::printOutput 
  0.5      5.31645e-34      4.05988e-20
5 === rate=1.64827e-39, T=0.125969, TIT=0.251939, IT=0.5
5 finished solveLinearSystemImpl_ 1
5 converged value: 1 7 1 let s go get convergedRemote
5 to  comm_.min(converged) 
6  before alpha = rho_new / h; 
6  before apply first correction to x 
6  before r = r - alpha*v 
6  before test stop criteria 
6 istlsolver::printOutput 
  0.5      5.31645e-34      4.05988e-20
6 === rate=1.64827e-39, T=0.126057, TIT=0.252113, IT=0.5
6 finished solveLinearSystemImpl_ 1
6 converged value: 1 7 1 let s go get convergedRemote
6 to  comm_.min(converged) 
3  before alpha = rho_new / h; 
3  before apply first correction to x 
3  before r = r - alpha*v 
3  before test stop criteria 
3 istlsolver::printOutput 
  0.5      5.31645e-34      4.05988e-20
3 === rate=1.64827e-39, T=0.125969, TIT=0.251938, IT=0.5
3 finished solveLinearSystemImpl_ 1
3 converged value: 1 7 1 let s go get convergedRemote
3 to  comm_.min(converged) 
1  before alpha = rho_new / h; 
1  before apply first correction to x 
1  before r = r - alpha*v 
1  before test stop criteria 
1 istlsolver::printOutput 
  0.5      5.31645e-34      4.05988e-20
1 === rate=1.64827e-39, T=0.125971, TIT=0.251942, IT=0.5
1 finished solveLinearSystemImpl_ 1
1 converged value: 1 7 1 let s go get convergedRemote
1 to  comm_.min(converged) 
4  before alpha = rho_new / h; 
4  before apply first correction to x 
4  before r = r - alpha*v 
4  before test stop criteria 
4 istlsolver::printOutput 
  0.5      5.31645e-34      4.05988e-20
4 === rate=1.64827e-39, T=0.126057, TIT=0.252114, IT=0.5
4 finished solveLinearSystemImpl_ 1
4 converged value: 1 7 1 let s go get convergedRemote
4 to  comm_.min(converged) 
2 did  comm_.min(converged) convergedRemote: 1
2 final convergedRemote: 1
2 Update: x^(k+1) = x^k - deltax^k 
2 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
2 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
2 Newton iteration  7 done2 , maximum relative shift = 3.678e-06
2 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
0 finished solveLinearSystemImpl_ 1
0 converged value: 1 7 1 let s go get convergedRemote
0 to  comm_.min(converged) 
0 did  comm_.min(converged) convergedRemote: 1
0 final convergedRemote: 1
0 Update: x^(k+1) = x^k - deltax^k 
0 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
0 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
0 Newton iteration  7 done0 , maximum relative shift = 3.678e-06
0 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
5 did  comm_.min(converged) convergedRemote: 1
5 final convergedRemote: 1
5 Update: x^(k+1) = x^k - deltax^k 
5 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
5 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
5 Newton iteration  7 done5 , maximum relative shift = 3.678e-06
5 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
6 did  comm_.min(converged) convergedRemote: 1
6 final convergedRemote: 1
6 Update: x^(k+1) = x^k - deltax^k 
6 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
6 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
6 Newton iteration  7 done6 , maximum relative shift = 3.678e-06
6 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
3 did  comm_.min(converged) convergedRemote: 1
3 final convergedRemote: 1
3 Update: x^(k+1) = x^k - deltax^k 
3 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
3 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
3 Newton iteration  7 done3 , maximum relative shift = 3.678e-06
3 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
4 did  comm_.min(converged) convergedRemote: 1
4 final convergedRemote: 1
4 Update: x^(k+1) = x^k - deltax^k 
4 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
4 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
4 Newton iteration  7 done4 , maximum relative shift = 3.678e-06
4 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
1 did  comm_.min(converged) convergedRemote: 1
1 final convergedRemote: 1
1 Update: x^(k+1) = x^k - deltax^k 
1 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
1 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
1 Newton iteration  7 done1 , maximum relative shift = 3.678e-06
1 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
4 Solve: M deltax^k = r 
4 Newton::solveLinearSystem : entering the try section 7
4 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
4 solveLinearSystemImpl_, before converged = ls.solve 
4 7 amgbackend::solve isParallel 1
3 Solve: M deltax^k = r 
3 Newton::solveLinearSystem : entering the try section 7
3 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
3 solveLinearSystemImpl_, before converged = ls.solve 
3 7 amgbackend::solve isParallel 1
2 Solve: M deltax^k = r 
2 Newton::solveLinearSystem : entering the try section 7
2 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
2 solveLinearSystemImpl_, before converged = ls.solve 
2 7 amgbackend::solve isParallel 1
0 Solve: M deltax^k = r 
0 Newton::solveLinearSystem : entering the try section 7
0 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
0 solveLinearSystemImpl_, before converged = ls.solve 
0 7 amgbackend::solve isParallel 1
5 Solve: M deltax^k = r 
5 Newton::solveLinearSystem : entering the try section 7
5 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
5 solveLinearSystemImpl_, before converged = ls.solve 
5 7 amgbackend::solve isParallel 1
6 Solve: M deltax^k = r 
6 Newton::solveLinearSystem : entering the try section 7
6 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
6 solveLinearSystemImpl_, before converged = ls.solve 
6 7 amgbackend::solve isParallel 1
1 Solve: M deltax^k = r 
1 Newton::solveLinearSystem : entering the try section 7
1 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
1 solveLinearSystemImpl_, before converged = ls.solve 
1 7 amgbackend::solve isParallel 1
3 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
3 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
2 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
2 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
0 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
0 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 7
5 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
5 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
1 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
1 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
6 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
6 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
4 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
4 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 7
1 7 before Dune::BiCGSTABSolver 
1 Dune::BiCGSTABSolver.apply(x, b, result_) 
1 BiCGSTABSolver::apply 
1 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
6 7 before Dune::BiCGSTABSolver 
6 Dune::BiCGSTABSolver.apply(x, b, result_) 
6 BiCGSTABSolver::apply 
6 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
5 7 before Dune::BiCGSTABSolver 
5 Dune::BiCGSTABSolver.apply(x, b, result_) 
5 BiCGSTABSolver::apply 
5 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
3 7 before Dune::BiCGSTABSolver 
3 Dune::BiCGSTABSolver.apply(x, b, result_) 
3 BiCGSTABSolver::apply 
3 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
2 7 before Dune::BiCGSTABSolver 
2 Dune::BiCGSTABSolver.apply(x, b, result_) 
2 BiCGSTABSolver::apply 
2 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
4 7 before Dune::BiCGSTABSolver 
4 Dune::BiCGSTABSolver.apply(x, b, result_) 
4 BiCGSTABSolver::apply 
4 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
Using a direct coarse solver (SuperLU)
Building hierarchy of 1 levels (inclusive coarse solver) took 0.11187 seconds.
0 7 before Dune::BiCGSTABSolver 
0 Dune::BiCGSTABSolver.apply(x, b, result_) 
0 BiCGSTABSolver::apply 
0 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
0 === BiCGSTABSolver
 Iter          Defect            Rate
    0      2.31338e-15
0  (it = 0.5; it < _maxit; it+=.5) 0.5 250
0  before p = r 
0  before  y = W^-1 * p 
0  before _prec->apply(y,p); 0.5 250
0 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 7
0 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
4 === BiCGSTABSolver
 Iter          Defect            Rate
    0      2.31338e-15
4  (it = 0.5; it < _maxit; it+=.5) 0.5 250
4  before p = r 
4  before  y = W^-1 * p 
4  before _prec->apply(y,p); 0.5 250
4 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
1 === BiCGSTABSolver
 Iter          Defect            Rate
    0      2.31338e-15
1  (it = 0.5; it < _maxit; it+=.5) 0.5 250
1  before p = r 
1  before  y = W^-1 * p 
1  before _prec->apply(y,p); 0.5 250
1 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
6 === BiCGSTABSolver
 Iter          Defect            Rate
    0      2.31338e-15
6  (it = 0.5; it < _maxit; it+=.5) 0.5 250
6  before p = r 
6  before  y = W^-1 * p 
6  before _prec->apply(y,p); 0.5 250
6 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
5 === BiCGSTABSolver
 Iter          Defect            Rate
    0      2.31338e-15
5  (it = 0.5; it < _maxit; it+=.5) 0.5 250
5  before p = r 
5  before  y = W^-1 * p 
5  before _prec->apply(y,p); 0.5 250
5 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
3 === BiCGSTABSolver
 Iter          Defect            Rate
    0      2.31338e-15
3  (it = 0.5; it < _maxit; it+=.5) 0.5 250
3  before p = r 
3  before  y = W^-1 * p 
3  before _prec->apply(y,p); 0.5 250
3 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
2 === BiCGSTABSolver
 Iter          Defect            Rate
    0      2.31338e-15
2  (it = 0.5; it < _maxit; it+=.5) 0.5 250
2  before p = r 
2  before  y = W^-1 * p 
2  before _prec->apply(y,p); 0.5 250
2 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
0 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
4 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
4 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
4  before v = A * y 
1 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
6 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
6 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
6  before v = A * y 
5 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
5 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
5  before v = A * y 
3 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
3 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
3  before v = A * y 
2 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
2 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
6  before  alpha = rho_new / < rt, v > 
5  before  alpha = rho_new / < rt, v > 
2  before v = A * y 
3  before  alpha = rho_new / < rt, v > 
0 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
0  before v = A * y 
4  before  alpha = rho_new / < rt, v > 
1 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
3  before alpha = rho_new / h; 
3  before apply first correction to x 
3  before r = r - alpha*v 
3  before test stop criteria 
3 istlsolver::printOutput 
  0.5      1.19244e-31      5.15455e-17
3 === rate=2.65694e-33, T=0.122762, TIT=0.245524, IT=0.5
3 finished solveLinearSystemImpl_ 1
3 converged value: 1 7 1 let s go get convergedRemote
3 to  comm_.min(converged) 
0  before  alpha = rho_new / < rt, v > 
0  before alpha = rho_new / h; 
0  before apply first correction to x 
0  before r = r - alpha*v 
0  before test stop criteria 
0 istlsolver::printOutput 
  0.5      1.19244e-31      5.15455e-17
0 === rate=2.65694e-33, T=0.0185035, TIT=0.037007, IT=0.5
4  before alpha = rho_new / h; 
4  before apply first correction to x 
4  before r = r - alpha*v 
4  before test stop criteria 
4 istlsolver::printOutput 
  0.5      1.19244e-31      5.15455e-17
4 === rate=2.65694e-33, T=0.122846, TIT=0.245693, IT=0.5
4 finished solveLinearSystemImpl_ 1
4 converged value: 1 7 1 let s go get convergedRemote
4 to  comm_.min(converged) 
5  before alpha = rho_new / h; 
5  before apply first correction to x 
5  before r = r - alpha*v 
5  before test stop criteria 
5 istlsolver::printOutput 
  0.5      1.19244e-31      5.15455e-17
5 === rate=2.65694e-33, T=0.12276, TIT=0.245521, IT=0.5
5 finished solveLinearSystemImpl_ 1
5 converged value: 1 7 1 let s go get convergedRemote
5 to  comm_.min(converged) 
6  before alpha = rho_new / h; 
6  before apply first correction to x 
6  before r = r - alpha*v 
6  before test stop criteria 
6 istlsolver::printOutput 
  0.5      1.19244e-31      5.15455e-17
6 === rate=2.65694e-33, T=0.122847, TIT=0.245695, IT=0.5
6 finished solveLinearSystemImpl_ 1
6 converged value: 1 7 1 let s go get convergedRemote
6 to  comm_.min(converged) 
1  before v = A * y 
1  before  alpha = rho_new / < rt, v > 
1  before alpha = rho_new / h; 
1  before apply first correction to x 
1  before r = r - alpha*v 
1  before test stop criteria 
1 istlsolver::printOutput 
  0.5      1.19244e-31      5.15455e-17
1 === rate=2.65694e-33, T=0.12276, TIT=0.245521, IT=0.5
1 finished solveLinearSystemImpl_ 1
1 converged value: 1 7 1 let s go get convergedRemote
1 to  comm_.min(converged) 
2  before  alpha = rho_new / < rt, v > 
2  before alpha = rho_new / h; 
2  before apply first correction to x 
2  before r = r - alpha*v 
2  before test stop criteria 
2 istlsolver::printOutput 
  0.5      1.19244e-31      5.15455e-17
2 === rate=2.65694e-33, T=0.122849, TIT=0.245697, IT=0.5
2 finished solveLinearSystemImpl_ 1
2 converged value: 1 7 1 let s go get convergedRemote
2 to  comm_.min(converged) 
5 did  comm_.min(converged) convergedRemote: 1
5 final convergedRemote: 1
5 Update: x^(k+1) = x^k - deltax^k 
5 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
5 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
5 Newton iteration  8 done5 , maximum relative shift = 6.498e-07
5 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
2 did  comm_.min(converged) convergedRemote: 1
2 final convergedRemote: 1
2 Update: x^(k+1) = x^k - deltax^k 
2 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
2 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
2 Newton iteration  8 done2 , maximum relative shift = 6.498e-07
2 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
1 did  comm_.min(converged) convergedRemote: 1
1 final convergedRemote: 1
1 Update: x^(k+1) = x^k - deltax^k 
1 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
1 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
1 Newton iteration  8 done1 , maximum relative shift = 6.498e-07
1 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
6 did  comm_.min(converged) convergedRemote: 1
6 final convergedRemote: 1
6 Update: x^(k+1) = x^k - deltax^k 
6 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
6 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
6 Newton iteration  8 done6 , maximum relative shift = 6.498e-07
6 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
3 did  comm_.min(converged) convergedRemote: 1
3 final convergedRemote: 1
3 Update: x^(k+1) = x^k - deltax^k 
3 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
3 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
3 Newton iteration  8 done3 , maximum relative shift = 6.498e-07
3 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
0 finished solveLinearSystemImpl_ 1
0 converged value: 1 7 1 let s go get convergedRemote
0 to  comm_.min(converged) 
0 did  comm_.min(converged) convergedRemote: 1
0 final convergedRemote: 1
0 Update: x^(k+1) = x^k - deltax^k 
0 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
0 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
0 Newton iteration  8 done0 , maximum relative shift = 6.498e-07
0 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
4 did  comm_.min(converged) convergedRemote: 1
4 final convergedRemote: 1
4 Update: x^(k+1) = x^k - deltax^k 
4 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
4 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
4 Newton iteration  8 done4 , maximum relative shift = 6.498e-07
4 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
0 Solve: M deltax^k = r 
0 Newton::solveLinearSystem : entering the try section 8
0 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
0 solveLinearSystemImpl_, before converged = ls.solve 
0 7 amgbackend::solve isParallel 1
5 Solve: M deltax^k = r 
5 Newton::solveLinearSystem : entering the try section 8
5 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
5 solveLinearSystemImpl_, before converged = ls.solve 
5 7 amgbackend::solve isParallel 1
4 Solve: M deltax^k = r 
4 Newton::solveLinearSystem : entering the try section 8
4 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
4 solveLinearSystemImpl_, before converged = ls.solve 
4 7 amgbackend::solve isParallel 1
2 Solve: M deltax^k = r 
2 Newton::solveLinearSystem : entering the try section 8
2 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
2 solveLinearSystemImpl_, before converged = ls.solve 
2 7 amgbackend::solve isParallel 1
1 Solve: M deltax^k = r 
1 Newton::solveLinearSystem : entering the try section 8
1 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
1 solveLinearSystemImpl_, before converged = ls.solve 
1 7 amgbackend::solve isParallel 1
6 Solve: M deltax^k = r 
6 Newton::solveLinearSystem : entering the try section 8
6 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
6 solveLinearSystemImpl_, before converged = ls.solve 
6 7 amgbackend::solve isParallel 1
3 Solve: M deltax^k = r 
3 Newton::solveLinearSystem : entering the try section 8
3 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
3 solveLinearSystemImpl_, before converged = ls.solve 
3 7 amgbackend::solve isParallel 1
6 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
6 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
5 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
5 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
0 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
0 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 7
3 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
3 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
4 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
4 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
2 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
2 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
1 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
1 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 7
3 7 before Dune::BiCGSTABSolver 
3 Dune::BiCGSTABSolver.apply(x, b, result_) 
3 BiCGSTABSolver::apply 
3 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
4 7 before Dune::BiCGSTABSolver 
4 Dune::BiCGSTABSolver.apply(x, b, result_) 
4 BiCGSTABSolver::apply 
4 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
2 7 before Dune::BiCGSTABSolver 
2 Dune::BiCGSTABSolver.apply(x, b, result_) 
2 BiCGSTABSolver::apply 
2 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
1 7 before Dune::BiCGSTABSolver 
1 Dune::BiCGSTABSolver.apply(x, b, result_) 
1 BiCGSTABSolver::apply 
1 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
6 7 before Dune::BiCGSTABSolver 
6 Dune::BiCGSTABSolver.apply(x, b, result_) 
6 BiCGSTABSolver::apply 
6 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
5 7 before Dune::BiCGSTABSolver 
5 Dune::BiCGSTABSolver.apply(x, b, result_) 
5 BiCGSTABSolver::apply 
5 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 7
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
Using a direct coarse solver (SuperLU)
Building hierarchy of 1 levels (inclusive coarse solver) took 0.112076 seconds.
0 7 before Dune::BiCGSTABSolver 
0 Dune::BiCGSTABSolver.apply(x, b, result_) 
0 BiCGSTABSolver::apply 
0 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
0 === BiCGSTABSolver
 Iter          Defect            Rate
    0      4.08693e-16
0  (it = 0.5; it < _maxit; it+=.5) 0.5 250
0  before p = r 
0  before  y = W^-1 * p 
0  before _prec->apply(y,p); 0.5 250
0 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 7
0 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
1 === BiCGSTABSolver
 Iter          Defect            Rate
    0      4.08693e-16
1  (it = 0.5; it < _maxit; it+=.5) 0.5 250
1  before p = r 
1  before  y = W^-1 * p 
1  before _prec->apply(y,p); 0.5 250
1 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
5 === BiCGSTABSolver
 Iter          Defect            Rate
    0      4.08693e-16
5  (it = 0.5; it < _maxit; it+=.5) 0.5 250
5  before p = r 
5  before  y = W^-1 * p 
5  before _prec->apply(y,p); 0.5 250
5 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
4 === BiCGSTABSolver
 Iter          Defect            Rate
    0      4.08693e-16
4  (it = 0.5; it < _maxit; it+=.5) 0.5 250
4  before p = r 
4  before  y = W^-1 * p 
4  before _prec->apply(y,p); 0.5 250
4 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
2 === BiCGSTABSolver
 Iter          Defect            Rate
    0      4.08693e-16
2  (it = 0.5; it < _maxit; it+=.5) 0.5 250
2  before p = r 
2  before  y = W^-1 * p 
2  before _prec->apply(y,p); 0.5 250
2 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
3 === BiCGSTABSolver
 Iter          Defect            Rate
    0      4.08693e-16
3  (it = 0.5; it < _maxit; it+=.5) 0.5 250
3  before p = r 
3  before  y = W^-1 * p 
3  before _prec->apply(y,p); 0.5 250
3 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
6 === BiCGSTABSolver
 Iter          Defect            Rate
    0      4.08693e-16
6  (it = 0.5; it < _maxit; it+=.5) 0.5 250
6  before p = r 
6  before  y = W^-1 * p 
6  before _prec->apply(y,p); 0.5 250
6 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
0 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
1 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
5 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
5 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
5  before v = A * y 
4 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
4 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
4  before v = A * y 
2 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
2 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
3 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
3 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
3  before v = A * y 
6 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
6 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
6  before v = A * y 
6  before  alpha = rho_new / < rt, v > 
4  before  alpha = rho_new / < rt, v > 
2  before v = A * y 
3  before  alpha = rho_new / < rt, v > 
0 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
1 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
5  before  alpha = rho_new / < rt, v > 
3  before alpha = rho_new / h; 
3  before apply first correction to x 
3  before r = r - alpha*v 
3  before test stop criteria 
3 istlsolver::printOutput 
  0.5        3.951e-33      9.66741e-18
3 === rate=9.34588e-35, T=0.122775, TIT=0.245551, IT=0.5
3 finished solveLinearSystemImpl_ 1
3 converged value: 1 7 1 let s go get convergedRemote
3 to  comm_.min(converged) 
0  before v = A * y 
0  before  alpha = rho_new / < rt, v > 
0  before alpha = rho_new / h; 
0  before apply first correction to x 
0  before r = r - alpha*v 
0  before test stop criteria 
0 istlsolver::printOutput 
  0.5        3.951e-33      9.66741e-18
0 === rate=9.34588e-35, T=0.0184349, TIT=0.0368699, IT=0.5
1  before v = A * y 
1  before  alpha = rho_new / < rt, v > 
1  before alpha = rho_new / h; 
1  before apply first correction to x 
1  before r = r - alpha*v 
1  before test stop criteria 
1 istlsolver::printOutput 
  0.5        3.951e-33      9.66741e-18
1 === rate=9.34588e-35, T=0.122774, TIT=0.245547, IT=0.5
1 finished solveLinearSystemImpl_ 1
1 converged value: 1 7 1 let s go get convergedRemote
1 to  comm_.min(converged) 
6  before alpha = rho_new / h; 
6  before apply first correction to x 
6  before r = r - alpha*v 
6  before test stop criteria 
6 istlsolver::printOutput 
  0.5        3.951e-33      9.66741e-18
6 === rate=9.34588e-35, T=0.122835, TIT=0.24567, IT=0.5
6 finished solveLinearSystemImpl_ 1
6 converged value: 1 7 1 let s go get convergedRemote
6 to  comm_.min(converged) 
4  before alpha = rho_new / h; 
4  before apply first correction to x 
4  before r = r - alpha*v 
4  before test stop criteria 
4 istlsolver::printOutput 
  0.5        3.951e-33      9.66741e-18
4 === rate=9.34588e-35, T=0.122834, TIT=0.245669, IT=0.5
4 finished solveLinearSystemImpl_ 1
4 converged value: 1 7 1 let s go get convergedRemote
4 to  comm_.min(converged) 
5  before alpha = rho_new / h; 
5  before apply first correction to x 
5  before r = r - alpha*v 
5  before test stop criteria 
5 istlsolver::printOutput 
  0.5        3.951e-33      9.66741e-18
5 === rate=9.34588e-35, T=0.122774, TIT=0.245548, IT=0.5
5 finished solveLinearSystemImpl_ 1
5 converged value: 1 7 1 let s go get convergedRemote
5 to  comm_.min(converged) 
2  before  alpha = rho_new / < rt, v > 
2  before alpha = rho_new / h; 
2  before apply first correction to x 
2  before r = r - alpha*v 
2  before test stop criteria 
2 istlsolver::printOutput 
  0.5        3.951e-33      9.66741e-18
2 === rate=9.34588e-35, T=0.122835, TIT=0.245669, IT=0.5
2 finished solveLinearSystemImpl_ 1
2 converged value: 1 7 1 let s go get convergedRemote
2 to  comm_.min(converged) 
1 did  comm_.min(converged) convergedRemote: 1
1 final convergedRemote: 1
1 Update: x^(k+1) = x^k - deltax^k 
1 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
1 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
1 Newton iteration  9 done1 , maximum relative shift = 1.148e-07
1 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
6 did  comm_.min(converged) convergedRemote: 1
6 final convergedRemote: 1
6 Update: x^(k+1) = x^k - deltax^k 
6 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
6 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
6 Newton iteration  9 done6 , maximum relative shift = 1.148e-07
6 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
2 did  comm_.min(converged) convergedRemote: 1
2 final convergedRemote: 1
2 Update: x^(k+1) = x^k - deltax^k 
2 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
2 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
2 Newton iteration  9 done2 , maximum relative shift = 1.148e-07
2 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
5 did  comm_.min(converged) convergedRemote: 1
5 final convergedRemote: 1
5 Update: x^(k+1) = x^k - deltax^k 
5 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
5 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
5 Newton iteration  9 done5 , maximum relative shift = 1.148e-07
5 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
4 did  comm_.min(converged) convergedRemote: 1
4 final convergedRemote: 1
4 Update: x^(k+1) = x^k - deltax^k 
4 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
4 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
4 Newton iteration  9 done4 , maximum relative shift = 1.148e-07
4 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
3 did  comm_.min(converged) convergedRemote: 1
3 final convergedRemote: 1
3 Update: x^(k+1) = x^k - deltax^k 
3 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
3 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
3 Newton iteration  9 done3 , maximum relative shift = 1.148e-07
3 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
0 finished solveLinearSystemImpl_ 1
0 converged value: 1 7 1 let s go get convergedRemote
0 to  comm_.min(converged) 
0 did  comm_.min(converged) convergedRemote: 1
0 final convergedRemote: 1
0 Update: x^(k+1) = x^k - deltax^k 
0 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
0 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
0 Newton iteration  9 done0 , maximum relative shift = 1.148e-07
0 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
4 Solve: M deltax^k = r 
4 Newton::solveLinearSystem : entering the try section 9
4 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
4 solveLinearSystemImpl_, before converged = ls.solve 
4 7 amgbackend::solve isParallel 1
3 Solve: M deltax^k = r 
3 Newton::solveLinearSystem : entering the try section 9
3 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
3 solveLinearSystemImpl_, before converged = ls.solve 
3 7 amgbackend::solve isParallel 1
6 Solve: M deltax^k = r 
6 Newton::solveLinearSystem : entering the try section 9
6 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
6 solveLinearSystemImpl_, before converged = ls.solve 
6 7 amgbackend::solve isParallel 1
1 Solve: M deltax^k = r 
1 Newton::solveLinearSystem : entering the try section 9
1 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
1 solveLinearSystemImpl_, before converged = ls.solve 
1 7 amgbackend::solve isParallel 1
0 Solve: M deltax^k = r 
0 Newton::solveLinearSystem : entering the try section 9
0 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
0 solveLinearSystemImpl_, before converged = ls.solve 
0 7 amgbackend::solve isParallel 1
2 Solve: M deltax^k = r 
2 Newton::solveLinearSystem : entering the try section 9
2 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
2 solveLinearSystemImpl_, before converged = ls.solve 
2 7 amgbackend::solve isParallel 1
5 Solve: M deltax^k = r 
5 Newton::solveLinearSystem : entering the try section 9
5 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
5 solveLinearSystemImpl_, before converged = ls.solve 
5 7 amgbackend::solve isParallel 1
0 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
0 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 7
2 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
2 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
1 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
1 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
4 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
4 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
3 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
3 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
6 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
6 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
5 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
5 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 7
2 7 before Dune::BiCGSTABSolver 
2 Dune::BiCGSTABSolver.apply(x, b, result_) 
2 BiCGSTABSolver::apply 
2 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
1 7 before Dune::BiCGSTABSolver 
1 Dune::BiCGSTABSolver.apply(x, b, result_) 
1 BiCGSTABSolver::apply 
1 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
4 7 before Dune::BiCGSTABSolver 
4 Dune::BiCGSTABSolver.apply(x, b, result_) 
4 BiCGSTABSolver::apply 
4 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
3 7 before Dune::BiCGSTABSolver 
3 Dune::BiCGSTABSolver.apply(x, b, result_) 
3 BiCGSTABSolver::apply 
3 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 7
6 7 before Dune::BiCGSTABSolver 
6 Dune::BiCGSTABSolver.apply(x, b, result_) 
6 BiCGSTABSolver::apply 
6 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
5 7 before Dune::BiCGSTABSolver 
5 Dune::BiCGSTABSolver.apply(x, b, result_) 
5 BiCGSTABSolver::apply 
5 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
Using a direct coarse solver (SuperLU)
Building hierarchy of 1 levels (inclusive coarse solver) took 0.11145 seconds.
0 7 before Dune::BiCGSTABSolver 
0 Dune::BiCGSTABSolver.apply(x, b, result_) 
0 BiCGSTABSolver::apply 
0 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
0 === BiCGSTABSolver
 Iter          Defect            Rate
    0      7.22876e-17
0  (it = 0.5; it < _maxit; it+=.5) 0.5 250
0  before p = r 
0  before  y = W^-1 * p 
0  before _prec->apply(y,p); 0.5 250
0 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
5 === BiCGSTABSolver
 Iter          Defect            Rate
    0      7.22876e-17
5  (it = 0.5; it < _maxit; it+=.5) 0.5 250
5  before p = r 
5  before  y = W^-1 * p 
5  before _prec->apply(y,p); 0.5 250
5 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
6 === BiCGSTABSolver
 Iter          Defect            Rate
    0      7.22876e-17
6  (it = 0.5; it < _maxit; it+=.5) 0.5 250
6  before p = r 
6  before  y = W^-1 * p 
6  before _prec->apply(y,p); 0.5 250
6 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
3 === BiCGSTABSolver
 Iter          Defect            Rate
    0      7.22876e-17
3  (it = 0.5; it < _maxit; it+=.5) 0.5 250
3  before p = r 
3  before  y = W^-1 * p 
3  before _prec->apply(y,p); 0.5 250
3 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
2 === BiCGSTABSolver
 Iter          Defect            Rate
    0      7.22876e-17
2  (it = 0.5; it < _maxit; it+=.5) 0.5 250
2  before p = r 
2  before  y = W^-1 * p 
2  before _prec->apply(y,p); 0.5 250
2 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
1 === BiCGSTABSolver
 Iter          Defect            Rate
    0      7.22876e-17
1  (it = 0.5; it < _maxit; it+=.5) 0.5 250
1  before p = r 
1  before  y = W^-1 * p 
1  before _prec->apply(y,p); 0.5 250
1 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
4 === BiCGSTABSolver
 Iter          Defect            Rate
    0      7.22876e-17
4  (it = 0.5; it < _maxit; it+=.5) 0.5 250
4  before p = r 
4  before  y = W^-1 * p 
4  before _prec->apply(y,p); 0.5 250
4 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 7
0 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
4 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
4 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
4  before v = A * y 
5 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
5 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
5  before v = A * y 
6 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
6 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
6  before v = A * y 
6  before  alpha = rho_new / < rt, v > 
3 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
3 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
3  before v = A * y 
2 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
2 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
1 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
0 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
1 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
1  before v = A * y 
4  before  alpha = rho_new / < rt, v > 
5  before  alpha = rho_new / < rt, v > 
3  before  alpha = rho_new / < rt, v > 
2  before v = A * y 
2  before  alpha = rho_new / < rt, v > 
0 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
0  before v = A * y 
4  before alpha = rho_new / h; 
4  before apply first correction to x 
4  before r = r - alpha*v 
4  before test stop criteria 
4 istlsolver::printOutput 
  0.5      3.60979e-34      4.99365e-18
4 === rate=2.49365e-35, T=0.122208, TIT=0.244416, IT=0.5
4 finished solveLinearSystemImpl_ 1
4 converged value: 1 7 1 let s go get convergedRemote
4 to  comm_.min(converged) 
6  before alpha = rho_new / h; 
6  before apply first correction to x 
6  before r = r - alpha*v 
6  before test stop criteria 
6 istlsolver::printOutput 
  0.5      3.60979e-34      4.99365e-18
6 === rate=2.49365e-35, T=0.122212, TIT=0.244424, IT=0.5
6 finished solveLinearSystemImpl_ 1
6 converged value: 1 7 1 let s go get convergedRemote
6 to  comm_.min(converged) 
5  before alpha = rho_new / h; 
5  before apply first correction to x 
5  before r = r - alpha*v 
5  before test stop criteria 
5 istlsolver::printOutput 
  0.5      3.60979e-34      4.99365e-18
5 === rate=2.49365e-35, T=0.122222, TIT=0.244444, IT=0.5
5 finished solveLinearSystemImpl_ 1
5 converged value: 1 7 1 let s go get convergedRemote
5 to  comm_.min(converged) 
3  before alpha = rho_new / h; 
3  before apply first correction to x 
3  before r = r - alpha*v 
3  before test stop criteria 
3 istlsolver::printOutput 
  0.5      3.60979e-34      4.99365e-18
3 === rate=2.49365e-35, T=0.122222, TIT=0.244445, IT=0.5
3 finished solveLinearSystemImpl_ 1
3 converged value: 1 7 1 let s go get convergedRemote
3 to  comm_.min(converged) 
0  before  alpha = rho_new / < rt, v > 
0  before alpha = rho_new / h; 
0  before apply first correction to x 
0  before r = r - alpha*v 
0  before test stop criteria 
0 istlsolver::printOutput 
  0.5      3.60979e-34      4.99365e-18
0 === rate=2.49365e-35, T=0.018514, TIT=0.0370281, IT=0.5
2  before alpha = rho_new / h; 
2  before apply first correction to x 
2  before r = r - alpha*v 
2  before test stop criteria 
2 istlsolver::printOutput 
  0.5      3.60979e-34      4.99365e-18
2 === rate=2.49365e-35, T=0.122209, TIT=0.244418, IT=0.5
2 finished solveLinearSystemImpl_ 1
2 converged value: 1 7 1 let s go get convergedRemote
2 to  comm_.min(converged) 
1  before  alpha = rho_new / < rt, v > 
1  before alpha = rho_new / h; 
1  before apply first correction to x 
1  before r = r - alpha*v 
1  before test stop criteria 
1 istlsolver::printOutput 
  0.5      3.60979e-34      4.99365e-18
1 === rate=2.49365e-35, T=0.122222, TIT=0.244444, IT=0.5
1 finished solveLinearSystemImpl_ 1
1 converged value: 1 7 1 let s go get convergedRemote
1 to  comm_.min(converged) 
1 did  comm_.min(converged) convergedRemote: 1
1 final convergedRemote: 1
1 Update: x^(k+1) = x^k - deltax^k 
1 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
1 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
1 Newton iteration 10 done1 , maximum relative shift = 2.029e-08
1 Assemble/solve/update time: 0.445464(25.2255%)/1.32027(74.7638%)/0.000187952(0.0106432%)
1 gridVariables->advanceTimeStep
1 before, nonLinearSolver->suggestTimeStepSize, current time: 606.573 simTime 57600
1 before assembler->setPreviousSolution, ddt: 8.87084
1 nonLinearSolver->solve
1 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
0 finished solveLinearSystemImpl_ 1
0 converged value: 1 7 1 let s go get convergedRemote
0 to  comm_.min(converged) 
0 did  comm_.min(converged) convergedRemote: 1
0 final convergedRemote: 1
0 Update: x^(k+1) = x^k - deltax^k 
0 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
0 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
0 Newton iteration 10 done0 , maximum relative shift = 2.029e-08
0 Assemble/solve/update time: 0.444313(25.1768%)/1.32024(74.8109%)/0.000216312(0.0122572%)
0 gridVariables->advanceTimeStep
0 before, nonLinearSolver->suggestTimeStepSize, current time: 606.573 simTime 57600
0 before assembler->setPreviousSolution, ddt: 8.87084
0 nonLinearSolver->solve
0 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
2 did  comm_.min(converged) convergedRemote: 1
2 final convergedRemote: 1
2 Update: x^(k+1) = x^k - deltax^k 
2 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
2 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
2 Newton iteration 10 done2 , maximum relative shift = 2.029e-08
2 Assemble/solve/update time: 0.445368(25.2208%)/1.3203(74.7676%)/0.000204295(0.0115691%)
2 gridVariables->advanceTimeStep
2 before, nonLinearSolver->suggestTimeStepSize, current time: 606.573 simTime 57600
2 before assembler->setPreviousSolution, ddt: 8.87084
2 nonLinearSolver->solve
2 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
3 did  comm_.min(converged) convergedRemote: 1
3 final convergedRemote: 1
3 Update: x^(k+1) = x^k - deltax^k 
3 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
3 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
3 Newton iteration 10 done3 , maximum relative shift = 2.029e-08
3 Assemble/solve/update time: 0.445429(25.2235%)/1.3203(74.7655%)/0.000194661(0.0110232%)
3 gridVariables->advanceTimeStep
3 before, nonLinearSolver->suggestTimeStepSize, current time: 606.573 simTime 57600
3 before assembler->setPreviousSolution, ddt: 8.87084
3 nonLinearSolver->solve
3 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
4 did  comm_.min(converged) convergedRemote: 1
4 final convergedRemote: 1
4 Update: x^(k+1) = x^k - deltax^k 
4 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
4 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
4 Newton iteration 10 done4 , maximum relative shift = 2.029e-08
4 Assemble/solve/update time: 0.445542(25.2288%)/1.32028(74.7605%)/0.000188752(0.0106881%)
4 gridVariables->advanceTimeStep
4 before, nonLinearSolver->suggestTimeStepSize, current time: 606.573 simTime 57600
4 before assembler->setPreviousSolution, ddt: 8.87084
4 nonLinearSolver->solve
4 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
6 did  comm_.min(converged) convergedRemote: 1
6 final convergedRemote: 1
6 Update: x^(k+1) = x^k - deltax^k 
6 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
6 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
6 Newton iteration 10 done6 , maximum relative shift = 2.029e-08
6 Assemble/solve/update time: 0.445545(25.229%)/1.3203(74.7623%)/0.00015274(0.00864892%)
6 gridVariables->advanceTimeStep
6 before, nonLinearSolver->suggestTimeStepSize, current time: 606.573 simTime 57600
6 before assembler->setPreviousSolution, ddt: 8.87084
6 nonLinearSolver->solve
6 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
5 did  comm_.min(converged) convergedRemote: 1
5 final convergedRemote: 1
5 Update: x^(k+1) = x^k - deltax^k 
5 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
5 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
5 Newton iteration 10 done5 , maximum relative shift = 2.029e-08
5 Assemble/solve/update time: 0.445575(25.2303%)/1.32027(74.7592%)/0.00018598(0.0105309%)
5 gridVariables->advanceTimeStep
5 before, nonLinearSolver->suggestTimeStepSize, current time: 606.573 simTime 57600
5 before assembler->setPreviousSolution, ddt: 8.87084
5 nonLinearSolver->solve
5 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
3 Solve: M deltax^k = r 
3 Newton::solveLinearSystem : entering the try section 0
3to  comm_.sum(norm2) 
3did  comm_.sum(norm2) 
3 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
3 solveLinearSystemImpl_, before converged = ls.solve 
3 7 amgbackend::solve isParallel 1
4 Solve: M deltax^k = r 
4 Newton::solveLinearSystem : entering the try section 0
4to  comm_.sum(norm2) 
4did  comm_.sum(norm2) 
4 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
4 solveLinearSystemImpl_, before converged = ls.solve 
4 7 amgbackend::solve isParallel 1
6 Solve: M deltax^k = r 
6 Newton::solveLinearSystem : entering the try section 0
6to  comm_.sum(norm2) 
6did  comm_.sum(norm2) 
6 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
6 solveLinearSystemImpl_, before converged = ls.solve 
6 7 amgbackend::solve isParallel 1
5 Solve: M deltax^k = r 
5 Newton::solveLinearSystem : entering the try section 0
5to  comm_.sum(norm2) 
5did  comm_.sum(norm2) 
5 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
5 solveLinearSystemImpl_, before converged = ls.solve 
5 7 amgbackend::solve isParallel 1
1 Solve: M deltax^k = r 
1 Newton::solveLinearSystem : entering the try section 0
1to  comm_.sum(norm2) 
1did  comm_.sum(norm2) 
1 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
1 solveLinearSystemImpl_, before converged = ls.solve 
1 7 amgbackend::solve isParallel 1
0 Solve: M deltax^k = r 
0 Newton::solveLinearSystem : entering the try section 0
0to  comm_.sum(norm2) 
0did  comm_.sum(norm2) 
0 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
0 solveLinearSystemImpl_, before converged = ls.solve 
0 7 amgbackend::solve isParallel 1
2 Solve: M deltax^k = r 
2 Newton::solveLinearSystem : entering the try section 0
2to  comm_.sum(norm2) 
2did  comm_.sum(norm2) 
2 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
2 solveLinearSystemImpl_, before converged = ls.solve 
2 7 amgbackend::solve isParallel 1
5 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
5 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
1 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
1 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
0 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
0 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 7
6 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
6 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
3 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
3 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
4 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
4 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
2 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
2 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 7
4 7 before Dune::BiCGSTABSolver 
4 Dune::BiCGSTABSolver.apply(x, b, result_) 
4 BiCGSTABSolver::apply 
4 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
5 7 before Dune::BiCGSTABSolver 
5 Dune::BiCGSTABSolver.apply(x, b, result_) 
5 BiCGSTABSolver::apply 
5 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
1 7 before Dune::BiCGSTABSolver 
1 Dune::BiCGSTABSolver.apply(x, b, result_) 
1 BiCGSTABSolver::apply 
1 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
2 7 before Dune::BiCGSTABSolver 
2 Dune::BiCGSTABSolver.apply(x, b, result_) 
2 BiCGSTABSolver::apply 
2 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
6 7 before Dune::BiCGSTABSolver 
6 Dune::BiCGSTABSolver.apply(x, b, result_) 
6 BiCGSTABSolver::apply 
6 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
3 7 before Dune::BiCGSTABSolver 
3 Dune::BiCGSTABSolver.apply(x, b, result_) 
3 BiCGSTABSolver::apply 
3 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 7
Using a direct coarse solver (SuperLU)
Building hierarchy of 1 levels (inclusive coarse solver) took 0.111352 seconds.
0 7 before Dune::BiCGSTABSolver 
0 Dune::BiCGSTABSolver.apply(x, b, result_) 
0 BiCGSTABSolver::apply 
0 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
0 === BiCGSTABSolver
 Iter          Defect            Rate
3 === BiCGSTABSolver
 Iter          Defect            Rate
    0      3.16239e-09
3  (it = 0.5; it < _maxit; it+=.5) 0.5 250
5 === BiCGSTABSolver
 Iter          Defect            Rate
    0      3.16239e-09
5  (it = 0.5; it < _maxit; it+=.5) 0.5 250
1 === BiCGSTABSolver
 Iter          Defect            Rate
    0      3.16239e-09
1  (it = 0.5; it < _maxit; it+=.5) 0.5 250
2 === BiCGSTABSolver
 Iter          Defect            Rate
    0      3.16239e-09
2  (it = 0.5; it < _maxit; it+=.5) 0.5 250
6 === BiCGSTABSolver
 Iter          Defect            Rate
    0      3.16239e-09
6  (it = 0.5; it < _maxit; it+=.5) 0.5 250
    0      3.16239e-09
0  (it = 0.5; it < _maxit; it+=.5) 0.5 250
0  before p = r 
0  before  y = W^-1 * p 
0  before _prec->apply(y,p); 0.5 250
0 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 7
0 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
6  before p = r 
6  before  y = W^-1 * p 
6  before _prec->apply(y,p); 0.5 250
6 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
4 === BiCGSTABSolver
 Iter          Defect            Rate
    0      3.16239e-09
4  (it = 0.5; it < _maxit; it+=.5) 0.5 250
4  before p = r 
4  before  y = W^-1 * p 
4  before _prec->apply(y,p); 0.5 250
4 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
3  before p = r 
3  before  y = W^-1 * p 
3  before _prec->apply(y,p); 0.5 250
3 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
5  before p = r 
5  before  y = W^-1 * p 
5  before _prec->apply(y,p); 0.5 250
5 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
1  before p = r 
1  before  y = W^-1 * p 
1  before _prec->apply(y,p); 0.5 250
1 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
2  before p = r 
2  before  y = W^-1 * p 
2  before _prec->apply(y,p); 0.5 250
2 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
5 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
5 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
5  before v = A * y 
1 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
2 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
2 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
0 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
6 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
6 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
6  before v = A * y 
4 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
4 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
4  before v = A * y 
3 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
3 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
3  before v = A * y 
5  before  alpha = rho_new / < rt, v > 
3  before  alpha = rho_new / < rt, v > 
0 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
6  before  alpha = rho_new / < rt, v > 
4  before  alpha = rho_new / < rt, v > 
1 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
2  before v = A * y 
2  before  alpha = rho_new / < rt, v > 
2  before alpha = rho_new / h; 
2  before apply first correction to x 
2  before r = r - alpha*v 
2  before test stop criteria 
2 istlsolver::printOutput 
  0.5      4.60761e-25        1.457e-16
2 === rate=2.12286e-32, T=0.124728, TIT=0.249456, IT=0.5
2 finished solveLinearSystemImpl_ 1
2 converged value: 1 7 1 let s go get convergedRemote
2 to  comm_.min(converged) 
3  before alpha = rho_new / h; 
3  before apply first correction to x 
3  before r = r - alpha*v 
3  before test stop criteria 
3 istlsolver::printOutput 
  0.5      4.60761e-25        1.457e-16
3 === rate=2.12286e-32, T=0.124851, TIT=0.249701, IT=0.5
3 finished solveLinearSystemImpl_ 1
3 converged value: 1 7 1 let s go get convergedRemote
3 to  comm_.min(converged) 
0  before v = A * y 
0  before  alpha = rho_new / < rt, v > 
0  before alpha = rho_new / h; 
0  before apply first correction to x 
0  before r = r - alpha*v 
0  before test stop criteria 
0 istlsolver::printOutput 
  0.5      4.60761e-25        1.457e-16
0 === rate=2.12286e-32, T=0.0209394, TIT=0.0418788, IT=0.5
6  before alpha = rho_new / h; 
6  before apply first correction to x 
6  before r = r - alpha*v 
6  before test stop criteria 
6 istlsolver::printOutput 
  0.5      4.60761e-25        1.457e-16
6 === rate=2.12286e-32, T=0.124728, TIT=0.249457, IT=0.5
6 finished solveLinearSystemImpl_ 1
6 converged value: 1 7 1 let s go get convergedRemote
6 to  comm_.min(converged) 
4  before alpha = rho_new / h; 
4  before apply first correction to x 
4  before r = r - alpha*v 
4  before test stop criteria 
4 istlsolver::printOutput 
  0.5      4.60761e-25        1.457e-16
4 === rate=2.12286e-32, T=0.124731, TIT=0.249461, IT=0.5
4 finished solveLinearSystemImpl_ 1
4 converged value: 1 7 1 let s go get convergedRemote
4 to  comm_.min(converged) 
5  before alpha = rho_new / h; 
5  before apply first correction to x 
5  before r = r - alpha*v 
5  before test stop criteria 
5 istlsolver::printOutput 
  0.5      4.60761e-25        1.457e-16
5 === rate=2.12286e-32, T=0.124849, TIT=0.249699, IT=0.5
5 finished solveLinearSystemImpl_ 1
5 converged value: 1 7 1 let s go get convergedRemote
5 to  comm_.min(converged) 
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
1  before v = A * y 
1  before  alpha = rho_new / < rt, v > 
1  before alpha = rho_new / h; 
1  before apply first correction to x 
1  before r = r - alpha*v 
1  before test stop criteria 
1 istlsolver::printOutput 
  0.5      4.60761e-25        1.457e-16
1 === rate=2.12286e-32, T=0.124853, TIT=0.249706, IT=0.5
1 finished solveLinearSystemImpl_ 1
1 converged value: 1 7 1 let s go get convergedRemote
1 to  comm_.min(converged) 
1 did  comm_.min(converged) convergedRemote: 1
1 final convergedRemote: 1
1 Update: x^(k+1) = x^k - deltax^k 
1 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
1 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
1 Newton iteration  1 done1 , maximum relative shift = 7.077e-02
1 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
5 did  comm_.min(converged) convergedRemote: 1
5 final convergedRemote: 1
5 Update: x^(k+1) = x^k - deltax^k 
5 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
5 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
5 Newton iteration  1 done5 , maximum relative shift = 7.077e-02
5 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
2 did  comm_.min(converged) convergedRemote: 1
2 final convergedRemote: 1
2 Update: x^(k+1) = x^k - deltax^k 
2 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
2 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
2 Newton iteration  1 done2 , maximum relative shift = 7.077e-02
2 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
3 did  comm_.min(converged) convergedRemote: 1
3 final convergedRemote: 1
3 Update: x^(k+1) = x^k - deltax^k 
3 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
3 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
3 Newton iteration  1 done3 , maximum relative shift = 7.077e-02
3 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
0 finished solveLinearSystemImpl_ 1
0 converged value: 1 7 1 let s go get convergedRemote
0 to  comm_.min(converged) 
0 did  comm_.min(converged) convergedRemote: 1
0 final convergedRemote: 1
0 Update: x^(k+1) = x^k - deltax^k 
0 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
0 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
0 Newton iteration  1 done0 , maximum relative shift = 7.077e-02
0 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
6 did  comm_.min(converged) convergedRemote: 1
6 final convergedRemote: 1
6 Update: x^(k+1) = x^k - deltax^k 
6 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
6 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
6 Newton iteration  1 done6 , maximum relative shift = 7.077e-02
6 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
4 did  comm_.min(converged) convergedRemote: 1
4 final convergedRemote: 1
4 Update: x^(k+1) = x^k - deltax^k 
4 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
4 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
4 Newton iteration  1 done4 , maximum relative shift = 7.077e-02
4 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
5 Solve: M deltax^k = r 
5 Newton::solveLinearSystem : entering the try section 1
5 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
5 solveLinearSystemImpl_, before converged = ls.solve 
5 7 amgbackend::solve isParallel 1
2 Solve: M deltax^k = r 
2 Newton::solveLinearSystem : entering the try section 1
2 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
2 solveLinearSystemImpl_, before converged = ls.solve 
2 7 amgbackend::solve isParallel 1
3 Solve: M deltax^k = r 
3 Newton::solveLinearSystem : entering the try section 1
3 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
3 solveLinearSystemImpl_, before converged = ls.solve 
3 7 amgbackend::solve isParallel 1
0 Solve: M deltax^k = r 
0 Newton::solveLinearSystem : entering the try section 1
0 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
0 solveLinearSystemImpl_, before converged = ls.solve 
0 7 amgbackend::solve isParallel 1
6 Solve: M deltax^k = r 
6 Newton::solveLinearSystem : entering the try section 1
6 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
6 solveLinearSystemImpl_, before converged = ls.solve 
6 7 amgbackend::solve isParallel 1
4 Solve: M deltax^k = r 
4 Newton::solveLinearSystem : entering the try section 1
4 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
4 solveLinearSystemImpl_, before converged = ls.solve 
4 7 amgbackend::solve isParallel 1
1 Solve: M deltax^k = r 
1 Newton::solveLinearSystem : entering the try section 1
1 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
1 solveLinearSystemImpl_, before converged = ls.solve 
1 7 amgbackend::solve isParallel 1
4 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
4 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
6 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
6 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
5 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
5 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
2 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
2 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
3 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
3 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
0 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
0 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 7
1 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
1 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 7
6 7 before Dune::BiCGSTABSolver 
6 Dune::BiCGSTABSolver.apply(x, b, result_) 
6 BiCGSTABSolver::apply 
6 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
5 7 before Dune::BiCGSTABSolver 
5 Dune::BiCGSTABSolver.apply(x, b, result_) 
5 BiCGSTABSolver::apply 
5 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
2 7 before Dune::BiCGSTABSolver 
2 Dune::BiCGSTABSolver.apply(x, b, result_) 
2 BiCGSTABSolver::apply 
2 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
3 7 before Dune::BiCGSTABSolver 
3 Dune::BiCGSTABSolver.apply(x, b, result_) 
3 BiCGSTABSolver::apply 
3 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 7
1 7 before Dune::BiCGSTABSolver 
1 Dune::BiCGSTABSolver.apply(x, b, result_) 
1 BiCGSTABSolver::apply 
1 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
4 7 before Dune::BiCGSTABSolver 
4 Dune::BiCGSTABSolver.apply(x, b, result_) 
4 BiCGSTABSolver::apply 
4 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
Using a direct coarse solver (SuperLU)
Building hierarchy of 1 levels (inclusive coarse solver) took 0.112155 seconds.
0 7 before Dune::BiCGSTABSolver 
0 Dune::BiCGSTABSolver.apply(x, b, result_) 
0 BiCGSTABSolver::apply 
0 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
0 === BiCGSTABSolver
 Iter          Defect            Rate
    0       4.2572e-11
0  (it = 0.5; it < _maxit; it+=.5) 0.5 250
1 === BiCGSTABSolver
 Iter          Defect            Rate
    0       4.2572e-11
1  (it = 0.5; it < _maxit; it+=.5) 0.5 250
1  before p = r 
1  before  y = W^-1 * p 
1  before _prec->apply(y,p); 0.5 250
1 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
3 === BiCGSTABSolver
 Iter          Defect            Rate
    0       4.2572e-11
3  (it = 0.5; it < _maxit; it+=.5) 0.5 250
3  before p = r 
3  before  y = W^-1 * p 
3  before _prec->apply(y,p); 0.5 250
3 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
6 === BiCGSTABSolver
 Iter          Defect            Rate
    0       4.2572e-11
6  (it = 0.5; it < _maxit; it+=.5) 0.5 250
6  before p = r 
6  before  y = W^-1 * p 
6  before _prec->apply(y,p); 0.5 250
6 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
5 === BiCGSTABSolver
 Iter          Defect            Rate
    0       4.2572e-11
5  (it = 0.5; it < _maxit; it+=.5) 0.5 250
5  before p = r 
5  before  y = W^-1 * p 
5  before _prec->apply(y,p); 0.5 250
5 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
2 === BiCGSTABSolver
 Iter          Defect            Rate
    0       4.2572e-11
2  (it = 0.5; it < _maxit; it+=.5) 0.5 250
2  before p = r 
2  before  y = W^-1 * p 
2  before _prec->apply(y,p); 0.5 250
2 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
0  before p = r 
0  before  y = W^-1 * p 
0  before _prec->apply(y,p); 0.5 250
0 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 7
0 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
4 === BiCGSTABSolver
 Iter          Defect            Rate
    0       4.2572e-11
4  (it = 0.5; it < _maxit; it+=.5) 0.5 250
4  before p = r 
4  before  y = W^-1 * p 
4  before _prec->apply(y,p); 0.5 250
4 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
1 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
3 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
3 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
3  before v = A * y 
6 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
6 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
6  before v = A * y 
5 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
5 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
5  before v = A * y 
4 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
4 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
4  before v = A * y 
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
0 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
2 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
2 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
0 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
0  before v = A * y 
5  before  alpha = rho_new / < rt, v > 
1 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
1  before v = A * y 
3  before  alpha = rho_new / < rt, v > 
6  before  alpha = rho_new / < rt, v > 
2  before v = A * y 
2  before  alpha = rho_new / < rt, v > 
4  before  alpha = rho_new / < rt, v > 
1  before  alpha = rho_new / < rt, v > 
1  before alpha = rho_new / h; 
1  before apply first correction to x 
1  before r = r - alpha*v 
1  before test stop criteria 
1 istlsolver::printOutput 
  0.5      6.48681e-27      1.52373e-16
1 === rate=2.32175e-32, T=0.122818, TIT=0.245635, IT=0.5
1 finished solveLinearSystemImpl_ 1
1 converged value: 1 7 1 let s go get convergedRemote
1 to  comm_.min(converged) 
3  before alpha = rho_new / h; 
3  before apply first correction to x 
3  before r = r - alpha*v 
3  before test stop criteria 
3 istlsolver::printOutput 
  0.5      6.48681e-27      1.52373e-16
3 === rate=2.32175e-32, T=0.122813, TIT=0.245626, IT=0.5
3 finished solveLinearSystemImpl_ 1
3 converged value: 1 7 1 let s go get convergedRemote
3 to  comm_.min(converged) 
4  before alpha = rho_new / h; 
4  before apply first correction to x 
4  before r = r - alpha*v 
4  before test stop criteria 
4 istlsolver::printOutput 
  0.5      6.48681e-27      1.52373e-16
4 === rate=2.32175e-32, T=0.122689, TIT=0.245377, IT=0.5
4 finished solveLinearSystemImpl_ 1
4 converged value: 1 7 1 let s go get convergedRemote
4 to  comm_.min(converged) 
2  before alpha = rho_new / h; 
2  before apply first correction to x 
2  before r = r - alpha*v 
2  before test stop criteria 
2 istlsolver::printOutput 
  0.5      6.48681e-27      1.52373e-16
2 === rate=2.32175e-32, T=0.122687, TIT=0.245375, IT=0.5
2 finished solveLinearSystemImpl_ 1
2 converged value: 1 7 1 let s go get convergedRemote
2 to  comm_.min(converged) 
6  before alpha = rho_new / h; 
6  before apply first correction to x 
6  before r = r - alpha*v 
6  before test stop criteria 
6 istlsolver::printOutput 
  0.5      6.48681e-27      1.52373e-16
6 === rate=2.32175e-32, T=0.12269, TIT=0.245381, IT=0.5
6 finished solveLinearSystemImpl_ 1
6 converged value: 1 7 1 let s go get convergedRemote
6 to  comm_.min(converged) 
0  before  alpha = rho_new / < rt, v > 
0  before alpha = rho_new / h; 
0  before apply first correction to x 
0  before r = r - alpha*v 
0  before test stop criteria 
0 istlsolver::printOutput 
  0.5      6.48681e-27      1.52373e-16
0 === rate=2.32175e-32, T=0.0185229, TIT=0.0370458, IT=0.5
5  before alpha = rho_new / h; 
5  before apply first correction to x 
5  before r = r - alpha*v 
5  before test stop criteria 
5 istlsolver::printOutput 
  0.5      6.48681e-27      1.52373e-16
5 === rate=2.32175e-32, T=0.122815, TIT=0.24563, IT=0.5
5 finished solveLinearSystemImpl_ 1
5 converged value: 1 7 1 let s go get convergedRemote
5 to  comm_.min(converged) 
6 did  comm_.min(converged) convergedRemote: 1
6 final convergedRemote: 1
6 Update: x^(k+1) = x^k - deltax^k 
6 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
6 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
6 Newton iteration  2 done6 , maximum relative shift = 6.668e-03
6 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
0 finished solveLinearSystemImpl_ 1
0 converged value: 1 7 1 let s go get convergedRemote
0 to  comm_.min(converged) 
0 did  comm_.min(converged) convergedRemote: 1
0 final convergedRemote: 1
0 Update: x^(k+1) = x^k - deltax^k 
0 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
0 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
0 Newton iteration  2 done0 , maximum relative shift = 6.668e-03
0 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
3 did  comm_.min(converged) convergedRemote: 1
3 final convergedRemote: 1
3 Update: x^(k+1) = x^k - deltax^k 
3 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
3 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
3 Newton iteration  2 done3 , maximum relative shift = 6.668e-03
3 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
1 did  comm_.min(converged) convergedRemote: 1
1 final convergedRemote: 1
1 Update: x^(k+1) = x^k - deltax^k 
1 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
1 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
1 Newton iteration  2 done1 , maximum relative shift = 6.668e-03
1 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
5 did  comm_.min(converged) convergedRemote: 1
5 final convergedRemote: 1
5 Update: x^(k+1) = x^k - deltax^k 
5 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
5 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
5 Newton iteration  2 done5 , maximum relative shift = 6.668e-03
5 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
4 did  comm_.min(converged) convergedRemote: 1
4 final convergedRemote: 1
4 Update: x^(k+1) = x^k - deltax^k 
4 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
4 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
4 Newton iteration  2 done4 , maximum relative shift = 6.668e-03
4 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
2 did  comm_.min(converged) convergedRemote: 1
2 final convergedRemote: 1
2 Update: x^(k+1) = x^k - deltax^k 
2 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
2 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
2 Newton iteration  2 done2 , maximum relative shift = 6.668e-03
2 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
6 Solve: M deltax^k = r 
6 Newton::solveLinearSystem : entering the try section 2
6 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
6 solveLinearSystemImpl_, before converged = ls.solve 
6 7 amgbackend::solve isParallel 1
0 Solve: M deltax^k = r 
0 Newton::solveLinearSystem : entering the try section 2
0 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
0 solveLinearSystemImpl_, before converged = ls.solve 
0 7 amgbackend::solve isParallel 1
3 Solve: M deltax^k = r 
3 Newton::solveLinearSystem : entering the try section 2
3 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
3 solveLinearSystemImpl_, before converged = ls.solve 
3 7 amgbackend::solve isParallel 1
2 Solve: M deltax^k = r 
2 Newton::solveLinearSystem : entering the try section 2
2 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
2 solveLinearSystemImpl_, before converged = ls.solve 
2 7 amgbackend::solve isParallel 1
5 Solve: M deltax^k = r 
5 Newton::solveLinearSystem : entering the try section 2
5 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
5 solveLinearSystemImpl_, before converged = ls.solve 
5 7 amgbackend::solve isParallel 1
4 Solve: M deltax^k = r 
4 Newton::solveLinearSystem : entering the try section 2
4 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
4 solveLinearSystemImpl_, before converged = ls.solve 
4 7 amgbackend::solve isParallel 1
1 Solve: M deltax^k = r 
1 Newton::solveLinearSystem : entering the try section 2
1 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
1 solveLinearSystemImpl_, before converged = ls.solve 
1 7 amgbackend::solve isParallel 1
2 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
2 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
5 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
5 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
4 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
4 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
3 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
3 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
6 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
6 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
0 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
0 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 7
1 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
1 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 7
1 7 before Dune::BiCGSTABSolver 
1 Dune::BiCGSTABSolver.apply(x, b, result_) 
1 BiCGSTABSolver::apply 
1 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
2 7 before Dune::BiCGSTABSolver 
2 Dune::BiCGSTABSolver.apply(x, b, result_) 
2 BiCGSTABSolver::apply 
2 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
5 7 before Dune::BiCGSTABSolver 
5 Dune::BiCGSTABSolver.apply(x, b, result_) 
5 BiCGSTABSolver::apply 
5 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
4 7 before Dune::BiCGSTABSolver 
4 Dune::BiCGSTABSolver.apply(x, b, result_) 
4 BiCGSTABSolver::apply 
4 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
3 7 before Dune::BiCGSTABSolver 
3 Dune::BiCGSTABSolver.apply(x, b, result_) 
3 BiCGSTABSolver::apply 
3 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
6 7 before Dune::BiCGSTABSolver 
6 Dune::BiCGSTABSolver.apply(x, b, result_) 
6 BiCGSTABSolver::apply 
6 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
Using a direct coarse solver (SuperLU)
Building hierarchy of 1 levels (inclusive coarse solver) took 0.111891 seconds.
0 7 before Dune::BiCGSTABSolver 
0 Dune::BiCGSTABSolver.apply(x, b, result_) 
0 BiCGSTABSolver::apply 
0 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
1 === BiCGSTABSolver
 Iter          Defect            Rate
    0      1.08058e-11
1  (it = 0.5; it < _maxit; it+=.5) 0.5 250
1  before p = r 
1  before  y = W^-1 * p 
1  before _prec->apply(y,p); 0.5 250
1 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
4 === BiCGSTABSolver
 Iter          Defect            Rate
    0      1.08058e-11
4  (it = 0.5; it < _maxit; it+=.5) 0.5 250
4  before p = r 
4  before  y = W^-1 * p 
4  before _prec->apply(y,p); 0.5 250
4 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
5 === BiCGSTABSolver
 Iter          Defect            Rate
    0      1.08058e-11
5  (it = 0.5; it < _maxit; it+=.5) 0.5 250
5  before p = r 
5  before  y = W^-1 * p 
5  before _prec->apply(y,p); 0.5 250
5 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
6 === BiCGSTABSolver
 Iter          Defect            Rate
    0      1.08058e-11
6  (it = 0.5; it < _maxit; it+=.5) 0.5 250
6  before p = r 
6  before  y = W^-1 * p 
6  before _prec->apply(y,p); 0.5 250
6 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
3 === BiCGSTABSolver
 Iter          Defect            Rate
    0      1.08058e-11
3  (it = 0.5; it < _maxit; it+=.5) 0.5 250
3  before p = r 
3  before  y = W^-1 * p 
3  before _prec->apply(y,p); 0.5 250
3 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
0 === BiCGSTABSolver
 Iter          Defect            Rate
    0      1.08058e-11
0  (it = 0.5; it < _maxit; it+=.5) 0.5 250
0  before p = r 
0  before  y = W^-1 * p 
0  before _prec->apply(y,p); 0.5 250
0 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 7
0 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
2 === BiCGSTABSolver
 Iter          Defect            Rate
    0      1.08058e-11
2  (it = 0.5; it < _maxit; it+=.5) 0.5 250
2  before p = r 
2  before  y = W^-1 * p 
2  before _prec->apply(y,p); 0.5 250
2 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
0 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
3 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
3 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
3  before v = A * y 
1 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
4 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
4 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
4  before v = A * y 
4  before  alpha = rho_new / < rt, v > 
5 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
5 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
5  before v = A * y 
6 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
6 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
6  before v = A * y 
6  before  alpha = rho_new / < rt, v > 
2 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
2 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
2  before v = A * y 
2  before  alpha = rho_new / < rt, v > 
2  before alpha = rho_new / h; 
2  before apply first correction to x 
2  before r = r - alpha*v 
2  before test stop criteria 
2 istlsolver::printOutput 
  0.5       7.9473e-28      7.35464e-17
2 === rate=5.40908e-33, T=0.122389, TIT=0.244778, IT=0.5
2 finished solveLinearSystemImpl_ 1
2 converged value: 1 7 1 let s go get convergedRemote
2 to  comm_.min(converged) 
0 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
0  before v = A * y 
0  before  alpha = rho_new / < rt, v > 
0  before alpha = rho_new / h; 
0  before apply first correction to x 
0  before r = r - alpha*v 
0  before test stop criteria 
0 istlsolver::printOutput 
  0.5       7.9473e-28      7.35464e-17
3  before  alpha = rho_new / < rt, v > 
3  before alpha = rho_new / h; 
3  before apply first correction to x 
3  before r = r - alpha*v 
3  before test stop criteria 
3 istlsolver::printOutput 
  0.5       7.9473e-28      7.35464e-17
3 === rate=5.40908e-33, T=0.12248, TIT=0.24496, IT=0.5
1 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
1  before v = A * y 
1  before  alpha = rho_new / < rt, v > 
1  before alpha = rho_new / h; 
1  before apply first correction to x 
1  before r = r - alpha*v 
1  before test stop criteria 
1 istlsolver::printOutput 
  0.5       7.9473e-28      7.35464e-17
1 === rate=5.40908e-33, T=0.122479, TIT=0.244959, IT=0.5
4  before alpha = rho_new / h; 
4  before apply first correction to x 
4  before r = r - alpha*v 
4  before test stop criteria 
4 istlsolver::printOutput 
  0.5       7.9473e-28      7.35464e-17
4 === rate=5.40908e-33, T=0.122388, TIT=0.244777, IT=0.5
4 finished solveLinearSystemImpl_ 1
4 converged value: 1 7 1 let s go get convergedRemote
4 to  comm_.min(converged) 
5  before  alpha = rho_new / < rt, v > 
5  before alpha = rho_new / h; 
5  before apply first correction to x 
5  before r = r - alpha*v 
5  before test stop criteria 
5 istlsolver::printOutput 
  0.5       7.9473e-28      7.35464e-17
5 === rate=5.40908e-33, T=0.122478, TIT=0.244956, IT=0.5
4 did  comm_.min(converged) convergedRemote: 1
4 final convergedRemote: 1
4 Update: x^(k+1) = x^k - deltax^k 
4 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
4 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
4 Newton iteration  3 done4 , maximum relative shift = 2.198e-03
4 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
5 finished solveLinearSystemImpl_ 1
5 converged value: 1 7 1 let s go get convergedRemote
5 to  comm_.min(converged) 
5 did  comm_.min(converged) convergedRemote: 1
5 final convergedRemote: 1
5 Update: x^(k+1) = x^k - deltax^k 
5 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
5 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
5 Newton iteration  3 done5 , maximum relative shift = 2.198e-03
5 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
6  before alpha = rho_new / h; 
6  before apply first correction to x 
6  before r = r - alpha*v 
6  before test stop criteria 
6 istlsolver::printOutput 
  0.5       7.9473e-28      7.35464e-17
6 === rate=5.40908e-33, T=0.122386, TIT=0.244772, IT=0.5
6 finished solveLinearSystemImpl_ 1
6 converged value: 1 7 1 let s go get convergedRemote
6 to  comm_.min(converged) 
6 did  comm_.min(converged) convergedRemote: 1
6 final convergedRemote: 1
6 Update: x^(k+1) = x^k - deltax^k 
6 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
6 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
6 Newton iteration  3 done6 , maximum relative shift = 2.198e-03
6 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
2 did  comm_.min(converged) convergedRemote: 1
2 final convergedRemote: 1
2 Update: x^(k+1) = x^k - deltax^k 
2 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
2 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
2 Newton iteration  3 done2 , maximum relative shift = 2.198e-03
2 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
0 === rate=5.40908e-33, T=0.0182149, TIT=0.0364299, IT=0.5
0 finished solveLinearSystemImpl_ 1
0 converged value: 1 7 1 let s go get convergedRemote
0 to  comm_.min(converged) 
0 did  comm_.min(converged) convergedRemote: 1
0 final convergedRemote: 1
0 Update: x^(k+1) = x^k - deltax^k 
0 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
0 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
0 Newton iteration  3 done0 , maximum relative shift = 2.198e-03
3 finished solveLinearSystemImpl_ 1
3 converged value: 1 7 1 let s go get convergedRemote
3 to  comm_.min(converged) 
3 did  comm_.min(converged) convergedRemote: 1
3 final convergedRemote: 1
3 Update: x^(k+1) = x^k - deltax^k 
3 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
3 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
3 Newton iteration  3 done3 , maximum relative shift = 2.198e-03
3 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
1 finished solveLinearSystemImpl_ 1
1 converged value: 1 7 1 let s go get convergedRemote
1 to  comm_.min(converged) 
1 did  comm_.min(converged) convergedRemote: 1
1 final convergedRemote: 1
1 Update: x^(k+1) = x^k - deltax^k 
1 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
1 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
1 Newton iteration  3 done1 , maximum relative shift = 2.198e-03
1 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
0 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
0 Solve: M deltax^k = r 
0 Newton::solveLinearSystem : entering the try section 3
0 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
0 solveLinearSystemImpl_, before converged = ls.solve 
0 7 amgbackend::solve isParallel 1
5 Solve: M deltax^k = r 
5 Newton::solveLinearSystem : entering the try section 3
5 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
5 solveLinearSystemImpl_, before converged = ls.solve 
5 7 amgbackend::solve isParallel 1
6 Solve: M deltax^k = r 
6 Newton::solveLinearSystem : entering the try section 3
6 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
6 solveLinearSystemImpl_, before converged = ls.solve 
6 7 amgbackend::solve isParallel 1
2 Solve: M deltax^k = r 
2 Newton::solveLinearSystem : entering the try section 3
2 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
2 solveLinearSystemImpl_, before converged = ls.solve 
2 7 amgbackend::solve isParallel 1
1 Solve: M deltax^k = r 
1 Newton::solveLinearSystem : entering the try section 3
1 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
1 solveLinearSystemImpl_, before converged = ls.solve 
1 7 amgbackend::solve isParallel 1
3 Solve: M deltax^k = r 
3 Newton::solveLinearSystem : entering the try section 3
3 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
3 solveLinearSystemImpl_, before converged = ls.solve 
3 7 amgbackend::solve isParallel 1
4 Solve: M deltax^k = r 
4 Newton::solveLinearSystem : entering the try section 3
4 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
4 solveLinearSystemImpl_, before converged = ls.solve 
4 7 amgbackend::solve isParallel 1
0 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
0 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 7
5 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
5 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
6 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
6 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
2 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
2 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
1 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
1 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
3 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
3 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
4 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
4 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 7
5 7 before Dune::BiCGSTABSolver 
5 Dune::BiCGSTABSolver.apply(x, b, result_) 
5 BiCGSTABSolver::apply 
5 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
6 7 before Dune::BiCGSTABSolver 
6 Dune::BiCGSTABSolver.apply(x, b, result_) 
6 BiCGSTABSolver::apply 
6 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
2 7 before Dune::BiCGSTABSolver 
2 Dune::BiCGSTABSolver.apply(x, b, result_) 
2 BiCGSTABSolver::apply 
2 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
1 7 before Dune::BiCGSTABSolver 
1 Dune::BiCGSTABSolver.apply(x, b, result_) 
1 BiCGSTABSolver::apply 
1 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 7
3 7 before Dune::BiCGSTABSolver 
3 Dune::BiCGSTABSolver.apply(x, b, result_) 
3 BiCGSTABSolver::apply 
3 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
4 7 before Dune::BiCGSTABSolver 
4 Dune::BiCGSTABSolver.apply(x, b, result_) 
4 BiCGSTABSolver::apply 
4 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
Using a direct coarse solver (SuperLU)
Building hierarchy of 1 levels (inclusive coarse solver) took 0.111917 seconds.
0 7 before Dune::BiCGSTABSolver 
0 Dune::BiCGSTABSolver.apply(x, b, result_) 
0 BiCGSTABSolver::apply 
0 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
0 === BiCGSTABSolver
 Iter          Defect            Rate
    0      8.23458e-13
0  (it = 0.5; it < _maxit; it+=.5) 0.5 250
6 === BiCGSTABSolver
 Iter          Defect            Rate
    0      8.23458e-13
6  (it = 0.5; it < _maxit; it+=.5) 0.5 250
6  before p = r 
6  before  y = W^-1 * p 
6  before _prec->apply(y,p); 0.5 250
6 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
2 === BiCGSTABSolver
 Iter          Defect            Rate
    0      8.23458e-13
2  (it = 0.5; it < _maxit; it+=.5) 0.5 250
2  before p = r 
2  before  y = W^-1 * p 
2  before _prec->apply(y,p); 0.5 250
2 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
0  before p = r 
0  before  y = W^-1 * p 
0  before _prec->apply(y,p); 0.5 250
0 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 7
0 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
4 === BiCGSTABSolver
 Iter          Defect            Rate
    0      8.23458e-13
4  (it = 0.5; it < _maxit; it+=.5) 0.5 250
4  before p = r 
4  before  y = W^-1 * p 
4  before _prec->apply(y,p); 0.5 250
4 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
5 === BiCGSTABSolver
 Iter          Defect            Rate
    0      8.23458e-13
5  (it = 0.5; it < _maxit; it+=.5) 0.5 250
5  before p = r 
5  before  y = W^-1 * p 
5  before _prec->apply(y,p); 0.5 250
5 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
3 === BiCGSTABSolver
 Iter          Defect            Rate
    0      8.23458e-13
3  (it = 0.5; it < _maxit; it+=.5) 0.5 250
3  before p = r 
3  before  y = W^-1 * p 
3  before _prec->apply(y,p); 0.5 250
3 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
1 === BiCGSTABSolver
 Iter          Defect            Rate
    0      8.23458e-13
1  (it = 0.5; it < _maxit; it+=.5) 0.5 250
1  before p = r 
1  before  y = W^-1 * p 
1  before _prec->apply(y,p); 0.5 250
1 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
2 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
2 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
6 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
6 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
6  before v = A * y 
1 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
4 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
4 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
4  before v = A * y 
5 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
5 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
5  before v = A * y 
3 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
3 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
3  before v = A * y 
0 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
0 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
4  before  alpha = rho_new / < rt, v > 
5  before  alpha = rho_new / < rt, v > 
6  before  alpha = rho_new / < rt, v > 
2  before v = A * y 
3  before  alpha = rho_new / < rt, v > 
2  before  alpha = rho_new / < rt, v > 
2  before alpha = rho_new / h; 
2  before apply first correction to x 
2  before r = r - alpha*v 
2  before test stop criteria 
2 istlsolver::printOutput 
  0.5      9.76305e-30      1.18562e-17
2 === rate=1.40568e-34, T=0.126042, TIT=0.252084, IT=0.5
2 finished solveLinearSystemImpl_ 1
2 converged value: 1 7 1 let s go get convergedRemote
2 to  comm_.min(converged) 
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
0  before v = A * y 
0  before  alpha = rho_new / < rt, v > 
0  before alpha = rho_new / h; 
0  before apply first correction to x 
0  before r = r - alpha*v 
0  before test stop criteria 
0 istlsolver::printOutput 
  0.5      9.76305e-30      1.18562e-17
0 === rate=1.40568e-34, T=0.0219325, TIT=0.0438651, IT=0.5
4  before alpha = rho_new / h; 
4  before apply first correction to x 
4  before r = r - alpha*v 
4  before test stop criteria 
4 istlsolver::printOutput 
  0.5      9.76305e-30      1.18562e-17
4 === rate=1.40568e-34, T=0.126032, TIT=0.252064, IT=0.5
4 finished solveLinearSystemImpl_ 1
4 converged value: 1 7 1 let s go get convergedRemote
4 to  comm_.min(converged) 
5  before alpha = rho_new / h; 
5  before apply first correction to x 
5  before r = r - alpha*v 
5  before test stop criteria 
5 istlsolver::printOutput 
  0.5      9.76305e-30      1.18562e-17
5 === rate=1.40568e-34, T=0.126063, TIT=0.252126, IT=0.5
5 finished solveLinearSystemImpl_ 1
5 converged value: 1 7 1 let s go get convergedRemote
5 to  comm_.min(converged) 
6  before alpha = rho_new / h; 
6  before apply first correction to x 
6  before r = r - alpha*v 
6  before test stop criteria 
6 istlsolver::printOutput 
  0.5      9.76305e-30      1.18562e-17
6 === rate=1.40568e-34, T=0.126046, TIT=0.252091, IT=0.5
6 finished solveLinearSystemImpl_ 1
6 converged value: 1 7 1 let s go get convergedRemote
6 to  comm_.min(converged) 
3  before alpha = rho_new / h; 
3  before apply first correction to x 
3  before r = r - alpha*v 
3  before test stop criteria 
3 istlsolver::printOutput 
  0.5      9.76305e-30      1.18562e-17
3 === rate=1.40568e-34, T=0.126063, TIT=0.252126, IT=0.5
3 finished solveLinearSystemImpl_ 1
3 converged value: 1 7 1 let s go get convergedRemote
3 to  comm_.min(converged) 
1 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
1  before v = A * y 
1  before  alpha = rho_new / < rt, v > 
1  before alpha = rho_new / h; 
1  before apply first correction to x 
1  before r = r - alpha*v 
1  before test stop criteria 
1 istlsolver::printOutput 
  0.5      9.76305e-30      1.18562e-17
1 === rate=1.40568e-34, T=0.126049, TIT=0.252099, IT=0.5
1 finished solveLinearSystemImpl_ 1
1 converged value: 1 7 1 let s go get convergedRemote
1 to  comm_.min(converged) 
6 did  comm_.min(converged) convergedRemote: 1
6 final convergedRemote: 1
6 Update: x^(k+1) = x^k - deltax^k 
6 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
6 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
6 Newton iteration  4 done6 , maximum relative shift = 2.234e-04
6 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
2 did  comm_.min(converged) convergedRemote: 1
2 final convergedRemote: 1
2 Update: x^(k+1) = x^k - deltax^k 
2 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
2 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
2 Newton iteration  4 done2 , maximum relative shift = 2.234e-04
2 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
0 finished solveLinearSystemImpl_ 1
0 converged value: 1 7 1 let s go get convergedRemote
0 to  comm_.min(converged) 
0 did  comm_.min(converged) convergedRemote: 1
0 final convergedRemote: 1
0 Update: x^(k+1) = x^k - deltax^k 
0 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
0 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
0 Newton iteration  4 done0 , maximum relative shift = 2.234e-04
0 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
4 did  comm_.min(converged) convergedRemote: 1
4 final convergedRemote: 1
4 Update: x^(k+1) = x^k - deltax^k 
4 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
4 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
4 Newton iteration  4 done4 , maximum relative shift = 2.234e-04
4 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
5 did  comm_.min(converged) convergedRemote: 1
5 final convergedRemote: 1
5 Update: x^(k+1) = x^k - deltax^k 
5 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
5 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
5 Newton iteration  4 done5 , maximum relative shift = 2.234e-04
5 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
1 did  comm_.min(converged) convergedRemote: 1
1 final convergedRemote: 1
1 Update: x^(k+1) = x^k - deltax^k 
1 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
1 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
1 Newton iteration  4 done1 , maximum relative shift = 2.234e-04
1 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
3 did  comm_.min(converged) convergedRemote: 1
3 final convergedRemote: 1
3 Update: x^(k+1) = x^k - deltax^k 
3 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
3 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
3 Newton iteration  4 done3 , maximum relative shift = 2.234e-04
3 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
1 Solve: M deltax^k = r 
1 Newton::solveLinearSystem : entering the try section 4
1 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
1 solveLinearSystemImpl_, before converged = ls.solve 
1 7 amgbackend::solve isParallel 1
5 Solve: M deltax^k = r 
5 Newton::solveLinearSystem : entering the try section 4
5 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
5 solveLinearSystemImpl_, before converged = ls.solve 
5 7 amgbackend::solve isParallel 1
6 Solve: M deltax^k = r 
6 Newton::solveLinearSystem : entering the try section 4
6 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
6 solveLinearSystemImpl_, before converged = ls.solve 
6 7 amgbackend::solve isParallel 1
2 Solve: M deltax^k = r 
2 Newton::solveLinearSystem : entering the try section 4
2 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
2 solveLinearSystemImpl_, before converged = ls.solve 
2 7 amgbackend::solve isParallel 1
0 Solve: M deltax^k = r 
0 Newton::solveLinearSystem : entering the try section 4
0 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
0 solveLinearSystemImpl_, before converged = ls.solve 
0 7 amgbackend::solve isParallel 1
4 Solve: M deltax^k = r 
4 Newton::solveLinearSystem : entering the try section 4
4 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
4 solveLinearSystemImpl_, before converged = ls.solve 
4 7 amgbackend::solve isParallel 1
3 Solve: M deltax^k = r 
3 Newton::solveLinearSystem : entering the try section 4
3 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
3 solveLinearSystemImpl_, before converged = ls.solve 
3 7 amgbackend::solve isParallel 1
3 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
3 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
4 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
4 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
1 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
1 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
5 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
5 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
6 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
6 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
2 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
2 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
0 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
0 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 7
3 7 before Dune::BiCGSTABSolver 
3 Dune::BiCGSTABSolver.apply(x, b, result_) 
3 BiCGSTABSolver::apply 
3 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
4 7 before Dune::BiCGSTABSolver 
4 Dune::BiCGSTABSolver.apply(x, b, result_) 
4 BiCGSTABSolver::apply 
4 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
1 7 before Dune::BiCGSTABSolver 
1 Dune::BiCGSTABSolver.apply(x, b, result_) 
1 BiCGSTABSolver::apply 
1 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
5 7 before Dune::BiCGSTABSolver 
5 Dune::BiCGSTABSolver.apply(x, b, result_) 
5 BiCGSTABSolver::apply 
5 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
6 7 before Dune::BiCGSTABSolver 
6 Dune::BiCGSTABSolver.apply(x, b, result_) 
6 BiCGSTABSolver::apply 
6 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
2 7 before Dune::BiCGSTABSolver 
2 Dune::BiCGSTABSolver.apply(x, b, result_) 
2 BiCGSTABSolver::apply 
2 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
Using a direct coarse solver (SuperLU)
Building hierarchy of 1 levels (inclusive coarse solver) took 0.113219 seconds.
0 7 before Dune::BiCGSTABSolver 
0 Dune::BiCGSTABSolver.apply(x, b, result_) 
0 BiCGSTABSolver::apply 
0 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
0 === BiCGSTABSolver
 Iter          Defect            Rate
    0        1.916e-13
0  (it = 0.5; it < _maxit; it+=.5) 0.5 250
1 === BiCGSTABSolver
 Iter          Defect            Rate
    0        1.916e-13
1  (it = 0.5; it < _maxit; it+=.5) 0.5 250
1  before p = r 
1  before  y = W^-1 * p 
1  before _prec->apply(y,p); 0.5 250
1 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
5 === BiCGSTABSolver
 Iter          Defect            Rate
    0        1.916e-13
5  (it = 0.5; it < _maxit; it+=.5) 0.5 250
5  before p = r 
5  before  y = W^-1 * p 
5  before _prec->apply(y,p); 0.5 250
5 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
6 === BiCGSTABSolver
 Iter          Defect            Rate
    0        1.916e-13
6  (it = 0.5; it < _maxit; it+=.5) 0.5 250
6  before p = r 
6  before  y = W^-1 * p 
6  before _prec->apply(y,p); 0.5 250
6 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
0  before p = r 
0  before  y = W^-1 * p 
0  before _prec->apply(y,p); 0.5 250
0 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 7
0 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
2 === BiCGSTABSolver
 Iter          Defect            Rate
    0        1.916e-13
2  (it = 0.5; it < _maxit; it+=.5) 0.5 250
2  before p = r 
2  before  y = W^-1 * p 
2  before _prec->apply(y,p); 0.5 250
2 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
3 === BiCGSTABSolver
 Iter          Defect            Rate
    0        1.916e-13
3  (it = 0.5; it < _maxit; it+=.5) 0.5 250
3  before p = r 
3  before  y = W^-1 * p 
3  before _prec->apply(y,p); 0.5 250
3 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
4 === BiCGSTABSolver
 Iter          Defect            Rate
    0        1.916e-13
4  (it = 0.5; it < _maxit; it+=.5) 0.5 250
4  before p = r 
4  before  y = W^-1 * p 
4  before _prec->apply(y,p); 0.5 250
4 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
6 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
6 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
6  before v = A * y 
1 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
5 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
5 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
5  before v = A * y 
4 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
4 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
4  before v = A * y 
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
0 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
2 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
2 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
3 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
3 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
3  before v = A * y 
3  before  alpha = rho_new / < rt, v > 
4  before  alpha = rho_new / < rt, v > 
0 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
0  before v = A * y 
2  before v = A * y 
2  before  alpha = rho_new / < rt, v > 
5  before  alpha = rho_new / < rt, v > 
6  before  alpha = rho_new / < rt, v > 
1 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
1  before v = A * y 
1  before  alpha = rho_new / < rt, v > 
1  before alpha = rho_new / h; 
1  before apply first correction to x 
1  before r = r - alpha*v 
1  before test stop criteria 
1 istlsolver::printOutput 
  0.5      2.52436e-29      1.31751e-16
1 === rate=1.73583e-32, T=0.12703, TIT=0.254059, IT=0.5
1 finished solveLinearSystemImpl_ 1
1 converged value: 1 7 1 let s go get convergedRemote
1 to  comm_.min(converged) 
3  before alpha = rho_new / h; 
3  before apply first correction to x 
3  before r = r - alpha*v 
3  before test stop criteria 
3 istlsolver::printOutput 
  0.5      2.52436e-29      1.31751e-16
3 === rate=1.73583e-32, T=0.127042, TIT=0.254085, IT=0.5
3 finished solveLinearSystemImpl_ 1
3 converged value: 1 7 1 let s go get convergedRemote
3 to  comm_.min(converged) 
4  before alpha = rho_new / h; 
4  before apply first correction to x 
4  before r = r - alpha*v 
4  before test stop criteria 
4 istlsolver::printOutput 
  0.5      2.52436e-29      1.31751e-16
4 === rate=1.73583e-32, T=0.126991, TIT=0.253983, IT=0.5
4 finished solveLinearSystemImpl_ 1
4 converged value: 1 7 1 let s go get convergedRemote
4 to  comm_.min(converged) 
0  before  alpha = rho_new / < rt, v > 
0  before alpha = rho_new / h; 
0  before apply first correction to x 
0  before r = r - alpha*v 
0  before test stop criteria 
0 istlsolver::printOutput 
  0.5      2.52436e-29      1.31751e-16
0 === rate=1.73583e-32, T=0.0218226, TIT=0.0436453, IT=0.5
0 finished solveLinearSystemImpl_ 1
0 converged value: 1 7 1 let s go get convergedRemote
0 to  comm_.min(converged) 
0 did  comm_.min(converged) convergedRemote: 1
0 final convergedRemote: 1
0 Update: x^(k+1) = x^k - deltax^k 
0 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
2  before alpha = rho_new / h; 
2  before apply first correction to x 
2  before r = r - alpha*v 
2  before test stop criteria 
2 istlsolver::printOutput 
  0.5      2.52436e-29      1.31751e-16
2 === rate=1.73583e-32, T=0.126992, TIT=0.253984, IT=0.5
2 finished solveLinearSystemImpl_ 1
2 converged value: 1 7 1 let s go get convergedRemote
2 to  comm_.min(converged) 
2 did  comm_.min(converged) convergedRemote: 1
2 final convergedRemote: 1
2 Update: x^(k+1) = x^k - deltax^k 
2 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
2 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
5  before alpha = rho_new / h; 
5  before apply first correction to x 
5  before r = r - alpha*v 
5  before test stop criteria 
5 istlsolver::printOutput 
  0.5      2.52436e-29      1.31751e-16
5 === rate=1.73583e-32, T=0.127041, TIT=0.254083, IT=0.5
5 finished solveLinearSystemImpl_ 1
5 converged value: 1 7 1 let s go get convergedRemote
5 to  comm_.min(converged) 
6  before alpha = rho_new / h; 
6  before apply first correction to x 
6  before r = r - alpha*v 
6  before test stop criteria 
6 istlsolver::printOutput 
  0.5      2.52436e-29      1.31751e-16
6 === rate=1.73583e-32, T=0.126993, TIT=0.253985, IT=0.5
6 finished solveLinearSystemImpl_ 1
6 converged value: 1 7 1 let s go get convergedRemote
6 to  comm_.min(converged) 
6 did  comm_.min(converged) convergedRemote: 1
6 final convergedRemote: 1
6 Update: x^(k+1) = x^k - deltax^k 
6 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
6 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
6 Newton iteration  5 done6 , maximum relative shift = 3.906e-05
6 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
5 did  comm_.min(converged) convergedRemote: 1
5 final convergedRemote: 1
5 Update: x^(k+1) = x^k - deltax^k 
5 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
5 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
5 Newton iteration  5 done5 , maximum relative shift = 3.906e-05
5 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
1 did  comm_.min(converged) convergedRemote: 1
1 final convergedRemote: 1
1 Update: x^(k+1) = x^k - deltax^k 
1 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
1 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
1 Newton iteration  5 done1 , maximum relative shift = 3.906e-05
1 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
3 did  comm_.min(converged) convergedRemote: 1
3 final convergedRemote: 1
3 Update: x^(k+1) = x^k - deltax^k 
3 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
3 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
3 Newton iteration  5 done3 , maximum relative shift = 3.906e-05
3 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
4 did  comm_.min(converged) convergedRemote: 1
4 final convergedRemote: 1
4 Update: x^(k+1) = x^k - deltax^k 
4 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
4 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
4 Newton iteration  5 done4 , maximum relative shift = 3.906e-05
4 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
0 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
0 Newton iteration  5 done0 , maximum relative shift = 3.906e-05
0 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
2 Newton iteration  5 done2 , maximum relative shift = 3.906e-05
2 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
2 Solve: M deltax^k = r 
2 Newton::solveLinearSystem : entering the try section 5
2 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
2 solveLinearSystemImpl_, before converged = ls.solve 
2 7 amgbackend::solve isParallel 1
5 Solve: M deltax^k = r 
5 Newton::solveLinearSystem : entering the try section 5
5 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
5 solveLinearSystemImpl_, before converged = ls.solve 
5 7 amgbackend::solve isParallel 1
1 Solve: M deltax^k = r 
1 Newton::solveLinearSystem : entering the try section 5
1 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
1 solveLinearSystemImpl_, before converged = ls.solve 
1 7 amgbackend::solve isParallel 1
3 Solve: M deltax^k = r 
3 Newton::solveLinearSystem : entering the try section 5
3 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
3 solveLinearSystemImpl_, before converged = ls.solve 
3 7 amgbackend::solve isParallel 1
4 Solve: M deltax^k = r 
4 Newton::solveLinearSystem : entering the try section 5
4 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
4 solveLinearSystemImpl_, before converged = ls.solve 
4 7 amgbackend::solve isParallel 1
0 Solve: M deltax^k = r 
0 Newton::solveLinearSystem : entering the try section 5
0 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
0 solveLinearSystemImpl_, before converged = ls.solve 
0 7 amgbackend::solve isParallel 1
6 Solve: M deltax^k = r 
6 Newton::solveLinearSystem : entering the try section 5
6 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
6 solveLinearSystemImpl_, before converged = ls.solve 
6 7 amgbackend::solve isParallel 1
6 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
6 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
2 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
2 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
5 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
5 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
1 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
1 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
3 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
3 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
4 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
4 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
0 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
0 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 7
5 7 before Dune::BiCGSTABSolver 
5 Dune::BiCGSTABSolver.apply(x, b, result_) 
5 BiCGSTABSolver::apply 
5 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
1 7 before Dune::BiCGSTABSolver 
1 Dune::BiCGSTABSolver.apply(x, b, result_) 
1 BiCGSTABSolver::apply 
1 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
3 7 before Dune::BiCGSTABSolver 
3 Dune::BiCGSTABSolver.apply(x, b, result_) 
3 BiCGSTABSolver::apply 
3 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
4 7 before Dune::BiCGSTABSolver 
4 Dune::BiCGSTABSolver.apply(x, b, result_) 
4 BiCGSTABSolver::apply 
4 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 7
6 7 before Dune::BiCGSTABSolver 
6 Dune::BiCGSTABSolver.apply(x, b, result_) 
6 BiCGSTABSolver::apply 
6 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
2 7 before Dune::BiCGSTABSolver 
2 Dune::BiCGSTABSolver.apply(x, b, result_) 
2 BiCGSTABSolver::apply 
2 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
Using a direct coarse solver (SuperLU)
Building hierarchy of 1 levels (inclusive coarse solver) took 0.112621 seconds.
0 7 before Dune::BiCGSTABSolver 
0 Dune::BiCGSTABSolver.apply(x, b, result_) 
0 BiCGSTABSolver::apply 
0 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
0 === BiCGSTABSolver
 Iter          Defect            Rate
1 === BiCGSTABSolver
 Iter          Defect            Rate
    0      1.43538e-14
1  (it = 0.5; it < _maxit; it+=.5) 0.5 250
1  before p = r 
1  before  y = W^-1 * p 
1  before _prec->apply(y,p); 0.5 250
1 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
3 === BiCGSTABSolver
 Iter          Defect            Rate
    0      1.43538e-14
3  (it = 0.5; it < _maxit; it+=.5) 0.5 250
3  before p = r 
3  before  y = W^-1 * p 
3  before _prec->apply(y,p); 0.5 250
3 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
    0      1.43538e-14
0  (it = 0.5; it < _maxit; it+=.5) 0.5 250
0  before p = r 
0  before  y = W^-1 * p 
0  before _prec->apply(y,p); 0.5 250
0 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 7
0 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
2 === BiCGSTABSolver
 Iter          Defect            Rate
    0      1.43538e-14
2  (it = 0.5; it < _maxit; it+=.5) 0.5 250
2  before p = r 
2  before  y = W^-1 * p 
2  before _prec->apply(y,p); 0.5 250
2 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
6 === BiCGSTABSolver
 Iter          Defect            Rate
    0      1.43538e-14
6  (it = 0.5; it < _maxit; it+=.5) 0.5 250
6  before p = r 
6  before  y = W^-1 * p 
6  before _prec->apply(y,p); 0.5 250
6 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
4 === BiCGSTABSolver
 Iter          Defect            Rate
    0      1.43538e-14
4  (it = 0.5; it < _maxit; it+=.5) 0.5 250
4  before p = r 
4  before  y = W^-1 * p 
4  before _prec->apply(y,p); 0.5 250
4 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
5 === BiCGSTABSolver
 Iter          Defect            Rate
    0      1.43538e-14
5  (it = 0.5; it < _maxit; it+=.5) 0.5 250
5  before p = r 
5  before  y = W^-1 * p 
5  before _prec->apply(y,p); 0.5 250
5 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
5 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
5 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
5  before v = A * y 
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
0 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
2 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
2 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
6 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
6 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
6  before v = A * y 
6  before  alpha = rho_new / < rt, v > 
4 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
4 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
4  before v = A * y 
4  before  alpha = rho_new / < rt, v > 
3 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
3 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
3  before v = A * y 
1 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
3  before  alpha = rho_new / < rt, v > 
1 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
5  before  alpha = rho_new / < rt, v > 
0 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
0  before v = A * y 
2  before v = A * y 
2  before  alpha = rho_new / < rt, v > 
6  before alpha = rho_new / h; 
6  before apply first correction to x 
6  before r = r - alpha*v 
6  before test stop criteria 
6 istlsolver::printOutput 
  0.5      5.38242e-34      3.74982e-20
6 === rate=1.40612e-39, T=0.123133, TIT=0.246266, IT=0.5
6 finished solveLinearSystemImpl_ 1
6 converged value: 1 7 1 let s go get convergedRemote
6 to  comm_.min(converged) 
4  before alpha = rho_new / h; 
4  before apply first correction to x 
4  before r = r - alpha*v 
4  before test stop criteria 
4 istlsolver::printOutput 
  0.5      5.38242e-34      3.74982e-20
4 === rate=1.40612e-39, T=0.123131, TIT=0.246263, IT=0.5
4 finished solveLinearSystemImpl_ 1
4 converged value: 1 7 1 let s go get convergedRemote
4 to  comm_.min(converged) 
3  before alpha = rho_new / h; 
3  before apply first correction to x 
3  before r = r - alpha*v 
3  before test stop criteria 
3 istlsolver::printOutput 
  0.5      5.38242e-34      3.74982e-20
3 === rate=1.40612e-39, T=0.123143, TIT=0.246286, IT=0.5
3 finished solveLinearSystemImpl_ 1
3 converged value: 1 7 1 let s go get convergedRemote
3 to  comm_.min(converged) 
2  before alpha = rho_new / h; 
2  before apply first correction to x 
2  before r = r - alpha*v 
2  before test stop criteria 
2 istlsolver::printOutput 
  0.5      5.38242e-34      3.74982e-20
2 === rate=1.40612e-39, T=0.123127, TIT=0.246253, IT=0.5
2 finished solveLinearSystemImpl_ 1
2 converged value: 1 7 1 let s go get convergedRemote
2 to  comm_.min(converged) 
1  before v = A * y 
1  before  alpha = rho_new / < rt, v > 
1  before alpha = rho_new / h; 
1  before apply first correction to x 
1  before r = r - alpha*v 
1  before test stop criteria 
1 istlsolver::printOutput 
  0.5      5.38242e-34      3.74982e-20
1 === rate=1.40612e-39, T=0.123147, TIT=0.246293, IT=0.5
1 finished solveLinearSystemImpl_ 1
1 converged value: 1 7 1 let s go get convergedRemote
1 to  comm_.min(converged) 
5  before alpha = rho_new / h; 
5  before apply first correction to x 
5  before r = r - alpha*v 
5  before test stop criteria 
5 istlsolver::printOutput 
  0.5      5.38242e-34      3.74982e-20
5 === rate=1.40612e-39, T=0.123142, TIT=0.246285, IT=0.5
5 finished solveLinearSystemImpl_ 1
5 converged value: 1 7 1 let s go get convergedRemote
5 to  comm_.min(converged) 
0  before  alpha = rho_new / < rt, v > 
0  before alpha = rho_new / h; 
0  before apply first correction to x 
0  before r = r - alpha*v 
0  before test stop criteria 
0 istlsolver::printOutput 
  0.5      5.38242e-34      3.74982e-20
0 === rate=1.40612e-39, T=0.018386, TIT=0.036772, IT=0.5
4 did  comm_.min(converged) convergedRemote: 1
4 final convergedRemote: 1
4 Update: x^(k+1) = x^k - deltax^k 
4 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
4 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
4 Newton iteration  6 done4 , maximum relative shift = 2.926e-06
4 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
0 finished solveLinearSystemImpl_ 1
0 converged value: 1 7 1 let s go get convergedRemote
0 to  comm_.min(converged) 
0 did  comm_.min(converged) convergedRemote: 1
0 final convergedRemote: 1
0 Update: x^(k+1) = x^k - deltax^k 
0 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
0 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
0 Newton iteration  6 done0 , maximum relative shift = 2.926e-06
0 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
2 did  comm_.min(converged) convergedRemote: 1
2 final convergedRemote: 1
2 Update: x^(k+1) = x^k - deltax^k 
2 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
2 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
2 Newton iteration  6 done2 , maximum relative shift = 2.926e-06
2 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
1 did  comm_.min(converged) convergedRemote: 1
1 final convergedRemote: 1
1 Update: x^(k+1) = x^k - deltax^k 
1 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
1 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
1 Newton iteration  6 done1 , maximum relative shift = 2.926e-06
1 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
5 did  comm_.min(converged) convergedRemote: 1
5 final convergedRemote: 1
5 Update: x^(k+1) = x^k - deltax^k 
5 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
5 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
5 Newton iteration  6 done5 , maximum relative shift = 2.926e-06
5 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
3 did  comm_.min(converged) convergedRemote: 1
3 final convergedRemote: 1
3 Update: x^(k+1) = x^k - deltax^k 
3 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
3 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
3 Newton iteration  6 done3 , maximum relative shift = 2.926e-06
3 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
6 did  comm_.min(converged) convergedRemote: 1
6 final convergedRemote: 1
6 Update: x^(k+1) = x^k - deltax^k 
6 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
6 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
6 Newton iteration  6 done6 , maximum relative shift = 2.926e-06
6 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
2 Solve: M deltax^k = r 
2 Newton::solveLinearSystem : entering the try section 6
2 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
2 solveLinearSystemImpl_, before converged = ls.solve 
2 7 amgbackend::solve isParallel 1
1 Solve: M deltax^k = r 
1 Newton::solveLinearSystem : entering the try section 6
1 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
1 solveLinearSystemImpl_, before converged = ls.solve 
1 7 amgbackend::solve isParallel 1
5 Solve: M deltax^k = r 
5 Newton::solveLinearSystem : entering the try section 6
5 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
5 solveLinearSystemImpl_, before converged = ls.solve 
5 7 amgbackend::solve isParallel 1
3 Solve: M deltax^k = r 
3 Newton::solveLinearSystem : entering the try section 6
3 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
3 solveLinearSystemImpl_, before converged = ls.solve 
3 7 amgbackend::solve isParallel 1
4 Solve: M deltax^k = r 
4 Newton::solveLinearSystem : entering the try section 6
4 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
4 solveLinearSystemImpl_, before converged = ls.solve 
4 7 amgbackend::solve isParallel 1
6 Solve: M deltax^k = r 
6 Newton::solveLinearSystem : entering the try section 6
6 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
6 solveLinearSystemImpl_, before converged = ls.solve 
6 7 amgbackend::solve isParallel 1
0 Solve: M deltax^k = r 
0 Newton::solveLinearSystem : entering the try section 6
0 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
0 solveLinearSystemImpl_, before converged = ls.solve 
0 7 amgbackend::solve isParallel 1
0 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
0 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 7
6 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
6 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
4 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
4 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
2 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
2 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
1 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
1 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
5 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
5 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
3 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
3 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 7
5 7 before Dune::BiCGSTABSolver 
5 Dune::BiCGSTABSolver.apply(x, b, result_) 
5 BiCGSTABSolver::apply 
5 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 7
3 7 before Dune::BiCGSTABSolver 
3 Dune::BiCGSTABSolver.apply(x, b, result_) 
3 BiCGSTABSolver::apply 
3 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
6 7 before Dune::BiCGSTABSolver 
6 Dune::BiCGSTABSolver.apply(x, b, result_) 
6 BiCGSTABSolver::apply 
6 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
4 7 before Dune::BiCGSTABSolver 
4 Dune::BiCGSTABSolver.apply(x, b, result_) 
4 BiCGSTABSolver::apply 
4 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
2 7 before Dune::BiCGSTABSolver 
2 Dune::BiCGSTABSolver.apply(x, b, result_) 
2 BiCGSTABSolver::apply 
2 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
1 7 before Dune::BiCGSTABSolver 
1 Dune::BiCGSTABSolver.apply(x, b, result_) 
1 BiCGSTABSolver::apply 
1 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
Using a direct coarse solver (SuperLU)
Building hierarchy of 1 levels (inclusive coarse solver) took 0.111544 seconds.
0 7 before Dune::BiCGSTABSolver 
0 Dune::BiCGSTABSolver.apply(x, b, result_) 
0 BiCGSTABSolver::apply 
0 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
0 === BiCGSTABSolver
 Iter          Defect            Rate
    0      1.07567e-15
0  (it = 0.5; it < _maxit; it+=.5) 0.5 250
2 === BiCGSTABSolver
 Iter          Defect            Rate
    0      1.07567e-15
2  (it = 0.5; it < _maxit; it+=.5) 0.5 250
2  before p = r 
2  before  y = W^-1 * p 
2  before _prec->apply(y,p); 0.5 250
2 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
3 === BiCGSTABSolver
 Iter          Defect            Rate
    0      1.07567e-15
3  (it = 0.5; it < _maxit; it+=.5) 0.5 250
3  before p = r 
3  before  y = W^-1 * p 
3  before _prec->apply(y,p); 0.5 250
3 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
0  before p = r 
0  before  y = W^-1 * p 
0  before _prec->apply(y,p); 0.5 250
0 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 7
0 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
5 === BiCGSTABSolver
 Iter          Defect            Rate
    0      1.07567e-15
5  (it = 0.5; it < _maxit; it+=.5) 0.5 250
5  before p = r 
5  before  y = W^-1 * p 
5  before _prec->apply(y,p); 0.5 250
5 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
1 === BiCGSTABSolver
 Iter          Defect            Rate
    0      1.07567e-15
1  (it = 0.5; it < _maxit; it+=.5) 0.5 250
1  before p = r 
1  before  y = W^-1 * p 
1  before _prec->apply(y,p); 0.5 250
1 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
6 === BiCGSTABSolver
 Iter          Defect            Rate
    0      1.07567e-15
6  (it = 0.5; it < _maxit; it+=.5) 0.5 250
6  before p = r 
6  before  y = W^-1 * p 
6  before _prec->apply(y,p); 0.5 250
6 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
4 === BiCGSTABSolver
 Iter          Defect            Rate
    0      1.07567e-15
4  (it = 0.5; it < _maxit; it+=.5) 0.5 250
4  before p = r 
4  before  y = W^-1 * p 
4  before _prec->apply(y,p); 0.5 250
4 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
3 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
3 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
3  before v = A * y 
3  before  alpha = rho_new / < rt, v > 
2 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
2 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
4 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
4 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
4  before v = A * y 
4  before  alpha = rho_new / < rt, v > 
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
0 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
0 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
5 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
5 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
5  before v = A * y 
5  before  alpha = rho_new / < rt, v > 
1 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
1 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
6 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
6 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
6  before v = A * y 
6  before  alpha = rho_new / < rt, v > 
0  before v = A * y 
0  before  alpha = rho_new / < rt, v > 
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
1  before v = A * y 
2  before v = A * y 
2  before  alpha = rho_new / < rt, v > 
3  before alpha = rho_new / h; 
3  before apply first correction to x 
3  before r = r - alpha*v 
3  before test stop criteria 
3 istlsolver::printOutput 
  0.5      3.61728e-32      3.36283e-17
3 === rate=1.13086e-33, T=0.122379, TIT=0.244758, IT=0.5
3 finished solveLinearSystemImpl_ 1
3 converged value: 1 7 1 let s go get convergedRemote
3 to  comm_.min(converged) 
2  before alpha = rho_new / h; 
2  before apply first correction to x 
2  before r = r - alpha*v 
2  before test stop criteria 
2 istlsolver::printOutput 
  0.5      3.61728e-32      3.36283e-17
2 === rate=1.13086e-33, T=0.122191, TIT=0.244382, IT=0.5
2 finished solveLinearSystemImpl_ 1
2 converged value: 1 7 1 let s go get convergedRemote
2 to  comm_.min(converged) 
4  before alpha = rho_new / h; 
4  before apply first correction to x 
4  before r = r - alpha*v 
4  before test stop criteria 
4 istlsolver::printOutput 
  0.5      3.61728e-32      3.36283e-17
4 === rate=1.13086e-33, T=0.122188, TIT=0.244376, IT=0.5
4 finished solveLinearSystemImpl_ 1
4 converged value: 1 7 1 let s go get convergedRemote
4 to  comm_.min(converged) 
6  before alpha = rho_new / h; 
6  before apply first correction to x 
6  before r = r - alpha*v 
6  before test stop criteria 
6 istlsolver::printOutput 
  0.5      3.61728e-32      3.36283e-17
6 === rate=1.13086e-33, T=0.122187, TIT=0.244374, IT=0.5
6 finished solveLinearSystemImpl_ 1
6 converged value: 1 7 1 let s go get convergedRemote
6 to  comm_.min(converged) 
5  before alpha = rho_new / h; 
5  before apply first correction to x 
5  before r = r - alpha*v 
5  before test stop criteria 
5 istlsolver::printOutput 
  0.5      3.61728e-32      3.36283e-17
5 === rate=1.13086e-33, T=0.122378, TIT=0.244757, IT=0.5
5 finished solveLinearSystemImpl_ 1
5 converged value: 1 7 1 let s go get convergedRemote
5 to  comm_.min(converged) 
0  before alpha = rho_new / h; 
0  before apply first correction to x 
0  before r = r - alpha*v 
0  before test stop criteria 
0 istlsolver::printOutput 
  0.5      3.61728e-32      3.36283e-17
0 === rate=1.13086e-33, T=0.0185051, TIT=0.0370101, IT=0.5
1  before  alpha = rho_new / < rt, v > 
1  before alpha = rho_new / h; 
1  before apply first correction to x 
1  before r = r - alpha*v 
1  before test stop criteria 
1 istlsolver::printOutput 
  0.5      3.61728e-32      3.36283e-17
1 === rate=1.13086e-33, T=0.122382, TIT=0.244764, IT=0.5
1 finished solveLinearSystemImpl_ 1
1 converged value: 1 7 1 let s go get convergedRemote
1 to  comm_.min(converged) 
3 did  comm_.min(converged) convergedRemote: 1
3 final convergedRemote: 1
3 Update: x^(k+1) = x^k - deltax^k 
3 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
3 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
3 Newton iteration  7 done3 , maximum relative shift = 1.755e-07
3 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
1 did  comm_.min(converged) convergedRemote: 1
1 final convergedRemote: 1
1 Update: x^(k+1) = x^k - deltax^k 
1 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
1 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
1 Newton iteration  7 done1 , maximum relative shift = 1.755e-07
1 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
2 did  comm_.min(converged) convergedRemote: 1
2 final convergedRemote: 1
2 Update: x^(k+1) = x^k - deltax^k 
2 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
2 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
2 Newton iteration  7 done2 , maximum relative shift = 1.755e-07
2 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
4 did  comm_.min(converged) convergedRemote: 1
4 final convergedRemote: 1
4 Update: x^(k+1) = x^k - deltax^k 
4 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
4 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
4 Newton iteration  7 done4 , maximum relative shift = 1.755e-07
4 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
6 did  comm_.min(converged) convergedRemote: 1
6 final convergedRemote: 1
6 Update: x^(k+1) = x^k - deltax^k 
6 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
6 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
6 Newton iteration  7 done6 , maximum relative shift = 1.755e-07
6 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
5 did  comm_.min(converged) convergedRemote: 1
5 final convergedRemote: 1
5 Update: x^(k+1) = x^k - deltax^k 
5 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
5 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
5 Newton iteration  7 done5 , maximum relative shift = 1.755e-07
5 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
0 finished solveLinearSystemImpl_ 1
0 converged value: 1 7 1 let s go get convergedRemote
0 to  comm_.min(converged) 
0 did  comm_.min(converged) convergedRemote: 1
0 final convergedRemote: 1
0 Update: x^(k+1) = x^k - deltax^k 
0 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
0 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
0 Newton iteration  7 done0 , maximum relative shift = 1.755e-07
0 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
5 Solve: M deltax^k = r 
5 Newton::solveLinearSystem : entering the try section 7
5 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
5 solveLinearSystemImpl_, before converged = ls.solve 
5 7 amgbackend::solve isParallel 1
3 Solve: M deltax^k = r 
3 Newton::solveLinearSystem : entering the try section 7
3 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
3 solveLinearSystemImpl_, before converged = ls.solve 
3 7 amgbackend::solve isParallel 1
0 Solve: M deltax^k = r 
0 Newton::solveLinearSystem : entering the try section 7
0 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
0 solveLinearSystemImpl_, before converged = ls.solve 
0 7 amgbackend::solve isParallel 1
1 Solve: M deltax^k = r 
1 Newton::solveLinearSystem : entering the try section 7
1 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
1 solveLinearSystemImpl_, before converged = ls.solve 
1 7 amgbackend::solve isParallel 1
2 Solve: M deltax^k = r 
2 Newton::solveLinearSystem : entering the try section 7
2 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
2 solveLinearSystemImpl_, before converged = ls.solve 
2 7 amgbackend::solve isParallel 1
4 Solve: M deltax^k = r 
4 Newton::solveLinearSystem : entering the try section 7
4 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
4 solveLinearSystemImpl_, before converged = ls.solve 
4 7 amgbackend::solve isParallel 1
6 Solve: M deltax^k = r 
6 Newton::solveLinearSystem : entering the try section 7
6 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
6 solveLinearSystemImpl_, before converged = ls.solve 
6 7 amgbackend::solve isParallel 1
5 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
5 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
6 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
6 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
0 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
0 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 7
1 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
1 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
2 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
2 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
4 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
4 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
3 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
3 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 7
6 7 before Dune::BiCGSTABSolver 
6 Dune::BiCGSTABSolver.apply(x, b, result_) 
6 BiCGSTABSolver::apply 
6 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
3 7 before Dune::BiCGSTABSolver 
3 Dune::BiCGSTABSolver.apply(x, b, result_) 
3 BiCGSTABSolver::apply 
3 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
1 7 before Dune::BiCGSTABSolver 
1 Dune::BiCGSTABSolver.apply(x, b, result_) 
1 BiCGSTABSolver::apply 
1 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
2 7 before Dune::BiCGSTABSolver 
2 Dune::BiCGSTABSolver.apply(x, b, result_) 
2 BiCGSTABSolver::apply 
2 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
4 7 before Dune::BiCGSTABSolver 
4 Dune::BiCGSTABSolver.apply(x, b, result_) 
4 BiCGSTABSolver::apply 
4 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
5 7 before Dune::BiCGSTABSolver 
5 Dune::BiCGSTABSolver.apply(x, b, result_) 
5 BiCGSTABSolver::apply 
5 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 7
Using a direct coarse solver (SuperLU)
Building hierarchy of 1 levels (inclusive coarse solver) took 0.111853 seconds.
0 7 before Dune::BiCGSTABSolver 
0 Dune::BiCGSTABSolver.apply(x, b, result_) 
0 BiCGSTABSolver::apply 
0 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
1 === BiCGSTABSolver
 Iter          Defect            Rate
    0       2.7939e-16
1  (it = 0.5; it < _maxit; it+=.5) 0.5 250
1  before p = r 
1  before  y = W^-1 * p 
1  before _prec->apply(y,p); 0.5 250
1 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
2 === BiCGSTABSolver
 Iter          Defect            Rate
    0       2.7939e-16
2  (it = 0.5; it < _maxit; it+=.5) 0.5 250
2  before p = r 
2  before  y = W^-1 * p 
2  before _prec->apply(y,p); 0.5 250
2 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
4 === BiCGSTABSolver
 Iter          Defect            Rate
    0       2.7939e-16
4  (it = 0.5; it < _maxit; it+=.5) 0.5 250
4  before p = r 
4  before  y = W^-1 * p 
4  before _prec->apply(y,p); 0.5 250
4 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
0 === BiCGSTABSolver
 Iter          Defect            Rate
    0       2.7939e-16
0  (it = 0.5; it < _maxit; it+=.5) 0.5 250
0  before p = r 
0  before  y = W^-1 * p 
0  before _prec->apply(y,p); 0.5 250
0 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 7
0 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
5 === BiCGSTABSolver
 Iter          Defect            Rate
    0       2.7939e-16
5  (it = 0.5; it < _maxit; it+=.5) 0.5 250
5  before p = r 
5  before  y = W^-1 * p 
5  before _prec->apply(y,p); 0.5 250
5 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
6 === BiCGSTABSolver
 Iter          Defect            Rate
    0       2.7939e-16
6  (it = 0.5; it < _maxit; it+=.5) 0.5 250
6  before p = r 
6  before  y = W^-1 * p 
6  before _prec->apply(y,p); 0.5 250
6 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
3 === BiCGSTABSolver
 Iter          Defect            Rate
    0       2.7939e-16
3  (it = 0.5; it < _maxit; it+=.5) 0.5 250
3  before p = r 
3  before  y = W^-1 * p 
3  before _prec->apply(y,p); 0.5 250
3 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
3 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
3 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
3  before v = A * y 
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
0 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
5 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
5 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
5  before v = A * y 
6 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
6 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
6  before v = A * y 
4 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
4 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
4  before v = A * y 
1 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
2 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
2 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
6  before  alpha = rho_new / < rt, v > 
4  before  alpha = rho_new / < rt, v > 
1 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
1  before v = A * y 
1  before  alpha = rho_new / < rt, v > 
2  before v = A * y 
3  before  alpha = rho_new / < rt, v > 
0 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
0  before v = A * y 
5  before  alpha = rho_new / < rt, v > 
2  before  alpha = rho_new / < rt, v > 
2  before alpha = rho_new / h; 
2  before apply first correction to x 
2  before r = r - alpha*v 
2  before test stop criteria 
2 istlsolver::printOutput 
  0.5      4.37565e-32      1.56614e-16
2 === rate=2.4528e-32, T=0.122429, TIT=0.244858, IT=0.5
2 finished solveLinearSystemImpl_ 1
2 converged value: 1 7 1 let s go get convergedRemote
2 to  comm_.min(converged) 
3  before alpha = rho_new / h; 
3  before apply first correction to x 
3  before r = r - alpha*v 
3  before test stop criteria 
3 istlsolver::printOutput 
  0.5      4.37565e-32      1.56614e-16
3 === rate=2.4528e-32, T=0.122412, TIT=0.244823, IT=0.5
3 finished solveLinearSystemImpl_ 1
3 converged value: 1 7 1 let s go get convergedRemote
3 to  comm_.min(converged) 
0  before  alpha = rho_new / < rt, v > 
0  before alpha = rho_new / h; 
0  before apply first correction to x 
0  before r = r - alpha*v 
0  before test stop criteria 
0 istlsolver::printOutput 
  0.5      4.37565e-32      1.56614e-16
0 === rate=2.4528e-32, T=0.0185264, TIT=0.0370528, IT=0.5
1  before alpha = rho_new / h; 
1  before apply first correction to x 
1  before r = r - alpha*v 
1  before test stop criteria 
1 istlsolver::printOutput 
  0.5      4.37565e-32      1.56614e-16
1 === rate=2.4528e-32, T=0.122412, TIT=0.244824, IT=0.5
1 finished solveLinearSystemImpl_ 1
1 converged value: 1 7 1 let s go get convergedRemote
1 to  comm_.min(converged) 
6  before alpha = rho_new / h; 
6  before apply first correction to x 
6  before r = r - alpha*v 
6  before test stop criteria 
6 istlsolver::printOutput 
  0.5      4.37565e-32      1.56614e-16
6 === rate=2.4528e-32, T=0.122431, TIT=0.244862, IT=0.5
6 finished solveLinearSystemImpl_ 1
6 converged value: 1 7 1 let s go get convergedRemote
6 to  comm_.min(converged) 
4  before alpha = rho_new / h; 
4  before apply first correction to x 
4  before r = r - alpha*v 
4  before test stop criteria 
4 istlsolver::printOutput 
  0.5      4.37565e-32      1.56614e-16
4 === rate=2.4528e-32, T=0.122427, TIT=0.244854, IT=0.5
4 finished solveLinearSystemImpl_ 1
4 converged value: 1 7 1 let s go get convergedRemote
4 to  comm_.min(converged) 
5  before alpha = rho_new / h; 
5  before apply first correction to x 
5  before r = r - alpha*v 
5  before test stop criteria 
5 istlsolver::printOutput 
  0.5      4.37565e-32      1.56614e-16
5 === rate=2.4528e-32, T=0.122411, TIT=0.244822, IT=0.5
5 finished solveLinearSystemImpl_ 1
5 converged value: 1 7 1 let s go get convergedRemote
5 to  comm_.min(converged) 
5 did  comm_.min(converged) convergedRemote: 1
5 final convergedRemote: 1
5 Update: x^(k+1) = x^k - deltax^k 
5 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
5 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
5 Newton iteration  8 done5 , maximum relative shift = 5.696e-08
5 Assemble/solve/update time: 0.35315(24.9607%)/1.06152(75.0286%)/0.000151342(0.0106969%)
5 gridVariables->advanceTimeStep
5 before, nonLinearSolver->suggestTimeStepSize, current time: 615.444 simTime 57600
5 before assembler->setPreviousSolution, ddt: 10.3493
5 nonLinearSolver->solve
5 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
4 did  comm_.min(converged) convergedRemote: 1
4 final convergedRemote: 1
4 Update: x^(k+1) = x^k - deltax^k 
4 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
4 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
4 Newton iteration  8 done4 , maximum relative shift = 5.696e-08
4 Assemble/solve/update time: 0.352995(24.9525%)/1.06152(75.0368%)/0.000151002(0.010674%)
4 gridVariables->advanceTimeStep
4 before, nonLinearSolver->suggestTimeStepSize, current time: 615.444 simTime 57600
4 before assembler->setPreviousSolution, ddt: 10.3493
4 nonLinearSolver->solve
4 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
2 did  comm_.min(converged) convergedRemote: 1
2 final convergedRemote: 1
2 Update: x^(k+1) = x^k - deltax^k 
2 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
2 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
2 Newton iteration  8 done2 , maximum relative shift = 5.696e-08
2 Assemble/solve/update time: 0.352827(24.9429%)/1.06155(75.0457%)/0.000161362(0.0114074%)
2 gridVariables->advanceTimeStep
2 before, nonLinearSolver->suggestTimeStepSize, current time: 615.444 simTime 57600
2 before assembler->setPreviousSolution, ddt: 10.3493
2 nonLinearSolver->solve
2 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
3 did  comm_.min(converged) convergedRemote: 1
3 final convergedRemote: 1
3 Update: x^(k+1) = x^k - deltax^k 
3 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
3 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
3 Newton iteration  8 done3 , maximum relative shift = 5.696e-08
3 Assemble/solve/update time: 0.353108(24.9577%)/1.06156(75.031%)/0.000159232(0.0112546%)
3 gridVariables->advanceTimeStep
3 before, nonLinearSolver->suggestTimeStepSize, current time: 615.444 simTime 57600
3 before assembler->setPreviousSolution, ddt: 10.3493
3 nonLinearSolver->solve
3 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
0 finished solveLinearSystemImpl_ 1
0 converged value: 1 7 1 let s go get convergedRemote
0 to  comm_.min(converged) 
0 did  comm_.min(converged) convergedRemote: 1
0 final convergedRemote: 1
0 Update: x^(k+1) = x^k - deltax^k 
0 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
0 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
0 Newton iteration  8 done0 , maximum relative shift = 5.696e-08
0 Assemble/solve/update time: 0.352046(24.902%)/1.0615(75.0852%)/0.000180262(0.0127508%)
0 gridVariables->advanceTimeStep
0 before, nonLinearSolver->suggestTimeStepSize, current time: 615.444 simTime 57600
0 before assembler->setPreviousSolution, ddt: 10.3493
0 nonLinearSolver->solve
0 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
1 did  comm_.min(converged) convergedRemote: 1
1 final convergedRemote: 1
1 Update: x^(k+1) = x^k - deltax^k 
1 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
1 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
1 Newton iteration  8 done1 , maximum relative shift = 5.696e-08
1 Assemble/solve/update time: 0.353137(24.9598%)/1.06153(75.0292%)/0.000155562(0.0109952%)
1 gridVariables->advanceTimeStep
1 before, nonLinearSolver->suggestTimeStepSize, current time: 615.444 simTime 57600
1 before assembler->setPreviousSolution, ddt: 10.3493
1 nonLinearSolver->solve
1 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
6 did  comm_.min(converged) convergedRemote: 1
6 final convergedRemote: 1
6 Update: x^(k+1) = x^k - deltax^k 
6 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
6 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
6 Newton iteration  8 done6 , maximum relative shift = 5.696e-08
6 Assemble/solve/update time: 0.35313(24.9598%)/1.06155(75.0321%)/0.000113972(0.00805572%)
6 gridVariables->advanceTimeStep
6 before, nonLinearSolver->suggestTimeStepSize, current time: 615.444 simTime 57600
6 before assembler->setPreviousSolution, ddt: 10.3493
6 nonLinearSolver->solve
6 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
2 Solve: M deltax^k = r 
2 Newton::solveLinearSystem : entering the try section 0
2to  comm_.sum(norm2) 
2did  comm_.sum(norm2) 
2 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
2 solveLinearSystemImpl_, before converged = ls.solve 
2 7 amgbackend::solve isParallel 1
3 Solve: M deltax^k = r 
3 Newton::solveLinearSystem : entering the try section 0
3to  comm_.sum(norm2) 
3did  comm_.sum(norm2) 
3 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
3 solveLinearSystemImpl_, before converged = ls.solve 
3 7 amgbackend::solve isParallel 1
0 Solve: M deltax^k = r 
0 Newton::solveLinearSystem : entering the try section 0
0to  comm_.sum(norm2) 
0did  comm_.sum(norm2) 
0 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
0 solveLinearSystemImpl_, before converged = ls.solve 
0 7 amgbackend::solve isParallel 1
1 Solve: M deltax^k = r 
1 Newton::solveLinearSystem : entering the try section 0
1to  comm_.sum(norm2) 
1did  comm_.sum(norm2) 
1 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
1 solveLinearSystemImpl_, before converged = ls.solve 
1 7 amgbackend::solve isParallel 1
6 Solve: M deltax^k = r 
6 Newton::solveLinearSystem : entering the try section 0
6to  comm_.sum(norm2) 
6did  comm_.sum(norm2) 
6 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
6 solveLinearSystemImpl_, before converged = ls.solve 
6 7 amgbackend::solve isParallel 1
5 Solve: M deltax^k = r 
5 Newton::solveLinearSystem : entering the try section 0
5to  comm_.sum(norm2) 
5did  comm_.sum(norm2) 
5 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
5 solveLinearSystemImpl_, before converged = ls.solve 
5 7 amgbackend::solve isParallel 1
4 Solve: M deltax^k = r 
4 Newton::solveLinearSystem : entering the try section 0
4to  comm_.sum(norm2) 
4did  comm_.sum(norm2) 
4 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
4 solveLinearSystemImpl_, before converged = ls.solve 
4 7 amgbackend::solve isParallel 1
1 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
1 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
2 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
2 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
3 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
3 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
0 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
0 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 7
4 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
4 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
6 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
6 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
5 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
5 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 7
5 7 before Dune::BiCGSTABSolver 
5 Dune::BiCGSTABSolver.apply(x, b, result_) 
5 BiCGSTABSolver::apply 
5 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
1 7 before Dune::BiCGSTABSolver 
1 Dune::BiCGSTABSolver.apply(x, b, result_) 
1 BiCGSTABSolver::apply 
1 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
2 7 before Dune::BiCGSTABSolver 
2 Dune::BiCGSTABSolver.apply(x, b, result_) 
2 BiCGSTABSolver::apply 
2 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
3 7 before Dune::BiCGSTABSolver 
3 Dune::BiCGSTABSolver.apply(x, b, result_) 
3 BiCGSTABSolver::apply 
3 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 7
4 7 before Dune::BiCGSTABSolver 
4 Dune::BiCGSTABSolver.apply(x, b, result_) 
4 BiCGSTABSolver::apply 
4 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
6 7 before Dune::BiCGSTABSolver 
6 Dune::BiCGSTABSolver.apply(x, b, result_) 
6 BiCGSTABSolver::apply 
6 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
Using a direct coarse solver (SuperLU)
Building hierarchy of 1 levels (inclusive coarse solver) took 0.11227 seconds.
0 7 before Dune::BiCGSTABSolver 
0 Dune::BiCGSTABSolver.apply(x, b, result_) 
0 BiCGSTABSolver::apply 
0 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
0 === BiCGSTABSolver
 Iter          Defect            Rate
    0      3.16231e-09
0  (it = 0.5; it < _maxit; it+=.5) 0.5 250
0  before p = r 
0  before  y = W^-1 * p 
0  before _prec->apply(y,p); 0.5 250
0 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
5 === BiCGSTABSolver
 Iter          Defect            Rate
    0      3.16231e-09
5  (it = 0.5; it < _maxit; it+=.5) 0.5 250
5  before p = r 
5  before  y = W^-1 * p 
5  before _prec->apply(y,p); 0.5 250
5 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
1 === BiCGSTABSolver
 Iter          Defect            Rate
    0      3.16231e-09
1  (it = 0.5; it < _maxit; it+=.5) 0.5 250
1  before p = r 
1  before  y = W^-1 * p 
1  before _prec->apply(y,p); 0.5 250
1 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
2 === BiCGSTABSolver
 Iter          Defect            Rate
    0      3.16231e-09
2  (it = 0.5; it < _maxit; it+=.5) 0.5 250
2  before p = r 
2  before  y = W^-1 * p 
2  before _prec->apply(y,p); 0.5 250
2 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 7
0 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
6 === BiCGSTABSolver
 Iter          Defect            Rate
    0      3.16231e-09
6  (it = 0.5; it < _maxit; it+=.5) 0.5 250
6  before p = r 
6  before  y = W^-1 * p 
6  before _prec->apply(y,p); 0.5 250
6 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
4 === BiCGSTABSolver
 Iter          Defect            Rate
    0      3.16231e-09
4  (it = 0.5; it < _maxit; it+=.5) 0.5 250
4  before p = r 
4  before  y = W^-1 * p 
4  before _prec->apply(y,p); 0.5 250
4 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
3 === BiCGSTABSolver
 Iter          Defect            Rate
    0      3.16231e-09
3  (it = 0.5; it < _maxit; it+=.5) 0.5 250
3  before p = r 
3  before  y = W^-1 * p 
3  before _prec->apply(y,p); 0.5 250
3 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
2 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
2 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
5 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
5 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
5  before v = A * y 
1 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
3 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
3 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
3  before v = A * y 
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
0 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
6 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
6 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
6  before v = A * y 
4 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
4 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
4  before v = A * y 
1 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
1  before v = A * y 
2  before v = A * y 
2  before  alpha = rho_new / < rt, v > 
5  before  alpha = rho_new / < rt, v > 
4  before  alpha = rho_new / < rt, v > 
3  before  alpha = rho_new / < rt, v > 
0 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
0  before v = A * y 
6  before  alpha = rho_new / < rt, v > 
0  before  alpha = rho_new / < rt, v > 
0  before alpha = rho_new / h; 
0  before apply first correction to x 
0  before r = r - alpha*v 
0  before test stop criteria 
0 istlsolver::printOutput 
  0.5      9.04136e-25       2.8591e-16
0 === rate=8.17445e-32, T=0.0188375, TIT=0.0376751, IT=0.5
5  before alpha = rho_new / h; 
5  before apply first correction to x 
5  before r = r - alpha*v 
5  before test stop criteria 
5 istlsolver::printOutput 
  0.5      9.04136e-25       2.8591e-16
5 === rate=8.17445e-32, T=0.123288, TIT=0.246576, IT=0.5
5 finished solveLinearSystemImpl_ 1
5 converged value: 1 7 1 let s go get convergedRemote
5 to  comm_.min(converged) 
1  before  alpha = rho_new / < rt, v > 
1  before alpha = rho_new / h; 
1  before apply first correction to x 
1  before r = r - alpha*v 
1  before test stop criteria 
1 istlsolver::printOutput 
  0.5      9.04136e-25       2.8591e-16
1 === rate=8.17445e-32, T=0.12329, TIT=0.246581, IT=0.5
1 finished solveLinearSystemImpl_ 1
1 converged value: 1 7 1 let s go get convergedRemote
1 to  comm_.min(converged) 
2  before alpha = rho_new / h; 
2  before apply first correction to x 
2  before r = r - alpha*v 
2  before test stop criteria 
2 istlsolver::printOutput 
  0.5      9.04136e-25       2.8591e-16
2 === rate=8.17445e-32, T=0.123278, TIT=0.246555, IT=0.5
2 finished solveLinearSystemImpl_ 1
2 converged value: 1 7 1 let s go get convergedRemote
2 to  comm_.min(converged) 
6  before alpha = rho_new / h; 
6  before apply first correction to x 
6  before r = r - alpha*v 
6  before test stop criteria 
6 istlsolver::printOutput 
  0.5      9.04136e-25       2.8591e-16
6 === rate=8.17445e-32, T=0.123277, TIT=0.246554, IT=0.5
6 finished solveLinearSystemImpl_ 1
6 converged value: 1 7 1 let s go get convergedRemote
6 to  comm_.min(converged) 
4  before alpha = rho_new / h; 
4  before apply first correction to x 
4  before r = r - alpha*v 
4  before test stop criteria 
4 istlsolver::printOutput 
  0.5      9.04136e-25       2.8591e-16
4 === rate=8.17445e-32, T=0.123276, TIT=0.246553, IT=0.5
4 finished solveLinearSystemImpl_ 1
4 converged value: 1 7 1 let s go get convergedRemote
4 to  comm_.min(converged) 
3  before alpha = rho_new / h; 
3  before apply first correction to x 
3  before r = r - alpha*v 
3  before test stop criteria 
3 istlsolver::printOutput 
  0.5      9.04136e-25       2.8591e-16
3 === rate=8.17445e-32, T=0.123288, TIT=0.246576, IT=0.5
3 finished solveLinearSystemImpl_ 1
3 converged value: 1 7 1 let s go get convergedRemote
3 to  comm_.min(converged) 
5 did  comm_.min(converged) convergedRemote: 1
5 final convergedRemote: 1
5 Update: x^(k+1) = x^k - deltax^k 
5 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
5 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
5 Newton iteration  1 done5 , maximum relative shift = 7.953e-02
5 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
1 did  comm_.min(converged) convergedRemote: 1
1 final convergedRemote: 1
1 Update: x^(k+1) = x^k - deltax^k 
1 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
1 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
1 Newton iteration  1 done1 , maximum relative shift = 7.953e-02
1 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
3 did  comm_.min(converged) convergedRemote: 1
3 final convergedRemote: 1
3 Update: x^(k+1) = x^k - deltax^k 
3 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
3 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
3 Newton iteration  1 done3 , maximum relative shift = 7.953e-02
3 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
6 did  comm_.min(converged) convergedRemote: 1
6 final convergedRemote: 1
6 Update: x^(k+1) = x^k - deltax^k 
6 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
6 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
6 Newton iteration  1 done6 , maximum relative shift = 7.953e-02
6 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
4 did  comm_.min(converged) convergedRemote: 1
4 final convergedRemote: 1
4 Update: x^(k+1) = x^k - deltax^k 
4 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
4 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
4 Newton iteration  1 done4 , maximum relative shift = 7.953e-02
4 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
2 did  comm_.min(converged) convergedRemote: 1
2 final convergedRemote: 1
2 Update: x^(k+1) = x^k - deltax^k 
2 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
2 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
2 Newton iteration  1 done2 , maximum relative shift = 7.953e-02
2 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
0 finished solveLinearSystemImpl_ 1
0 converged value: 1 7 1 let s go get convergedRemote
0 to  comm_.min(converged) 
0 did  comm_.min(converged) convergedRemote: 1
0 final convergedRemote: 1
0 Update: x^(k+1) = x^k - deltax^k 
0 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
0 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
0 Newton iteration  1 done0 , maximum relative shift = 7.953e-02
0 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
1 Solve: M deltax^k = r 
1 Newton::solveLinearSystem : entering the try section 1
1 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
1 solveLinearSystemImpl_, before converged = ls.solve 
1 7 amgbackend::solve isParallel 1
5 Solve: M deltax^k = r 
5 Newton::solveLinearSystem : entering the try section 1
5 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
5 solveLinearSystemImpl_, before converged = ls.solve 
5 7 amgbackend::solve isParallel 1
0 Solve: M deltax^k = r 
0 Newton::solveLinearSystem : entering the try section 1
0 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
0 solveLinearSystemImpl_, before converged = ls.solve 
0 7 amgbackend::solve isParallel 1
3 Solve: M deltax^k = r 
3 Newton::solveLinearSystem : entering the try section 1
3 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
3 solveLinearSystemImpl_, before converged = ls.solve 
3 7 amgbackend::solve isParallel 1
6 Solve: M deltax^k = r 
6 Newton::solveLinearSystem : entering the try section 1
6 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
6 solveLinearSystemImpl_, before converged = ls.solve 
6 7 amgbackend::solve isParallel 1
4 Solve: M deltax^k = r 
4 Newton::solveLinearSystem : entering the try section 1
4 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
4 solveLinearSystemImpl_, before converged = ls.solve 
4 7 amgbackend::solve isParallel 1
2 Solve: M deltax^k = r 
2 Newton::solveLinearSystem : entering the try section 1
2 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
2 solveLinearSystemImpl_, before converged = ls.solve 
2 7 amgbackend::solve isParallel 1
4 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
4 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
5 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
5 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
1 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
1 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
2 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
2 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
0 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
0 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 7
3 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
3 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
6 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
6 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 7
3 7 before Dune::BiCGSTABSolver 
3 Dune::BiCGSTABSolver.apply(x, b, result_) 
3 BiCGSTABSolver::apply 
3 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
1 7 before Dune::BiCGSTABSolver 
1 Dune::BiCGSTABSolver.apply(x, b, result_) 
1 BiCGSTABSolver::apply 
1 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
4 7 before Dune::BiCGSTABSolver 
4 Dune::BiCGSTABSolver.apply(x, b, result_) 
4 BiCGSTABSolver::apply 
4 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
5 7 before Dune::BiCGSTABSolver 
5 Dune::BiCGSTABSolver.apply(x, b, result_) 
5 BiCGSTABSolver::apply 
5 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 7
2 7 before Dune::BiCGSTABSolver 
2 Dune::BiCGSTABSolver.apply(x, b, result_) 
2 BiCGSTABSolver::apply 
2 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
6 7 before Dune::BiCGSTABSolver 
6 Dune::BiCGSTABSolver.apply(x, b, result_) 
6 BiCGSTABSolver::apply 
6 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
Using a direct coarse solver (SuperLU)
Building hierarchy of 1 levels (inclusive coarse solver) took 0.11044 seconds.
0 7 before Dune::BiCGSTABSolver 
0 Dune::BiCGSTABSolver.apply(x, b, result_) 
0 BiCGSTABSolver::apply 
0 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
6 === BiCGSTABSolver
 Iter          Defect            Rate
    0      2.75286e-11
6  (it = 0.5; it < _maxit; it+=.5) 0.5 250
6  before p = r 
6  before  y = W^-1 * p 
6  before _prec->apply(y,p); 0.5 250
6 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
2 === BiCGSTABSolver
 Iter          Defect            Rate
    0      2.75286e-11
2  (it = 0.5; it < _maxit; it+=.5) 0.5 250
2  before p = r 
2  before  y = W^-1 * p 
2  before _prec->apply(y,p); 0.5 250
2 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
5 === BiCGSTABSolver
 Iter          Defect            Rate
    0      2.75286e-11
5  (it = 0.5; it < _maxit; it+=.5) 0.5 250
5  before p = r 
5  before  y = W^-1 * p 
5  before _prec->apply(y,p); 0.5 250
5 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
3 === BiCGSTABSolver
 Iter          Defect            Rate
    0      2.75286e-11
3  (it = 0.5; it < _maxit; it+=.5) 0.5 250
3  before p = r 
3  before  y = W^-1 * p 
3  before _prec->apply(y,p); 0.5 250
3 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
1 === BiCGSTABSolver
 Iter          Defect            Rate
    0      2.75286e-11
1  (it = 0.5; it < _maxit; it+=.5) 0.5 250
1  before p = r 
1  before  y = W^-1 * p 
1  before _prec->apply(y,p); 0.5 250
1 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
4 === BiCGSTABSolver
 Iter          Defect            Rate
    0      2.75286e-11
4  (it = 0.5; it < _maxit; it+=.5) 0.5 250
4  before p = r 
4  before  y = W^-1 * p 
4  before _prec->apply(y,p); 0.5 250
4 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
0 === BiCGSTABSolver
 Iter          Defect            Rate
    0      2.75286e-11
0  (it = 0.5; it < _maxit; it+=.5) 0.5 250
0  before p = r 
0  before  y = W^-1 * p 
0  before _prec->apply(y,p); 0.5 250
0 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 7
0 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
0 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
4 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
4 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
4  before v = A * y 
6 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
6 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
6  before v = A * y 
2 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
2 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
5 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
5 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
5  before v = A * y 
3 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
3 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
3  before v = A * y 
1 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
3  before  alpha = rho_new / < rt, v > 
1 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
1  before v = A * y 
0 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
0  before v = A * y 
4  before  alpha = rho_new / < rt, v > 
6  before  alpha = rho_new / < rt, v > 
2  before v = A * y 
5  before  alpha = rho_new / < rt, v > 
1  before  alpha = rho_new / < rt, v > 
1  before alpha = rho_new / h; 
1  before apply first correction to x 
1  before r = r - alpha*v 
1  before test stop criteria 
1 istlsolver::printOutput 
  0.5      2.60751e-27      9.47201e-17
1 === rate=8.9719e-33, T=0.122439, TIT=0.244878, IT=0.5
1 finished solveLinearSystemImpl_ 1
1 converged value: 1 7 1 let s go get convergedRemote
1 to  comm_.min(converged) 
0  before  alpha = rho_new / < rt, v > 
0  before alpha = rho_new / h; 
0  before apply first correction to x 
0  before r = r - alpha*v 
0  before test stop criteria 
0 istlsolver::printOutput 
  0.5      2.60751e-27      9.47201e-17
0 === rate=8.9719e-33, T=0.0198278, TIT=0.0396555, IT=0.5
4  before alpha = rho_new / h; 
4  before apply first correction to x 
4  before r = r - alpha*v 
4  before test stop criteria 
4 istlsolver::printOutput 
  0.5      2.60751e-27      9.47201e-17
4 === rate=8.9719e-33, T=0.122346, TIT=0.244693, IT=0.5
4 finished solveLinearSystemImpl_ 1
4 converged value: 1 7 1 let s go get convergedRemote
4 to  comm_.min(converged) 
6  before alpha = rho_new / h; 
6  before apply first correction to x 
6  before r = r - alpha*v 
6  before test stop criteria 
6 istlsolver::printOutput 
  0.5      2.60751e-27      9.47201e-17
6 === rate=8.9719e-33, T=0.122348, TIT=0.244697, IT=0.5
6 finished solveLinearSystemImpl_ 1
6 converged value: 1 7 1 let s go get convergedRemote
6 to  comm_.min(converged) 
2  before  alpha = rho_new / < rt, v > 
2  before alpha = rho_new / h; 
2  before apply first correction to x 
2  before r = r - alpha*v 
2  before test stop criteria 
2 istlsolver::printOutput 
  0.5      2.60751e-27      9.47201e-17
2 === rate=8.9719e-33, T=0.12235, TIT=0.2447, IT=0.5
2 finished solveLinearSystemImpl_ 1
2 converged value: 1 7 1 let s go get convergedRemote
2 to  comm_.min(converged) 
3  before alpha = rho_new / h; 
3  before apply first correction to x 
3  before r = r - alpha*v 
3  before test stop criteria 
3 istlsolver::printOutput 
  0.5      2.60751e-27      9.47201e-17
3 === rate=8.9719e-33, T=0.122437, TIT=0.244874, IT=0.5
3 finished solveLinearSystemImpl_ 1
3 converged value: 1 7 1 let s go get convergedRemote
3 to  comm_.min(converged) 
5  before alpha = rho_new / h; 
5  before apply first correction to x 
5  before r = r - alpha*v 
5  before test stop criteria 
5 istlsolver::printOutput 
  0.5      2.60751e-27      9.47201e-17
5 === rate=8.9719e-33, T=0.122435, TIT=0.244871, IT=0.5
5 finished solveLinearSystemImpl_ 1
5 converged value: 1 7 1 let s go get convergedRemote
5 to  comm_.min(converged) 
5 did  comm_.min(converged) convergedRemote: 1
5 final convergedRemote: 1
5 Update: x^(k+1) = x^k - deltax^k 
5 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
5 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
5 Newton iteration  2 done5 , maximum relative shift = 7.986e-03
5 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
3 did  comm_.min(converged) convergedRemote: 1
3 final convergedRemote: 1
3 Update: x^(k+1) = x^k - deltax^k 
3 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
3 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
3 Newton iteration  2 done3 , maximum relative shift = 7.986e-03
3 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
1 did  comm_.min(converged) convergedRemote: 1
1 final convergedRemote: 1
1 Update: x^(k+1) = x^k - deltax^k 
1 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
1 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
1 Newton iteration  2 done1 , maximum relative shift = 7.986e-03
1 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
0 finished solveLinearSystemImpl_ 1
0 converged value: 1 7 1 let s go get convergedRemote
0 to  comm_.min(converged) 
0 did  comm_.min(converged) convergedRemote: 1
0 final convergedRemote: 1
0 Update: x^(k+1) = x^k - deltax^k 
0 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
0 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
0 Newton iteration  2 done0 , maximum relative shift = 7.986e-03
0 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
4 did  comm_.min(converged) convergedRemote: 1
4 final convergedRemote: 1
4 Update: x^(k+1) = x^k - deltax^k 
4 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
4 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
4 Newton iteration  2 done4 , maximum relative shift = 7.986e-03
4 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
6 did  comm_.min(converged) convergedRemote: 1
6 final convergedRemote: 1
6 Update: x^(k+1) = x^k - deltax^k 
6 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
6 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
6 Newton iteration  2 done6 , maximum relative shift = 7.986e-03
6 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
2 did  comm_.min(converged) convergedRemote: 1
2 final convergedRemote: 1
2 Update: x^(k+1) = x^k - deltax^k 
2 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
2 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
2 Newton iteration  2 done2 , maximum relative shift = 7.986e-03
2 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
6 Solve: M deltax^k = r 
6 Newton::solveLinearSystem : entering the try section 2
6 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
6 solveLinearSystemImpl_, before converged = ls.solve 
6 7 amgbackend::solve isParallel 1
2 Solve: M deltax^k = r 
2 Newton::solveLinearSystem : entering the try section 2
2 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
2 solveLinearSystemImpl_, before converged = ls.solve 
2 7 amgbackend::solve isParallel 1
5 Solve: M deltax^k = r 
5 Newton::solveLinearSystem : entering the try section 2
5 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
5 solveLinearSystemImpl_, before converged = ls.solve 
5 7 amgbackend::solve isParallel 1
3 Solve: M deltax^k = r 
3 Newton::solveLinearSystem : entering the try section 2
3 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
3 solveLinearSystemImpl_, before converged = ls.solve 
3 7 amgbackend::solve isParallel 1
1 Solve: M deltax^k = r 
1 Newton::solveLinearSystem : entering the try section 2
1 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
1 solveLinearSystemImpl_, before converged = ls.solve 
1 7 amgbackend::solve isParallel 1
0 Solve: M deltax^k = r 
0 Newton::solveLinearSystem : entering the try section 2
0 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
0 solveLinearSystemImpl_, before converged = ls.solve 
0 7 amgbackend::solve isParallel 1
4 Solve: M deltax^k = r 
4 Newton::solveLinearSystem : entering the try section 2
4 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
4 solveLinearSystemImpl_, before converged = ls.solve 
4 7 amgbackend::solve isParallel 1
2 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
2 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
5 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
5 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
3 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
3 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
1 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
1 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
0 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
0 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 7
6 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
6 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
4 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
4 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 7
6 7 before Dune::BiCGSTABSolver 
6 Dune::BiCGSTABSolver.apply(x, b, result_) 
6 BiCGSTABSolver::apply 
6 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
2 7 before Dune::BiCGSTABSolver 
2 Dune::BiCGSTABSolver.apply(x, b, result_) 
2 BiCGSTABSolver::apply 
2 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
5 7 before Dune::BiCGSTABSolver 
5 Dune::BiCGSTABSolver.apply(x, b, result_) 
5 BiCGSTABSolver::apply 
5 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
3 7 before Dune::BiCGSTABSolver 
3 Dune::BiCGSTABSolver.apply(x, b, result_) 
3 BiCGSTABSolver::apply 
3 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
1 7 before Dune::BiCGSTABSolver 
1 Dune::BiCGSTABSolver.apply(x, b, result_) 
1 BiCGSTABSolver::apply 
1 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
4 7 before Dune::BiCGSTABSolver 
4 Dune::BiCGSTABSolver.apply(x, b, result_) 
4 BiCGSTABSolver::apply 
4 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 7
Using a direct coarse solver (SuperLU)
Building hierarchy of 1 levels (inclusive coarse solver) took 0.110246 seconds.
0 7 before Dune::BiCGSTABSolver 
0 Dune::BiCGSTABSolver.apply(x, b, result_) 
0 BiCGSTABSolver::apply 
0 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
0 === BiCGSTABSolver
 Iter          Defect            Rate
    0      7.67653e-12
0  (it = 0.5; it < _maxit; it+=.5) 0.5 250
2 === BiCGSTABSolver
 Iter          Defect            Rate
    0      7.67653e-12
2  (it = 0.5; it < _maxit; it+=.5) 0.5 250
2  before p = r 
2  before  y = W^-1 * p 
2  before _prec->apply(y,p); 0.5 250
2 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
5 === BiCGSTABSolver
 Iter          Defect            Rate
    0      7.67653e-12
5  (it = 0.5; it < _maxit; it+=.5) 0.5 250
5  before p = r 
5  before  y = W^-1 * p 
5  before _prec->apply(y,p); 0.5 250
5 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
3 === BiCGSTABSolver
 Iter          Defect            Rate
    0      7.67653e-12
3  (it = 0.5; it < _maxit; it+=.5) 0.5 250
3  before p = r 
3  before  y = W^-1 * p 
3  before _prec->apply(y,p); 0.5 250
3 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
1 === BiCGSTABSolver
 Iter          Defect            Rate
    0      7.67653e-12
1  (it = 0.5; it < _maxit; it+=.5) 0.5 250
1  before p = r 
1  before  y = W^-1 * p 
1  before _prec->apply(y,p); 0.5 250
1 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
0  before p = r 
0  before  y = W^-1 * p 
0  before _prec->apply(y,p); 0.5 250
0 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 7
0 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
4 === BiCGSTABSolver
 Iter          Defect            Rate
    0      7.67653e-12
4  (it = 0.5; it < _maxit; it+=.5) 0.5 250
4  before p = r 
4  before  y = W^-1 * p 
4  before _prec->apply(y,p); 0.5 250
4 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
6 === BiCGSTABSolver
 Iter          Defect            Rate
    0      7.67653e-12
6  (it = 0.5; it < _maxit; it+=.5) 0.5 250
6  before p = r 
6  before  y = W^-1 * p 
6  before _prec->apply(y,p); 0.5 250
6 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
2 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
2 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
5 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
5 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
5  before v = A * y 
3 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
3 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
3  before v = A * y 
6 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
6 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
6  before v = A * y 
6  before  alpha = rho_new / < rt, v > 
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
0 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
4 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
4 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
4  before v = A * y 
1 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
0 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
0  before v = A * y 
4  before  alpha = rho_new / < rt, v > 
3  before  alpha = rho_new / < rt, v > 
2  before v = A * y 
2  before  alpha = rho_new / < rt, v > 
5  before  alpha = rho_new / < rt, v > 
1 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
1  before v = A * y 
5  before alpha = rho_new / h; 
5  before apply first correction to x 
5  before r = r - alpha*v 
5  before test stop criteria 
5 istlsolver::printOutput 
  0.5      6.98529e-31      9.09955e-20
5 === rate=8.28018e-39, T=0.12209, TIT=0.244181, IT=0.5
5 finished solveLinearSystemImpl_ 1
5 converged value: 1 7 1 let s go get convergedRemote
5 to  comm_.min(converged) 
0  before  alpha = rho_new / < rt, v > 
0  before alpha = rho_new / h; 
0  before apply first correction to x 
0  before r = r - alpha*v 
0  before test stop criteria 
0 istlsolver::printOutput 
  0.5      6.98529e-31      9.09955e-20
0 === rate=8.28018e-39, T=0.0196499, TIT=0.0392999, IT=0.5
4  before alpha = rho_new / h; 
4  before apply first correction to x 
4  before r = r - alpha*v 
4  before test stop criteria 
4 istlsolver::printOutput 
  0.5      6.98529e-31      9.09955e-20
4 === rate=8.28018e-39, T=0.1221, TIT=0.244201, IT=0.5
4 finished solveLinearSystemImpl_ 1
4 converged value: 1 7 1 let s go get convergedRemote
4 to  comm_.min(converged) 
3  before alpha = rho_new / h; 
3  before apply first correction to x 
3  before r = r - alpha*v 
3  before test stop criteria 
3 istlsolver::printOutput 
  0.5      6.98529e-31      9.09955e-20
3 === rate=8.28018e-39, T=0.12209, TIT=0.24418, IT=0.5
3 finished solveLinearSystemImpl_ 1
3 converged value: 1 7 1 let s go get convergedRemote
3 to  comm_.min(converged) 
2  before alpha = rho_new / h; 
2  before apply first correction to x 
2  before r = r - alpha*v 
2  before test stop criteria 
2 istlsolver::printOutput 
  0.5      6.98529e-31      9.09955e-20
2 === rate=8.28018e-39, T=0.122101, TIT=0.244203, IT=0.5
2 finished solveLinearSystemImpl_ 1
2 converged value: 1 7 1 let s go get convergedRemote
2 to  comm_.min(converged) 
1  before  alpha = rho_new / < rt, v > 
1  before alpha = rho_new / h; 
1  before apply first correction to x 
1  before r = r - alpha*v 
1  before test stop criteria 
1 istlsolver::printOutput 
  0.5      6.98529e-31      9.09955e-20
1 === rate=8.28018e-39, T=0.12209, TIT=0.24418, IT=0.5
1 finished solveLinearSystemImpl_ 1
1 converged value: 1 7 1 let s go get convergedRemote
1 to  comm_.min(converged) 
6  before alpha = rho_new / h; 
6  before apply first correction to x 
6  before r = r - alpha*v 
6  before test stop criteria 
6 istlsolver::printOutput 
  0.5      6.98529e-31      9.09955e-20
6 === rate=8.28018e-39, T=0.122102, TIT=0.244205, IT=0.5
6 finished solveLinearSystemImpl_ 1
6 converged value: 1 7 1 let s go get convergedRemote
6 to  comm_.min(converged) 
6 did  comm_.min(converged) convergedRemote: 1
6 final convergedRemote: 1
6 Update: x^(k+1) = x^k - deltax^k 
6 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
6 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
6 Newton iteration  3 done6 , maximum relative shift = 1.412e-03
6 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
1 did  comm_.min(converged) convergedRemote: 1
1 final convergedRemote: 1
1 Update: x^(k+1) = x^k - deltax^k 
1 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
1 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
1 Newton iteration  3 done1 , maximum relative shift = 1.412e-03
1 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
2 did  comm_.min(converged) convergedRemote: 1
2 final convergedRemote: 1
2 Update: x^(k+1) = x^k - deltax^k 
2 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
2 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
2 Newton iteration  3 done2 , maximum relative shift = 1.412e-03
2 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
5 did  comm_.min(converged) convergedRemote: 1
5 final convergedRemote: 1
5 Update: x^(k+1) = x^k - deltax^k 
5 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
5 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
5 Newton iteration  3 done5 , maximum relative shift = 1.412e-03
5 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
0 finished solveLinearSystemImpl_ 1
0 converged value: 1 7 1 let s go get convergedRemote
0 to  comm_.min(converged) 
0 did  comm_.min(converged) convergedRemote: 1
0 final convergedRemote: 1
0 Update: x^(k+1) = x^k - deltax^k 
0 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
0 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
0 Newton iteration  3 done0 , maximum relative shift = 1.412e-03
0 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
4 did  comm_.min(converged) convergedRemote: 1
4 final convergedRemote: 1
4 Update: x^(k+1) = x^k - deltax^k 
4 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
4 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
4 Newton iteration  3 done4 , maximum relative shift = 1.412e-03
4 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
3 did  comm_.min(converged) convergedRemote: 1
3 final convergedRemote: 1
3 Update: x^(k+1) = x^k - deltax^k 
3 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
3 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
3 Newton iteration  3 done3 , maximum relative shift = 1.412e-03
3 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
1 Solve: M deltax^k = r 
1 Newton::solveLinearSystem : entering the try section 3
1 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
1 solveLinearSystemImpl_, before converged = ls.solve 
1 7 amgbackend::solve isParallel 1
2 Solve: M deltax^k = r 
2 Newton::solveLinearSystem : entering the try section 3
2 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
2 solveLinearSystemImpl_, before converged = ls.solve 
2 7 amgbackend::solve isParallel 1
5 Solve: M deltax^k = r 
5 Newton::solveLinearSystem : entering the try section 3
5 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
5 solveLinearSystemImpl_, before converged = ls.solve 
5 7 amgbackend::solve isParallel 1
0 Solve: M deltax^k = r 
0 Newton::solveLinearSystem : entering the try section 3
0 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
0 solveLinearSystemImpl_, before converged = ls.solve 
0 7 amgbackend::solve isParallel 1
4 Solve: M deltax^k = r 
4 Newton::solveLinearSystem : entering the try section 3
4 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
4 solveLinearSystemImpl_, before converged = ls.solve 
4 7 amgbackend::solve isParallel 1
3 Solve: M deltax^k = r 
3 Newton::solveLinearSystem : entering the try section 3
3 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
3 solveLinearSystemImpl_, before converged = ls.solve 
3 7 amgbackend::solve isParallel 1
6 Solve: M deltax^k = r 
6 Newton::solveLinearSystem : entering the try section 3
6 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
6 solveLinearSystemImpl_, before converged = ls.solve 
6 7 amgbackend::solve isParallel 1
3 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
3 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
4 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
4 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
1 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
1 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
2 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
2 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
5 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
5 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
0 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
0 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 7
6 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
6 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 7
5 7 before Dune::BiCGSTABSolver 
5 Dune::BiCGSTABSolver.apply(x, b, result_) 
5 BiCGSTABSolver::apply 
5 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 7
6 7 before Dune::BiCGSTABSolver 
6 Dune::BiCGSTABSolver.apply(x, b, result_) 
6 BiCGSTABSolver::apply 
6 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
3 7 before Dune::BiCGSTABSolver 
3 Dune::BiCGSTABSolver.apply(x, b, result_) 
3 BiCGSTABSolver::apply 
3 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
4 7 before Dune::BiCGSTABSolver 
4 Dune::BiCGSTABSolver.apply(x, b, result_) 
4 BiCGSTABSolver::apply 
4 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
1 7 before Dune::BiCGSTABSolver 
1 Dune::BiCGSTABSolver.apply(x, b, result_) 
1 BiCGSTABSolver::apply 
1 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
2 7 before Dune::BiCGSTABSolver 
2 Dune::BiCGSTABSolver.apply(x, b, result_) 
2 BiCGSTABSolver::apply 
2 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
Using a direct coarse solver (SuperLU)
Building hierarchy of 1 levels (inclusive coarse solver) took 0.111163 seconds.
0 7 before Dune::BiCGSTABSolver 
0 Dune::BiCGSTABSolver.apply(x, b, result_) 
0 BiCGSTABSolver::apply 
0 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
0 === BiCGSTABSolver
 Iter          Defect            Rate
    0      1.68402e-12
0  (it = 0.5; it < _maxit; it+=.5) 0.5 250
0  before p = r 
0  before  y = W^-1 * p 
0  before _prec->apply(y,p); 0.5 250
0 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
2 === BiCGSTABSolver
 Iter          Defect            Rate
    0      1.68402e-12
2  (it = 0.5; it < _maxit; it+=.5) 0.5 250
2  before p = r 
2  before  y = W^-1 * p 
2  before _prec->apply(y,p); 0.5 250
2 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
6 === BiCGSTABSolver
 Iter          Defect            Rate
    0      1.68402e-12
6  (it = 0.5; it < _maxit; it+=.5) 0.5 250
6  before p = r 
6  before  y = W^-1 * p 
6  before _prec->apply(y,p); 0.5 250
6 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
3 === BiCGSTABSolver
 Iter          Defect            Rate
    0      1.68402e-12
3  (it = 0.5; it < _maxit; it+=.5) 0.5 250
3  before p = r 
3  before  y = W^-1 * p 
3  before _prec->apply(y,p); 0.5 250
3 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
4 === BiCGSTABSolver
 Iter          Defect            Rate
    0      1.68402e-12
4  (it = 0.5; it < _maxit; it+=.5) 0.5 250
4  before p = r 
4  before  y = W^-1 * p 
4  before _prec->apply(y,p); 0.5 250
4 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
1 === BiCGSTABSolver
 Iter          Defect            Rate
    0      1.68402e-12
1  (it = 0.5; it < _maxit; it+=.5) 0.5 250
1  before p = r 
1  before  y = W^-1 * p 
1  before _prec->apply(y,p); 0.5 250
1 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
5 === BiCGSTABSolver
 Iter          Defect            Rate
    0      1.68402e-12
5  (it = 0.5; it < _maxit; it+=.5) 0.5 250
5  before p = r 
5  before  y = W^-1 * p 
5  before _prec->apply(y,p); 0.5 250
5 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 7
0 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
1 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
0 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
5 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
5 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
5  before v = A * y 
2 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
2 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
6 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
6 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
6  before v = A * y 
6  before  alpha = rho_new / < rt, v > 
3 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
3 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
3  before v = A * y 
4 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
4 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
4  before v = A * y 
4  before  alpha = rho_new / < rt, v > 
0 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
0  before v = A * y 
0  before  alpha = rho_new / < rt, v > 
5  before  alpha = rho_new / < rt, v > 
2  before v = A * y 
2  before  alpha = rho_new / < rt, v > 
3  before  alpha = rho_new / < rt, v > 
1 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
4  before alpha = rho_new / h; 
4  before apply first correction to x 
4  before r = r - alpha*v 
4  before test stop criteria 
4 istlsolver::printOutput 
  0.5      1.42012e-28      8.43287e-17
4 === rate=7.11133e-33, T=0.12321, TIT=0.246421, IT=0.5
4 finished solveLinearSystemImpl_ 1
4 converged value: 1 7 1 let s go get convergedRemote
4 to  comm_.min(converged) 
4 did  comm_.min(converged) convergedRemote: 1
4 final convergedRemote: 1
4 Update: x^(k+1) = x^k - deltax^k 
4 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
4 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
4 Newton iteration  4 done4 , maximum relative shift = 3.873e-04
4 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
0  before alpha = rho_new / h; 
0  before apply first correction to x 
0  before r = r - alpha*v 
0  before test stop criteria 
0 istlsolver::printOutput 
  0.5      1.42012e-28      8.43287e-17
0 === rate=7.11133e-33, T=0.0200463, TIT=0.0400925, IT=0.5
0 finished solveLinearSystemImpl_ 1
0 converged value: 1 7 1 let s go get convergedRemote
0 to  comm_.min(converged) 
0 did  comm_.min(converged) convergedRemote: 1
0 final convergedRemote: 1
0 Update: x^(k+1) = x^k - deltax^k 
0 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
0 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
0 Newton iteration  4 done0 , maximum relative shift = 3.873e-04
0 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
5  before alpha = rho_new / h; 
5  before apply first correction to x 
5  before r = r - alpha*v 
5  before test stop criteria 
5 istlsolver::printOutput 
  0.5      1.42012e-28      8.43287e-17
5 === rate=7.11133e-33, T=0.123319, TIT=0.246639, IT=0.5
5 finished solveLinearSystemImpl_ 1
5 converged value: 1 7 1 let s go get convergedRemote
5 to  comm_.min(converged) 
5 did  comm_.min(converged) convergedRemote: 1
5 final convergedRemote: 1
5 Update: x^(k+1) = x^k - deltax^k 
5 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
5 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
5 Newton iteration  4 done5 , maximum relative shift = 3.873e-04
5 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
6  before alpha = rho_new / h; 
6  before apply first correction to x 
6  before r = r - alpha*v 
6  before test stop criteria 
6 istlsolver::printOutput 
  0.5      1.42012e-28      8.43287e-17
6 === rate=7.11133e-33, T=0.12321, TIT=0.24642, IT=0.5
6 finished solveLinearSystemImpl_ 1
6 converged value: 1 7 1 let s go get convergedRemote
6 to  comm_.min(converged) 
6 did  comm_.min(converged) convergedRemote: 1
6 final convergedRemote: 1
6 Update: x^(k+1) = x^k - deltax^k 
6 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
6 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
6 Newton iteration  4 done6 , maximum relative shift = 3.873e-04
6 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
2  before alpha = rho_new / h; 
2  before apply first correction to x 
2  before r = r - alpha*v 
2  before test stop criteria 
2 istlsolver::printOutput 
  0.5      1.42012e-28      8.43287e-17
2 === rate=7.11133e-33, T=0.123211, TIT=0.246422, IT=0.5
2 finished solveLinearSystemImpl_ 1
2 converged value: 1 7 1 let s go get convergedRemote
2 to  comm_.min(converged) 
2 did  comm_.min(converged) convergedRemote: 1
2 final convergedRemote: 1
2 Update: x^(k+1) = x^k - deltax^k 
2 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
2 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
2 Newton iteration  4 done2 , maximum relative shift = 3.873e-04
2 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
3  before alpha = rho_new / h; 
3  before apply first correction to x 
3  before r = r - alpha*v 
3  before test stop criteria 
3 istlsolver::printOutput 
  0.5      1.42012e-28      8.43287e-17
3 === rate=7.11133e-33, T=0.123321, TIT=0.246642, IT=0.5
3 finished solveLinearSystemImpl_ 1
3 converged value: 1 7 1 let s go get convergedRemote
3 to  comm_.min(converged) 
3 did  comm_.min(converged) convergedRemote: 1
3 final convergedRemote: 1
3 Update: x^(k+1) = x^k - deltax^k 
3 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
3 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
3 Newton iteration  4 done3 , maximum relative shift = 3.873e-04
3 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
1  before v = A * y 
1  before  alpha = rho_new / < rt, v > 
1  before alpha = rho_new / h; 
1  before apply first correction to x 
1  before r = r - alpha*v 
1  before test stop criteria 
1 istlsolver::printOutput 
  0.5      1.42012e-28      8.43287e-17
1 === rate=7.11133e-33, T=0.123319, TIT=0.246638, IT=0.5
1 finished solveLinearSystemImpl_ 1
1 converged value: 1 7 1 let s go get convergedRemote
1 to  comm_.min(converged) 
1 did  comm_.min(converged) convergedRemote: 1
1 final convergedRemote: 1
1 Update: x^(k+1) = x^k - deltax^k 
1 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
1 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
1 Newton iteration  4 done1 , maximum relative shift = 3.873e-04
1 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
4 Solve: M deltax^k = r 
4 Newton::solveLinearSystem : entering the try section 4
4 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
4 solveLinearSystemImpl_, before converged = ls.solve 
4 7 amgbackend::solve isParallel 1
0 Solve: M deltax^k = r 
0 Newton::solveLinearSystem : entering the try section 4
0 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
0 solveLinearSystemImpl_, before converged = ls.solve 
0 7 amgbackend::solve isParallel 1
5 Solve: M deltax^k = r 
5 Newton::solveLinearSystem : entering the try section 4
5 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
5 solveLinearSystemImpl_, before converged = ls.solve 
5 7 amgbackend::solve isParallel 1
6 Solve: M deltax^k = r 
6 Newton::solveLinearSystem : entering the try section 4
6 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
6 solveLinearSystemImpl_, before converged = ls.solve 
6 7 amgbackend::solve isParallel 1
2 Solve: M deltax^k = r 
2 Newton::solveLinearSystem : entering the try section 4
2 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
2 solveLinearSystemImpl_, before converged = ls.solve 
2 7 amgbackend::solve isParallel 1
1 Solve: M deltax^k = r 
1 Newton::solveLinearSystem : entering the try section 4
1 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
1 solveLinearSystemImpl_, before converged = ls.solve 
1 7 amgbackend::solve isParallel 1
3 Solve: M deltax^k = r 
3 Newton::solveLinearSystem : entering the try section 4
3 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
3 solveLinearSystemImpl_, before converged = ls.solve 
3 7 amgbackend::solve isParallel 1
5 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
5 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
6 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
6 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
3 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
3 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
1 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
1 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
2 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
2 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
4 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
4 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
0 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
0 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 7
6 7 before Dune::BiCGSTABSolver 
6 Dune::BiCGSTABSolver.apply(x, b, result_) 
6 BiCGSTABSolver::apply 
6 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
5 7 before Dune::BiCGSTABSolver 
5 Dune::BiCGSTABSolver.apply(x, b, result_) 
5 BiCGSTABSolver::apply 
5 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 7
3 7 before Dune::BiCGSTABSolver 
3 Dune::BiCGSTABSolver.apply(x, b, result_) 
3 BiCGSTABSolver::apply 
3 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
1 7 before Dune::BiCGSTABSolver 
1 Dune::BiCGSTABSolver.apply(x, b, result_) 
1 BiCGSTABSolver::apply 
1 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
2 7 before Dune::BiCGSTABSolver 
2 Dune::BiCGSTABSolver.apply(x, b, result_) 
2 BiCGSTABSolver::apply 
2 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
4 7 before Dune::BiCGSTABSolver 
4 Dune::BiCGSTABSolver.apply(x, b, result_) 
4 BiCGSTABSolver::apply 
4 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
Using a direct coarse solver (SuperLU)
Building hierarchy of 1 levels (inclusive coarse solver) took 0.111489 seconds.
0 7 before Dune::BiCGSTABSolver 
0 Dune::BiCGSTABSolver.apply(x, b, result_) 
0 BiCGSTABSolver::apply 
0 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
0 === BiCGSTABSolver
 Iter          Defect            Rate
    0      4.24774e-14
0  (it = 0.5; it < _maxit; it+=.5) 0.5 250
3 === BiCGSTABSolver
 Iter          Defect            Rate
    0      4.24774e-14
3  (it = 0.5; it < _maxit; it+=.5) 0.5 250
3  before p = r 
3  before  y = W^-1 * p 
3  before _prec->apply(y,p); 0.5 250
3 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
1 === BiCGSTABSolver
 Iter          Defect            Rate
    0      4.24774e-14
1  (it = 0.5; it < _maxit; it+=.5) 0.5 250
1  before p = r 
1  before  y = W^-1 * p 
1  before _prec->apply(y,p); 0.5 250
1 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
2 === BiCGSTABSolver
 Iter          Defect            Rate
    0      4.24774e-14
2  (it = 0.5; it < _maxit; it+=.5) 0.5 250
2  before p = r 
2  before  y = W^-1 * p 
2  before _prec->apply(y,p); 0.5 250
2 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
5 === BiCGSTABSolver
 Iter          Defect            Rate
    0      4.24774e-14
5  (it = 0.5; it < _maxit; it+=.5) 0.5 250
5  before p = r 
5  before  y = W^-1 * p 
5  before _prec->apply(y,p); 0.5 250
5 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
6 === BiCGSTABSolver
 Iter          Defect            Rate
    0      4.24774e-14
6  (it = 0.5; it < _maxit; it+=.5) 0.5 250
6  before p = r 
6  before  y = W^-1 * p 
6  before _prec->apply(y,p); 0.5 250
6 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
0  before p = r 
0  before  y = W^-1 * p 
0  before _prec->apply(y,p); 0.5 250
0 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 7
0 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
4 === BiCGSTABSolver
 Iter          Defect            Rate
    0      4.24774e-14
4  (it = 0.5; it < _maxit; it+=.5) 0.5 250
4  before p = r 
4  before  y = W^-1 * p 
4  before _prec->apply(y,p); 0.5 250
4 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
2 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
2 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
5 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
5 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
5  before v = A * y 
4 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
4 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
4  before v = A * y 
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
0 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
6 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
6 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
6  before v = A * y 
6  before  alpha = rho_new / < rt, v > 
3 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
3 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
3  before v = A * y 
1 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
1 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
1  before v = A * y 
4  before  alpha = rho_new / < rt, v > 
0 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
0  before v = A * y 
3  before  alpha = rho_new / < rt, v > 
5  before  alpha = rho_new / < rt, v > 
2  before v = A * y 
2  before  alpha = rho_new / < rt, v > 
2  before alpha = rho_new / h; 
2  before apply first correction to x 
2  before r = r - alpha*v 
2  before test stop criteria 
2 istlsolver::printOutput 
  0.5      4.88987e-33      1.15117e-19
2 === rate=1.32519e-38, T=0.122174, TIT=0.244348, IT=0.5
2 finished solveLinearSystemImpl_ 1
2 converged value: 1 7 1 let s go get convergedRemote
2 to  comm_.min(converged) 
1  before  alpha = rho_new / < rt, v > 
1  before alpha = rho_new / h; 
1  before apply first correction to x 
1  before r = r - alpha*v 
1  before test stop criteria 
1 istlsolver::printOutput 
  0.5      4.88987e-33      1.15117e-19
1 === rate=1.32519e-38, T=0.122264, TIT=0.244528, IT=0.5
1 finished solveLinearSystemImpl_ 1
1 converged value: 1 7 1 let s go get convergedRemote
1 to  comm_.min(converged) 
4  before alpha = rho_new / h; 
4  before apply first correction to x 
4  before r = r - alpha*v 
4  before test stop criteria 
4 istlsolver::printOutput 
  0.5      4.88987e-33      1.15117e-19
4 === rate=1.32519e-38, T=0.122174, TIT=0.244348, IT=0.5
4 finished solveLinearSystemImpl_ 1
4 converged value: 1 7 1 let s go get convergedRemote
4 to  comm_.min(converged) 
6  before alpha = rho_new / h; 
6  before apply first correction to x 
6  before r = r - alpha*v 
6  before test stop criteria 
6 istlsolver::printOutput 
  0.5      4.88987e-33      1.15117e-19
6 === rate=1.32519e-38, T=0.122174, TIT=0.244349, IT=0.5
6 finished solveLinearSystemImpl_ 1
6 converged value: 1 7 1 let s go get convergedRemote
6 to  comm_.min(converged) 
0  before  alpha = rho_new / < rt, v > 
0  before alpha = rho_new / h; 
0  before apply first correction to x 
0  before r = r - alpha*v 
0  before test stop criteria 
0 istlsolver::printOutput 
  0.5      4.88987e-33      1.15117e-19
0 === rate=1.32519e-38, T=0.0186551, TIT=0.0373103, IT=0.5
3  before alpha = rho_new / h; 
3  before apply first correction to x 
3  before r = r - alpha*v 
3  before test stop criteria 
3 istlsolver::printOutput 
  0.5      4.88987e-33      1.15117e-19
3 === rate=1.32519e-38, T=0.122265, TIT=0.244529, IT=0.5
3 finished solveLinearSystemImpl_ 1
3 converged value: 1 7 1 let s go get convergedRemote
3 to  comm_.min(converged) 
5  before alpha = rho_new / h; 
5  before apply first correction to x 
5  before r = r - alpha*v 
5  before test stop criteria 
5 istlsolver::printOutput 
  0.5      4.88987e-33      1.15117e-19
5 === rate=1.32519e-38, T=0.122264, TIT=0.244528, IT=0.5
5 finished solveLinearSystemImpl_ 1
5 converged value: 1 7 1 let s go get convergedRemote
5 to  comm_.min(converged) 
1 did  comm_.min(converged) convergedRemote: 1
1 final convergedRemote: 1
1 Update: x^(k+1) = x^k - deltax^k 
1 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
1 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
1 Newton iteration  5 done1 , maximum relative shift = 9.768e-06
1 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
4 did  comm_.min(converged) convergedRemote: 1
4 final convergedRemote: 1
4 Update: x^(k+1) = x^k - deltax^k 
4 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
4 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
4 Newton iteration  5 done4 , maximum relative shift = 9.768e-06
4 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
6 did  comm_.min(converged) convergedRemote: 1
6 final convergedRemote: 1
6 Update: x^(k+1) = x^k - deltax^k 
6 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
6 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
6 Newton iteration  5 done6 , maximum relative shift = 9.768e-06
6 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
0 finished solveLinearSystemImpl_ 1
0 converged value: 1 7 1 let s go get convergedRemote
0 to  comm_.min(converged) 
0 did  comm_.min(converged) convergedRemote: 1
0 final convergedRemote: 1
0 Update: x^(k+1) = x^k - deltax^k 
0 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
0 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
0 Newton iteration  5 done0 , maximum relative shift = 9.768e-06
0 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
3 did  comm_.min(converged) convergedRemote: 1
3 final convergedRemote: 1
3 Update: x^(k+1) = x^k - deltax^k 
3 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
3 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
3 Newton iteration  5 done3 , maximum relative shift = 9.768e-06
3 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
5 did  comm_.min(converged) convergedRemote: 1
5 final convergedRemote: 1
5 Update: x^(k+1) = x^k - deltax^k 
5 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
5 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
5 Newton iteration  5 done5 , maximum relative shift = 9.768e-06
5 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
2 did  comm_.min(converged) convergedRemote: 1
2 final convergedRemote: 1
2 Update: x^(k+1) = x^k - deltax^k 
2 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
2 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
2 Newton iteration  5 done2 , maximum relative shift = 9.768e-06
2 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
1 Solve: M deltax^k = r 
1 Newton::solveLinearSystem : entering the try section 5
1 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
1 solveLinearSystemImpl_, before converged = ls.solve 
1 7 amgbackend::solve isParallel 1
4 Solve: M deltax^k = r 
4 Newton::solveLinearSystem : entering the try section 5
4 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
4 solveLinearSystemImpl_, before converged = ls.solve 
4 7 amgbackend::solve isParallel 1
6 Solve: M deltax^k = r 
6 Newton::solveLinearSystem : entering the try section 5
6 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
6 solveLinearSystemImpl_, before converged = ls.solve 
6 7 amgbackend::solve isParallel 1
0 Solve: M deltax^k = r 
0 Newton::solveLinearSystem : entering the try section 5
0 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
0 solveLinearSystemImpl_, before converged = ls.solve 
0 7 amgbackend::solve isParallel 1
2 Solve: M deltax^k = r 
2 Newton::solveLinearSystem : entering the try section 5
2 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
2 solveLinearSystemImpl_, before converged = ls.solve 
2 7 amgbackend::solve isParallel 1
5 Solve: M deltax^k = r 
5 Newton::solveLinearSystem : entering the try section 5
5 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
5 solveLinearSystemImpl_, before converged = ls.solve 
5 7 amgbackend::solve isParallel 1
3 Solve: M deltax^k = r 
3 Newton::solveLinearSystem : entering the try section 5
3 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
3 solveLinearSystemImpl_, before converged = ls.solve 
3 7 amgbackend::solve isParallel 1
4 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
4 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
6 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
6 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
3 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
3 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
2 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
2 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
5 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
5 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
0 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
0 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 7
1 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
1 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 7
3 7 before Dune::BiCGSTABSolver 
3 Dune::BiCGSTABSolver.apply(x, b, result_) 
3 BiCGSTABSolver::apply 
3 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
2 7 before Dune::BiCGSTABSolver 
2 Dune::BiCGSTABSolver.apply(x, b, result_) 
2 BiCGSTABSolver::apply 
2 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
5 7 before Dune::BiCGSTABSolver 
5 Dune::BiCGSTABSolver.apply(x, b, result_) 
5 BiCGSTABSolver::apply 
5 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
1 7 before Dune::BiCGSTABSolver 
1 Dune::BiCGSTABSolver.apply(x, b, result_) 
1 BiCGSTABSolver::apply 
1 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
6 7 before Dune::BiCGSTABSolver 
6 Dune::BiCGSTABSolver.apply(x, b, result_) 
6 BiCGSTABSolver::apply 
6 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
4 7 before Dune::BiCGSTABSolver 
4 Dune::BiCGSTABSolver.apply(x, b, result_) 
4 BiCGSTABSolver::apply 
4 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
Using a direct coarse solver (SuperLU)
Building hierarchy of 1 levels (inclusive coarse solver) took 0.110605 seconds.
0 7 before Dune::BiCGSTABSolver 
0 Dune::BiCGSTABSolver.apply(x, b, result_) 
0 BiCGSTABSolver::apply 
0 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
0 === BiCGSTABSolver
 Iter          Defect            Rate
    0      1.08124e-15
0  (it = 0.5; it < _maxit; it+=.5) 0.5 250
4 === BiCGSTABSolver
 Iter          Defect            Rate
    0      1.08124e-15
4  (it = 0.5; it < _maxit; it+=.5) 0.5 250
4  before p = r 
4  before  y = W^-1 * p 
4  before _prec->apply(y,p); 0.5 250
4 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
3 === BiCGSTABSolver
 Iter          Defect            Rate
    0      1.08124e-15
3  (it = 0.5; it < _maxit; it+=.5) 0.5 250
3  before p = r 
3  before  y = W^-1 * p 
3  before _prec->apply(y,p); 0.5 250
3 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
2 === BiCGSTABSolver
 Iter          Defect            Rate
    0      1.08124e-15
2  (it = 0.5; it < _maxit; it+=.5) 0.5 250
2  before p = r 
2  before  y = W^-1 * p 
2  before _prec->apply(y,p); 0.5 250
2 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
5 === BiCGSTABSolver
 Iter          Defect            Rate
    0      1.08124e-15
5  (it = 0.5; it < _maxit; it+=.5) 0.5 250
5  before p = r 
5  before  y = W^-1 * p 
5  before _prec->apply(y,p); 0.5 250
5 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
1 === BiCGSTABSolver
 Iter          Defect            Rate
    0      1.08124e-15
1  (it = 0.5; it < _maxit; it+=.5) 0.5 250
1  before p = r 
1  before  y = W^-1 * p 
1  before _prec->apply(y,p); 0.5 250
1 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
6 === BiCGSTABSolver
 Iter          Defect            Rate
    0      1.08124e-15
6  (it = 0.5; it < _maxit; it+=.5) 0.5 250
6  before p = r 
6  before  y = W^-1 * p 
6  before _prec->apply(y,p); 0.5 250
6 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
0  before p = r 
0  before  y = W^-1 * p 
0  before _prec->apply(y,p); 0.5 250
0 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 7
0 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
4 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
4 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
4  before v = A * y 
3 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
3 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
3  before v = A * y 
2 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
2 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
5 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
5 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
5  before v = A * y 
1 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
6 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
6 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
6  before v = A * y 
6  before  alpha = rho_new / < rt, v > 
0 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
0 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
1 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
4  before  alpha = rho_new / < rt, v > 
3  before  alpha = rho_new / < rt, v > 
2  before v = A * y 
5  before  alpha = rho_new / < rt, v > 
6  before alpha = rho_new / h; 
6  before apply first correction to x 
6  before r = r - alpha*v 
6  before test stop criteria 
6 istlsolver::printOutput 
  0.5      7.71741e-32      7.13752e-17
6 === rate=5.09442e-33, T=0.12107, TIT=0.242141, IT=0.5
6 finished solveLinearSystemImpl_ 1
6 converged value: 1 7 1 let s go get convergedRemote
6 to  comm_.min(converged) 
0  before v = A * y 
0  before  alpha = rho_new / < rt, v > 
0  before alpha = rho_new / h; 
0  before apply first correction to x 
0  before r = r - alpha*v 
0  before test stop criteria 
0 istlsolver::printOutput 
  0.5      7.71741e-32      7.13752e-17
0 === rate=5.09442e-33, T=0.0183948, TIT=0.0367896, IT=0.5
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
1  before v = A * y 
1  before  alpha = rho_new / < rt, v > 
1  before alpha = rho_new / h; 
1  before apply first correction to x 
1  before r = r - alpha*v 
1  before test stop criteria 
1 istlsolver::printOutput 
  0.5      7.71741e-32      7.13752e-17
1 === rate=5.09442e-33, T=0.12105, TIT=0.2421, IT=0.5
1 finished solveLinearSystemImpl_ 1
1 converged value: 1 7 1 let s go get convergedRemote
1 to  comm_.min(converged) 
4  before alpha = rho_new / h; 
4  before apply first correction to x 
4  before r = r - alpha*v 
4  before test stop criteria 
4 istlsolver::printOutput 
  0.5      7.71741e-32      7.13752e-17
4 === rate=5.09442e-33, T=0.121069, TIT=0.242138, IT=0.5
4 finished solveLinearSystemImpl_ 1
4 converged value: 1 7 1 let s go get convergedRemote
4 to  comm_.min(converged) 
3  before alpha = rho_new / h; 
3  before apply first correction to x 
3  before r = r - alpha*v 
3  before test stop criteria 
3 istlsolver::printOutput 
  0.5      7.71741e-32      7.13752e-17
3 === rate=5.09442e-33, T=0.121047, TIT=0.242094, IT=0.5
3 finished solveLinearSystemImpl_ 1
3 converged value: 1 7 1 let s go get convergedRemote
3 to  comm_.min(converged) 
2  before  alpha = rho_new / < rt, v > 
2  before alpha = rho_new / h; 
2  before apply first correction to x 
2  before r = r - alpha*v 
2  before test stop criteria 
2 istlsolver::printOutput 
  0.5      7.71741e-32      7.13752e-17
2 === rate=5.09442e-33, T=0.121069, TIT=0.242138, IT=0.5
2 finished solveLinearSystemImpl_ 1
2 converged value: 1 7 1 let s go get convergedRemote
2 to  comm_.min(converged) 
5  before alpha = rho_new / h; 
5  before apply first correction to x 
5  before r = r - alpha*v 
5  before test stop criteria 
5 istlsolver::printOutput 
  0.5      7.71741e-32      7.13752e-17
5 === rate=5.09442e-33, T=0.121048, TIT=0.242097, IT=0.5
5 finished solveLinearSystemImpl_ 1
5 converged value: 1 7 1 let s go get convergedRemote
5 to  comm_.min(converged) 
1 did  comm_.min(converged) convergedRemote: 1
1 final convergedRemote: 1
1 Update: x^(k+1) = x^k - deltax^k 
1 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
1 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
1 Newton iteration  6 done1 , maximum relative shift = 3.314e-07
1 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
4 did  comm_.min(converged) convergedRemote: 1
4 final convergedRemote: 1
4 Update: x^(k+1) = x^k - deltax^k 
4 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
4 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
4 Newton iteration  6 done4 , maximum relative shift = 3.314e-07
4 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
3 did  comm_.min(converged) convergedRemote: 1
3 final convergedRemote: 1
3 Update: x^(k+1) = x^k - deltax^k 
3 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
3 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
3 Newton iteration  6 done3 , maximum relative shift = 3.314e-07
3 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
5 did  comm_.min(converged) convergedRemote: 1
5 final convergedRemote: 1
5 Update: x^(k+1) = x^k - deltax^k 
5 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
5 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
5 Newton iteration  6 done5 , maximum relative shift = 3.314e-07
5 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
2 did  comm_.min(converged) convergedRemote: 1
2 final convergedRemote: 1
2 Update: x^(k+1) = x^k - deltax^k 
2 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
2 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
2 Newton iteration  6 done2 , maximum relative shift = 3.314e-07
2 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
6 did  comm_.min(converged) convergedRemote: 1
6 final convergedRemote: 1
6 Update: x^(k+1) = x^k - deltax^k 
6 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
6 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
6 Newton iteration  6 done6 , maximum relative shift = 3.314e-07
6 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
0 finished solveLinearSystemImpl_ 1
0 converged value: 1 7 1 let s go get convergedRemote
0 to  comm_.min(converged) 
0 did  comm_.min(converged) convergedRemote: 1
0 final convergedRemote: 1
0 Update: x^(k+1) = x^k - deltax^k 
0 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
0 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
0 Newton iteration  6 done0 , maximum relative shift = 3.314e-07
0 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
5 Solve: M deltax^k = r 
5 Newton::solveLinearSystem : entering the try section 6
5 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
5 solveLinearSystemImpl_, before converged = ls.solve 
5 7 amgbackend::solve isParallel 1
2 Solve: M deltax^k = r 
2 Newton::solveLinearSystem : entering the try section 6
2 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
2 solveLinearSystemImpl_, before converged = ls.solve 
2 7 amgbackend::solve isParallel 1
6 Solve: M deltax^k = r 
6 Newton::solveLinearSystem : entering the try section 6
6 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
6 solveLinearSystemImpl_, before converged = ls.solve 
6 7 amgbackend::solve isParallel 1
3 Solve: M deltax^k = r 
3 Newton::solveLinearSystem : entering the try section 6
3 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
3 solveLinearSystemImpl_, before converged = ls.solve 
3 7 amgbackend::solve isParallel 1
1 Solve: M deltax^k = r 
1 Newton::solveLinearSystem : entering the try section 6
1 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
1 solveLinearSystemImpl_, before converged = ls.solve 
1 7 amgbackend::solve isParallel 1
4 Solve: M deltax^k = r 
4 Newton::solveLinearSystem : entering the try section 6
4 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
4 solveLinearSystemImpl_, before converged = ls.solve 
4 7 amgbackend::solve isParallel 1
0 Solve: M deltax^k = r 
0 Newton::solveLinearSystem : entering the try section 6
0 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
0 solveLinearSystemImpl_, before converged = ls.solve 
0 7 amgbackend::solve isParallel 1
2 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
2 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
6 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
6 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
3 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
3 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
1 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
1 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
0 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
0 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 7
4 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
4 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
5 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
5 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 7
4 7 before Dune::BiCGSTABSolver 
4 Dune::BiCGSTABSolver.apply(x, b, result_) 
4 BiCGSTABSolver::apply 
4 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
1 7 before Dune::BiCGSTABSolver 
1 Dune::BiCGSTABSolver.apply(x, b, result_) 
1 BiCGSTABSolver::apply 
1 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
2 7 before Dune::BiCGSTABSolver 
2 Dune::BiCGSTABSolver.apply(x, b, result_) 
2 BiCGSTABSolver::apply 
2 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
6 7 before Dune::BiCGSTABSolver 
6 Dune::BiCGSTABSolver.apply(x, b, result_) 
6 BiCGSTABSolver::apply 
6 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
3 7 before Dune::BiCGSTABSolver 
3 Dune::BiCGSTABSolver.apply(x, b, result_) 
3 BiCGSTABSolver::apply 
3 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 7
5 7 before Dune::BiCGSTABSolver 
5 Dune::BiCGSTABSolver.apply(x, b, result_) 
5 BiCGSTABSolver::apply 
5 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
Using a direct coarse solver (SuperLU)
Building hierarchy of 1 levels (inclusive coarse solver) took 0.111475 seconds.
0 7 before Dune::BiCGSTABSolver 
0 Dune::BiCGSTABSolver.apply(x, b, result_) 
0 BiCGSTABSolver::apply 
0 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
0 === BiCGSTABSolver
 Iter          Defect            Rate
    0      3.23182e-16
0  (it = 0.5; it < _maxit; it+=.5) 0.5 250
4 === BiCGSTABSolver
 Iter          Defect            Rate
    0      3.23182e-16
4  (it = 0.5; it < _maxit; it+=.5) 0.5 250
4  before p = r 
4  before  y = W^-1 * p 
4  before _prec->apply(y,p); 0.5 250
4 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
1 === BiCGSTABSolver
 Iter          Defect            Rate
    0      3.23182e-16
1  (it = 0.5; it < _maxit; it+=.5) 0.5 250
1  before p = r 
1  before  y = W^-1 * p 
1  before _prec->apply(y,p); 0.5 250
1 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
2 === BiCGSTABSolver
 Iter          Defect            Rate
    0      3.23182e-16
2  (it = 0.5; it < _maxit; it+=.5) 0.5 250
2  before p = r 
2  before  y = W^-1 * p 
2  before _prec->apply(y,p); 0.5 250
2 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
6 === BiCGSTABSolver
 Iter          Defect            Rate
    0      3.23182e-16
6  (it = 0.5; it < _maxit; it+=.5) 0.5 250
6  before p = r 
6  before  y = W^-1 * p 
6  before _prec->apply(y,p); 0.5 250
6 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
0  before p = r 
0  before  y = W^-1 * p 
0  before _prec->apply(y,p); 0.5 250
0 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 7
0 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
5 === BiCGSTABSolver
 Iter          Defect            Rate
    0      3.23182e-16
5  (it = 0.5; it < _maxit; it+=.5) 0.5 250
5  before p = r 
5  before  y = W^-1 * p 
5  before _prec->apply(y,p); 0.5 250
5 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
3 === BiCGSTABSolver
 Iter          Defect            Rate
    0      3.23182e-16
3  (it = 0.5; it < _maxit; it+=.5) 0.5 250
3  before p = r 
3  before  y = W^-1 * p 
3  before _prec->apply(y,p); 0.5 250
3 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
4 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
4 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
4  before v = A * y 
1 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
2 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
2 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
3 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
3 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
3  before v = A * y 
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
5 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
5 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
5  before v = A * y 
6 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
6 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
6  before v = A * y 
1 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
6  before  alpha = rho_new / < rt, v > 
3  before  alpha = rho_new / < rt, v > 
0 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
0 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
0  before v = A * y 
5  before  alpha = rho_new / < rt, v > 
2  before v = A * y 
4  before  alpha = rho_new / < rt, v > 
2  before  alpha = rho_new / < rt, v > 
2  before alpha = rho_new / h; 
2  before apply first correction to x 
2  before r = r - alpha*v 
2  before test stop criteria 
2 istlsolver::printOutput 
  0.5      4.93047e-32       1.5256e-16
2 === rate=2.32746e-32, T=0.122188, TIT=0.244376, IT=0.5
2 finished solveLinearSystemImpl_ 1
2 converged value: 1 7 1 let s go get convergedRemote
2 to  comm_.min(converged) 
1  before v = A * y 
1  before  alpha = rho_new / < rt, v > 
1  before alpha = rho_new / h; 
1  before apply first correction to x 
1  before r = r - alpha*v 
1  before test stop criteria 
1 istlsolver::printOutput 
  0.5      4.93047e-32       1.5256e-16
1 === rate=2.32746e-32, T=0.12235, TIT=0.244701, IT=0.5
1 finished solveLinearSystemImpl_ 1
1 converged value: 1 7 1 let s go get convergedRemote
1 to  comm_.min(converged) 
4  before alpha = rho_new / h; 
4  before apply first correction to x 
4  before r = r - alpha*v 
4  before test stop criteria 
4 istlsolver::printOutput 
  0.5      4.93047e-32       1.5256e-16
4 === rate=2.32746e-32, T=0.122189, TIT=0.244378, IT=0.5
4 finished solveLinearSystemImpl_ 1
4 converged value: 1 7 1 let s go get convergedRemote
4 to  comm_.min(converged) 
6  before alpha = rho_new / h; 
6  before apply first correction to x 
6  before r = r - alpha*v 
6  before test stop criteria 
6 istlsolver::printOutput 
  0.5      4.93047e-32       1.5256e-16
6 === rate=2.32746e-32, T=0.122187, TIT=0.244373, IT=0.5
6 finished solveLinearSystemImpl_ 1
6 converged value: 1 7 1 let s go get convergedRemote
6 to  comm_.min(converged) 
3  before alpha = rho_new / h; 
3  before apply first correction to x 
3  before r = r - alpha*v 
3  before test stop criteria 
3 istlsolver::printOutput 
  0.5      4.93047e-32       1.5256e-16
3 === rate=2.32746e-32, T=0.122357, TIT=0.244714, IT=0.5
3 finished solveLinearSystemImpl_ 1
3 converged value: 1 7 1 let s go get convergedRemote
3 to  comm_.min(converged) 
0  before  alpha = rho_new / < rt, v > 
0  before alpha = rho_new / h; 
0  before apply first correction to x 
0  before r = r - alpha*v 
0  before test stop criteria 
0 istlsolver::printOutput 
  0.5      4.93047e-32       1.5256e-16
0 === rate=2.32746e-32, T=0.0185122, TIT=0.0370244, IT=0.5
5  before alpha = rho_new / h; 
5  before apply first correction to x 
5  before r = r - alpha*v 
5  before test stop criteria 
5 istlsolver::printOutput 
  0.5      4.93047e-32       1.5256e-16
5 === rate=2.32746e-32, T=0.12236, TIT=0.24472, IT=0.5
5 finished solveLinearSystemImpl_ 1
5 converged value: 1 7 1 let s go get convergedRemote
5 to  comm_.min(converged) 
5 did  comm_.min(converged) convergedRemote: 1
5 final convergedRemote: 1
5 Update: x^(k+1) = x^k - deltax^k 
5 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
5 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
5 Newton iteration  7 done5 , maximum relative shift = 5.946e-08
5 Assemble/solve/update time: 0.308636(25.143%)/0.918758(74.8465%)/0.000129432(0.0105442%)
5 gridVariables->advanceTimeStep
5 before, nonLinearSolver->suggestTimeStepSize, current time: 625.793 simTime 57600
5 before assembler->setPreviousSolution, ddt: 12.9366
5 nonLinearSolver->solve
5 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
4 did  comm_.min(converged) convergedRemote: 1
4 final convergedRemote: 1
4 Update: x^(k+1) = x^k - deltax^k 
4 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
4 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
4 Newton iteration  7 done4 , maximum relative shift = 5.946e-08
4 Assemble/solve/update time: 0.308209(25.117%)/0.918754(74.8723%)/0.000131272(0.0106978%)
4 gridVariables->advanceTimeStep
4 before, nonLinearSolver->suggestTimeStepSize, current time: 625.793 simTime 57600
4 before assembler->setPreviousSolution, ddt: 12.9366
4 nonLinearSolver->solve
4 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
6 did  comm_.min(converged) convergedRemote: 1
6 final convergedRemote: 1
6 Update: x^(k+1) = x^k - deltax^k 
6 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
6 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
6 Newton iteration  7 done6 , maximum relative shift = 5.946e-08
6 Assemble/solve/update time: 0.308351(25.1258%)/0.918776(74.8658%)/0.000102791(0.00837585%)
6 gridVariables->advanceTimeStep
6 before, nonLinearSolver->suggestTimeStepSize, current time: 625.793 simTime 57600
6 before assembler->setPreviousSolution, ddt: 12.9366
6 nonLinearSolver->solve
6 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
3 did  comm_.min(converged) convergedRemote: 1
3 final convergedRemote: 1
3 Update: x^(k+1) = x^k - deltax^k 
3 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
3 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
3 Newton iteration  7 done3 , maximum relative shift = 5.946e-08
3 Assemble/solve/update time: 0.308601(25.1403%)/0.918781(74.849%)/0.000131762(0.0107341%)
3 gridVariables->advanceTimeStep
3 before, nonLinearSolver->suggestTimeStepSize, current time: 625.793 simTime 57600
3 before assembler->setPreviousSolution, ddt: 12.9366
3 nonLinearSolver->solve
3 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
0 finished solveLinearSystemImpl_ 1
0 converged value: 1 7 1 let s go get convergedRemote
0 to  comm_.min(converged) 
0 did  comm_.min(converged) convergedRemote: 1
0 final convergedRemote: 1
0 Update: x^(k+1) = x^k - deltax^k 
0 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
0 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
0 Newton iteration  7 done0 , maximum relative shift = 5.946e-08
0 Assemble/solve/update time: 0.307403(25.0678%)/0.918732(74.9199%)/0.000151092(0.0123211%)
0 gridVariables->advanceTimeStep
0 before, nonLinearSolver->suggestTimeStepSize, current time: 625.793 simTime 57600
0 before assembler->setPreviousSolution, ddt: 12.9366
0 nonLinearSolver->solve
1 did  comm_.min(converged) convergedRemote: 1
1 final convergedRemote: 1
1 Update: x^(k+1) = x^k - deltax^k 
1 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
1 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
1 Newton iteration  7 done1 , maximum relative shift = 5.946e-08
1 Assemble/solve/update time: 0.308622(25.1421%)/0.918756(74.8471%)/0.000132271(0.0107756%)
1 gridVariables->advanceTimeStep
1 before, nonLinearSolver->suggestTimeStepSize, current time: 625.793 simTime 57600
1 before assembler->setPreviousSolution, ddt: 12.9366
1 nonLinearSolver->solve
1 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
2 did  comm_.min(converged) convergedRemote: 1
2 final convergedRemote: 1
2 Update: x^(k+1) = x^k - deltax^k 
2 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
2 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
2 Newton iteration  7 done2 , maximum relative shift = 5.946e-08
2 Assemble/solve/update time: 0.30818(25.1147%)/0.918779(74.8746%)/0.000130982(0.0106742%)
2 gridVariables->advanceTimeStep
2 before, nonLinearSolver->suggestTimeStepSize, current time: 625.793 simTime 57600
2 before assembler->setPreviousSolution, ddt: 12.9366
2 nonLinearSolver->solve
2 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
0 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
3 Solve: M deltax^k = r 
3 Newton::solveLinearSystem : entering the try section 0
3to  comm_.sum(norm2) 
3did  comm_.sum(norm2) 
3 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
3 solveLinearSystemImpl_, before converged = ls.solve 
3 7 amgbackend::solve isParallel 1
2 Solve: M deltax^k = r 
2 Newton::solveLinearSystem : entering the try section 0
2to  comm_.sum(norm2) 
2did  comm_.sum(norm2) 
2 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
2 solveLinearSystemImpl_, before converged = ls.solve 
2 7 amgbackend::solve isParallel 1
1 Solve: M deltax^k = r 
1 Newton::solveLinearSystem : entering the try section 0
1to  comm_.sum(norm2) 
1did  comm_.sum(norm2) 
1 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
1 solveLinearSystemImpl_, before converged = ls.solve 
1 7 amgbackend::solve isParallel 1
0 Solve: M deltax^k = r 
0 Newton::solveLinearSystem : entering the try section 0
0to  comm_.sum(norm2) 
0did  comm_.sum(norm2) 
0 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
0 solveLinearSystemImpl_, before converged = ls.solve 
0 7 amgbackend::solve isParallel 1
5 Solve: M deltax^k = r 
5 Newton::solveLinearSystem : entering the try section 0
5to  comm_.sum(norm2) 
5did  comm_.sum(norm2) 
5 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
5 solveLinearSystemImpl_, before converged = ls.solve 
5 7 amgbackend::solve isParallel 1
4 Solve: M deltax^k = r 
4 Newton::solveLinearSystem : entering the try section 0
4to  comm_.sum(norm2) 
4did  comm_.sum(norm2) 
4 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
4 solveLinearSystemImpl_, before converged = ls.solve 
4 7 amgbackend::solve isParallel 1
6 Solve: M deltax^k = r 
6 Newton::solveLinearSystem : entering the try section 0
6to  comm_.sum(norm2) 
6did  comm_.sum(norm2) 
6 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
6 solveLinearSystemImpl_, before converged = ls.solve 
6 7 amgbackend::solve isParallel 1
6 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
6 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
0 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
0 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 7
5 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
5 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
4 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
4 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
1 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
1 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
3 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
3 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
2 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
2 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 7
6 7 before Dune::BiCGSTABSolver 
6 Dune::BiCGSTABSolver.apply(x, b, result_) 
6 BiCGSTABSolver::apply 
6 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
2 7 before Dune::BiCGSTABSolver 
2 Dune::BiCGSTABSolver.apply(x, b, result_) 
2 BiCGSTABSolver::apply 
2 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
5 7 before Dune::BiCGSTABSolver 
5 Dune::BiCGSTABSolver.apply(x, b, result_) 
5 BiCGSTABSolver::apply 
5 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
4 7 before Dune::BiCGSTABSolver 
4 Dune::BiCGSTABSolver.apply(x, b, result_) 
4 BiCGSTABSolver::apply 
4 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
1 7 before Dune::BiCGSTABSolver 
1 Dune::BiCGSTABSolver.apply(x, b, result_) 
1 BiCGSTABSolver::apply 
1 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
3 7 before Dune::BiCGSTABSolver 
3 Dune::BiCGSTABSolver.apply(x, b, result_) 
3 BiCGSTABSolver::apply 
3 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 7
Using a direct coarse solver (SuperLU)
Building hierarchy of 1 levels (inclusive coarse solver) took 0.110013 seconds.
0 7 before Dune::BiCGSTABSolver 
0 Dune::BiCGSTABSolver.apply(x, b, result_) 
0 BiCGSTABSolver::apply 
0 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
0 === BiCGSTABSolver
 Iter          Defect            Rate
4 === BiCGSTABSolver
 Iter          Defect            Rate
    0      3.16222e-09
4  (it = 0.5; it < _maxit; it+=.5) 0.5 250
4  before p = r 
4  before  y = W^-1 * p 
4  before _prec->apply(y,p); 0.5 250
4 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
1 === BiCGSTABSolver
 Iter          Defect            Rate
    0      3.16222e-09
1  (it = 0.5; it < _maxit; it+=.5) 0.5 250
1  before p = r 
1  before  y = W^-1 * p 
1  before _prec->apply(y,p); 0.5 250
1 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
    0      3.16222e-09
0  (it = 0.5; it < _maxit; it+=.5) 0.5 250
0  before p = r 
0  before  y = W^-1 * p 
0  before _prec->apply(y,p); 0.5 250
0 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 7
0 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
3 === BiCGSTABSolver
 Iter          Defect            Rate
    0      3.16222e-09
3  (it = 0.5; it < _maxit; it+=.5) 0.5 250
3  before p = r 
3  before  y = W^-1 * p 
3  before _prec->apply(y,p); 0.5 250
3 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
6 === BiCGSTABSolver
 Iter          Defect            Rate
    0      3.16222e-09
6  (it = 0.5; it < _maxit; it+=.5) 0.5 250
6  before p = r 
6  before  y = W^-1 * p 
6  before _prec->apply(y,p); 0.5 250
6 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
2 === BiCGSTABSolver
 Iter          Defect            Rate
    0      3.16222e-09
2  (it = 0.5; it < _maxit; it+=.5) 0.5 250
2  before p = r 
2  before  y = W^-1 * p 
2  before _prec->apply(y,p); 0.5 250
2 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
5 === BiCGSTABSolver
 Iter          Defect            Rate
    0      3.16222e-09
5  (it = 0.5; it < _maxit; it+=.5) 0.5 250
5  before p = r 
5  before  y = W^-1 * p 
5  before _prec->apply(y,p); 0.5 250
5 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
1 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
4 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
4 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
4  before v = A * y 
5 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
5 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
5  before v = A * y 
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
0 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
3 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
3 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
3  before v = A * y 
6 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
6 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
6  before v = A * y 
2 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
2 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
0 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
0  before v = A * y 
0  before  alpha = rho_new / < rt, v > 
3  before  alpha = rho_new / < rt, v > 
6  before  alpha = rho_new / < rt, v > 
4  before  alpha = rho_new / < rt, v > 
1 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
2  before v = A * y 
2  before  alpha = rho_new / < rt, v > 
5  before  alpha = rho_new / < rt, v > 
2  before alpha = rho_new / h; 
2  before apply first correction to x 
2  before r = r - alpha*v 
2  before test stop criteria 
2 istlsolver::printOutput 
  0.5      4.79616e-25      1.51671e-16
2 === rate=2.30039e-32, T=0.120895, TIT=0.241791, IT=0.5
2 finished solveLinearSystemImpl_ 1
2 converged value: 1 7 1 let s go get convergedRemote
2 to  comm_.min(converged) 
1  before v = A * y 
1  before  alpha = rho_new / < rt, v > 
1  before alpha = rho_new / h; 
1  before apply first correction to x 
1  before r = r - alpha*v 
1  before test stop criteria 
1 istlsolver::printOutput 
  0.5      4.79616e-25      1.51671e-16
1 === rate=2.30039e-32, T=0.121001, TIT=0.242002, IT=0.5
1 finished solveLinearSystemImpl_ 1
1 converged value: 1 7 1 let s go get convergedRemote
1 to  comm_.min(converged) 
0  before alpha = rho_new / h; 
0  before apply first correction to x 
0  before r = r - alpha*v 
0  before test stop criteria 
0 istlsolver::printOutput 
  0.5      4.79616e-25      1.51671e-16
0 === rate=2.30039e-32, T=0.0186813, TIT=0.0373625, IT=0.5
3  before alpha = rho_new / h; 
3  before apply first correction to x 
3  before r = r - alpha*v 
3  before test stop criteria 
3 istlsolver::printOutput 
  0.5      4.79616e-25      1.51671e-16
3 === rate=2.30039e-32, T=0.121, TIT=0.242001, IT=0.5
3 finished solveLinearSystemImpl_ 1
3 converged value: 1 7 1 let s go get convergedRemote
3 to  comm_.min(converged) 
6  before alpha = rho_new / h; 
6  before apply first correction to x 
6  before r = r - alpha*v 
6  before test stop criteria 
6 istlsolver::printOutput 
  0.5      4.79616e-25      1.51671e-16
6 === rate=2.30039e-32, T=0.120893, TIT=0.241785, IT=0.5
6 finished solveLinearSystemImpl_ 1
6 converged value: 1 7 1 let s go get convergedRemote
6 to  comm_.min(converged) 
4  before alpha = rho_new / h; 
4  before apply first correction to x 
4  before r = r - alpha*v 
4  before test stop criteria 
4 istlsolver::printOutput 
  0.5      4.79616e-25      1.51671e-16
4 === rate=2.30039e-32, T=0.120895, TIT=0.241789, IT=0.5
4 finished solveLinearSystemImpl_ 1
4 converged value: 1 7 1 let s go get convergedRemote
4 to  comm_.min(converged) 
5  before alpha = rho_new / h; 
5  before apply first correction to x 
5  before r = r - alpha*v 
5  before test stop criteria 
5 istlsolver::printOutput 
  0.5      4.79616e-25      1.51671e-16
5 === rate=2.30039e-32, T=0.120998, TIT=0.241997, IT=0.5
5 finished solveLinearSystemImpl_ 1
5 converged value: 1 7 1 let s go get convergedRemote
5 to  comm_.min(converged) 
3 did  comm_.min(converged) convergedRemote: 1
3 final convergedRemote: 1
3 Update: x^(k+1) = x^k - deltax^k 
3 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
3 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
3 Newton iteration  1 done3 , maximum relative shift = 1.251e-01
3 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
6 did  comm_.min(converged) convergedRemote: 1
6 final convergedRemote: 1
6 Update: x^(k+1) = x^k - deltax^k 
6 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
6 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
6 Newton iteration  1 done6 , maximum relative shift = 1.251e-01
6 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
5 did  comm_.min(converged) convergedRemote: 1
5 final convergedRemote: 1
5 Update: x^(k+1) = x^k - deltax^k 
5 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
5 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
5 Newton iteration  1 done5 , maximum relative shift = 1.251e-01
5 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
4 did  comm_.min(converged) convergedRemote: 1
4 final convergedRemote: 1
4 Update: x^(k+1) = x^k - deltax^k 
4 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
4 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
4 Newton iteration  1 done4 , maximum relative shift = 1.251e-01
4 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
2 did  comm_.min(converged) convergedRemote: 1
2 final convergedRemote: 1
2 Update: x^(k+1) = x^k - deltax^k 
2 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
2 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
2 Newton iteration  1 done2 , maximum relative shift = 1.251e-01
2 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
1 did  comm_.min(converged) convergedRemote: 1
1 final convergedRemote: 1
1 Update: x^(k+1) = x^k - deltax^k 
1 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
1 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
1 Newton iteration  1 done1 , maximum relative shift = 1.251e-01
1 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
0 finished solveLinearSystemImpl_ 1
0 converged value: 1 7 1 let s go get convergedRemote
0 to  comm_.min(converged) 
0 did  comm_.min(converged) convergedRemote: 1
0 final convergedRemote: 1
0 Update: x^(k+1) = x^k - deltax^k 
0 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
0 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
0 Newton iteration  1 done0 , maximum relative shift = 1.251e-01
0 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
2 Solve: M deltax^k = r 
2 Newton::solveLinearSystem : entering the try section 1
2 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
2 solveLinearSystemImpl_, before converged = ls.solve 
2 7 amgbackend::solve isParallel 1
1 Solve: M deltax^k = r 
1 Newton::solveLinearSystem : entering the try section 1
1 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
1 solveLinearSystemImpl_, before converged = ls.solve 
1 7 amgbackend::solve isParallel 1
6 Solve: M deltax^k = r 
6 Newton::solveLinearSystem : entering the try section 1
6 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
6 solveLinearSystemImpl_, before converged = ls.solve 
6 7 amgbackend::solve isParallel 1
3 Solve: M deltax^k = r 
3 Newton::solveLinearSystem : entering the try section 1
3 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
3 solveLinearSystemImpl_, before converged = ls.solve 
3 7 amgbackend::solve isParallel 1
0 Solve: M deltax^k = r 
0 Newton::solveLinearSystem : entering the try section 1
0 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
0 solveLinearSystemImpl_, before converged = ls.solve 
0 7 amgbackend::solve isParallel 1
5 Solve: M deltax^k = r 
5 Newton::solveLinearSystem : entering the try section 1
5 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
5 solveLinearSystemImpl_, before converged = ls.solve 
5 7 amgbackend::solve isParallel 1
4 Solve: M deltax^k = r 
4 Newton::solveLinearSystem : entering the try section 1
4 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
4 solveLinearSystemImpl_, before converged = ls.solve 
4 7 amgbackend::solve isParallel 1
0 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
0 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 7
5 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
5 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
3 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
3 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
2 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
2 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
1 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
1 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
6 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
6 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
4 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
4 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 7
3 7 before Dune::BiCGSTABSolver 
3 Dune::BiCGSTABSolver.apply(x, b, result_) 
3 BiCGSTABSolver::apply 
3 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
2 7 before Dune::BiCGSTABSolver 
2 Dune::BiCGSTABSolver.apply(x, b, result_) 
2 BiCGSTABSolver::apply 
1 7 before Dune::BiCGSTABSolver 
1 Dune::BiCGSTABSolver.apply(x, b, result_) 
1 BiCGSTABSolver::apply 
1 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 7
6 7 before Dune::BiCGSTABSolver 
6 Dune::BiCGSTABSolver.apply(x, b, result_) 
6 BiCGSTABSolver::apply 
6 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
4 7 before Dune::BiCGSTABSolver 
4 Dune::BiCGSTABSolver.apply(x, b, result_) 
4 BiCGSTABSolver::apply 
4 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
5 7 before Dune::BiCGSTABSolver 
5 Dune::BiCGSTABSolver.apply(x, b, result_) 
5 BiCGSTABSolver::apply 
5 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
2 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
Using a direct coarse solver (SuperLU)
Building hierarchy of 1 levels (inclusive coarse solver) took 0.111094 seconds.
0 7 before Dune::BiCGSTABSolver 
0 Dune::BiCGSTABSolver.apply(x, b, result_) 
0 BiCGSTABSolver::apply 
0 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
0 === BiCGSTABSolver
 Iter          Defect            Rate
1 === BiCGSTABSolver
 Iter          Defect            Rate
    0       1.0423e-10
1  (it = 0.5; it < _maxit; it+=.5) 0.5 250
1  before p = r 
1  before  y = W^-1 * p 
1  before _prec->apply(y,p); 0.5 250
1 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
3 === BiCGSTABSolver
 Iter          Defect            Rate
    0       1.0423e-10
3  (it = 0.5; it < _maxit; it+=.5) 0.5 250
3  before p = r 
3  before  y = W^-1 * p 
3  before _prec->apply(y,p); 0.5 250
3 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
    0       1.0423e-10
0  (it = 0.5; it < _maxit; it+=.5) 0.5 250
0  before p = r 
0  before  y = W^-1 * p 
0  before _prec->apply(y,p); 0.5 250
0 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 7
0 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
2 === BiCGSTABSolver
 Iter          Defect            Rate
    0       1.0423e-10
2  (it = 0.5; it < _maxit; it+=.5) 0.5 250
2  before p = r 
2  before  y = W^-1 * p 
2  before _prec->apply(y,p); 0.5 250
2 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
6 === BiCGSTABSolver
 Iter          Defect            Rate
    0       1.0423e-10
6  (it = 0.5; it < _maxit; it+=.5) 0.5 250
6  before p = r 
6  before  y = W^-1 * p 
6  before _prec->apply(y,p); 0.5 250
6 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
4 === BiCGSTABSolver
 Iter          Defect            Rate
    0       1.0423e-10
4  (it = 0.5; it < _maxit; it+=.5) 0.5 250
4  before p = r 
4  before  y = W^-1 * p 
4  before _prec->apply(y,p); 0.5 250
4 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
5 === BiCGSTABSolver
 Iter          Defect            Rate
    0       1.0423e-10
5  (it = 0.5; it < _maxit; it+=.5) 0.5 250
5  before p = r 
5  before  y = W^-1 * p 
5  before _prec->apply(y,p); 0.5 250
5 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
0 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
2 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
2 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
6 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
6 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
6  before v = A * y 
6  before  alpha = rho_new / < rt, v > 
4 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
4 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
4  before v = A * y 
3 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
3 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
3  before v = A * y 
1 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
5 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
5 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
5  before v = A * y 
1 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
1  before v = A * y 
0 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
0  before v = A * y 
2  before v = A * y 
2  before  alpha = rho_new / < rt, v > 
4  before  alpha = rho_new / < rt, v > 
3  before  alpha = rho_new / < rt, v > 
5  before  alpha = rho_new / < rt, v > 
6  before alpha = rho_new / h; 
6  before apply first correction to x 
6  before r = r - alpha*v 
6  before test stop criteria 
6 istlsolver::printOutput 
  0.5      6.78195e-28      6.50671e-18
6 === rate=4.23373e-35, T=0.124604, TIT=0.249208, IT=0.5
6 finished solveLinearSystemImpl_ 1
6 converged value: 1 7 1 let s go get convergedRemote
6 to  comm_.min(converged) 
2  before alpha = rho_new / h; 
2  before apply first correction to x 
2  before r = r - alpha*v 
2  before test stop criteria 
2 istlsolver::printOutput 
  0.5      6.78195e-28      6.50671e-18
2 === rate=4.23373e-35, T=0.124597, TIT=0.249193, IT=0.5
2 finished solveLinearSystemImpl_ 1
2 converged value: 1 7 1 let s go get convergedRemote
2 to  comm_.min(converged) 
4  before alpha = rho_new / h; 
4  before apply first correction to x 
4  before r = r - alpha*v 
4  before test stop criteria 
4 istlsolver::printOutput 
  0.5      6.78195e-28      6.50671e-18
4 === rate=4.23373e-35, T=0.124594, TIT=0.249188, IT=0.5
4 finished solveLinearSystemImpl_ 1
4 converged value: 1 7 1 let s go get convergedRemote
4 to  comm_.min(converged) 
5  before alpha = rho_new / h; 
5  before apply first correction to x 
5  before r = r - alpha*v 
5  before test stop criteria 
5 istlsolver::printOutput 
  0.5      6.78195e-28      6.50671e-18
5 === rate=4.23373e-35, T=0.124643, TIT=0.249286, IT=0.5
5 finished solveLinearSystemImpl_ 1
5 converged value: 1 7 1 let s go get convergedRemote
5 to  comm_.min(converged) 
3  before alpha = rho_new / h; 
3  before apply first correction to x 
3  before r = r - alpha*v 
3  before test stop criteria 
3 istlsolver::printOutput 
  0.5      6.78195e-28      6.50671e-18
3 === rate=4.23373e-35, T=0.124646, TIT=0.249292, IT=0.5
3 finished solveLinearSystemImpl_ 1
3 converged value: 1 7 1 let s go get convergedRemote
3 to  comm_.min(converged) 
1  before  alpha = rho_new / < rt, v > 
1  before alpha = rho_new / h; 
1  before apply first correction to x 
1  before r = r - alpha*v 
1  before test stop criteria 
1 istlsolver::printOutput 
  0.5      6.78195e-28      6.50671e-18
1 === rate=4.23373e-35, T=0.124646, TIT=0.249291, IT=0.5
1 finished solveLinearSystemImpl_ 1
1 converged value: 1 7 1 let s go get convergedRemote
1 to  comm_.min(converged) 
0  before  alpha = rho_new / < rt, v > 
0  before alpha = rho_new / h; 
0  before apply first correction to x 
0  before r = r - alpha*v 
0  before test stop criteria 
0 istlsolver::printOutput 
  0.5      6.78195e-28      6.50671e-18
0 === rate=4.23373e-35, T=0.0212795, TIT=0.042559, IT=0.5
4 did  comm_.min(converged) convergedRemote: 1
4 final convergedRemote: 1
4 Update: x^(k+1) = x^k - deltax^k 
4 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
2 did  comm_.min(converged) convergedRemote: 1
2 final convergedRemote: 1
2 Update: x^(k+1) = x^k - deltax^k 
2 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
0 finished solveLinearSystemImpl_ 1
0 converged value: 1 7 1 let s go get convergedRemote
0 to  comm_.min(converged) 
0 did  comm_.min(converged) convergedRemote: 1
0 final convergedRemote: 1
0 Update: x^(k+1) = x^k - deltax^k 
0 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
5 did  comm_.min(converged) convergedRemote: 1
5 final convergedRemote: 1
5 Update: x^(k+1) = x^k - deltax^k 
5 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
3 did  comm_.min(converged) convergedRemote: 1
3 final convergedRemote: 1
3 Update: x^(k+1) = x^k - deltax^k 
3 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
1 did  comm_.min(converged) convergedRemote: 1
1 final convergedRemote: 1
1 Update: x^(k+1) = x^k - deltax^k 
1 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
1 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
1 Newton iteration  2 done1 , maximum relative shift = 2.900e-02
1 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
0 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
0 Newton iteration  2 done0 , maximum relative shift = 2.900e-02
0 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
5 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
5 Newton iteration  2 done5 , maximum relative shift = 2.900e-02
5 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
3 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
3 Newton iteration  2 done3 , maximum relative shift = 2.900e-02
3 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
2 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
2 Newton iteration  2 done2 , maximum relative shift = 2.900e-02
2 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
6 did  comm_.min(converged) convergedRemote: 1
6 final convergedRemote: 1
6 Update: x^(k+1) = x^k - deltax^k 
6 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
6 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
6 Newton iteration  2 done6 , maximum relative shift = 2.900e-02
6 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
4 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
4 Newton iteration  2 done4 , maximum relative shift = 2.900e-02
4 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
1 Solve: M deltax^k = r 
1 Newton::solveLinearSystem : entering the try section 2
1 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
1 solveLinearSystemImpl_, before converged = ls.solve 
1 7 amgbackend::solve isParallel 1
0 Solve: M deltax^k = r 
0 Newton::solveLinearSystem : entering the try section 2
0 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
0 solveLinearSystemImpl_, before converged = ls.solve 
0 7 amgbackend::solve isParallel 1
5 Solve: M deltax^k = r 
5 Newton::solveLinearSystem : entering the try section 2
5 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
5 solveLinearSystemImpl_, before converged = ls.solve 
5 7 amgbackend::solve isParallel 1
3 Solve: M deltax^k = r 
3 Newton::solveLinearSystem : entering the try section 2
3 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
3 solveLinearSystemImpl_, before converged = ls.solve 
3 7 amgbackend::solve isParallel 1
2 Solve: M deltax^k = r 
2 Newton::solveLinearSystem : entering the try section 2
2 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
2 solveLinearSystemImpl_, before converged = ls.solve 
2 7 amgbackend::solve isParallel 1
6 Solve: M deltax^k = r 
6 Newton::solveLinearSystem : entering the try section 2
6 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
6 solveLinearSystemImpl_, before converged = ls.solve 
6 7 amgbackend::solve isParallel 1
4 Solve: M deltax^k = r 
4 Newton::solveLinearSystem : entering the try section 2
4 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
4 solveLinearSystemImpl_, before converged = ls.solve 
4 7 amgbackend::solve isParallel 1
0 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
0 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 7
5 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
5 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
3 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
3 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
2 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
2 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
4 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
4 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
6 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
6 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
1 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
1 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 7
2 7 before Dune::BiCGSTABSolver 
2 Dune::BiCGSTABSolver.apply(x, b, result_) 
2 BiCGSTABSolver::apply 
2 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
1 7 before Dune::BiCGSTABSolver 
1 Dune::BiCGSTABSolver.apply(x, b, result_) 
1 BiCGSTABSolver::apply 
1 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
5 7 before Dune::BiCGSTABSolver 
5 Dune::BiCGSTABSolver.apply(x, b, result_) 
5 BiCGSTABSolver::apply 
5 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
3 7 before Dune::BiCGSTABSolver 
3 Dune::BiCGSTABSolver.apply(x, b, result_) 
3 BiCGSTABSolver::apply 
3 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 7
4 7 before Dune::BiCGSTABSolver 
4 Dune::BiCGSTABSolver.apply(x, b, result_) 
4 BiCGSTABSolver::apply 
4 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
6 7 before Dune::BiCGSTABSolver 
6 Dune::BiCGSTABSolver.apply(x, b, result_) 
6 BiCGSTABSolver::apply 
6 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
Using a direct coarse solver (SuperLU)
Building hierarchy of 1 levels (inclusive coarse solver) took 0.11098 seconds.
0 7 before Dune::BiCGSTABSolver 
0 Dune::BiCGSTABSolver.apply(x, b, result_) 
0 BiCGSTABSolver::apply 
0 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
0 === BiCGSTABSolver
 Iter          Defect            Rate
    0      5.71209e-12
0  (it = 0.5; it < _maxit; it+=.5) 0.5 250
0  before p = r 
0  before  y = W^-1 * p 
0  before _prec->apply(y,p); 0.5 250
0 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 7
0 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
5 === BiCGSTABSolver
 Iter          Defect            Rate
    0      5.71209e-12
5  (it = 0.5; it < _maxit; it+=.5) 0.5 250
5  before p = r 
5  before  y = W^-1 * p 
5  before _prec->apply(y,p); 0.5 250
5 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
6 === BiCGSTABSolver
 Iter          Defect            Rate
    0      5.71209e-12
6  (it = 0.5; it < _maxit; it+=.5) 0.5 250
6  before p = r 
6  before  y = W^-1 * p 
6  before _prec->apply(y,p); 0.5 250
6 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
4 === BiCGSTABSolver
 Iter          Defect            Rate
    0      5.71209e-12
4  (it = 0.5; it < _maxit; it+=.5) 0.5 250
4  before p = r 
4  before  y = W^-1 * p 
4  before _prec->apply(y,p); 0.5 250
4 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
2 === BiCGSTABSolver
 Iter          Defect            Rate
    0      5.71209e-12
2  (it = 0.5; it < _maxit; it+=.5) 0.5 250
2  before p = r 
2  before  y = W^-1 * p 
2  before _prec->apply(y,p); 0.5 250
2 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
3 === BiCGSTABSolver
 Iter          Defect            Rate
    0      5.71209e-12
3  (it = 0.5; it < _maxit; it+=.5) 0.5 250
3  before p = r 
3  before  y = W^-1 * p 
3  before _prec->apply(y,p); 0.5 250
3 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
1 === BiCGSTABSolver
 Iter          Defect            Rate
    0      5.71209e-12
1  (it = 0.5; it < _maxit; it+=.5) 0.5 250
1  before p = r 
1  before  y = W^-1 * p 
1  before _prec->apply(y,p); 0.5 250
1 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
0 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
5 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
5 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
5  before v = A * y 
6 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
6 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
6  before v = A * y 
4 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
4 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
4  before v = A * y 
2 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
2 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
3 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
3 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
3  before v = A * y 
1 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
1 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
3  before  alpha = rho_new / < rt, v > 
0 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
5  before  alpha = rho_new / < rt, v > 
6  before  alpha = rho_new / < rt, v > 
4  before  alpha = rho_new / < rt, v > 
2  before v = A * y 
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
1  before v = A * y 
2  before  alpha = rho_new / < rt, v > 
2  before alpha = rho_new / h; 
2  before apply first correction to x 
2  before r = r - alpha*v 
2  before test stop criteria 
2 istlsolver::printOutput 
  0.5      4.60552e-28      8.06275e-17
2 === rate=6.50079e-33, T=0.121434, TIT=0.242869, IT=0.5
2 finished solveLinearSystemImpl_ 1
2 converged value: 1 7 1 let s go get convergedRemote
2 to  comm_.min(converged) 
3  before alpha = rho_new / h; 
3  before apply first correction to x 
3  before r = r - alpha*v 
3  before test stop criteria 
3 istlsolver::printOutput 
  0.5      4.60552e-28      8.06275e-17
3 === rate=6.50079e-33, T=0.121509, TIT=0.243018, IT=0.5
3 finished solveLinearSystemImpl_ 1
3 converged value: 1 7 1 let s go get convergedRemote
3 to  comm_.min(converged) 
0  before v = A * y 
0  before  alpha = rho_new / < rt, v > 
0  before alpha = rho_new / h; 
0  before apply first correction to x 
0  before r = r - alpha*v 
0  before test stop criteria 
0 istlsolver::printOutput 
  0.5      4.60552e-28      8.06275e-17
0 === rate=6.50079e-33, T=0.0184648, TIT=0.0369296, IT=0.5
5  before alpha = rho_new / h; 
5  before apply first correction to x 
5  before r = r - alpha*v 
5  before test stop criteria 
5 istlsolver::printOutput 
  0.5      4.60552e-28      8.06275e-17
5 === rate=6.50079e-33, T=0.121511, TIT=0.243023, IT=0.5
5 finished solveLinearSystemImpl_ 1
5 converged value: 1 7 1 let s go get convergedRemote
5 to  comm_.min(converged) 
6  before alpha = rho_new / h; 
6  before apply first correction to x 
6  before r = r - alpha*v 
6  before test stop criteria 
6 istlsolver::printOutput 
  0.5      4.60552e-28      8.06275e-17
6 === rate=6.50079e-33, T=0.121567, TIT=0.243134, IT=0.5
6 finished solveLinearSystemImpl_ 1
6 converged value: 1 7 1 let s go get convergedRemote
6 to  comm_.min(converged) 
4  before alpha = rho_new / h; 
4  before apply first correction to x 
4  before r = r - alpha*v 
4  before test stop criteria 
4 istlsolver::printOutput 
  0.5      4.60552e-28      8.06275e-17
4 === rate=6.50079e-33, T=0.121567, TIT=0.243134, IT=0.5
4 finished solveLinearSystemImpl_ 1
4 converged value: 1 7 1 let s go get convergedRemote
4 to  comm_.min(converged) 
1  before  alpha = rho_new / < rt, v > 
1  before alpha = rho_new / h; 
1  before apply first correction to x 
1  before r = r - alpha*v 
1  before test stop criteria 
1 istlsolver::printOutput 
  0.5      4.60552e-28      8.06275e-17
1 === rate=6.50079e-33, T=0.121512, TIT=0.243023, IT=0.5
1 finished solveLinearSystemImpl_ 1
1 converged value: 1 7 1 let s go get convergedRemote
1 to  comm_.min(converged) 
5 did  comm_.min(converged) convergedRemote: 1
5 final convergedRemote: 1
5 Update: x^(k+1) = x^k - deltax^k 
5 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
5 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
5 Newton iteration  3 done5 , maximum relative shift = 1.267e-03
5 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
6 did  comm_.min(converged) convergedRemote: 1
6 final convergedRemote: 1
6 Update: x^(k+1) = x^k - deltax^k 
6 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
6 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
1 did  comm_.min(converged) convergedRemote: 1
1 final convergedRemote: 1
1 Update: x^(k+1) = x^k - deltax^k 
1 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
1 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
1 Newton iteration  3 done1 , maximum relative shift = 1.267e-03
1 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
4 did  comm_.min(converged) convergedRemote: 1
4 final convergedRemote: 1
4 Update: x^(k+1) = x^k - deltax^k 
4 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
4 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
2 did  comm_.min(converged) convergedRemote: 1
2 final convergedRemote: 1
2 Update: x^(k+1) = x^k - deltax^k 
2 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
2 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
3 did  comm_.min(converged) convergedRemote: 1
3 final convergedRemote: 1
3 Update: x^(k+1) = x^k - deltax^k 
3 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
3 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
3 Newton iteration  3 done3 , maximum relative shift = 1.267e-03
3 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
0 finished solveLinearSystemImpl_ 1
0 converged value: 1 7 1 let s go get convergedRemote
0 to  comm_.min(converged) 
0 did  comm_.min(converged) convergedRemote: 1
0 final convergedRemote: 1
0 Update: x^(k+1) = x^k - deltax^k 
0 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
0 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
0 Newton iteration  3 done0 , maximum relative shift = 1.267e-03
0 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
4 Newton iteration  3 done4 , maximum relative shift = 1.267e-03
4 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
2 Newton iteration  3 done2 , maximum relative shift = 1.267e-03
2 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
6 Newton iteration  3 done6 , maximum relative shift = 1.267e-03
6 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
6 Solve: M deltax^k = r 
6 Newton::solveLinearSystem : entering the try section 3
6 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
6 solveLinearSystemImpl_, before converged = ls.solve 
6 7 amgbackend::solve isParallel 1
1 Solve: M deltax^k = r 
1 Newton::solveLinearSystem : entering the try section 3
1 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
1 solveLinearSystemImpl_, before converged = ls.solve 
1 7 amgbackend::solve isParallel 1
0 Solve: M deltax^k = r 
0 Newton::solveLinearSystem : entering the try section 3
0 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
0 solveLinearSystemImpl_, before converged = ls.solve 
0 7 amgbackend::solve isParallel 1
4 Solve: M deltax^k = r 
4 Newton::solveLinearSystem : entering the try section 3
4 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
4 solveLinearSystemImpl_, before converged = ls.solve 
4 7 amgbackend::solve isParallel 1
3 Solve: M deltax^k = r 
3 Newton::solveLinearSystem : entering the try section 3
3 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
3 solveLinearSystemImpl_, before converged = ls.solve 
3 7 amgbackend::solve isParallel 1
2 Solve: M deltax^k = r 
2 Newton::solveLinearSystem : entering the try section 3
2 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
2 solveLinearSystemImpl_, before converged = ls.solve 
2 7 amgbackend::solve isParallel 1
5 Solve: M deltax^k = r 
5 Newton::solveLinearSystem : entering the try section 3
5 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
5 solveLinearSystemImpl_, before converged = ls.solve 
5 7 amgbackend::solve isParallel 1
1 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
1 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
0 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
0 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 7
4 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
4 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
3 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
3 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
2 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
2 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
5 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
5 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
6 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
6 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 7
5 7 before Dune::BiCGSTABSolver 
5 Dune::BiCGSTABSolver.apply(x, b, result_) 
5 BiCGSTABSolver::apply 
5 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
2 7 before Dune::BiCGSTABSolver 
2 Dune::BiCGSTABSolver.apply(x, b, result_) 
2 BiCGSTABSolver::apply 
2 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
1 7 before Dune::BiCGSTABSolver 
1 Dune::BiCGSTABSolver.apply(x, b, result_) 
1 BiCGSTABSolver::apply 
1 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
6 7 before Dune::BiCGSTABSolver 
6 Dune::BiCGSTABSolver.apply(x, b, result_) 
6 BiCGSTABSolver::apply 
6 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
4 7 before Dune::BiCGSTABSolver 
4 Dune::BiCGSTABSolver.apply(x, b, result_) 
4 BiCGSTABSolver::apply 
4 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
3 7 before Dune::BiCGSTABSolver 
3 Dune::BiCGSTABSolver.apply(x, b, result_) 
3 BiCGSTABSolver::apply 
3 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 7
Using a direct coarse solver (SuperLU)
Building hierarchy of 1 levels (inclusive coarse solver) took 0.111475 seconds.
0 7 before Dune::BiCGSTABSolver 
0 Dune::BiCGSTABSolver.apply(x, b, result_) 
0 BiCGSTABSolver::apply 
0 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
0 === BiCGSTABSolver
 Iter          Defect            Rate
    0      9.80043e-13
0  (it = 0.5; it < _maxit; it+=.5) 0.5 250
0  before p = r 
0  before  y = W^-1 * p 
0  before _prec->apply(y,p); 0.5 250
0 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
1 === BiCGSTABSolver
 Iter          Defect            Rate
    0      9.80043e-13
1  (it = 0.5; it < _maxit; it+=.5) 0.5 250
1  before p = r 
1  before  y = W^-1 * p 
1  before _prec->apply(y,p); 0.5 250
1 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
6 === BiCGSTABSolver
 Iter          Defect            Rate
    0      9.80043e-13
6  (it = 0.5; it < _maxit; it+=.5) 0.5 250
6  before p = r 
6  before  y = W^-1 * p 
6  before _prec->apply(y,p); 0.5 250
6 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
4 === BiCGSTABSolver
 Iter          Defect            Rate
    0      9.80043e-13
4  (it = 0.5; it < _maxit; it+=.5) 0.5 250
4  before p = r 
4  before  y = W^-1 * p 
4  before _prec->apply(y,p); 0.5 250
4 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 7
0 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
3 === BiCGSTABSolver
 Iter          Defect            Rate
    0      9.80043e-13
3  (it = 0.5; it < _maxit; it+=.5) 0.5 250
3  before p = r 
3  before  y = W^-1 * p 
3  before _prec->apply(y,p); 0.5 250
3 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
5 === BiCGSTABSolver
 Iter          Defect            Rate
    0      9.80043e-13
5  (it = 0.5; it < _maxit; it+=.5) 0.5 250
5  before p = r 
5  before  y = W^-1 * p 
5  before _prec->apply(y,p); 0.5 250
5 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
2 === BiCGSTABSolver
 Iter          Defect            Rate
    0      9.80043e-13
2  (it = 0.5; it < _maxit; it+=.5) 0.5 250
2  before p = r 
2  before  y = W^-1 * p 
2  before _prec->apply(y,p); 0.5 250
2 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
0 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
3 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
3 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
3  before v = A * y 
5 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
5 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
5  before v = A * y 
4 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
4 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
4  before v = A * y 
1 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
6 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
6 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
6  before v = A * y 
2 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
2 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
4  before  alpha = rho_new / < rt, v > 
1 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
2  before v = A * y 
2  before  alpha = rho_new / < rt, v > 
6  before  alpha = rho_new / < rt, v > 
0 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
0  before v = A * y 
3  before  alpha = rho_new / < rt, v > 
5  before  alpha = rho_new / < rt, v > 
0  before  alpha = rho_new / < rt, v > 
0  before alpha = rho_new / h; 
0  before apply first correction to x 
0  before r = r - alpha*v 
0  before test stop criteria 
0 istlsolver::printOutput 
  0.5      7.02299e-32        7.166e-20
0 === rate=5.13516e-39, T=0.0188196, TIT=0.0376392, IT=0.5
3  before alpha = rho_new / h; 
3  before apply first correction to x 
3  before r = r - alpha*v 
3  before test stop criteria 
3 istlsolver::printOutput 
  0.5      7.02299e-32        7.166e-20
3 === rate=5.13516e-39, T=0.122634, TIT=0.245267, IT=0.5
3 finished solveLinearSystemImpl_ 1
3 converged value: 1 7 1 let s go get convergedRemote
3 to  comm_.min(converged) 
1  before v = A * y 
1  before  alpha = rho_new / < rt, v > 
1  before alpha = rho_new / h; 
1  before apply first correction to x 
1  before r = r - alpha*v 
1  before test stop criteria 
1 istlsolver::printOutput 
  0.5      7.02299e-32        7.166e-20
1 === rate=5.13516e-39, T=0.122489, TIT=0.244978, IT=0.5
1 finished solveLinearSystemImpl_ 1
1 converged value: 1 7 1 let s go get convergedRemote
1 to  comm_.min(converged) 
4  before alpha = rho_new / h; 
4  before apply first correction to x 
4  before r = r - alpha*v 
4  before test stop criteria 
4 istlsolver::printOutput 
  0.5      7.02299e-32        7.166e-20
4 === rate=5.13516e-39, T=0.122419, TIT=0.244837, IT=0.5
4 finished solveLinearSystemImpl_ 1
4 converged value: 1 7 1 let s go get convergedRemote
4 to  comm_.min(converged) 
5  before alpha = rho_new / h; 
5  before apply first correction to x 
5  before r = r - alpha*v 
5  before test stop criteria 
5 istlsolver::printOutput 
  0.5      7.02299e-32        7.166e-20
5 === rate=5.13516e-39, T=0.122632, TIT=0.245265, IT=0.5
5 finished solveLinearSystemImpl_ 1
5 converged value: 1 7 1 let s go get convergedRemote
5 to  comm_.min(converged) 
2  before alpha = rho_new / h; 
2  before apply first correction to x 
2  before r = r - alpha*v 
2  before test stop criteria 
2 istlsolver::printOutput 
  0.5      7.02299e-32        7.166e-20
2 === rate=5.13516e-39, T=0.122418, TIT=0.244836, IT=0.5
2 finished solveLinearSystemImpl_ 1
2 converged value: 1 7 1 let s go get convergedRemote
2 to  comm_.min(converged) 
6  before alpha = rho_new / h; 
6  before apply first correction to x 
6  before r = r - alpha*v 
6  before test stop criteria 
6 istlsolver::printOutput 
  0.5      7.02299e-32        7.166e-20
6 === rate=5.13516e-39, T=0.122422, TIT=0.244844, IT=0.5
6 finished solveLinearSystemImpl_ 1
6 converged value: 1 7 1 let s go get convergedRemote
6 to  comm_.min(converged) 
5 did  comm_.min(converged) convergedRemote: 1
5 final convergedRemote: 1
5 Update: x^(k+1) = x^k - deltax^k 
5 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
5 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
5 Newton iteration  4 done5 , maximum relative shift = 2.716e-04
5 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
2 did  comm_.min(converged) convergedRemote: 1
2 final convergedRemote: 1
2 Update: x^(k+1) = x^k - deltax^k 
2 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
2 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
2 Newton iteration  4 done2 , maximum relative shift = 2.716e-04
2 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
4 did  comm_.min(converged) convergedRemote: 1
4 final convergedRemote: 1
4 Update: x^(k+1) = x^k - deltax^k 
4 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
4 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
4 Newton iteration  4 done4 , maximum relative shift = 2.716e-04
4 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
0 finished solveLinearSystemImpl_ 1
0 converged value: 1 7 1 let s go get convergedRemote
0 to  comm_.min(converged) 
0 did  comm_.min(converged) convergedRemote: 1
0 final convergedRemote: 1
0 Update: x^(k+1) = x^k - deltax^k 
0 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
0 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
0 Newton iteration  4 done0 , maximum relative shift = 2.716e-04
0 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
3 did  comm_.min(converged) convergedRemote: 1
3 final convergedRemote: 1
3 Update: x^(k+1) = x^k - deltax^k 
3 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
3 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
3 Newton iteration  4 done3 , maximum relative shift = 2.716e-04
3 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
1 did  comm_.min(converged) convergedRemote: 1
1 final convergedRemote: 1
1 Update: x^(k+1) = x^k - deltax^k 
1 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
1 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
1 Newton iteration  4 done1 , maximum relative shift = 2.716e-04
1 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
6 did  comm_.min(converged) convergedRemote: 1
6 final convergedRemote: 1
6 Update: x^(k+1) = x^k - deltax^k 
6 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
6 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
6 Newton iteration  4 done6 , maximum relative shift = 2.716e-04
6 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
3 Solve: M deltax^k = r 
3 Newton::solveLinearSystem : entering the try section 4
3 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
3 solveLinearSystemImpl_, before converged = ls.solve 
3 7 amgbackend::solve isParallel 1
6 Solve: M deltax^k = r 
6 Newton::solveLinearSystem : entering the try section 4
6 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
6 solveLinearSystemImpl_, before converged = ls.solve 
6 7 amgbackend::solve isParallel 1
1 Solve: M deltax^k = r 
1 Newton::solveLinearSystem : entering the try section 4
1 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
1 solveLinearSystemImpl_, before converged = ls.solve 
1 7 amgbackend::solve isParallel 1
5 Solve: M deltax^k = r 
5 Newton::solveLinearSystem : entering the try section 4
5 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
5 solveLinearSystemImpl_, before converged = ls.solve 
5 7 amgbackend::solve isParallel 1
2 Solve: M deltax^k = r 
2 Newton::solveLinearSystem : entering the try section 4
2 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
2 solveLinearSystemImpl_, before converged = ls.solve 
2 7 amgbackend::solve isParallel 1
4 Solve: M deltax^k = r 
4 Newton::solveLinearSystem : entering the try section 4
4 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
4 solveLinearSystemImpl_, before converged = ls.solve 
4 7 amgbackend::solve isParallel 1
0 Solve: M deltax^k = r 
0 Newton::solveLinearSystem : entering the try section 4
0 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
0 solveLinearSystemImpl_, before converged = ls.solve 
0 7 amgbackend::solve isParallel 1
0 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
0 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 7
6 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
6 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
1 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
1 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
5 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
5 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
2 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
2 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
4 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
4 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
3 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
3 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 7
3 7 before Dune::BiCGSTABSolver 
3 Dune::BiCGSTABSolver.apply(x, b, result_) 
3 BiCGSTABSolver::apply 
3 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
6 7 before Dune::BiCGSTABSolver 
6 Dune::BiCGSTABSolver.apply(x, b, result_) 
6 BiCGSTABSolver::apply 
6 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
1 7 before Dune::BiCGSTABSolver 
1 Dune::BiCGSTABSolver.apply(x, b, result_) 
1 BiCGSTABSolver::apply 
1 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
5 7 before Dune::BiCGSTABSolver 
5 Dune::BiCGSTABSolver.apply(x, b, result_) 
5 BiCGSTABSolver::apply 
5 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
2 7 before Dune::BiCGSTABSolver 
2 Dune::BiCGSTABSolver.apply(x, b, result_) 
2 BiCGSTABSolver::apply 
2 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
4 7 before Dune::BiCGSTABSolver 
4 Dune::BiCGSTABSolver.apply(x, b, result_) 
4 BiCGSTABSolver::apply 
4 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
Using a direct coarse solver (SuperLU)
Building hierarchy of 1 levels (inclusive coarse solver) took 0.109997 seconds.
0 7 before Dune::BiCGSTABSolver 
0 Dune::BiCGSTABSolver.apply(x, b, result_) 
0 BiCGSTABSolver::apply 
0 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
6 === BiCGSTABSolver
 Iter          Defect            Rate
    0      3.54639e-14
6  (it = 0.5; it < _maxit; it+=.5) 0.5 250
6  before p = r 
6  before  y = W^-1 * p 
6  before _prec->apply(y,p); 0.5 250
6 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
3 === BiCGSTABSolver
 Iter          Defect            Rate
    0      3.54639e-14
3  (it = 0.5; it < _maxit; it+=.5) 0.5 250
3  before p = r 
3  before  y = W^-1 * p 
3  before _prec->apply(y,p); 0.5 250
3 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
1 === BiCGSTABSolver
 Iter          Defect            Rate
    0      3.54639e-14
1  (it = 0.5; it < _maxit; it+=.5) 0.5 250
1  before p = r 
1  before  y = W^-1 * p 
1  before _prec->apply(y,p); 0.5 250
1 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
5 === BiCGSTABSolver
 Iter          Defect            Rate
    0      3.54639e-14
5  (it = 0.5; it < _maxit; it+=.5) 0.5 250
5  before p = r 
5  before  y = W^-1 * p 
5  before _prec->apply(y,p); 0.5 250
5 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
0 === BiCGSTABSolver
 Iter          Defect            Rate
    0      3.54639e-14
0  (it = 0.5; it < _maxit; it+=.5) 0.5 250
0  before p = r 
0  before  y = W^-1 * p 
0  before _prec->apply(y,p); 0.5 250
0 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 7
0 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
4 === BiCGSTABSolver
 Iter          Defect            Rate
    0      3.54639e-14
4  (it = 0.5; it < _maxit; it+=.5) 0.5 250
4  before p = r 
4  before  y = W^-1 * p 
4  before _prec->apply(y,p); 0.5 250
4 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
2 === BiCGSTABSolver
 Iter          Defect            Rate
    0      3.54639e-14
2  (it = 0.5; it < _maxit; it+=.5) 0.5 250
2  before p = r 
2  before  y = W^-1 * p 
2  before _prec->apply(y,p); 0.5 250
2 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
3 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
3 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
3  before v = A * y 
1 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
2 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
2 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
4 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
4 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
4  before v = A * y 
5 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
5 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
5  before v = A * y 
6 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
6 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
6  before v = A * y 
4  before  alpha = rho_new / < rt, v > 
5  before  alpha = rho_new / < rt, v > 
1 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
3  before  alpha = rho_new / < rt, v > 
6  before  alpha = rho_new / < rt, v > 
0 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
0 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
4  before alpha = rho_new / h; 
4  before apply first correction to x 
4  before r = r - alpha*v 
4  before test stop criteria 
4 istlsolver::printOutput 
  0.5      5.50097e-30      1.55115e-16
4 === rate=2.40606e-32, T=0.121137, TIT=0.242273, IT=0.5
4 finished solveLinearSystemImpl_ 1
4 converged value: 1 7 1 let s go get convergedRemote
4 to  comm_.min(converged) 
5  before alpha = rho_new / h; 
5  before apply first correction to x 
5  before r = r - alpha*v 
5  before test stop criteria 
5 istlsolver::printOutput 
  0.5      5.50097e-30      1.55115e-16
5 === rate=2.40606e-32, T=0.121246, TIT=0.242492, IT=0.5
5 finished solveLinearSystemImpl_ 1
5 converged value: 1 7 1 let s go get convergedRemote
5 to  comm_.min(converged) 
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
1  before v = A * y 
1  before  alpha = rho_new / < rt, v > 
1  before alpha = rho_new / h; 
1  before apply first correction to x 
1  before r = r - alpha*v 
1  before test stop criteria 
1 istlsolver::printOutput 
  0.5      5.50097e-30      1.55115e-16
1 === rate=2.40606e-32, T=0.121251, TIT=0.242503, IT=0.5
1 finished solveLinearSystemImpl_ 1
1 converged value: 1 7 1 let s go get convergedRemote
1 to  comm_.min(converged) 
0  before v = A * y 
0  before  alpha = rho_new / < rt, v > 
0  before alpha = rho_new / h; 
0  before apply first correction to x 
0  before r = r - alpha*v 
0  before test stop criteria 
0 istlsolver::printOutput 
  0.5      5.50097e-30      1.55115e-16
0 === rate=2.40606e-32, T=0.0185988, TIT=0.0371976, IT=0.5
2  before v = A * y 
2  before  alpha = rho_new / < rt, v > 
2  before alpha = rho_new / h; 
2  before apply first correction to x 
2  before r = r - alpha*v 
2  before test stop criteria 
2 istlsolver::printOutput 
  0.5      5.50097e-30      1.55115e-16
2 === rate=2.40606e-32, T=0.121137, TIT=0.242275, IT=0.5
2 finished solveLinearSystemImpl_ 1
2 converged value: 1 7 1 let s go get convergedRemote
2 to  comm_.min(converged) 
6  before alpha = rho_new / h; 
6  before apply first correction to x 
6  before r = r - alpha*v 
6  before test stop criteria 
6 istlsolver::printOutput 
  0.5      5.50097e-30      1.55115e-16
6 === rate=2.40606e-32, T=0.121137, TIT=0.242273, IT=0.5
6 finished solveLinearSystemImpl_ 1
6 converged value: 1 7 1 let s go get convergedRemote
6 to  comm_.min(converged) 
3  before alpha = rho_new / h; 
3  before apply first correction to x 
3  before r = r - alpha*v 
3  before test stop criteria 
3 istlsolver::printOutput 
  0.5      5.50097e-30      1.55115e-16
3 === rate=2.40606e-32, T=0.121248, TIT=0.242496, IT=0.5
3 finished solveLinearSystemImpl_ 1
3 converged value: 1 7 1 let s go get convergedRemote
3 to  comm_.min(converged) 
2 did  comm_.min(converged) convergedRemote: 1
2 final convergedRemote: 1
2 Update: x^(k+1) = x^k - deltax^k 
2 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
2 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
2 Newton iteration  5 done2 , maximum relative shift = 7.865e-06
2 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
6 did  comm_.min(converged) convergedRemote: 1
6 final convergedRemote: 1
6 Update: x^(k+1) = x^k - deltax^k 
6 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
6 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
6 Newton iteration  5 done6 , maximum relative shift = 7.865e-06
6 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
1 did  comm_.min(converged) convergedRemote: 1
1 final convergedRemote: 1
1 Update: x^(k+1) = x^k - deltax^k 
1 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
1 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
1 Newton iteration  5 done1 , maximum relative shift = 7.865e-06
1 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
4 did  comm_.min(converged) convergedRemote: 1
4 final convergedRemote: 1
4 Update: x^(k+1) = x^k - deltax^k 
4 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
4 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
4 Newton iteration  5 done4 , maximum relative shift = 7.865e-06
4 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
5 did  comm_.min(converged) convergedRemote: 1
5 final convergedRemote: 1
5 Update: x^(k+1) = x^k - deltax^k 
5 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
5 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
5 Newton iteration  5 done5 , maximum relative shift = 7.865e-06
5 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
3 did  comm_.min(converged) convergedRemote: 1
3 final convergedRemote: 1
3 Update: x^(k+1) = x^k - deltax^k 
3 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
3 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
3 Newton iteration  5 done3 , maximum relative shift = 7.865e-06
3 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
0 finished solveLinearSystemImpl_ 1
0 converged value: 1 7 1 let s go get convergedRemote
0 to  comm_.min(converged) 
0 did  comm_.min(converged) convergedRemote: 1
0 final convergedRemote: 1
0 Update: x^(k+1) = x^k - deltax^k 
0 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
0 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
0 Newton iteration  5 done0 , maximum relative shift = 7.865e-06
0 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
1 Solve: M deltax^k = r 
1 Newton::solveLinearSystem : entering the try section 5
1 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
1 solveLinearSystemImpl_, before converged = ls.solve 
1 7 amgbackend::solve isParallel 1
4 Solve: M deltax^k = r 
4 Newton::solveLinearSystem : entering the try section 5
4 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
4 solveLinearSystemImpl_, before converged = ls.solve 
4 7 amgbackend::solve isParallel 1
3 Solve: M deltax^k = r 
3 Newton::solveLinearSystem : entering the try section 5
3 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
3 solveLinearSystemImpl_, before converged = ls.solve 
3 7 amgbackend::solve isParallel 1
5 Solve: M deltax^k = r 
5 Newton::solveLinearSystem : entering the try section 5
5 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
5 solveLinearSystemImpl_, before converged = ls.solve 
5 7 amgbackend::solve isParallel 1
2 Solve: M deltax^k = r 
2 Newton::solveLinearSystem : entering the try section 5
2 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
2 solveLinearSystemImpl_, before converged = ls.solve 
2 7 amgbackend::solve isParallel 1
6 Solve: M deltax^k = r 
6 Newton::solveLinearSystem : entering the try section 5
6 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
6 solveLinearSystemImpl_, before converged = ls.solve 
6 7 amgbackend::solve isParallel 1
6 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
6 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
4 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
4 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
3 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
3 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
5 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
5 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
2 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
2 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
0 Solve: M deltax^k = r 
0 Newton::solveLinearSystem : entering the try section 5
0 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
0 solveLinearSystemImpl_, before converged = ls.solve 
0 7 amgbackend::solve isParallel 1
0 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
0 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 7
1 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
1 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 7
6 7 before Dune::BiCGSTABSolver 
6 Dune::BiCGSTABSolver.apply(x, b, result_) 
6 BiCGSTABSolver::apply 
6 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
4 7 before Dune::BiCGSTABSolver 
4 Dune::BiCGSTABSolver.apply(x, b, result_) 
4 BiCGSTABSolver::apply 
4 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
3 7 before Dune::BiCGSTABSolver 
3 Dune::BiCGSTABSolver.apply(x, b, result_) 
3 BiCGSTABSolver::apply 
3 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
5 7 before Dune::BiCGSTABSolver 
5 Dune::BiCGSTABSolver.apply(x, b, result_) 
5 BiCGSTABSolver::apply 
5 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
2 7 before Dune::BiCGSTABSolver 
2 Dune::BiCGSTABSolver.apply(x, b, result_) 
2 BiCGSTABSolver::apply 
2 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
1 7 before Dune::BiCGSTABSolver 
1 Dune::BiCGSTABSolver.apply(x, b, result_) 
1 BiCGSTABSolver::apply 
1 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
Using a direct coarse solver (SuperLU)
Building hierarchy of 1 levels (inclusive coarse solver) took 0.11092 seconds.
0 7 before Dune::BiCGSTABSolver 
0 Dune::BiCGSTABSolver.apply(x, b, result_) 
0 BiCGSTABSolver::apply 
0 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
0 === BiCGSTABSolver
 Iter          Defect            Rate
    0      6.05556e-15
0  (it = 0.5; it < _maxit; it+=.5) 0.5 250
0  before p = r 
0  before  y = W^-1 * p 
0  before _prec->apply(y,p); 0.5 250
0 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 7
0 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
1 === BiCGSTABSolver
 Iter          Defect            Rate
    0      6.05556e-15
1  (it = 0.5; it < _maxit; it+=.5) 0.5 250
1  before p = r 
1  before  y = W^-1 * p 
1  before _prec->apply(y,p); 0.5 250
1 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
6 === BiCGSTABSolver
 Iter          Defect            Rate
    0      6.05556e-15
6  (it = 0.5; it < _maxit; it+=.5) 0.5 250
6  before p = r 
6  before  y = W^-1 * p 
6  before _prec->apply(y,p); 0.5 250
6 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
4 === BiCGSTABSolver
 Iter          Defect            Rate
    0      6.05556e-15
4  (it = 0.5; it < _maxit; it+=.5) 0.5 250
4  before p = r 
4  before  y = W^-1 * p 
4  before _prec->apply(y,p); 0.5 250
4 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
3 === BiCGSTABSolver
 Iter          Defect            Rate
    0      6.05556e-15
3  (it = 0.5; it < _maxit; it+=.5) 0.5 250
3  before p = r 
3  before  y = W^-1 * p 
3  before _prec->apply(y,p); 0.5 250
3 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
5 === BiCGSTABSolver
 Iter          Defect            Rate
    0      6.05556e-15
5  (it = 0.5; it < _maxit; it+=.5) 0.5 250
5  before p = r 
5  before  y = W^-1 * p 
5  before _prec->apply(y,p); 0.5 250
5 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
2 === BiCGSTABSolver
 Iter          Defect            Rate
    0      6.05556e-15
2  (it = 0.5; it < _maxit; it+=.5) 0.5 250
2  before p = r 
2  before  y = W^-1 * p 
2  before _prec->apply(y,p); 0.5 250
2 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
3 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
3 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
3  before v = A * y 
5 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
5 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
5  before v = A * y 
2 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
2 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
0 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
1 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
6 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
6 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
6  before v = A * y 
4 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
4 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
4  before v = A * y 
4  before  alpha = rho_new / < rt, v > 
0 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
1 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
6  before  alpha = rho_new / < rt, v > 
5  before  alpha = rho_new / < rt, v > 
3  before  alpha = rho_new / < rt, v > 
6  before alpha = rho_new / h; 
6  before apply first correction to x 
6  before r = r - alpha*v 
6  before test stop criteria 
6 istlsolver::printOutput 
  0.5       1.1656e-31      1.92484e-17
6 === rate=3.70502e-34, T=0.121621, TIT=0.243242, IT=0.5
6 finished solveLinearSystemImpl_ 1
6 converged value: 1 7 1 let s go get convergedRemote
6 to  comm_.min(converged) 
5  before alpha = rho_new / h; 
5  before apply first correction to x 
5  before r = r - alpha*v 
5  before test stop criteria 
5 istlsolver::printOutput 
  0.5       1.1656e-31      1.92484e-17
5 === rate=3.70502e-34, T=0.121576, TIT=0.243152, IT=0.5
5 finished solveLinearSystemImpl_ 1
5 converged value: 1 7 1 let s go get convergedRemote
5 to  comm_.min(converged) 
3  before alpha = rho_new / h; 
3  before apply first correction to x 
3  before r = r - alpha*v 
3  before test stop criteria 
3 istlsolver::printOutput 
  0.5       1.1656e-31      1.92484e-17
3 === rate=3.70502e-34, T=0.121577, TIT=0.243153, IT=0.5
3 finished solveLinearSystemImpl_ 1
3 converged value: 1 7 1 let s go get convergedRemote
3 to  comm_.min(converged) 
2  before v = A * y 
2  before  alpha = rho_new / < rt, v > 
2  before alpha = rho_new / h; 
2  before apply first correction to x 
2  before r = r - alpha*v 
2  before test stop criteria 
2 istlsolver::printOutput 
  0.5       1.1656e-31      1.92484e-17
2 === rate=3.70502e-34, T=0.121618, TIT=0.243237, IT=0.5
2 finished solveLinearSystemImpl_ 1
2 converged value: 1 7 1 let s go get convergedRemote
2 to  comm_.min(converged) 
4  before alpha = rho_new / h; 
4  before apply first correction to x 
4  before r = r - alpha*v 
4  before test stop criteria 
4 istlsolver::printOutput 
  0.5       1.1656e-31      1.92484e-17
4 === rate=3.70502e-34, T=0.121618, TIT=0.243236, IT=0.5
4 finished solveLinearSystemImpl_ 1
4 converged value: 1 7 1 let s go get convergedRemote
4 to  comm_.min(converged) 
0  before v = A * y 
0  before  alpha = rho_new / < rt, v > 
0  before alpha = rho_new / h; 
0  before apply first correction to x 
0  before r = r - alpha*v 
0  before test stop criteria 
0 istlsolver::printOutput 
  0.5       1.1656e-31      1.92484e-17
0 === rate=3.70502e-34, T=0.0185298, TIT=0.0370596, IT=0.5
1  before v = A * y 
1  before  alpha = rho_new / < rt, v > 
1  before alpha = rho_new / h; 
1  before apply first correction to x 
1  before r = r - alpha*v 
1  before test stop criteria 
1 istlsolver::printOutput 
  0.5       1.1656e-31      1.92484e-17
1 === rate=3.70502e-34, T=0.121577, TIT=0.243153, IT=0.5
1 finished solveLinearSystemImpl_ 1
1 converged value: 1 7 1 let s go get convergedRemote
1 to  comm_.min(converged) 
2 did  comm_.min(converged) convergedRemote: 1
2 final convergedRemote: 1
2 Update: x^(k+1) = x^k - deltax^k 
2 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
2 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
2 Newton iteration  6 done2 , maximum relative shift = 1.678e-06
2 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
4 did  comm_.min(converged) convergedRemote: 1
4 final convergedRemote: 1
4 Update: x^(k+1) = x^k - deltax^k 
4 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
4 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
4 Newton iteration  6 done4 , maximum relative shift = 1.678e-06
4 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
0 finished solveLinearSystemImpl_ 1
0 converged value: 1 7 1 let s go get convergedRemote
0 to  comm_.min(converged) 
0 did  comm_.min(converged) convergedRemote: 1
0 final convergedRemote: 1
0 Update: x^(k+1) = x^k - deltax^k 
0 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
0 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
0 Newton iteration  6 done0 , maximum relative shift = 1.678e-06
0 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
5 did  comm_.min(converged) convergedRemote: 1
5 final convergedRemote: 1
5 Update: x^(k+1) = x^k - deltax^k 
5 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
5 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
5 Newton iteration  6 done5 , maximum relative shift = 1.678e-06
5 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
6 did  comm_.min(converged) convergedRemote: 1
6 final convergedRemote: 1
6 Update: x^(k+1) = x^k - deltax^k 
6 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
6 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
6 Newton iteration  6 done6 , maximum relative shift = 1.678e-06
6 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
1 did  comm_.min(converged) convergedRemote: 1
1 final convergedRemote: 1
1 Update: x^(k+1) = x^k - deltax^k 
1 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
1 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
1 Newton iteration  6 done1 , maximum relative shift = 1.678e-06
1 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
3 did  comm_.min(converged) convergedRemote: 1
3 final convergedRemote: 1
3 Update: x^(k+1) = x^k - deltax^k 
3 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
3 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
3 Newton iteration  6 done3 , maximum relative shift = 1.678e-06
3 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
6 Solve: M deltax^k = r 
6 Newton::solveLinearSystem : entering the try section 6
6 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
6 solveLinearSystemImpl_, before converged = ls.solve 
6 7 amgbackend::solve isParallel 1
2 Solve: M deltax^k = r 
2 Newton::solveLinearSystem : entering the try section 6
2 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
2 solveLinearSystemImpl_, before converged = ls.solve 
2 7 amgbackend::solve isParallel 1
4 Solve: M deltax^k = r 
4 Newton::solveLinearSystem : entering the try section 6
4 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
4 solveLinearSystemImpl_, before converged = ls.solve 
4 7 amgbackend::solve isParallel 1
0 Solve: M deltax^k = r 
0 Newton::solveLinearSystem : entering the try section 6
0 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
0 solveLinearSystemImpl_, before converged = ls.solve 
0 7 amgbackend::solve isParallel 1
5 Solve: M deltax^k = r 
5 Newton::solveLinearSystem : entering the try section 6
5 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
5 solveLinearSystemImpl_, before converged = ls.solve 
5 7 amgbackend::solve isParallel 1
3 Solve: M deltax^k = r 
3 Newton::solveLinearSystem : entering the try section 6
3 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
3 solveLinearSystemImpl_, before converged = ls.solve 
3 7 amgbackend::solve isParallel 1
1 Solve: M deltax^k = r 
1 Newton::solveLinearSystem : entering the try section 6
1 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
1 solveLinearSystemImpl_, before converged = ls.solve 
1 7 amgbackend::solve isParallel 1
6 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
6 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
2 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
2 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
4 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
4 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
0 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
0 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 7
1 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
1 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
3 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
3 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
5 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
5 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 7
1 7 before Dune::BiCGSTABSolver 
1 Dune::BiCGSTABSolver.apply(x, b, result_) 
1 BiCGSTABSolver::apply 
1 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
3 7 before Dune::BiCGSTABSolver 
3 Dune::BiCGSTABSolver.apply(x, b, result_) 
3 BiCGSTABSolver::apply 
3 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
5 7 before Dune::BiCGSTABSolver 
5 Dune::BiCGSTABSolver.apply(x, b, result_) 
5 BiCGSTABSolver::apply 
5 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
6 7 before Dune::BiCGSTABSolver 
6 Dune::BiCGSTABSolver.apply(x, b, result_) 
6 BiCGSTABSolver::apply 
6 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
2 7 before Dune::BiCGSTABSolver 
2 Dune::BiCGSTABSolver.apply(x, b, result_) 
2 BiCGSTABSolver::apply 
2 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
4 7 before Dune::BiCGSTABSolver 
4 Dune::BiCGSTABSolver.apply(x, b, result_) 
4 BiCGSTABSolver::apply 
4 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
Using a direct coarse solver (SuperLU)
Building hierarchy of 1 levels (inclusive coarse solver) took 0.111679 seconds.
0 7 before Dune::BiCGSTABSolver 
0 Dune::BiCGSTABSolver.apply(x, b, result_) 
0 BiCGSTABSolver::apply 
0 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
0 === BiCGSTABSolver
 Iter          Defect            Rate
    0      2.20025e-16
0  (it = 0.5; it < _maxit; it+=.5) 0.5 250
3 === BiCGSTABSolver
 Iter          Defect            Rate
    0      2.20025e-16
3  (it = 0.5; it < _maxit; it+=.5) 0.5 250
3  before p = r 
3  before  y = W^-1 * p 
3  before _prec->apply(y,p); 0.5 250
3 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
5 === BiCGSTABSolver
 Iter          Defect            Rate
    0      2.20025e-16
5  (it = 0.5; it < _maxit; it+=.5) 0.5 250
5  before p = r 
5  before  y = W^-1 * p 
5  before _prec->apply(y,p); 0.5 250
5 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
6 === BiCGSTABSolver
 Iter          Defect            Rate
    0      2.20025e-16
6  (it = 0.5; it < _maxit; it+=.5) 0.5 250
6  before p = r 
6  before  y = W^-1 * p 
6  before _prec->apply(y,p); 0.5 250
6 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
2 === BiCGSTABSolver
 Iter          Defect            Rate
    0      2.20025e-16
2  (it = 0.5; it < _maxit; it+=.5) 0.5 250
2  before p = r 
2  before  y = W^-1 * p 
2  before _prec->apply(y,p); 0.5 250
2 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
0  before p = r 
0  before  y = W^-1 * p 
0  before _prec->apply(y,p); 0.5 250
0 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 7
0 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
4 === BiCGSTABSolver
 Iter          Defect            Rate
    0      2.20025e-16
4  (it = 0.5; it < _maxit; it+=.5) 0.5 250
4  before p = r 
4  before  y = W^-1 * p 
4  before _prec->apply(y,p); 0.5 250
4 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
1 === BiCGSTABSolver
 Iter          Defect            Rate
    0      2.20025e-16
1  (it = 0.5; it < _maxit; it+=.5) 0.5 250
1  before p = r 
1  before  y = W^-1 * p 
1  before _prec->apply(y,p); 0.5 250
1 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
1 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
0 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
4 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
4 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
4  before v = A * y 
2 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
2 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
3 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
3 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
3  before v = A * y 
5 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
5 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
5  before v = A * y 
6 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
6 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
6  before v = A * y 
4  before  alpha = rho_new / < rt, v > 
2  before v = A * y 
2  before  alpha = rho_new / < rt, v > 
3  before  alpha = rho_new / < rt, v > 
5  before  alpha = rho_new / < rt, v > 
6  before  alpha = rho_new / < rt, v > 
1 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
0 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
0  before v = A * y 
6  before alpha = rho_new / h; 
6  before apply first correction to x 
6  before r = r - alpha*v 
6  before test stop criteria 
6 istlsolver::printOutput 
  0.5      3.52712e-34      1.60305e-18
6 === rate=2.56978e-36, T=0.120913, TIT=0.241826, IT=0.5
6 finished solveLinearSystemImpl_ 1
6 converged value: 1 7 1 let s go get convergedRemote
6 to  comm_.min(converged) 
6 did  comm_.min(converged) convergedRemote: 1
6 final convergedRemote: 1
6 Update: x^(k+1) = x^k - deltax^k 
6 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
6 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
6 Newton iteration  7 done6 , maximum relative shift = 4.879e-08
6 Assemble/solve/update time: 0.308747(25.0965%)/0.92113(74.8741%)/0.000361555(0.029389%)
6 gridVariables->advanceTimeStep
6 before, nonLinearSolver->suggestTimeStepSize, current time: 638.73 simTime 57600
6 before assembler->setPreviousSolution, ddt: 16.1708
6 nonLinearSolver->solve
6 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
1  before v = A * y 
1  before  alpha = rho_new / < rt, v > 
1  before alpha = rho_new / h; 
1  before apply first correction to x 
1  before r = r - alpha*v 
1  before test stop criteria 
1 istlsolver::printOutput 
  0.5      3.52712e-34      1.60305e-18
1 === rate=2.56978e-36, T=0.121085, TIT=0.24217, IT=0.5
1 finished solveLinearSystemImpl_ 1
1 converged value: 1 7 1 let s go get convergedRemote
1 to  comm_.min(converged) 
1 did  comm_.min(converged) convergedRemote: 1
1 final convergedRemote: 1
1 Update: x^(k+1) = x^k - deltax^k 
1 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
1 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
1 Newton iteration  7 done1 , maximum relative shift = 4.879e-08
1 Assemble/solve/update time: 0.309066(25.1181%)/0.916506(74.4854%)/0.00487839(0.396472%)
1 gridVariables->advanceTimeStep
1 before, nonLinearSolver->suggestTimeStepSize, current time: 638.73 simTime 57600
1 before assembler->setPreviousSolution, ddt: 16.1708
1 nonLinearSolver->solve
1 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
5  before alpha = rho_new / h; 
5  before apply first correction to x 
5  before r = r - alpha*v 
5  before test stop criteria 
5 istlsolver::printOutput 
  0.5      3.52712e-34      1.60305e-18
5 === rate=2.56978e-36, T=0.121083, TIT=0.242165, IT=0.5
5 finished solveLinearSystemImpl_ 1
5 converged value: 1 7 1 let s go get convergedRemote
5 to  comm_.min(converged) 
5 did  comm_.min(converged) convergedRemote: 1
5 final convergedRemote: 1
5 Update: x^(k+1) = x^k - deltax^k 
5 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
5 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
5 Newton iteration  7 done5 , maximum relative shift = 4.879e-08
5 Assemble/solve/update time: 0.309075(25.1187%)/0.916508(74.4851%)/0.00487595(0.396271%)
5 gridVariables->advanceTimeStep
5 before, nonLinearSolver->suggestTimeStepSize, current time: 638.73 simTime 57600
5 before assembler->setPreviousSolution, ddt: 16.1708
5 nonLinearSolver->solve
5 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
4  before alpha = rho_new / h; 
4  before apply first correction to x 
4  before r = r - alpha*v 
4  before test stop criteria 
4 istlsolver::printOutput 
  0.5      3.52712e-34      1.60305e-18
4 === rate=2.56978e-36, T=0.12091, TIT=0.24182, IT=0.5
4 finished solveLinearSystemImpl_ 1
4 converged value: 1 7 1 let s go get convergedRemote
4 to  comm_.min(converged) 
4 did  comm_.min(converged) convergedRemote: 1
4 final convergedRemote: 1
4 Update: x^(k+1) = x^k - deltax^k 
4 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
4 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
4 Newton iteration  7 done4 , maximum relative shift = 4.879e-08
4 Assemble/solve/update time: 0.308709(25.0942%)/0.916604(74.5084%)/0.00488847(0.397372%)
4 gridVariables->advanceTimeStep
4 before, nonLinearSolver->suggestTimeStepSize, current time: 638.73 simTime 57600
4 before assembler->setPreviousSolution, ddt: 16.1708
4 nonLinearSolver->solve
2  before alpha = rho_new / h; 
2  before apply first correction to x 
2  before r = r - alpha*v 
2  before test stop criteria 
2 istlsolver::printOutput 
  0.5      3.52712e-34      1.60305e-18
2 === rate=2.56978e-36, T=0.120912, TIT=0.241824, IT=0.5
2 finished solveLinearSystemImpl_ 1
2 converged value: 1 7 1 let s go get convergedRemote
2 to  comm_.min(converged) 
2 did  comm_.min(converged) convergedRemote: 1
2 final convergedRemote: 1
2 Update: x^(k+1) = x^k - deltax^k 
2 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
2 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
2 Newton iteration  7 done2 , maximum relative shift = 4.879e-08
2 Assemble/solve/update time: 0.308343(25.0713%)/0.916619(74.53%)/0.00490388(0.398733%)
2 gridVariables->advanceTimeStep
2 before, nonLinearSolver->suggestTimeStepSize, current time: 638.73 simTime 57600
2 before assembler->setPreviousSolution, ddt: 16.1708
2 nonLinearSolver->solve
2 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
3  before alpha = rho_new / h; 
3  before apply first correction to x 
3  before r = r - alpha*v 
3  before test stop criteria 
3 istlsolver::printOutput 
  0.5      3.52712e-34      1.60305e-18
3 === rate=2.56978e-36, T=0.121085, TIT=0.24217, IT=0.5
3 finished solveLinearSystemImpl_ 1
3 converged value: 1 7 1 let s go get convergedRemote
3 to  comm_.min(converged) 
3 did  comm_.min(converged) convergedRemote: 1
3 final convergedRemote: 1
3 Update: x^(k+1) = x^k - deltax^k 
3 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
3 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
3 Newton iteration  7 done3 , maximum relative shift = 4.879e-08
3 Assemble/solve/update time: 0.309049(25.1167%)/0.916522(74.4866%)/0.00488116(0.396697%)
3 gridVariables->advanceTimeStep
3 before, nonLinearSolver->suggestTimeStepSize, current time: 638.73 simTime 57600
3 before assembler->setPreviousSolution, ddt: 16.1708
3 nonLinearSolver->solve
3 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
0  before  alpha = rho_new / < rt, v > 
0  before alpha = rho_new / h; 
0  before apply first correction to x 
0  before r = r - alpha*v 
0  before test stop criteria 
0 istlsolver::printOutput 
  0.5      3.52712e-34      1.60305e-18
0 === rate=2.56978e-36, T=0.01842, TIT=0.0368399, IT=0.5
0 finished solveLinearSystemImpl_ 1
0 converged value: 1 7 1 let s go get convergedRemote
0 to  comm_.min(converged) 
0 did  comm_.min(converged) convergedRemote: 1
0 final convergedRemote: 1
0 Update: x^(k+1) = x^k - deltax^k 
0 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
0 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
0 Newton iteration  7 done0 , maximum relative shift = 4.879e-08
0 Assemble/solve/update time: 0.308146(25.066%)/0.916289(74.5351%)/0.00490348(0.398872%)
0 gridVariables->advanceTimeStep
4 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
0 before, nonLinearSolver->suggestTimeStepSize, current time: 638.73 simTime 57600
0 before assembler->setPreviousSolution, ddt: 16.1708
0 nonLinearSolver->solve
0 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
0 Solve: M deltax^k = r 
0 Newton::solveLinearSystem : entering the try section 0
0to  comm_.sum(norm2) 
0did  comm_.sum(norm2) 
0 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
0 solveLinearSystemImpl_, before converged = ls.solve 
0 7 amgbackend::solve isParallel 1
3 Solve: M deltax^k = r 
3 Newton::solveLinearSystem : entering the try section 0
3to  comm_.sum(norm2) 
3did  comm_.sum(norm2) 
3 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
3 solveLinearSystemImpl_, before converged = ls.solve 
3 7 amgbackend::solve isParallel 1
6 Solve: M deltax^k = r 
6 Newton::solveLinearSystem : entering the try section 0
6to  comm_.sum(norm2) 
6did  comm_.sum(norm2) 
6 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
6 solveLinearSystemImpl_, before converged = ls.solve 
6 7 amgbackend::solve isParallel 1
1 Solve: M deltax^k = r 
1 Newton::solveLinearSystem : entering the try section 0
1to  comm_.sum(norm2) 
1did  comm_.sum(norm2) 
1 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
1 solveLinearSystemImpl_, before converged = ls.solve 
1 7 amgbackend::solve isParallel 1
5 Solve: M deltax^k = r 
5 Newton::solveLinearSystem : entering the try section 0
5to  comm_.sum(norm2) 
5did  comm_.sum(norm2) 
5 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
5 solveLinearSystemImpl_, before converged = ls.solve 
5 7 amgbackend::solve isParallel 1
4 Solve: M deltax^k = r 
4 Newton::solveLinearSystem : entering the try section 0
4to  comm_.sum(norm2) 
4did  comm_.sum(norm2) 
4 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
4 solveLinearSystemImpl_, before converged = ls.solve 
4 7 amgbackend::solve isParallel 1
2 Solve: M deltax^k = r 
2 Newton::solveLinearSystem : entering the try section 0
2to  comm_.sum(norm2) 
2did  comm_.sum(norm2) 
2 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
2 solveLinearSystemImpl_, before converged = ls.solve 
2 7 amgbackend::solve isParallel 1
6 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
6 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
1 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
1 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
5 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
5 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
4 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
4 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
2 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
2 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
0 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
0 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 7
3 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
3 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 7
2 7 before Dune::BiCGSTABSolver 
2 Dune::BiCGSTABSolver.apply(x, b, result_) 
2 BiCGSTABSolver::apply 
2 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
3 7 before Dune::BiCGSTABSolver 
3 Dune::BiCGSTABSolver.apply(x, b, result_) 
3 BiCGSTABSolver::apply 
3 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
4 7 before Dune::BiCGSTABSolver 
4 Dune::BiCGSTABSolver.apply(x, b, result_) 
4 BiCGSTABSolver::apply 
4 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
6 7 before Dune::BiCGSTABSolver 
6 Dune::BiCGSTABSolver.apply(x, b, result_) 
6 BiCGSTABSolver::apply 
6 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
1 7 before Dune::BiCGSTABSolver 
1 Dune::BiCGSTABSolver.apply(x, b, result_) 
1 BiCGSTABSolver::apply 
1 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
5 7 before Dune::BiCGSTABSolver 
5 Dune::BiCGSTABSolver.apply(x, b, result_) 
5 BiCGSTABSolver::apply 
5 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 7
Using a direct coarse solver (SuperLU)
Building hierarchy of 1 levels (inclusive coarse solver) took 0.112821 seconds.
0 7 before Dune::BiCGSTABSolver 
0 Dune::BiCGSTABSolver.apply(x, b, result_) 
0 BiCGSTABSolver::apply 
0 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
5 === BiCGSTABSolver
 Iter          Defect            Rate
    0      3.16211e-09
5  (it = 0.5; it < _maxit; it+=.5) 0.5 250
5  before p = r 
5  before  y = W^-1 * p 
5  before _prec->apply(y,p); 0.5 250
5 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
2 === BiCGSTABSolver
 Iter          Defect            Rate
    0      3.16211e-09
2  (it = 0.5; it < _maxit; it+=.5) 0.5 250
2  before p = r 
2  before  y = W^-1 * p 
2  before _prec->apply(y,p); 0.5 250
2 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
3 === BiCGSTABSolver
 Iter          Defect            Rate
    0      3.16211e-09
3  (it = 0.5; it < _maxit; it+=.5) 0.5 250
3  before p = r 
3  before  y = W^-1 * p 
3  before _prec->apply(y,p); 0.5 250
3 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
4 === BiCGSTABSolver
 Iter          Defect            Rate
    0      3.16211e-09
4  (it = 0.5; it < _maxit; it+=.5) 0.5 250
4  before p = r 
4  before  y = W^-1 * p 
4  before _prec->apply(y,p); 0.5 250
4 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
6 === BiCGSTABSolver
 Iter          Defect            Rate
    0      3.16211e-09
6  (it = 0.5; it < _maxit; it+=.5) 0.5 250
6  before p = r 
6  before  y = W^-1 * p 
6  before _prec->apply(y,p); 0.5 250
6 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
1 === BiCGSTABSolver
 Iter          Defect            Rate
    0      3.16211e-09
1  (it = 0.5; it < _maxit; it+=.5) 0.5 250
1  before p = r 
1  before  y = W^-1 * p 
1  before _prec->apply(y,p); 0.5 250
1 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
0 === BiCGSTABSolver
 Iter          Defect            Rate
    0      3.16211e-09
0  (it = 0.5; it < _maxit; it+=.5) 0.5 250
0  before p = r 
0  before  y = W^-1 * p 
0  before _prec->apply(y,p); 0.5 250
0 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 7
0 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
3 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
3 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
3  before v = A * y 
4 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
4 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
4  before v = A * y 
6 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
6 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
6  before v = A * y 
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
0 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
1 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
5 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
5 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
5  before v = A * y 
2 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
2 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
3  before  alpha = rho_new / < rt, v > 
4  before  alpha = rho_new / < rt, v > 
2  before v = A * y 
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
0 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
1 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
5  before  alpha = rho_new / < rt, v > 
6  before  alpha = rho_new / < rt, v > 
4  before alpha = rho_new / h; 
4  before apply first correction to x 
4  before r = r - alpha*v 
4  before test stop criteria 
4 istlsolver::printOutput 
  0.5      5.20658e-25      1.64655e-16
4 === rate=2.71114e-32, T=0.123496, TIT=0.246992, IT=0.5
4 finished solveLinearSystemImpl_ 1
4 converged value: 1 7 1 let s go get convergedRemote
4 to  comm_.min(converged) 
3  before alpha = rho_new / h; 
3  before apply first correction to x 
3  before r = r - alpha*v 
3  before test stop criteria 
3 istlsolver::printOutput 
  0.5      5.20658e-25      1.64655e-16
3 === rate=2.71114e-32, T=0.123473, TIT=0.246946, IT=0.5
3 finished solveLinearSystemImpl_ 1
3 converged value: 1 7 1 let s go get convergedRemote
3 to  comm_.min(converged) 
6  before alpha = rho_new / h; 
6  before apply first correction to x 
6  before r = r - alpha*v 
6  before test stop criteria 
6 istlsolver::printOutput 
  0.5      5.20658e-25      1.64655e-16
6 === rate=2.71114e-32, T=0.123497, TIT=0.246993, IT=0.5
6 finished solveLinearSystemImpl_ 1
6 converged value: 1 7 1 let s go get convergedRemote
6 to  comm_.min(converged) 
2  before  alpha = rho_new / < rt, v > 
2  before alpha = rho_new / h; 
2  before apply first correction to x 
2  before r = r - alpha*v 
2  before test stop criteria 
2 istlsolver::printOutput 
  0.5      5.20658e-25      1.64655e-16
2 === rate=2.71114e-32, T=0.123497, TIT=0.246993, IT=0.5
2 finished solveLinearSystemImpl_ 1
2 converged value: 1 7 1 let s go get convergedRemote
2 to  comm_.min(converged) 
0  before v = A * y 
0  before  alpha = rho_new / < rt, v > 
0  before alpha = rho_new / h; 
0  before apply first correction to x 
0  before r = r - alpha*v 
0  before test stop criteria 
0 istlsolver::printOutput 
  0.5      5.20658e-25      1.64655e-16
0 === rate=2.71114e-32, T=0.0184861, TIT=0.0369723, IT=0.5
1  before v = A * y 
1  before  alpha = rho_new / < rt, v > 
1  before alpha = rho_new / h; 
1  before apply first correction to x 
1  before r = r - alpha*v 
1  before test stop criteria 
1 istlsolver::printOutput 
  0.5      5.20658e-25      1.64655e-16
1 === rate=2.71114e-32, T=0.123476, TIT=0.246952, IT=0.5
1 finished solveLinearSystemImpl_ 1
1 converged value: 1 7 1 let s go get convergedRemote
1 to  comm_.min(converged) 
5  before alpha = rho_new / h; 
5  before apply first correction to x 
5  before r = r - alpha*v 
5  before test stop criteria 
5 istlsolver::printOutput 
  0.5      5.20658e-25      1.64655e-16
5 === rate=2.71114e-32, T=0.123474, TIT=0.246948, IT=0.5
5 finished solveLinearSystemImpl_ 1
5 converged value: 1 7 1 let s go get convergedRemote
5 to  comm_.min(converged) 
1 did  comm_.min(converged) convergedRemote: 1
1 final convergedRemote: 1
1 Update: x^(k+1) = x^k - deltax^k 
1 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
1 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
1 Newton iteration  1 done1 , maximum relative shift = 9.211e-02
1 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
3 did  comm_.min(converged) convergedRemote: 1
3 final convergedRemote: 1
3 Update: x^(k+1) = x^k - deltax^k 
3 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
3 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
3 Newton iteration  1 done3 , maximum relative shift = 9.211e-02
3 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
4 did  comm_.min(converged) convergedRemote: 1
4 final convergedRemote: 1
4 Update: x^(k+1) = x^k - deltax^k 
4 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
4 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
4 Newton iteration  1 done4 , maximum relative shift = 9.211e-02
4 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
5 did  comm_.min(converged) convergedRemote: 1
5 final convergedRemote: 1
5 Update: x^(k+1) = x^k - deltax^k 
5 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
5 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
5 Newton iteration  1 done5 , maximum relative shift = 9.211e-02
5 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
6 did  comm_.min(converged) convergedRemote: 1
6 final convergedRemote: 1
6 Update: x^(k+1) = x^k - deltax^k 
6 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
6 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
6 Newton iteration  1 done6 , maximum relative shift = 9.211e-02
6 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
2 did  comm_.min(converged) convergedRemote: 1
2 final convergedRemote: 1
2 Update: x^(k+1) = x^k - deltax^k 
2 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
2 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
2 Newton iteration  1 done2 , maximum relative shift = 9.211e-02
2 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
0 finished solveLinearSystemImpl_ 1
0 converged value: 1 7 1 let s go get convergedRemote
0 to  comm_.min(converged) 
0 did  comm_.min(converged) convergedRemote: 1
0 final convergedRemote: 1
0 Update: x^(k+1) = x^k - deltax^k 
0 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
0 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
0 Newton iteration  1 done0 , maximum relative shift = 9.211e-02
0 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
5 Solve: M deltax^k = r 
5 Newton::solveLinearSystem : entering the try section 1
5 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
5 solveLinearSystemImpl_, before converged = ls.solve 
5 7 amgbackend::solve isParallel 1
6 Solve: M deltax^k = r 
6 Newton::solveLinearSystem : entering the try section 1
6 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
6 solveLinearSystemImpl_, before converged = ls.solve 
6 7 amgbackend::solve isParallel 1
2 Solve: M deltax^k = r 
2 Newton::solveLinearSystem : entering the try section 1
2 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
2 solveLinearSystemImpl_, before converged = ls.solve 
2 7 amgbackend::solve isParallel 1
4 Solve: M deltax^k = r 
4 Newton::solveLinearSystem : entering the try section 1
4 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
4 solveLinearSystemImpl_, before converged = ls.solve 
4 7 amgbackend::solve isParallel 1
1 Solve: M deltax^k = r 
1 Newton::solveLinearSystem : entering the try section 1
1 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
1 solveLinearSystemImpl_, before converged = ls.solve 
1 7 amgbackend::solve isParallel 1
3 Solve: M deltax^k = r 
3 Newton::solveLinearSystem : entering the try section 1
3 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
3 solveLinearSystemImpl_, before converged = ls.solve 
3 7 amgbackend::solve isParallel 1
0 Solve: M deltax^k = r 
0 Newton::solveLinearSystem : entering the try section 1
0 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
0 solveLinearSystemImpl_, before converged = ls.solve 
0 7 amgbackend::solve isParallel 1
6 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
6 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
2 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
2 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
4 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
4 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
1 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
1 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
0 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
0 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 7
3 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
3 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
5 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
5 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 7
5 7 before Dune::BiCGSTABSolver 
5 Dune::BiCGSTABSolver.apply(x, b, result_) 
5 BiCGSTABSolver::apply 
5 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
3 7 before Dune::BiCGSTABSolver 
3 Dune::BiCGSTABSolver.apply(x, b, result_) 
3 BiCGSTABSolver::apply 
3 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
1 7 before Dune::BiCGSTABSolver 
1 Dune::BiCGSTABSolver.apply(x, b, result_) 
1 BiCGSTABSolver::apply 
1 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
6 7 before Dune::BiCGSTABSolver 
6 Dune::BiCGSTABSolver.apply(x, b, result_) 
6 BiCGSTABSolver::apply 
6 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
2 7 before Dune::BiCGSTABSolver 
2 Dune::BiCGSTABSolver.apply(x, b, result_) 
2 BiCGSTABSolver::apply 
2 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
4 7 before Dune::BiCGSTABSolver 
4 Dune::BiCGSTABSolver.apply(x, b, result_) 
4 BiCGSTABSolver::apply 
4 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 7
Using a direct coarse solver (SuperLU)
Building hierarchy of 1 levels (inclusive coarse solver) took 0.111464 seconds.
0 7 before Dune::BiCGSTABSolver 
0 Dune::BiCGSTABSolver.apply(x, b, result_) 
0 BiCGSTABSolver::apply 
0 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
0 === BiCGSTABSolver
 Iter          Defect            Rate
    0      6.21684e-11
0  (it = 0.5; it < _maxit; it+=.5) 0.5 250
0  before p = r 
0  before  y = W^-1 * p 
0  before _prec->apply(y,p); 0.5 250
0 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 7
0 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
4 === BiCGSTABSolver
 Iter          Defect            Rate
    0      6.21684e-11
4  (it = 0.5; it < _maxit; it+=.5) 0.5 250
4  before p = r 
4  before  y = W^-1 * p 
4  before _prec->apply(y,p); 0.5 250
4 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
5 === BiCGSTABSolver
 Iter          Defect            Rate
    0      6.21684e-11
5  (it = 0.5; it < _maxit; it+=.5) 0.5 250
5  before p = r 
5  before  y = W^-1 * p 
5  before _prec->apply(y,p); 0.5 250
5 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
3 === BiCGSTABSolver
 Iter          Defect            Rate
    0      6.21684e-11
3  (it = 0.5; it < _maxit; it+=.5) 0.5 250
3  before p = r 
3  before  y = W^-1 * p 
3  before _prec->apply(y,p); 0.5 250
3 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
1 === BiCGSTABSolver
 Iter          Defect            Rate
    0      6.21684e-11
1  (it = 0.5; it < _maxit; it+=.5) 0.5 250
1  before p = r 
1  before  y = W^-1 * p 
1  before _prec->apply(y,p); 0.5 250
1 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
6 === BiCGSTABSolver
 Iter          Defect            Rate
    0      6.21684e-11
6  (it = 0.5; it < _maxit; it+=.5) 0.5 250
6  before p = r 
6  before  y = W^-1 * p 
6  before _prec->apply(y,p); 0.5 250
6 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
2 === BiCGSTABSolver
 Iter          Defect            Rate
    0      6.21684e-11
2  (it = 0.5; it < _maxit; it+=.5) 0.5 250
2  before p = r 
2  before  y = W^-1 * p 
2  before _prec->apply(y,p); 0.5 250
2 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
4 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
4 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
4  before v = A * y 
5 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
5 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
5  before v = A * y 
3 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
3 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
3  before v = A * y 
1 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
6 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
6 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
6  before v = A * y 
2 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
2 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
3  before  alpha = rho_new / < rt, v > 
1 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
2  before v = A * y 
6  before  alpha = rho_new / < rt, v > 
0 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
0 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
4  before  alpha = rho_new / < rt, v > 
5  before  alpha = rho_new / < rt, v > 
0  before v = A * y 
0  before  alpha = rho_new / < rt, v > 
0  before alpha = rho_new / h; 
0  before apply first correction to x 
0  before r = r - alpha*v 
0  before test stop criteria 
0 istlsolver::printOutput 
  0.5      1.33254e-26      2.14343e-16
0 === rate=4.5943e-32, T=0.0185685, TIT=0.0371371, IT=0.5
4  before alpha = rho_new / h; 
4  before apply first correction to x 
4  before r = r - alpha*v 
4  before test stop criteria 
4 istlsolver::printOutput 
  0.5      1.33254e-26      2.14343e-16
4 === rate=4.5943e-32, T=0.122201, TIT=0.244402, IT=0.5
4 finished solveLinearSystemImpl_ 1
4 converged value: 1 7 1 let s go get convergedRemote
4 to  comm_.min(converged) 
1  before v = A * y 
1  before  alpha = rho_new / < rt, v > 
1  before alpha = rho_new / h; 
1  before apply first correction to x 
1  before r = r - alpha*v 
1  before test stop criteria 
1 istlsolver::printOutput 
  0.5      1.33254e-26      2.14343e-16
1 === rate=4.5943e-32, T=0.122114, TIT=0.244228, IT=0.5
1 finished solveLinearSystemImpl_ 1
1 converged value: 1 7 1 let s go get convergedRemote
1 to  comm_.min(converged) 
3  before alpha = rho_new / h; 
3  before apply first correction to x 
3  before r = r - alpha*v 
3  before test stop criteria 
3 istlsolver::printOutput 
  0.5      1.33254e-26      2.14343e-16
3 === rate=4.5943e-32, T=0.122116, TIT=0.244233, IT=0.5
3 finished solveLinearSystemImpl_ 1
3 converged value: 1 7 1 let s go get convergedRemote
3 to  comm_.min(converged) 
5  before alpha = rho_new / h; 
5  before apply first correction to x 
5  before r = r - alpha*v 
5  before test stop criteria 
5 istlsolver::printOutput 
  0.5      1.33254e-26      2.14343e-16
5 === rate=4.5943e-32, T=0.122114, TIT=0.244227, IT=0.5
5 finished solveLinearSystemImpl_ 1
5 converged value: 1 7 1 let s go get convergedRemote
5 to  comm_.min(converged) 
2  before  alpha = rho_new / < rt, v > 
2  before alpha = rho_new / h; 
2  before apply first correction to x 
2  before r = r - alpha*v 
2  before test stop criteria 
2 istlsolver::printOutput 
  0.5      1.33254e-26      2.14343e-16
2 === rate=4.5943e-32, T=0.122202, TIT=0.244404, IT=0.5
2 finished solveLinearSystemImpl_ 1
2 converged value: 1 7 1 let s go get convergedRemote
2 to  comm_.min(converged) 
6  before alpha = rho_new / h; 
6  before apply first correction to x 
6  before r = r - alpha*v 
6  before test stop criteria 
6 istlsolver::printOutput 
  0.5      1.33254e-26      2.14343e-16
6 === rate=4.5943e-32, T=0.122203, TIT=0.244406, IT=0.5
6 finished solveLinearSystemImpl_ 1
6 converged value: 1 7 1 let s go get convergedRemote
6 to  comm_.min(converged) 
5 did  comm_.min(converged) convergedRemote: 1
5 final convergedRemote: 1
5 Update: x^(k+1) = x^k - deltax^k 
5 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
5 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
5 Newton iteration  2 done5 , maximum relative shift = 1.975e-02
5 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
2 did  comm_.min(converged) convergedRemote: 1
2 final convergedRemote: 1
2 Update: x^(k+1) = x^k - deltax^k 
2 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
2 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
2 Newton iteration  2 done2 , maximum relative shift = 1.975e-02
2 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
3 did  comm_.min(converged) convergedRemote: 1
3 final convergedRemote: 1
3 Update: x^(k+1) = x^k - deltax^k 
3 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
3 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
3 Newton iteration  2 done3 , maximum relative shift = 1.975e-02
3 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
0 finished solveLinearSystemImpl_ 1
0 converged value: 1 7 1 let s go get convergedRemote
0 to  comm_.min(converged) 
0 did  comm_.min(converged) convergedRemote: 1
0 final convergedRemote: 1
0 Update: x^(k+1) = x^k - deltax^k 
0 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
0 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
0 Newton iteration  2 done0 , maximum relative shift = 1.975e-02
0 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
4 did  comm_.min(converged) convergedRemote: 1
4 final convergedRemote: 1
4 Update: x^(k+1) = x^k - deltax^k 
4 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
4 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
4 Newton iteration  2 done4 , maximum relative shift = 1.975e-02
4 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
1 did  comm_.min(converged) convergedRemote: 1
1 final convergedRemote: 1
1 Update: x^(k+1) = x^k - deltax^k 
1 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
1 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
1 Newton iteration  2 done1 , maximum relative shift = 1.975e-02
1 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
6 did  comm_.min(converged) convergedRemote: 1
6 final convergedRemote: 1
6 Update: x^(k+1) = x^k - deltax^k 
6 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
6 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
6 Newton iteration  2 done6 , maximum relative shift = 1.975e-02
6 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
1 Solve: M deltax^k = r 
1 Newton::solveLinearSystem : entering the try section 2
1 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
1 solveLinearSystemImpl_, before converged = ls.solve 
1 7 amgbackend::solve isParallel 1
5 Solve: M deltax^k = r 
5 Newton::solveLinearSystem : entering the try section 2
5 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
5 solveLinearSystemImpl_, before converged = ls.solve 
5 7 amgbackend::solve isParallel 1
2 Solve: M deltax^k = r 
2 Newton::solveLinearSystem : entering the try section 2
2 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
2 solveLinearSystemImpl_, before converged = ls.solve 
2 7 amgbackend::solve isParallel 1
3 Solve: M deltax^k = r 
3 Newton::solveLinearSystem : entering the try section 2
3 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
3 solveLinearSystemImpl_, before converged = ls.solve 
3 7 amgbackend::solve isParallel 1
0 Solve: M deltax^k = r 
0 Newton::solveLinearSystem : entering the try section 2
0 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
0 solveLinearSystemImpl_, before converged = ls.solve 
0 7 amgbackend::solve isParallel 1
4 Solve: M deltax^k = r 
4 Newton::solveLinearSystem : entering the try section 2
4 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
4 solveLinearSystemImpl_, before converged = ls.solve 
4 7 amgbackend::solve isParallel 1
6 Solve: M deltax^k = r 
6 Newton::solveLinearSystem : entering the try section 2
6 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
6 solveLinearSystemImpl_, before converged = ls.solve 
6 7 amgbackend::solve isParallel 1
3 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
3 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
0 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
0 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 7
6 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
6 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
4 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
4 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
1 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
1 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
5 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
5 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
2 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
2 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 7
1 7 before Dune::BiCGSTABSolver 
1 Dune::BiCGSTABSolver.apply(x, b, result_) 
1 BiCGSTABSolver::apply 
1 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
5 7 before Dune::BiCGSTABSolver 
5 Dune::BiCGSTABSolver.apply(x, b, result_) 
5 BiCGSTABSolver::apply 
5 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
2 7 before Dune::BiCGSTABSolver 
2 Dune::BiCGSTABSolver.apply(x, b, result_) 
2 BiCGSTABSolver::apply 
2 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
3 7 before Dune::BiCGSTABSolver 
3 Dune::BiCGSTABSolver.apply(x, b, result_) 
3 BiCGSTABSolver::apply 
3 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 7
6 7 before Dune::BiCGSTABSolver 
6 Dune::BiCGSTABSolver.apply(x, b, result_) 
6 BiCGSTABSolver::apply 
6 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
4 7 before Dune::BiCGSTABSolver 
4 Dune::BiCGSTABSolver.apply(x, b, result_) 
4 BiCGSTABSolver::apply 
4 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
Using a direct coarse solver (SuperLU)
Building hierarchy of 1 levels (inclusive coarse solver) took 0.112429 seconds.
0 7 before Dune::BiCGSTABSolver 
0 Dune::BiCGSTABSolver.apply(x, b, result_) 
0 BiCGSTABSolver::apply 
0 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
0 === BiCGSTABSolver
 Iter          Defect            Rate
    0      5.99835e-12
0  (it = 0.5; it < _maxit; it+=.5) 0.5 250
0  before p = r 
0  before  y = W^-1 * p 
0  before _prec->apply(y,p); 0.5 250
0 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
3 === BiCGSTABSolver
 Iter          Defect            Rate
    0      5.99835e-12
3  (it = 0.5; it < _maxit; it+=.5) 0.5 250
3  before p = r 
3  before  y = W^-1 * p 
3  before _prec->apply(y,p); 0.5 250
3 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
6 === BiCGSTABSolver
 Iter          Defect            Rate
    0      5.99835e-12
6  (it = 0.5; it < _maxit; it+=.5) 0.5 250
6  before p = r 
6  before  y = W^-1 * p 
6  before _prec->apply(y,p); 0.5 250
6 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
5 === BiCGSTABSolver
 Iter          Defect            Rate
    0      5.99835e-12
5  (it = 0.5; it < _maxit; it+=.5) 0.5 250
5  before p = r 
5  before  y = W^-1 * p 
5  before _prec->apply(y,p); 0.5 250
5 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
4 === BiCGSTABSolver
 Iter          Defect            Rate
    0      5.99835e-12
4  (it = 0.5; it < _maxit; it+=.5) 0.5 250
4  before p = r 
4  before  y = W^-1 * p 
4  before _prec->apply(y,p); 0.5 250
4 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
1 === BiCGSTABSolver
 Iter          Defect            Rate
    0      5.99835e-12
1  (it = 0.5; it < _maxit; it+=.5) 0.5 250
1  before p = r 
1  before  y = W^-1 * p 
1  before _prec->apply(y,p); 0.5 250
1 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
2 === BiCGSTABSolver
 Iter          Defect            Rate
    0      5.99835e-12
2  (it = 0.5; it < _maxit; it+=.5) 0.5 250
2  before p = r 
2  before  y = W^-1 * p 
2  before _prec->apply(y,p); 0.5 250
2 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 7
0 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
2 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
2 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
3 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
3 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
3  before v = A * y 
6 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
6 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
6  before v = A * y 
5 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
5 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
5  before v = A * y 
4 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
4 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
4  before v = A * y 
1 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
0 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
1 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
2  before v = A * y 
3  before  alpha = rho_new / < rt, v > 
6  before  alpha = rho_new / < rt, v > 
5  before  alpha = rho_new / < rt, v > 
4  before  alpha = rho_new / < rt, v > 
0 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
0  before v = A * y 
2  before  alpha = rho_new / < rt, v > 
2  before alpha = rho_new / h; 
2  before apply first correction to x 
2  before r = r - alpha*v 
2  before test stop criteria 
2 istlsolver::printOutput 
  0.5      1.49836e-28      2.49796e-17
2 === rate=6.23978e-34, T=0.123259, TIT=0.246519, IT=0.5
2 finished solveLinearSystemImpl_ 1
2 converged value: 1 7 1 let s go get convergedRemote
2 to  comm_.min(converged) 
3  before alpha = rho_new / h; 
3  before apply first correction to x 
3  before r = r - alpha*v 
3  before test stop criteria 
3 istlsolver::printOutput 
  0.5      1.49836e-28      2.49796e-17
3 === rate=6.23978e-34, T=0.123321, TIT=0.246643, IT=0.5
3 finished solveLinearSystemImpl_ 1
3 converged value: 1 7 1 let s go get convergedRemote
3 to  comm_.min(converged) 
6  before alpha = rho_new / h; 
6  before apply first correction to x 
6  before r = r - alpha*v 
6  before test stop criteria 
6 istlsolver::printOutput 
  0.5      1.49836e-28      2.49796e-17
6 === rate=6.23978e-34, T=0.12326, TIT=0.246519, IT=0.5
6 finished solveLinearSystemImpl_ 1
6 converged value: 1 7 1 let s go get convergedRemote
6 to  comm_.min(converged) 
5  before alpha = rho_new / h; 
5  before apply first correction to x 
5  before r = r - alpha*v 
5  before test stop criteria 
5 istlsolver::printOutput 
  0.5      1.49836e-28      2.49796e-17
5 === rate=6.23978e-34, T=0.12332, TIT=0.24664, IT=0.5
5 finished solveLinearSystemImpl_ 1
5 converged value: 1 7 1 let s go get convergedRemote
5 to  comm_.min(converged) 
0  before  alpha = rho_new / < rt, v > 
0  before alpha = rho_new / h; 
0  before apply first correction to x 
0  before r = r - alpha*v 
0  before test stop criteria 
0 istlsolver::printOutput 
  0.5      1.49836e-28      2.49796e-17
0 === rate=6.23978e-34, T=0.0189428, TIT=0.0378856, IT=0.5
4  before alpha = rho_new / h; 
4  before apply first correction to x 
4  before r = r - alpha*v 
4  before test stop criteria 
4 istlsolver::printOutput 
  0.5      1.49836e-28      2.49796e-17
4 === rate=6.23978e-34, T=0.123258, TIT=0.246516, IT=0.5
4 finished solveLinearSystemImpl_ 1
4 converged value: 1 7 1 let s go get convergedRemote
4 to  comm_.min(converged) 
1  before v = A * y 
1  before  alpha = rho_new / < rt, v > 
1  before alpha = rho_new / h; 
1  before apply first correction to x 
1  before r = r - alpha*v 
1  before test stop criteria 
1 istlsolver::printOutput 
  0.5      1.49836e-28      2.49796e-17
1 === rate=6.23978e-34, T=0.123223, TIT=0.246446, IT=0.5
1 finished solveLinearSystemImpl_ 1
1 converged value: 1 7 1 let s go get convergedRemote
1 to  comm_.min(converged) 
0 finished solveLinearSystemImpl_ 1
0 converged value: 1 7 1 let s go get convergedRemote
0 to  comm_.min(converged) 
0 did  comm_.min(converged) convergedRemote: 1
0 final convergedRemote: 1
0 Update: x^(k+1) = x^k - deltax^k 
0 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
0 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
0 Newton iteration  3 done0 , maximum relative shift = 1.992e-03
4 did  comm_.min(converged) convergedRemote: 1
4 final convergedRemote: 1
4 Update: x^(k+1) = x^k - deltax^k 
4 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
4 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
4 Newton iteration  3 done4 , maximum relative shift = 1.992e-03
4 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
5 did  comm_.min(converged) convergedRemote: 1
5 final convergedRemote: 1
5 Update: x^(k+1) = x^k - deltax^k 
5 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
5 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
5 Newton iteration  3 done5 , maximum relative shift = 1.992e-03
5 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
2 did  comm_.min(converged) convergedRemote: 1
2 final convergedRemote: 1
2 Update: x^(k+1) = x^k - deltax^k 
2 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
2 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
2 Newton iteration  3 done2 , maximum relative shift = 1.992e-03
2 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
3 did  comm_.min(converged) convergedRemote: 1
3 final convergedRemote: 1
3 Update: x^(k+1) = x^k - deltax^k 
3 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
3 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
3 Newton iteration  3 done3 , maximum relative shift = 1.992e-03
3 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
6 did  comm_.min(converged) convergedRemote: 1
6 final convergedRemote: 1
6 Update: x^(k+1) = x^k - deltax^k 
6 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
6 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
6 Newton iteration  3 done6 , maximum relative shift = 1.992e-03
6 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
1 did  comm_.min(converged) convergedRemote: 1
1 final convergedRemote: 1
1 Update: x^(k+1) = x^k - deltax^k 
1 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
1 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
1 Newton iteration  3 done1 , maximum relative shift = 1.992e-03
1 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
0 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
4 Solve: M deltax^k = r 
4 Newton::solveLinearSystem : entering the try section 3
4 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
4 solveLinearSystemImpl_, before converged = ls.solve 
4 7 amgbackend::solve isParallel 1
5 Solve: M deltax^k = r 
5 Newton::solveLinearSystem : entering the try section 3
5 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
5 solveLinearSystemImpl_, before converged = ls.solve 
5 7 amgbackend::solve isParallel 1
2 Solve: M deltax^k = r 
2 Newton::solveLinearSystem : entering the try section 3
2 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
2 solveLinearSystemImpl_, before converged = ls.solve 
2 7 amgbackend::solve isParallel 1
3 Solve: M deltax^k = r 
3 Newton::solveLinearSystem : entering the try section 3
3 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
3 solveLinearSystemImpl_, before converged = ls.solve 
3 7 amgbackend::solve isParallel 1
0 Solve: M deltax^k = r 
0 Newton::solveLinearSystem : entering the try section 3
0 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
0 solveLinearSystemImpl_, before converged = ls.solve 
0 7 amgbackend::solve isParallel 1
6 Solve: M deltax^k = r 
6 Newton::solveLinearSystem : entering the try section 3
6 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
6 solveLinearSystemImpl_, before converged = ls.solve 
6 7 amgbackend::solve isParallel 1
1 Solve: M deltax^k = r 
1 Newton::solveLinearSystem : entering the try section 3
1 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
1 solveLinearSystemImpl_, before converged = ls.solve 
1 7 amgbackend::solve isParallel 1
0 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
0 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 7
6 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
6 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
3 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
3 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
4 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
4 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
5 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
5 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
2 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
2 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
1 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
1 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 7
6 7 before Dune::BiCGSTABSolver 
6 Dune::BiCGSTABSolver.apply(x, b, result_) 
6 BiCGSTABSolver::apply 
6 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
3 7 before Dune::BiCGSTABSolver 
3 Dune::BiCGSTABSolver.apply(x, b, result_) 
3 BiCGSTABSolver::apply 
3 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
4 7 before Dune::BiCGSTABSolver 
4 Dune::BiCGSTABSolver.apply(x, b, result_) 
4 BiCGSTABSolver::apply 
4 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
5 7 before Dune::BiCGSTABSolver 
5 Dune::BiCGSTABSolver.apply(x, b, result_) 
5 BiCGSTABSolver::apply 
5 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 7
2 7 before Dune::BiCGSTABSolver 
2 Dune::BiCGSTABSolver.apply(x, b, result_) 
2 BiCGSTABSolver::apply 
2 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
1 7 before Dune::BiCGSTABSolver 
1 Dune::BiCGSTABSolver.apply(x, b, result_) 
1 BiCGSTABSolver::apply 
1 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
Using a direct coarse solver (SuperLU)
Building hierarchy of 1 levels (inclusive coarse solver) took 0.112287 seconds.
0 7 before Dune::BiCGSTABSolver 
0 Dune::BiCGSTABSolver.apply(x, b, result_) 
0 BiCGSTABSolver::apply 
0 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
0 === BiCGSTABSolver
 Iter          Defect            Rate
    0      6.86456e-13
0  (it = 0.5; it < _maxit; it+=.5) 0.5 250
0  before p = r 
0  before  y = W^-1 * p 
0  before _prec->apply(y,p); 0.5 250
0 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 7
0 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
1 === BiCGSTABSolver
 Iter          Defect            Rate
    0      6.86456e-13
1  (it = 0.5; it < _maxit; it+=.5) 0.5 250
1  before p = r 
1  before  y = W^-1 * p 
1  before _prec->apply(y,p); 0.5 250
1 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
2 === BiCGSTABSolver
 Iter          Defect            Rate
    0      6.86456e-13
2  (it = 0.5; it < _maxit; it+=.5) 0.5 250
2  before p = r 
2  before  y = W^-1 * p 
2  before _prec->apply(y,p); 0.5 250
2 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
3 === BiCGSTABSolver
 Iter          Defect            Rate
    0      6.86456e-13
3  (it = 0.5; it < _maxit; it+=.5) 0.5 250
3  before p = r 
3  before  y = W^-1 * p 
3  before _prec->apply(y,p); 0.5 250
3 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
6 === BiCGSTABSolver
 Iter          Defect            Rate
    0      6.86456e-13
6  (it = 0.5; it < _maxit; it+=.5) 0.5 250
6  before p = r 
6  before  y = W^-1 * p 
6  before _prec->apply(y,p); 0.5 250
6 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
5 === BiCGSTABSolver
 Iter          Defect            Rate
    0      6.86456e-13
5  (it = 0.5; it < _maxit; it+=.5) 0.5 250
5  before p = r 
5  before  y = W^-1 * p 
5  before _prec->apply(y,p); 0.5 250
5 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
4 === BiCGSTABSolver
 Iter          Defect            Rate
    0      6.86456e-13
4  (it = 0.5; it < _maxit; it+=.5) 0.5 250
4  before p = r 
4  before  y = W^-1 * p 
4  before _prec->apply(y,p); 0.5 250
4 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
1 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
2 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
2 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
3 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
3 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
3  before v = A * y 
6 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
6 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
6  before v = A * y 
5 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
5 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
5  before v = A * y 
4 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
4 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
4  before v = A * y 
6  before  alpha = rho_new / < rt, v > 
4  before  alpha = rho_new / < rt, v > 
5  before  alpha = rho_new / < rt, v > 
0 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
0 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
1 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
3  before  alpha = rho_new / < rt, v > 
5  before alpha = rho_new / h; 
5  before apply first correction to x 
5  before r = r - alpha*v 
5  before test stop criteria 
5 istlsolver::printOutput 
  0.5      3.15741e-30      4.59958e-18
5 === rate=2.11561e-35, T=0.122953, TIT=0.245907, IT=0.5
5 finished solveLinearSystemImpl_ 1
5 converged value: 1 7 1 let s go get convergedRemote
5 to  comm_.min(converged) 
0  before v = A * y 
0  before  alpha = rho_new / < rt, v > 
0  before alpha = rho_new / h; 
0  before apply first correction to x 
0  before r = r - alpha*v 
0  before test stop criteria 
0 istlsolver::printOutput 
  0.5      3.15741e-30      4.59958e-18
0 === rate=2.11561e-35, T=0.0186095, TIT=0.0372189, IT=0.5
0 finished solveLinearSystemImpl_ 1
0 converged value: 1 7 1 let s go get convergedRemote
0 to  comm_.min(converged) 
0 did  comm_.min(converged) convergedRemote: 1
0 final convergedRemote: 1
0 Update: x^(k+1) = x^k - deltax^k 
0 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
2  before v = A * y 
2  before  alpha = rho_new / < rt, v > 
2  before alpha = rho_new / h; 
2  before apply first correction to x 
2  before r = r - alpha*v 
2  before test stop criteria 
2 istlsolver::printOutput 
  0.5      3.15741e-30      4.59958e-18
2 === rate=2.11561e-35, T=0.122989, TIT=0.245979, IT=0.5
2 finished solveLinearSystemImpl_ 1
2 converged value: 1 7 1 let s go get convergedRemote
2 to  comm_.min(converged) 
2 did  comm_.min(converged) convergedRemote: 1
2 final convergedRemote: 1
2 Update: x^(k+1) = x^k - deltax^k 
2 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
2 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
2 Newton iteration  4 done2 , maximum relative shift = 2.288e-04
1  before v = A * y 
1  before  alpha = rho_new / < rt, v > 
1  before alpha = rho_new / h; 
1  before apply first correction to x 
1  before r = r - alpha*v 
1  before test stop criteria 
1 istlsolver::printOutput 
  0.5      3.15741e-30      4.59958e-18
1 === rate=2.11561e-35, T=0.122954, TIT=0.245909, IT=0.5
1 finished solveLinearSystemImpl_ 1
1 converged value: 1 7 1 let s go get convergedRemote
1 to  comm_.min(converged) 
1 did  comm_.min(converged) convergedRemote: 1
1 final convergedRemote: 1
6  before alpha = rho_new / h; 
6  before apply first correction to x 
6  before r = r - alpha*v 
6  before test stop criteria 
6 istlsolver::printOutput 
  0.5      3.15741e-30      4.59958e-18
6 === rate=2.11561e-35, T=0.122988, TIT=0.245975, IT=0.5
6 finished solveLinearSystemImpl_ 1
6 converged value: 1 7 1 let s go get convergedRemote
6 to  comm_.min(converged) 
6 did  comm_.min(converged) convergedRemote: 1
6 final convergedRemote: 1
6 Update: x^(k+1) = x^k - deltax^k 
6 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
6 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
6 Newton iteration  4 done6 , maximum relative shift = 2.288e-04
6 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
3  before alpha = rho_new / h; 
3  before apply first correction to x 
3  before r = r - alpha*v 
3  before test stop criteria 
3 istlsolver::printOutput 
  0.5      3.15741e-30      4.59958e-18
3 === rate=2.11561e-35, T=0.122955, TIT=0.24591, IT=0.5
3 finished solveLinearSystemImpl_ 1
3 converged value: 1 7 1 let s go get convergedRemote
3 to  comm_.min(converged) 
3 did  comm_.min(converged) convergedRemote: 1
3 final convergedRemote: 1
3 Update: x^(k+1) = x^k - deltax^k 
3 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
3 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
3 Newton iteration  4 done3 , maximum relative shift = 2.288e-04
3 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
4  before alpha = rho_new / h; 
4  before apply first correction to x 
4  before r = r - alpha*v 
4  before test stop criteria 
4 istlsolver::printOutput 
  0.5      3.15741e-30      4.59958e-18
4 === rate=2.11561e-35, T=0.122989, TIT=0.245979, IT=0.5
4 finished solveLinearSystemImpl_ 1
4 converged value: 1 7 1 let s go get convergedRemote
4 to  comm_.min(converged) 
4 did  comm_.min(converged) convergedRemote: 1
4 final convergedRemote: 1
4 Update: x^(k+1) = x^k - deltax^k 
4 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
4 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
4 Newton iteration  4 done4 , maximum relative shift = 2.288e-04
4 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
5 did  comm_.min(converged) convergedRemote: 1
5 final convergedRemote: 1
5 Update: x^(k+1) = x^k - deltax^k 
5 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
5 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
5 Newton iteration  4 done5 , maximum relative shift = 2.288e-04
5 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
0 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
0 Newton iteration  4 done0 , maximum relative shift = 2.288e-04
0 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
2 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
1 Update: x^(k+1) = x^k - deltax^k 
1 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
1 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
1 Newton iteration  4 done1 , maximum relative shift = 2.288e-04
1 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
3 Solve: M deltax^k = r 
3 Newton::solveLinearSystem : entering the try section 4
3 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
3 solveLinearSystemImpl_, before converged = ls.solve 
3 7 amgbackend::solve isParallel 1
6 Solve: M deltax^k = r 
6 Newton::solveLinearSystem : entering the try section 4
6 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
6 solveLinearSystemImpl_, before converged = ls.solve 
6 7 amgbackend::solve isParallel 1
4 Solve: M deltax^k = r 
4 Newton::solveLinearSystem : entering the try section 4
4 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
4 solveLinearSystemImpl_, before converged = ls.solve 
4 7 amgbackend::solve isParallel 1
5 Solve: M deltax^k = r 
5 Newton::solveLinearSystem : entering the try section 4
5 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
5 solveLinearSystemImpl_, before converged = ls.solve 
5 7 amgbackend::solve isParallel 1
0 Solve: M deltax^k = r 
0 Newton::solveLinearSystem : entering the try section 4
0 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
0 solveLinearSystemImpl_, before converged = ls.solve 
0 7 amgbackend::solve isParallel 1
2 Solve: M deltax^k = r 
2 Newton::solveLinearSystem : entering the try section 4
2 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
2 solveLinearSystemImpl_, before converged = ls.solve 
2 7 amgbackend::solve isParallel 1
1 Solve: M deltax^k = r 
1 Newton::solveLinearSystem : entering the try section 4
1 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
1 solveLinearSystemImpl_, before converged = ls.solve 
1 7 amgbackend::solve isParallel 1
1 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
1 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
2 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
2 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
3 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
3 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
6 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
6 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
4 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
4 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
5 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
5 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
0 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
0 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 7
3 7 before Dune::BiCGSTABSolver 
3 Dune::BiCGSTABSolver.apply(x, b, result_) 
3 BiCGSTABSolver::apply 
3 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
6 7 before Dune::BiCGSTABSolver 
6 Dune::BiCGSTABSolver.apply(x, b, result_) 
6 BiCGSTABSolver::apply 
6 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
4 7 before Dune::BiCGSTABSolver 
4 Dune::BiCGSTABSolver.apply(x, b, result_) 
4 BiCGSTABSolver::apply 
4 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
5 7 before Dune::BiCGSTABSolver 
5 Dune::BiCGSTABSolver.apply(x, b, result_) 
5 BiCGSTABSolver::apply 
5 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 7
1 7 before Dune::BiCGSTABSolver 
1 Dune::BiCGSTABSolver.apply(x, b, result_) 
1 BiCGSTABSolver::apply 
1 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
2 7 before Dune::BiCGSTABSolver 
2 Dune::BiCGSTABSolver.apply(x, b, result_) 
2 BiCGSTABSolver::apply 
2 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
Using a direct coarse solver (SuperLU)
Building hierarchy of 1 levels (inclusive coarse solver) took 0.112125 seconds.
0 7 before Dune::BiCGSTABSolver 
0 Dune::BiCGSTABSolver.apply(x, b, result_) 
0 BiCGSTABSolver::apply 
0 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
0 === BiCGSTABSolver
 Iter          Defect            Rate
    0      7.78551e-14
0  (it = 0.5; it < _maxit; it+=.5) 0.5 250
0  before p = r 
0  before  y = W^-1 * p 
0  before _prec->apply(y,p); 0.5 250
0 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 7
0 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
3 === BiCGSTABSolver
 Iter          Defect            Rate
    0      7.78551e-14
3  (it = 0.5; it < _maxit; it+=.5) 0.5 250
3  before p = r 
3  before  y = W^-1 * p 
3  before _prec->apply(y,p); 0.5 250
3 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
2 === BiCGSTABSolver
 Iter          Defect            Rate
    0      7.78551e-14
2  (it = 0.5; it < _maxit; it+=.5) 0.5 250
2  before p = r 
2  before  y = W^-1 * p 
2  before _prec->apply(y,p); 0.5 250
2 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
1 === BiCGSTABSolver
 Iter          Defect            Rate
    0      7.78551e-14
1  (it = 0.5; it < _maxit; it+=.5) 0.5 250
1  before p = r 
1  before  y = W^-1 * p 
1  before _prec->apply(y,p); 0.5 250
1 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
5 === BiCGSTABSolver
 Iter          Defect            Rate
    0      7.78551e-14
5  (it = 0.5; it < _maxit; it+=.5) 0.5 250
5  before p = r 
5  before  y = W^-1 * p 
5  before _prec->apply(y,p); 0.5 250
5 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
6 === BiCGSTABSolver
 Iter          Defect            Rate
    0      7.78551e-14
6  (it = 0.5; it < _maxit; it+=.5) 0.5 250
6  before p = r 
6  before  y = W^-1 * p 
6  before _prec->apply(y,p); 0.5 250
6 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
4 === BiCGSTABSolver
 Iter          Defect            Rate
    0      7.78551e-14
4  (it = 0.5; it < _maxit; it+=.5) 0.5 250
4  before p = r 
4  before  y = W^-1 * p 
4  before _prec->apply(y,p); 0.5 250
4 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
3 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
3 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
3  before v = A * y 
2 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
2 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
1 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
5 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
5 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
5  before v = A * y 
6 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
6 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
6  before v = A * y 
6  before  alpha = rho_new / < rt, v > 
4 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
4 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
4  before v = A * y 
4  before  alpha = rho_new / < rt, v > 
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
0 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
3  before  alpha = rho_new / < rt, v > 
2  before v = A * y 
1 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
5  before  alpha = rho_new / < rt, v > 
0 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
0  before v = A * y 
6  before alpha = rho_new / h; 
6  before apply first correction to x 
6  before r = r - alpha*v 
6  before test stop criteria 
6 istlsolver::printOutput 
  0.5      4.66525e-30      5.99222e-17
6 === rate=3.59066e-33, T=0.122777, TIT=0.245553, IT=0.5
6 finished solveLinearSystemImpl_ 1
6 converged value: 1 7 1 let s go get convergedRemote
6 to  comm_.min(converged) 
5  before alpha = rho_new / h; 
5  before apply first correction to x 
5  before r = r - alpha*v 
5  before test stop criteria 
5 istlsolver::printOutput 
  0.5      4.66525e-30      5.99222e-17
5 === rate=3.59066e-33, T=0.122816, TIT=0.245632, IT=0.5
5 finished solveLinearSystemImpl_ 1
5 converged value: 1 7 1 let s go get convergedRemote
5 to  comm_.min(converged) 
3  before alpha = rho_new / h; 
3  before apply first correction to x 
3  before r = r - alpha*v 
3  before test stop criteria 
3 istlsolver::printOutput 
  0.5      4.66525e-30      5.99222e-17
3 === rate=3.59066e-33, T=0.122817, TIT=0.245633, IT=0.5
3 finished solveLinearSystemImpl_ 1
3 converged value: 1 7 1 let s go get convergedRemote
3 to  comm_.min(converged) 
2  before  alpha = rho_new / < rt, v > 
2  before alpha = rho_new / h; 
2  before apply first correction to x 
2  before r = r - alpha*v 
2  before test stop criteria 
2 istlsolver::printOutput 
  0.5      4.66525e-30      5.99222e-17
2 === rate=3.59066e-33, T=0.122775, TIT=0.245551, IT=0.5
2 finished solveLinearSystemImpl_ 1
2 converged value: 1 7 1 let s go get convergedRemote
2 to  comm_.min(converged) 
1  before v = A * y 
1  before  alpha = rho_new / < rt, v > 
1  before alpha = rho_new / h; 
1  before apply first correction to x 
1  before r = r - alpha*v 
1  before test stop criteria 
1 istlsolver::printOutput 
  0.5      4.66525e-30      5.99222e-17
1 === rate=3.59066e-33, T=0.122816, TIT=0.245632, IT=0.5
1 finished solveLinearSystemImpl_ 1
1 converged value: 1 7 1 let s go get convergedRemote
1 to  comm_.min(converged) 
0  before  alpha = rho_new / < rt, v > 
0  before alpha = rho_new / h; 
0  before apply first correction to x 
0  before r = r - alpha*v 
0  before test stop criteria 
0 istlsolver::printOutput 
  0.5      4.66525e-30      5.99222e-17
0 === rate=3.59066e-33, T=0.0183384, TIT=0.0366769, IT=0.5
4  before alpha = rho_new / h; 
4  before apply first correction to x 
4  before r = r - alpha*v 
4  before test stop criteria 
4 istlsolver::printOutput 
  0.5      4.66525e-30      5.99222e-17
4 === rate=3.59066e-33, T=0.122777, TIT=0.245553, IT=0.5
4 finished solveLinearSystemImpl_ 1
4 converged value: 1 7 1 let s go get convergedRemote
4 to  comm_.min(converged) 
3 did  comm_.min(converged) convergedRemote: 1
3 final convergedRemote: 1
3 Update: x^(k+1) = x^k - deltax^k 
3 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
3 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
3 Newton iteration  5 done3 , maximum relative shift = 2.077e-05
3 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
2 did  comm_.min(converged) convergedRemote: 1
2 final convergedRemote: 1
2 Update: x^(k+1) = x^k - deltax^k 
2 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
2 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
2 Newton iteration  5 done2 , maximum relative shift = 2.077e-05
2 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
4 did  comm_.min(converged) convergedRemote: 1
4 final convergedRemote: 1
4 Update: x^(k+1) = x^k - deltax^k 
4 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
4 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
4 Newton iteration  5 done4 , maximum relative shift = 2.077e-05
4 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
0 finished solveLinearSystemImpl_ 1
0 converged value: 1 7 1 let s go get convergedRemote
0 to  comm_.min(converged) 
0 did  comm_.min(converged) convergedRemote: 1
0 final convergedRemote: 1
0 Update: x^(k+1) = x^k - deltax^k 
0 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
0 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
0 Newton iteration  5 done0 , maximum relative shift = 2.077e-05
0 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
1 did  comm_.min(converged) convergedRemote: 1
1 final convergedRemote: 1
1 Update: x^(k+1) = x^k - deltax^k 
1 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
1 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
1 Newton iteration  5 done1 , maximum relative shift = 2.077e-05
1 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
6 did  comm_.min(converged) convergedRemote: 1
6 final convergedRemote: 1
6 Update: x^(k+1) = x^k - deltax^k 
6 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
6 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
6 Newton iteration  5 done6 , maximum relative shift = 2.077e-05
6 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
5 did  comm_.min(converged) convergedRemote: 1
5 final convergedRemote: 1
5 Update: x^(k+1) = x^k - deltax^k 
5 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
5 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
5 Newton iteration  5 done5 , maximum relative shift = 2.077e-05
5 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
5 Solve: M deltax^k = r 
5 Newton::solveLinearSystem : entering the try section 5
5 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
5 solveLinearSystemImpl_, before converged = ls.solve 
5 7 amgbackend::solve isParallel 1
4 Solve: M deltax^k = r 
4 Newton::solveLinearSystem : entering the try section 5
4 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
4 solveLinearSystemImpl_, before converged = ls.solve 
4 7 amgbackend::solve isParallel 1
0 Solve: M deltax^k = r 
0 Newton::solveLinearSystem : entering the try section 5
0 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
0 solveLinearSystemImpl_, before converged = ls.solve 
0 7 amgbackend::solve isParallel 1
1 Solve: M deltax^k = r 
1 Newton::solveLinearSystem : entering the try section 5
1 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
1 solveLinearSystemImpl_, before converged = ls.solve 
1 7 amgbackend::solve isParallel 1
6 Solve: M deltax^k = r 
6 Newton::solveLinearSystem : entering the try section 5
6 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
6 solveLinearSystemImpl_, before converged = ls.solve 
6 7 amgbackend::solve isParallel 1
2 Solve: M deltax^k = r 
2 Newton::solveLinearSystem : entering the try section 5
2 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
2 solveLinearSystemImpl_, before converged = ls.solve 
2 7 amgbackend::solve isParallel 1
3 Solve: M deltax^k = r 
3 Newton::solveLinearSystem : entering the try section 5
3 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
3 solveLinearSystemImpl_, before converged = ls.solve 
3 7 amgbackend::solve isParallel 1
2 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
2 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
3 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
3 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
5 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
5 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
4 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
4 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
0 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
0 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 7
1 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
1 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
6 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
6 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 7
6 7 before Dune::BiCGSTABSolver 
6 Dune::BiCGSTABSolver.apply(x, b, result_) 
6 BiCGSTABSolver::apply 
6 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
1 7 before Dune::BiCGSTABSolver 
1 Dune::BiCGSTABSolver.apply(x, b, result_) 
1 BiCGSTABSolver::apply 
1 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
2 7 before Dune::BiCGSTABSolver 
2 Dune::BiCGSTABSolver.apply(x, b, result_) 
2 BiCGSTABSolver::apply 
2 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 7
3 7 before Dune::BiCGSTABSolver 
3 Dune::BiCGSTABSolver.apply(x, b, result_) 
3 BiCGSTABSolver::apply 
3 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
5 7 before Dune::BiCGSTABSolver 
5 Dune::BiCGSTABSolver.apply(x, b, result_) 
5 BiCGSTABSolver::apply 
5 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
4 7 before Dune::BiCGSTABSolver 
4 Dune::BiCGSTABSolver.apply(x, b, result_) 
4 BiCGSTABSolver::apply 
4 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
Using a direct coarse solver (SuperLU)
Building hierarchy of 1 levels (inclusive coarse solver) took 0.11142 seconds.
0 7 before Dune::BiCGSTABSolver 
0 Dune::BiCGSTABSolver.apply(x, b, result_) 
0 BiCGSTABSolver::apply 
0 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
4 === BiCGSTABSolver
 Iter          Defect            Rate
    0      8.47801e-15
4  (it = 0.5; it < _maxit; it+=.5) 0.5 250
4  before p = r 
4  before  y = W^-1 * p 
4  before _prec->apply(y,p); 0.5 250
4 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
3 === BiCGSTABSolver
 Iter          Defect            Rate
    0      8.47801e-15
3  (it = 0.5; it < _maxit; it+=.5) 0.5 250
3  before p = r 
3  before  y = W^-1 * p 
3  before _prec->apply(y,p); 0.5 250
3 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
5 === BiCGSTABSolver
 Iter          Defect            Rate
    0      8.47801e-15
5  (it = 0.5; it < _maxit; it+=.5) 0.5 250
5  before p = r 
5  before  y = W^-1 * p 
5  before _prec->apply(y,p); 0.5 250
5 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
2 === BiCGSTABSolver
 Iter          Defect            Rate
    0      8.47801e-15
2  (it = 0.5; it < _maxit; it+=.5) 0.5 250
2  before p = r 
2  before  y = W^-1 * p 
2  before _prec->apply(y,p); 0.5 250
2 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
6 === BiCGSTABSolver
 Iter          Defect            Rate
    0      8.47801e-15
6  (it = 0.5; it < _maxit; it+=.5) 0.5 250
6  before p = r 
6  before  y = W^-1 * p 
6  before _prec->apply(y,p); 0.5 250
6 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
1 === BiCGSTABSolver
 Iter          Defect            Rate
    0      8.47801e-15
1  (it = 0.5; it < _maxit; it+=.5) 0.5 250
1  before p = r 
1  before  y = W^-1 * p 
1  before _prec->apply(y,p); 0.5 250
1 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
0 === BiCGSTABSolver
 Iter          Defect            Rate
    0      8.47801e-15
0  (it = 0.5; it < _maxit; it+=.5) 0.5 250
0  before p = r 
0  before  y = W^-1 * p 
0  before _prec->apply(y,p); 0.5 250
0 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 7
0 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
1 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
4 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
4 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
4  before v = A * y 
3 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
3 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
3  before v = A * y 
5 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
5 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
5  before v = A * y 
2 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
2 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
6 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
6 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
6  before v = A * y 
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
0 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
5  before  alpha = rho_new / < rt, v > 
2  before v = A * y 
0 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
0  before v = A * y 
6  before  alpha = rho_new / < rt, v > 
1 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
4  before  alpha = rho_new / < rt, v > 
3  before  alpha = rho_new / < rt, v > 
3  before alpha = rho_new / h; 
3  before apply first correction to x 
3  before r = r - alpha*v 
3  before test stop criteria 
3 istlsolver::printOutput 
  0.5      1.24602e-30      1.46971e-16
3 === rate=2.16004e-32, T=0.122583, TIT=0.245166, IT=0.5
3 finished solveLinearSystemImpl_ 1
3 converged value: 1 7 1 let s go get convergedRemote
3 to  comm_.min(converged) 
0  before  alpha = rho_new / < rt, v > 
0  before alpha = rho_new / h; 
0  before apply first correction to x 
0  before r = r - alpha*v 
0  before test stop criteria 
0 istlsolver::printOutput 
  0.5      1.24602e-30      1.46971e-16
0 === rate=2.16004e-32, T=0.0185584, TIT=0.0371169, IT=0.5
6  before alpha = rho_new / h; 
6  before apply first correction to x 
6  before r = r - alpha*v 
6  before test stop criteria 
6 istlsolver::printOutput 
  0.5      1.24602e-30      1.46971e-16
6 === rate=2.16004e-32, T=0.122432, TIT=0.244863, IT=0.5
6 finished solveLinearSystemImpl_ 1
6 converged value: 1 7 1 let s go get convergedRemote
6 to  comm_.min(converged) 
1  before v = A * y 
1  before  alpha = rho_new / < rt, v > 
1  before alpha = rho_new / h; 
1  before apply first correction to x 
1  before r = r - alpha*v 
1  before test stop criteria 
1 istlsolver::printOutput 
  0.5      1.24602e-30      1.46971e-16
1 === rate=2.16004e-32, T=0.122586, TIT=0.245171, IT=0.5
1 finished solveLinearSystemImpl_ 1
1 converged value: 1 7 1 let s go get convergedRemote
1 to  comm_.min(converged) 
4  before alpha = rho_new / h; 
4  before apply first correction to x 
4  before r = r - alpha*v 
4  before test stop criteria 
4 istlsolver::printOutput 
  0.5      1.24602e-30      1.46971e-16
4 === rate=2.16004e-32, T=0.122435, TIT=0.24487, IT=0.5
4 finished solveLinearSystemImpl_ 1
4 converged value: 1 7 1 let s go get convergedRemote
4 to  comm_.min(converged) 
2  before  alpha = rho_new / < rt, v > 
2  before alpha = rho_new / h; 
2  before apply first correction to x 
2  before r = r - alpha*v 
2  before test stop criteria 
2 istlsolver::printOutput 
  0.5      1.24602e-30      1.46971e-16
2 === rate=2.16004e-32, T=0.122437, TIT=0.244875, IT=0.5
2 finished solveLinearSystemImpl_ 1
2 converged value: 1 7 1 let s go get convergedRemote
2 to  comm_.min(converged) 
5  before alpha = rho_new / h; 
5  before apply first correction to x 
5  before r = r - alpha*v 
5  before test stop criteria 
5 istlsolver::printOutput 
  0.5      1.24602e-30      1.46971e-16
5 === rate=2.16004e-32, T=0.122582, TIT=0.245164, IT=0.5
5 finished solveLinearSystemImpl_ 1
5 converged value: 1 7 1 let s go get convergedRemote
5 to  comm_.min(converged) 
2 did  comm_.min(converged) convergedRemote: 1
2 final convergedRemote: 1
2 Update: x^(k+1) = x^k - deltax^k 
2 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
2 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
2 Newton iteration  6 done2 , maximum relative shift = 2.262e-06
2 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
5 did  comm_.min(converged) convergedRemote: 1
5 final convergedRemote: 1
5 Update: x^(k+1) = x^k - deltax^k 
5 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
5 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
5 Newton iteration  6 done5 , maximum relative shift = 2.262e-06
5 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
3 did  comm_.min(converged) convergedRemote: 1
3 final convergedRemote: 1
3 Update: x^(k+1) = x^k - deltax^k 
3 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
3 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
3 Newton iteration  6 done3 , maximum relative shift = 2.262e-06
3 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
0 finished solveLinearSystemImpl_ 1
0 converged value: 1 7 1 let s go get convergedRemote
0 to  comm_.min(converged) 
0 did  comm_.min(converged) convergedRemote: 1
0 final convergedRemote: 1
0 Update: x^(k+1) = x^k - deltax^k 
0 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
0 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
0 Newton iteration  6 done0 , maximum relative shift = 2.262e-06
0 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
6 did  comm_.min(converged) convergedRemote: 1
6 final convergedRemote: 1
6 Update: x^(k+1) = x^k - deltax^k 
6 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
6 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
6 Newton iteration  6 done6 , maximum relative shift = 2.262e-06
6 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
1 did  comm_.min(converged) convergedRemote: 1
1 final convergedRemote: 1
1 Update: x^(k+1) = x^k - deltax^k 
1 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
1 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
1 Newton iteration  6 done1 , maximum relative shift = 2.262e-06
1 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
4 did  comm_.min(converged) convergedRemote: 1
4 final convergedRemote: 1
4 Update: x^(k+1) = x^k - deltax^k 
4 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
4 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
4 Newton iteration  6 done4 , maximum relative shift = 2.262e-06
4 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
2 Solve: M deltax^k = r 
2 Newton::solveLinearSystem : entering the try section 6
2 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
2 solveLinearSystemImpl_, before converged = ls.solve 
2 7 amgbackend::solve isParallel 1
4 Solve: M deltax^k = r 
4 Newton::solveLinearSystem : entering the try section 6
4 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
4 solveLinearSystemImpl_, before converged = ls.solve 
4 7 amgbackend::solve isParallel 1
5 Solve: M deltax^k = r 
5 Newton::solveLinearSystem : entering the try section 6
5 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
5 solveLinearSystemImpl_, before converged = ls.solve 
5 7 amgbackend::solve isParallel 1
3 Solve: M deltax^k = r 
3 Newton::solveLinearSystem : entering the try section 6
3 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
3 solveLinearSystemImpl_, before converged = ls.solve 
3 7 amgbackend::solve isParallel 1
0 Solve: M deltax^k = r 
0 Newton::solveLinearSystem : entering the try section 6
0 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
0 solveLinearSystemImpl_, before converged = ls.solve 
0 7 amgbackend::solve isParallel 1
6 Solve: M deltax^k = r 
6 Newton::solveLinearSystem : entering the try section 6
6 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
6 solveLinearSystemImpl_, before converged = ls.solve 
6 7 amgbackend::solve isParallel 1
1 Solve: M deltax^k = r 
1 Newton::solveLinearSystem : entering the try section 6
1 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
1 solveLinearSystemImpl_, before converged = ls.solve 
1 7 amgbackend::solve isParallel 1
4 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
4 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
5 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
5 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
3 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
3 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
0 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
0 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 7
6 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
6 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
2 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
2 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
1 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
1 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 7
5 7 before Dune::BiCGSTABSolver 
5 Dune::BiCGSTABSolver.apply(x, b, result_) 
5 BiCGSTABSolver::apply 
5 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
3 7 before Dune::BiCGSTABSolver 
3 Dune::BiCGSTABSolver.apply(x, b, result_) 
3 BiCGSTABSolver::apply 
3 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
1 7 before Dune::BiCGSTABSolver 
1 Dune::BiCGSTABSolver.apply(x, b, result_) 
1 BiCGSTABSolver::apply 
1 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
6 7 before Dune::BiCGSTABSolver 
6 Dune::BiCGSTABSolver.apply(x, b, result_) 
6 BiCGSTABSolver::apply 
6 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 7
2 7 before Dune::BiCGSTABSolver 
2 Dune::BiCGSTABSolver.apply(x, b, result_) 
2 BiCGSTABSolver::apply 
2 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
4 7 before Dune::BiCGSTABSolver 
4 Dune::BiCGSTABSolver.apply(x, b, result_) 
4 BiCGSTABSolver::apply 
4 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
Using a direct coarse solver (SuperLU)
Building hierarchy of 1 levels (inclusive coarse solver) took 0.113375 seconds.
0 7 before Dune::BiCGSTABSolver 
0 Dune::BiCGSTABSolver.apply(x, b, result_) 
0 BiCGSTABSolver::apply 
0 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
0 === BiCGSTABSolver
 Iter          Defect            Rate
    0      9.23317e-16
0  (it = 0.5; it < _maxit; it+=.5) 0.5 250
6 === BiCGSTABSolver
 Iter          Defect            Rate
    0      9.23317e-16
6  (it = 0.5; it < _maxit; it+=.5) 0.5 250
6  before p = r 
6  before  y = W^-1 * p 
6  before _prec->apply(y,p); 0.5 250
6 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
5 === BiCGSTABSolver
 Iter          Defect            Rate
    0      9.23317e-16
5  (it = 0.5; it < _maxit; it+=.5) 0.5 250
5  before p = r 
5  before  y = W^-1 * p 
5  before _prec->apply(y,p); 0.5 250
5 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
3 === BiCGSTABSolver
 Iter          Defect            Rate
    0      9.23317e-16
3  (it = 0.5; it < _maxit; it+=.5) 0.5 250
3  before p = r 
3  before  y = W^-1 * p 
3  before _prec->apply(y,p); 0.5 250
3 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
1 === BiCGSTABSolver
 Iter          Defect            Rate
    0      9.23317e-16
1  (it = 0.5; it < _maxit; it+=.5) 0.5 250
1  before p = r 
1  before  y = W^-1 * p 
1  before _prec->apply(y,p); 0.5 250
1 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
0  before p = r 
0  before  y = W^-1 * p 
0  before _prec->apply(y,p); 0.5 250
0 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 7
0 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
4 === BiCGSTABSolver
 Iter          Defect            Rate
    0      9.23317e-16
4  (it = 0.5; it < _maxit; it+=.5) 0.5 250
4  before p = r 
4  before  y = W^-1 * p 
4  before _prec->apply(y,p); 0.5 250
4 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
2 === BiCGSTABSolver
 Iter          Defect            Rate
    0      9.23317e-16
2  (it = 0.5; it < _maxit; it+=.5) 0.5 250
2  before p = r 
2  before  y = W^-1 * p 
2  before _prec->apply(y,p); 0.5 250
2 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
6 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
6 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
6  before v = A * y 
5 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
5 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
5  before v = A * y 
3 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
3 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
3  before v = A * y 
2 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
2 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
0 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
4 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
4 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
4  before v = A * y 
1 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
0 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
4  before  alpha = rho_new / < rt, v > 
3  before  alpha = rho_new / < rt, v > 
6  before  alpha = rho_new / < rt, v > 
5  before  alpha = rho_new / < rt, v > 
1 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
4  before alpha = rho_new / h; 
4  before apply first correction to x 
4  before r = r - alpha*v 
4  before test stop criteria 
4 istlsolver::printOutput 
  0.5      1.56232e-32      1.69207e-17
4 === rate=2.86312e-34, T=0.123741, TIT=0.247482, IT=0.5
4 finished solveLinearSystemImpl_ 1
4 converged value: 1 7 1 let s go get convergedRemote
4 to  comm_.min(converged) 
3  before alpha = rho_new / h; 
3  before apply first correction to x 
3  before r = r - alpha*v 
3  before test stop criteria 
3 istlsolver::printOutput 
  0.5      1.56232e-32      1.69207e-17
3 === rate=2.86312e-34, T=0.123784, TIT=0.247568, IT=0.5
3 finished solveLinearSystemImpl_ 1
3 converged value: 1 7 1 let s go get convergedRemote
3 to  comm_.min(converged) 
6  before alpha = rho_new / h; 
6  before apply first correction to x 
6  before r = r - alpha*v 
6  before test stop criteria 
6 istlsolver::printOutput 
  0.5      1.56232e-32      1.69207e-17
6 === rate=2.86312e-34, T=0.123741, TIT=0.247481, IT=0.5
6 finished solveLinearSystemImpl_ 1
6 converged value: 1 7 1 let s go get convergedRemote
6 to  comm_.min(converged) 
1  before v = A * y 
1  before  alpha = rho_new / < rt, v > 
1  before alpha = rho_new / h; 
1  before apply first correction to x 
1  before r = r - alpha*v 
1  before test stop criteria 
1 istlsolver::printOutput 
  0.5      1.56232e-32      1.69207e-17
1 === rate=2.86312e-34, T=0.123784, TIT=0.247568, IT=0.5
1 finished solveLinearSystemImpl_ 1
1 converged value: 1 7 1 let s go get convergedRemote
1 to  comm_.min(converged) 
2  before v = A * y 
2  before  alpha = rho_new / < rt, v > 
2  before alpha = rho_new / h; 
2  before apply first correction to x 
2  before r = r - alpha*v 
2  before test stop criteria 
2 istlsolver::printOutput 
  0.5      1.56232e-32      1.69207e-17
2 === rate=2.86312e-34, T=0.123741, TIT=0.247483, IT=0.5
2 finished solveLinearSystemImpl_ 1
2 converged value: 1 7 1 let s go get convergedRemote
2 to  comm_.min(converged) 
5  before alpha = rho_new / h; 
5  before apply first correction to x 
5  before r = r - alpha*v 
5  before test stop criteria 
5 istlsolver::printOutput 
  0.5      1.56232e-32      1.69207e-17
5 === rate=2.86312e-34, T=0.123795, TIT=0.247589, IT=0.5
5 finished solveLinearSystemImpl_ 1
5 converged value: 1 7 1 let s go get convergedRemote
5 to  comm_.min(converged) 
0  before v = A * y 
0  before  alpha = rho_new / < rt, v > 
0  before alpha = rho_new / h; 
0  before apply first correction to x 
0  before r = r - alpha*v 
0  before test stop criteria 
0 istlsolver::printOutput 
  0.5      1.56232e-32      1.69207e-17
0 === rate=2.86312e-34, T=0.0182151, TIT=0.0364301, IT=0.5
5 did  comm_.min(converged) convergedRemote: 1
5 final convergedRemote: 1
5 Update: x^(k+1) = x^k - deltax^k 
5 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
5 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
5 Newton iteration  7 done5 , maximum relative shift = 2.463e-07
5 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
6 did  comm_.min(converged) convergedRemote: 1
6 final convergedRemote: 1
6 Update: x^(k+1) = x^k - deltax^k 
6 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
6 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
6 Newton iteration  7 done6 , maximum relative shift = 2.463e-07
6 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
4 did  comm_.min(converged) convergedRemote: 1
4 final convergedRemote: 1
4 Update: x^(k+1) = x^k - deltax^k 
4 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
4 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
4 Newton iteration  7 done4 , maximum relative shift = 2.463e-07
4 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
3 did  comm_.min(converged) convergedRemote: 1
3 final convergedRemote: 1
3 Update: x^(k+1) = x^k - deltax^k 
3 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
3 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
3 Newton iteration  7 done3 , maximum relative shift = 2.463e-07
3 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
0 finished solveLinearSystemImpl_ 1
0 converged value: 1 7 1 let s go get convergedRemote
0 to  comm_.min(converged) 
0 did  comm_.min(converged) convergedRemote: 1
0 final convergedRemote: 1
0 Update: x^(k+1) = x^k - deltax^k 
0 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
0 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
0 Newton iteration  7 done0 , maximum relative shift = 2.463e-07
0 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
1 did  comm_.min(converged) convergedRemote: 1
1 final convergedRemote: 1
1 Update: x^(k+1) = x^k - deltax^k 
1 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
1 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
1 Newton iteration  7 done1 , maximum relative shift = 2.463e-07
1 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
2 did  comm_.min(converged) convergedRemote: 1
2 final convergedRemote: 1
2 Update: x^(k+1) = x^k - deltax^k 
2 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
2 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
2 Newton iteration  7 done2 , maximum relative shift = 2.463e-07
2 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
0 Solve: M deltax^k = r 
0 Newton::solveLinearSystem : entering the try section 7
0 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
0 solveLinearSystemImpl_, before converged = ls.solve 
0 7 amgbackend::solve isParallel 1
1 Solve: M deltax^k = r 
1 Newton::solveLinearSystem : entering the try section 7
1 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
1 solveLinearSystemImpl_, before converged = ls.solve 
1 7 amgbackend::solve isParallel 1
3 Solve: M deltax^k = r 
3 Newton::solveLinearSystem : entering the try section 7
3 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
3 solveLinearSystemImpl_, before converged = ls.solve 
3 7 amgbackend::solve isParallel 1
5 Solve: M deltax^k = r 
5 Newton::solveLinearSystem : entering the try section 7
5 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
5 solveLinearSystemImpl_, before converged = ls.solve 
5 7 amgbackend::solve isParallel 1
6 Solve: M deltax^k = r 
6 Newton::solveLinearSystem : entering the try section 7
6 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
6 solveLinearSystemImpl_, before converged = ls.solve 
6 7 amgbackend::solve isParallel 1
4 Solve: M deltax^k = r 
4 Newton::solveLinearSystem : entering the try section 7
4 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
4 solveLinearSystemImpl_, before converged = ls.solve 
4 7 amgbackend::solve isParallel 1
2 Solve: M deltax^k = r 
2 Newton::solveLinearSystem : entering the try section 7
2 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
2 solveLinearSystemImpl_, before converged = ls.solve 
2 7 amgbackend::solve isParallel 1
5 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
5 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
6 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
6 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
2 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
2 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
4 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
4 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
0 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
0 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 7
1 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
1 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
3 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
3 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 7
2 7 before Dune::BiCGSTABSolver 
2 Dune::BiCGSTABSolver.apply(x, b, result_) 
2 BiCGSTABSolver::apply 
2 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
4 7 before Dune::BiCGSTABSolver 
4 Dune::BiCGSTABSolver.apply(x, b, result_) 
4 BiCGSTABSolver::apply 
4 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
3 7 before Dune::BiCGSTABSolver 
3 Dune::BiCGSTABSolver.apply(x, b, result_) 
3 BiCGSTABSolver::apply 
3 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
1 7 before Dune::BiCGSTABSolver 
1 Dune::BiCGSTABSolver.apply(x, b, result_) 
1 BiCGSTABSolver::apply 
1 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
6 7 before Dune::BiCGSTABSolver 
6 Dune::BiCGSTABSolver.apply(x, b, result_) 
6 BiCGSTABSolver::apply 
6 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
5 7 before Dune::BiCGSTABSolver 
5 Dune::BiCGSTABSolver.apply(x, b, result_) 
5 BiCGSTABSolver::apply 
5 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 7
Using a direct coarse solver (SuperLU)
Building hierarchy of 1 levels (inclusive coarse solver) took 0.112171 seconds.
0 7 before Dune::BiCGSTABSolver 
0 Dune::BiCGSTABSolver.apply(x, b, result_) 
0 BiCGSTABSolver::apply 
0 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
5 === BiCGSTABSolver
 Iter          Defect            Rate
    0      1.00495e-16
5  (it = 0.5; it < _maxit; it+=.5) 0.5 250
5  before p = r 
5  before  y = W^-1 * p 
5  before _prec->apply(y,p); 0.5 250
5 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
2 === BiCGSTABSolver
 Iter          Defect            Rate
    0      1.00495e-16
2  (it = 0.5; it < _maxit; it+=.5) 0.5 250
2  before p = r 
2  before  y = W^-1 * p 
2  before _prec->apply(y,p); 0.5 250
2 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
4 === BiCGSTABSolver
 Iter          Defect            Rate
    0      1.00495e-16
4  (it = 0.5; it < _maxit; it+=.5) 0.5 250
4  before p = r 
4  before  y = W^-1 * p 
4  before _prec->apply(y,p); 0.5 250
4 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
3 === BiCGSTABSolver
 Iter          Defect            Rate
    0      1.00495e-16
3  (it = 0.5; it < _maxit; it+=.5) 0.5 250
3  before p = r 
3  before  y = W^-1 * p 
3  before _prec->apply(y,p); 0.5 250
3 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
1 === BiCGSTABSolver
 Iter          Defect            Rate
    0      1.00495e-16
1  (it = 0.5; it < _maxit; it+=.5) 0.5 250
1  before p = r 
1  before  y = W^-1 * p 
1  before _prec->apply(y,p); 0.5 250
1 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
6 === BiCGSTABSolver
 Iter          Defect            Rate
    0      1.00495e-16
6  (it = 0.5; it < _maxit; it+=.5) 0.5 250
6  before p = r 
6  before  y = W^-1 * p 
6  before _prec->apply(y,p); 0.5 250
6 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
0 === BiCGSTABSolver
 Iter          Defect            Rate
    0      1.00495e-16
0  (it = 0.5; it < _maxit; it+=.5) 0.5 250
0  before p = r 
0  before  y = W^-1 * p 
0  before _prec->apply(y,p); 0.5 250
0 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 7
0 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
0 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
6 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
6 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
6  before v = A * y 
5 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
5 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
5  before v = A * y 
2 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
2 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
4 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
4 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
4  before v = A * y 
3 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
3 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
3  before v = A * y 
1 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
4  before  alpha = rho_new / < rt, v > 
3  before  alpha = rho_new / < rt, v > 
1 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
0 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
6  before  alpha = rho_new / < rt, v > 
5  before  alpha = rho_new / < rt, v > 
3  before alpha = rho_new / h; 
3  before apply first correction to x 
3  before r = r - alpha*v 
3  before test stop criteria 
3 istlsolver::printOutput 
  0.5      9.99154e-33      9.94237e-17
3 === rate=9.88507e-33, T=0.122908, TIT=0.245816, IT=0.5
3 finished solveLinearSystemImpl_ 1
3 converged value: 1 7 1 let s go get convergedRemote
3 to  comm_.min(converged) 
4  before alpha = rho_new / h; 
4  before apply first correction to x 
4  before r = r - alpha*v 
4  before test stop criteria 
4 istlsolver::printOutput 
  0.5      9.99154e-33      9.94237e-17
4 === rate=9.88507e-33, T=0.122846, TIT=0.245692, IT=0.5
4 finished solveLinearSystemImpl_ 1
4 converged value: 1 7 1 let s go get convergedRemote
4 to  comm_.min(converged) 
5  before alpha = rho_new / h; 
5  before apply first correction to x 
5  before r = r - alpha*v 
5  before test stop criteria 
5 istlsolver::printOutput 
  0.5      9.99154e-33      9.94237e-17
5 === rate=9.88507e-33, T=0.122906, TIT=0.245812, IT=0.5
5 finished solveLinearSystemImpl_ 1
5 converged value: 1 7 1 let s go get convergedRemote
5 to  comm_.min(converged) 
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
1  before v = A * y 
1  before  alpha = rho_new / < rt, v > 
1  before alpha = rho_new / h; 
1  before apply first correction to x 
1  before r = r - alpha*v 
1  before test stop criteria 
1 istlsolver::printOutput 
  0.5      9.99154e-33      9.94237e-17
1 === rate=9.88507e-33, T=0.122906, TIT=0.245813, IT=0.5
1 finished solveLinearSystemImpl_ 1
1 converged value: 1 7 1 let s go get convergedRemote
1 to  comm_.min(converged) 
0  before v = A * y 
0  before  alpha = rho_new / < rt, v > 
0  before alpha = rho_new / h; 
0  before apply first correction to x 
0  before r = r - alpha*v 
0  before test stop criteria 
0 istlsolver::printOutput 
  0.5      9.99154e-33      9.94237e-17
0 === rate=9.88507e-33, T=0.0184678, TIT=0.0369357, IT=0.5
6  before alpha = rho_new / h; 
6  before apply first correction to x 
6  before r = r - alpha*v 
6  before test stop criteria 
6 istlsolver::printOutput 
  0.5      9.99154e-33      9.94237e-17
6 === rate=9.88507e-33, T=0.122847, TIT=0.245694, IT=0.5
6 finished solveLinearSystemImpl_ 1
6 converged value: 1 7 1 let s go get convergedRemote
6 to  comm_.min(converged) 
2  before v = A * y 
2  before  alpha = rho_new / < rt, v > 
2  before alpha = rho_new / h; 
2  before apply first correction to x 
2  before r = r - alpha*v 
2  before test stop criteria 
2 istlsolver::printOutput 
  0.5      9.99154e-33      9.94237e-17
2 === rate=9.88507e-33, T=0.122846, TIT=0.245692, IT=0.5
2 finished solveLinearSystemImpl_ 1
2 converged value: 1 7 1 let s go get convergedRemote
2 to  comm_.min(converged) 
5 did  comm_.min(converged) convergedRemote: 1
5 final convergedRemote: 1
5 Update: x^(k+1) = x^k - deltax^k 
5 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
5 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
5 Newton iteration  8 done5 , maximum relative shift = 2.681e-08
5 Assemble/solve/update time: 0.352834(25.0648%)/1.05457(74.9155%)/0.000277283(0.0196978%)
5 gridVariables->advanceTimeStep
5 before, nonLinearSolver->suggestTimeStepSize, current time: 654.901 simTime 57600
5 before assembler->setPreviousSolution, ddt: 18.8659
5 nonLinearSolver->solve
5 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
1 did  comm_.min(converged) convergedRemote: 1
1 final convergedRemote: 1
1 Update: x^(k+1) = x^k - deltax^k 
1 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
1 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
1 Newton iteration  8 done1 , maximum relative shift = 2.681e-08
1 Assemble/solve/update time: 0.352822(25.064%)/1.05458(74.9161%)/0.000279563(0.0198598%)
1 gridVariables->advanceTimeStep
1 before, nonLinearSolver->suggestTimeStepSize, current time: 654.901 simTime 57600
1 before assembler->setPreviousSolution, ddt: 18.8659
1 nonLinearSolver->solve
1 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
0 finished solveLinearSystemImpl_ 1
0 converged value: 1 7 1 let s go get convergedRemote
0 to  comm_.min(converged) 
0 did  comm_.min(converged) convergedRemote: 1
0 final convergedRemote: 1
0 Update: x^(k+1) = x^k - deltax^k 
0 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
0 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
0 Newton iteration  8 done0 , maximum relative shift = 2.681e-08
0 Assemble/solve/update time: 0.351928(25.017%)/1.05467(74.9713%)/0.000164411(0.0116872%)
0 gridVariables->advanceTimeStep
0 before, nonLinearSolver->suggestTimeStepSize, current time: 654.901 simTime 57600
0 before assembler->setPreviousSolution, ddt: 18.8659
0 nonLinearSolver->solve
0 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
6 did  comm_.min(converged) convergedRemote: 1
6 final convergedRemote: 1
6 Update: x^(k+1) = x^k - deltax^k 
6 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
6 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
6 Newton iteration  8 done6 , maximum relative shift = 2.681e-08
6 Assemble/solve/update time: 0.352745(25.0605%)/1.05471(74.9312%)/0.000117591(0.00835416%)
6 gridVariables->advanceTimeStep
6 before, nonLinearSolver->suggestTimeStepSize, current time: 654.901 simTime 57600
6 before assembler->setPreviousSolution, ddt: 18.8659
6 nonLinearSolver->solve
6 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
4 did  comm_.min(converged) convergedRemote: 1
4 final convergedRemote: 1
4 Update: x^(k+1) = x^k - deltax^k 
4 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
4 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
4 Newton iteration  8 done4 , maximum relative shift = 2.681e-08
4 Assemble/solve/update time: 0.352502(25.0476%)/1.05469(74.9425%)/0.000139191(0.00989046%)
4 gridVariables->advanceTimeStep
4 before, nonLinearSolver->suggestTimeStepSize, current time: 654.901 simTime 57600
4 before assembler->setPreviousSolution, ddt: 18.8659
4 nonLinearSolver->solve
4 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
3 did  comm_.min(converged) convergedRemote: 1
3 final convergedRemote: 1
3 Update: x^(k+1) = x^k - deltax^k 
3 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
3 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
3 Newton iteration  8 done3 , maximum relative shift = 2.681e-08
3 Assemble/solve/update time: 0.35279(25.0618%)/1.0546(74.9177%)/0.000287673(0.020436%)
3 gridVariables->advanceTimeStep
3 before, nonLinearSolver->suggestTimeStepSize, current time: 654.901 simTime 57600
3 before assembler->setPreviousSolution, ddt: 18.8659
3 nonLinearSolver->solve
3 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
2 did  comm_.min(converged) convergedRemote: 1
2 final convergedRemote: 1
2 Update: x^(k+1) = x^k - deltax^k 
2 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
2 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
2 Newton iteration  8 done2 , maximum relative shift = 2.681e-08
2 Assemble/solve/update time: 0.352705(25.0576%)/1.05471(74.9306%)/0.000166522(0.0118304%)
2 gridVariables->advanceTimeStep
2 before, nonLinearSolver->suggestTimeStepSize, current time: 654.901 simTime 57600
2 before assembler->setPreviousSolution, ddt: 18.8659
2 nonLinearSolver->solve
2 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
2 Solve: M deltax^k = r 
2 Newton::solveLinearSystem : entering the try section 0
2to  comm_.sum(norm2) 
2did  comm_.sum(norm2) 
2 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
2 solveLinearSystemImpl_, before converged = ls.solve 
2 7 amgbackend::solve isParallel 1
3 Solve: M deltax^k = r 
3 Newton::solveLinearSystem : entering the try section 0
3to  comm_.sum(norm2) 
3did  comm_.sum(norm2) 
3 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
3 solveLinearSystemImpl_, before converged = ls.solve 
3 7 amgbackend::solve isParallel 1
5 Solve: M deltax^k = r 
5 Newton::solveLinearSystem : entering the try section 0
5to  comm_.sum(norm2) 
5did  comm_.sum(norm2) 
5 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
5 solveLinearSystemImpl_, before converged = ls.solve 
5 7 amgbackend::solve isParallel 1
1 Solve: M deltax^k = r 
1 Newton::solveLinearSystem : entering the try section 0
1to  comm_.sum(norm2) 
1did  comm_.sum(norm2) 
1 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
1 solveLinearSystemImpl_, before converged = ls.solve 
1 7 amgbackend::solve isParallel 1
0 Solve: M deltax^k = r 
0 Newton::solveLinearSystem : entering the try section 0
0to  comm_.sum(norm2) 
0did  comm_.sum(norm2) 
0 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
0 solveLinearSystemImpl_, before converged = ls.solve 
0 7 amgbackend::solve isParallel 1
6 Solve: M deltax^k = r 
6 Newton::solveLinearSystem : entering the try section 0
6to  comm_.sum(norm2) 
6did  comm_.sum(norm2) 
6 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
6 solveLinearSystemImpl_, before converged = ls.solve 
6 7 amgbackend::solve isParallel 1
4 Solve: M deltax^k = r 
4 Newton::solveLinearSystem : entering the try section 0
4to  comm_.sum(norm2) 
4did  comm_.sum(norm2) 
4 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
4 solveLinearSystemImpl_, before converged = ls.solve 
4 7 amgbackend::solve isParallel 1
4 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
4 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
2 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
2 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
3 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
3 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
5 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
5 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
1 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
1 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
0 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
0 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 7
6 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
6 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 7
6 7 before Dune::BiCGSTABSolver 
6 Dune::BiCGSTABSolver.apply(x, b, result_) 
6 BiCGSTABSolver::apply 
6 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 7
4 7 before Dune::BiCGSTABSolver 
4 Dune::BiCGSTABSolver.apply(x, b, result_) 
4 BiCGSTABSolver::apply 
4 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
2 7 before Dune::BiCGSTABSolver 
2 Dune::BiCGSTABSolver.apply(x, b, result_) 
2 BiCGSTABSolver::apply 
2 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
3 7 before Dune::BiCGSTABSolver 
3 Dune::BiCGSTABSolver.apply(x, b, result_) 
3 BiCGSTABSolver::apply 
3 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
5 7 before Dune::BiCGSTABSolver 
5 Dune::BiCGSTABSolver.apply(x, b, result_) 
5 BiCGSTABSolver::apply 
5 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
1 7 before Dune::BiCGSTABSolver 
1 Dune::BiCGSTABSolver.apply(x, b, result_) 
1 BiCGSTABSolver::apply 
1 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
Using a direct coarse solver (SuperLU)
Building hierarchy of 1 levels (inclusive coarse solver) took 0.112604 seconds.
0 7 before Dune::BiCGSTABSolver 
0 Dune::BiCGSTABSolver.apply(x, b, result_) 
0 BiCGSTABSolver::apply 
0 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
1 === BiCGSTABSolver
 Iter          Defect            Rate
    0      3.16197e-09
1  (it = 0.5; it < _maxit; it+=.5) 0.5 250
1  before p = r 
1  before  y = W^-1 * p 
1  before _prec->apply(y,p); 0.5 250
1 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
4 === BiCGSTABSolver
 Iter          Defect            Rate
    0      3.16197e-09
4  (it = 0.5; it < _maxit; it+=.5) 0.5 250
4  before p = r 
4  before  y = W^-1 * p 
4  before _prec->apply(y,p); 0.5 250
4 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
2 === BiCGSTABSolver
 Iter          Defect            Rate
    0      3.16197e-09
2  (it = 0.5; it < _maxit; it+=.5) 0.5 250
2  before p = r 
2  before  y = W^-1 * p 
2  before _prec->apply(y,p); 0.5 250
2 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
3 === BiCGSTABSolver
 Iter          Defect            Rate
    0      3.16197e-09
3  (it = 0.5; it < _maxit; it+=.5) 0.5 250
3  before p = r 
3  before  y = W^-1 * p 
3  before _prec->apply(y,p); 0.5 250
3 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
5 === BiCGSTABSolver
 Iter          Defect            Rate
    0      3.16197e-09
5  (it = 0.5; it < _maxit; it+=.5) 0.5 250
5  before p = r 
5  before  y = W^-1 * p 
5  before _prec->apply(y,p); 0.5 250
5 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
6 === BiCGSTABSolver
 Iter          Defect            Rate
    0      3.16197e-09
6  (it = 0.5; it < _maxit; it+=.5) 0.5 250
6  before p = r 
6  before  y = W^-1 * p 
6  before _prec->apply(y,p); 0.5 250
6 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
0 === BiCGSTABSolver
 Iter          Defect            Rate
    0      3.16197e-09
0  (it = 0.5; it < _maxit; it+=.5) 0.5 250
0  before p = r 
0  before  y = W^-1 * p 
0  before _prec->apply(y,p); 0.5 250
0 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 7
0 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
0 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
6 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
6 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
6  before v = A * y 
1 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
4 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
4 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
4  before v = A * y 
2 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
2 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
3 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
3 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
3  before v = A * y 
5 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
5 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
5  before v = A * y 
6  before  alpha = rho_new / < rt, v > 
1 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
4  before  alpha = rho_new / < rt, v > 
2  before v = A * y 
3  before  alpha = rho_new / < rt, v > 
5  before  alpha = rho_new / < rt, v > 
0 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
5  before alpha = rho_new / h; 
5  before apply first correction to x 
5  before r = r - alpha*v 
5  before test stop criteria 
5 istlsolver::printOutput 
  0.5      8.68401e-25       2.7464e-16
5 === rate=7.5427e-32, T=0.123271, TIT=0.246542, IT=0.5
5 finished solveLinearSystemImpl_ 1
5 converged value: 1 7 1 let s go get convergedRemote
5 to  comm_.min(converged) 
3  before alpha = rho_new / h; 
3  before apply first correction to x 
3  before r = r - alpha*v 
3  before test stop criteria 
3 istlsolver::printOutput 
  0.5      8.68401e-25       2.7464e-16
3 === rate=7.5427e-32, T=0.123367, TIT=0.246734, IT=0.5
3 finished solveLinearSystemImpl_ 1
3 converged value: 1 7 1 let s go get convergedRemote
3 to  comm_.min(converged) 
6  before alpha = rho_new / h; 
6  before apply first correction to x 
6  before r = r - alpha*v 
6  before test stop criteria 
6 istlsolver::printOutput 
  0.5      8.68401e-25       2.7464e-16
6 === rate=7.5427e-32, T=0.123338, TIT=0.246676, IT=0.5
6 finished solveLinearSystemImpl_ 1
6 converged value: 1 7 1 let s go get convergedRemote
6 to  comm_.min(converged) 
1  before v = A * y 
1  before  alpha = rho_new / < rt, v > 
1  before alpha = rho_new / h; 
1  before apply first correction to x 
1  before r = r - alpha*v 
1  before test stop criteria 
1 istlsolver::printOutput 
  0.5      8.68401e-25       2.7464e-16
1 === rate=7.5427e-32, T=0.12327, TIT=0.246541, IT=0.5
1 finished solveLinearSystemImpl_ 1
1 converged value: 1 7 1 let s go get convergedRemote
1 to  comm_.min(converged) 
4  before alpha = rho_new / h; 
4  before apply first correction to x 
4  before r = r - alpha*v 
4  before test stop criteria 
4 istlsolver::printOutput 
  0.5      8.68401e-25       2.7464e-16
4 === rate=7.5427e-32, T=0.123338, TIT=0.246676, IT=0.5
4 finished solveLinearSystemImpl_ 1
4 converged value: 1 7 1 let s go get convergedRemote
4 to  comm_.min(converged) 
2  before  alpha = rho_new / < rt, v > 
2  before alpha = rho_new / h; 
2  before apply first correction to x 
2  before r = r - alpha*v 
2  before test stop criteria 
2 istlsolver::printOutput 
  0.5      8.68401e-25       2.7464e-16
2 === rate=7.5427e-32, T=0.123337, TIT=0.246674, IT=0.5
2 finished solveLinearSystemImpl_ 1
2 converged value: 1 7 1 let s go get convergedRemote
2 to  comm_.min(converged) 
0  before v = A * y 
0  before  alpha = rho_new / < rt, v > 
0  before alpha = rho_new / h; 
0  before apply first correction to x 
0  before r = r - alpha*v 
0  before test stop criteria 
0 istlsolver::printOutput 
  0.5      8.68401e-25       2.7464e-16
0 === rate=7.5427e-32, T=0.0185344, TIT=0.0370689, IT=0.5
3 did  comm_.min(converged) convergedRemote: 1
3 final convergedRemote: 1
3 Update: x^(k+1) = x^k - deltax^k 
3 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
3 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
3 Newton iteration  1 done3 , maximum relative shift = 1.269e-01
3 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
6 did  comm_.min(converged) convergedRemote: 1
6 final convergedRemote: 1
6 Update: x^(k+1) = x^k - deltax^k 
6 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
6 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
6 Newton iteration  1 done6 , maximum relative shift = 1.269e-01
6 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
1 did  comm_.min(converged) convergedRemote: 1
1 final convergedRemote: 1
1 Update: x^(k+1) = x^k - deltax^k 
1 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
1 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
1 Newton iteration  1 done1 , maximum relative shift = 1.269e-01
1 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
4 did  comm_.min(converged) convergedRemote: 1
4 final convergedRemote: 1
4 Update: x^(k+1) = x^k - deltax^k 
4 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
4 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
4 Newton iteration  1 done4 , maximum relative shift = 1.269e-01
4 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
0 finished solveLinearSystemImpl_ 1
0 converged value: 1 7 1 let s go get convergedRemote
0 to  comm_.min(converged) 
0 did  comm_.min(converged) convergedRemote: 1
0 final convergedRemote: 1
0 Update: x^(k+1) = x^k - deltax^k 
0 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
0 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
0 Newton iteration  1 done0 , maximum relative shift = 1.269e-01
0 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
2 did  comm_.min(converged) convergedRemote: 1
2 final convergedRemote: 1
2 Update: x^(k+1) = x^k - deltax^k 
2 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
2 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
2 Newton iteration  1 done2 , maximum relative shift = 1.269e-01
2 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
5 did  comm_.min(converged) convergedRemote: 1
5 final convergedRemote: 1
5 Update: x^(k+1) = x^k - deltax^k 
5 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
5 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
5 Newton iteration  1 done5 , maximum relative shift = 1.269e-01
5 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
0 Solve: M deltax^k = r 
0 Newton::solveLinearSystem : entering the try section 1
0 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
0 solveLinearSystemImpl_, before converged = ls.solve 
0 7 amgbackend::solve isParallel 1
2 Solve: M deltax^k = r 
2 Newton::solveLinearSystem : entering the try section 1
2 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
2 solveLinearSystemImpl_, before converged = ls.solve 
2 7 amgbackend::solve isParallel 1
4 Solve: M deltax^k = r 
4 Newton::solveLinearSystem : entering the try section 1
4 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
4 solveLinearSystemImpl_, before converged = ls.solve 
4 7 amgbackend::solve isParallel 1
3 Solve: M deltax^k = r 
3 Newton::solveLinearSystem : entering the try section 1
3 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
3 solveLinearSystemImpl_, before converged = ls.solve 
3 7 amgbackend::solve isParallel 1
6 Solve: M deltax^k = r 
6 Newton::solveLinearSystem : entering the try section 1
6 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
6 solveLinearSystemImpl_, before converged = ls.solve 
6 7 amgbackend::solve isParallel 1
1 Solve: M deltax^k = r 
1 Newton::solveLinearSystem : entering the try section 1
1 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
1 solveLinearSystemImpl_, before converged = ls.solve 
1 7 amgbackend::solve isParallel 1
5 Solve: M deltax^k = r 
5 Newton::solveLinearSystem : entering the try section 1
5 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
5 solveLinearSystemImpl_, before converged = ls.solve 
5 7 amgbackend::solve isParallel 1
3 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
3 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
6 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
6 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
5 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
5 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
1 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
1 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
0 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
0 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 7
2 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
2 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
4 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
4 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 7
3 7 before Dune::BiCGSTABSolver 
3 Dune::BiCGSTABSolver.apply(x, b, result_) 
3 BiCGSTABSolver::apply 
3 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 7
6 7 before Dune::BiCGSTABSolver 
6 Dune::BiCGSTABSolver.apply(x, b, result_) 
6 BiCGSTABSolver::apply 
6 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
5 7 before Dune::BiCGSTABSolver 
5 Dune::BiCGSTABSolver.apply(x, b, result_) 
5 BiCGSTABSolver::apply 
5 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
1 7 before Dune::BiCGSTABSolver 
1 Dune::BiCGSTABSolver.apply(x, b, result_) 
1 BiCGSTABSolver::apply 
1 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
4 7 before Dune::BiCGSTABSolver 
4 Dune::BiCGSTABSolver.apply(x, b, result_) 
4 BiCGSTABSolver::apply 
4 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
2 7 before Dune::BiCGSTABSolver 
2 Dune::BiCGSTABSolver.apply(x, b, result_) 
2 BiCGSTABSolver::apply 
2 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
Using a direct coarse solver (SuperLU)
Building hierarchy of 1 levels (inclusive coarse solver) took 0.112432 seconds.
0 7 before Dune::BiCGSTABSolver 
0 Dune::BiCGSTABSolver.apply(x, b, result_) 
0 BiCGSTABSolver::apply 
0 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
6 === BiCGSTABSolver
 Iter          Defect            Rate
    0      4.09212e-11
6  (it = 0.5; it < _maxit; it+=.5) 0.5 250
6  before p = r 
6  before  y = W^-1 * p 
6  before _prec->apply(y,p); 0.5 250
6 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
5 === BiCGSTABSolver
 Iter          Defect            Rate
    0      4.09212e-11
5  (it = 0.5; it < _maxit; it+=.5) 0.5 250
5  before p = r 
5  before  y = W^-1 * p 
5  before _prec->apply(y,p); 0.5 250
5 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
1 === BiCGSTABSolver
 Iter          Defect            Rate
    0      4.09212e-11
1  (it = 0.5; it < _maxit; it+=.5) 0.5 250
1  before p = r 
1  before  y = W^-1 * p 
1  before _prec->apply(y,p); 0.5 250
1 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
4 === BiCGSTABSolver
 Iter          Defect            Rate
    0      4.09212e-11
4  (it = 0.5; it < _maxit; it+=.5) 0.5 250
4  before p = r 
4  before  y = W^-1 * p 
4  before _prec->apply(y,p); 0.5 250
4 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
3 === BiCGSTABSolver
 Iter          Defect            Rate
    0      4.09212e-11
3  (it = 0.5; it < _maxit; it+=.5) 0.5 250
3  before p = r 
3  before  y = W^-1 * p 
3  before _prec->apply(y,p); 0.5 250
3 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
2 === BiCGSTABSolver
 Iter          Defect            Rate
    0      4.09212e-11
2  (it = 0.5; it < _maxit; it+=.5) 0.5 250
2  before p = r 
2  before  y = W^-1 * p 
2  before _prec->apply(y,p); 0.5 250
2 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
0 === BiCGSTABSolver
 Iter          Defect            Rate
    0      4.09212e-11
0  (it = 0.5; it < _maxit; it+=.5) 0.5 250
0  before p = r 
0  before  y = W^-1 * p 
0  before _prec->apply(y,p); 0.5 250
0 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 7
0 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
2 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
2 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
6 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
6 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
6  before v = A * y 
5 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
5 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
5  before v = A * y 
1 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
4 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
4 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
4  before v = A * y 
3 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
3 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
3  before v = A * y 
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
0 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
2  before v = A * y 
6  before  alpha = rho_new / < rt, v > 
5  before  alpha = rho_new / < rt, v > 
1 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
4  before  alpha = rho_new / < rt, v > 
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
0 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
0  before v = A * y 
3  before  alpha = rho_new / < rt, v > 
5  before alpha = rho_new / h; 
5  before apply first correction to x 
5  before r = r - alpha*v 
5  before test stop criteria 
5 istlsolver::printOutput 
  0.5      2.66163e-27      6.50428e-17
5 === rate=4.23056e-33, T=0.123374, TIT=0.246748, IT=0.5
5 finished solveLinearSystemImpl_ 1
5 converged value: 1 7 1 let s go get convergedRemote
5 to  comm_.min(converged) 
1  before v = A * y 
1  before  alpha = rho_new / < rt, v > 
1  before alpha = rho_new / h; 
1  before apply first correction to x 
1  before r = r - alpha*v 
1  before test stop criteria 
1 istlsolver::printOutput 
  0.5      2.66163e-27      6.50428e-17
1 === rate=4.23056e-33, T=0.123377, TIT=0.246754, IT=0.5
1 finished solveLinearSystemImpl_ 1
1 converged value: 1 7 1 let s go get convergedRemote
1 to  comm_.min(converged) 
3  before alpha = rho_new / h; 
3  before apply first correction to x 
3  before r = r - alpha*v 
3  before test stop criteria 
3 istlsolver::printOutput 
  0.5      2.66163e-27      6.50428e-17
3 === rate=4.23056e-33, T=0.123375, TIT=0.24675, IT=0.5
3 finished solveLinearSystemImpl_ 1
3 converged value: 1 7 1 let s go get convergedRemote
3 to  comm_.min(converged) 
0  before  alpha = rho_new / < rt, v > 
0  before alpha = rho_new / h; 
0  before apply first correction to x 
0  before r = r - alpha*v 
0  before test stop criteria 
0 istlsolver::printOutput 
  0.5      2.66163e-27      6.50428e-17
0 === rate=4.23056e-33, T=0.0185691, TIT=0.0371381, IT=0.5
4  before alpha = rho_new / h; 
4  before apply first correction to x 
4  before r = r - alpha*v 
4  before test stop criteria 
4 istlsolver::printOutput 
  0.5      2.66163e-27      6.50428e-17
4 === rate=4.23056e-33, T=0.123329, TIT=0.246659, IT=0.5
4 finished solveLinearSystemImpl_ 1
4 converged value: 1 7 1 let s go get convergedRemote
4 to  comm_.min(converged) 
2  before  alpha = rho_new / < rt, v > 
2  before alpha = rho_new / h; 
2  before apply first correction to x 
2  before r = r - alpha*v 
2  before test stop criteria 
2 istlsolver::printOutput 
  0.5      2.66163e-27      6.50428e-17
2 === rate=4.23056e-33, T=0.123329, TIT=0.246659, IT=0.5
2 finished solveLinearSystemImpl_ 1
2 converged value: 1 7 1 let s go get convergedRemote
2 to  comm_.min(converged) 
6  before alpha = rho_new / h; 
6  before apply first correction to x 
6  before r = r - alpha*v 
6  before test stop criteria 
6 istlsolver::printOutput 
  0.5      2.66163e-27      6.50428e-17
6 === rate=4.23056e-33, T=0.123331, TIT=0.246661, IT=0.5
6 finished solveLinearSystemImpl_ 1
6 converged value: 1 7 1 let s go get convergedRemote
6 to  comm_.min(converged) 
3 did  comm_.min(converged) convergedRemote: 1
3 final convergedRemote: 1
3 Update: x^(k+1) = x^k - deltax^k 
3 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
3 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
3 Newton iteration  2 done3 , maximum relative shift = 1.488e-02
3 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
0 finished solveLinearSystemImpl_ 1
0 converged value: 1 7 1 let s go get convergedRemote
0 to  comm_.min(converged) 
0 did  comm_.min(converged) convergedRemote: 1
0 final convergedRemote: 1
0 Update: x^(k+1) = x^k - deltax^k 
0 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
0 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
0 Newton iteration  2 done0 , maximum relative shift = 1.488e-02
0 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
4 did  comm_.min(converged) convergedRemote: 1
4 final convergedRemote: 1
4 Update: x^(k+1) = x^k - deltax^k 
4 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
4 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
4 Newton iteration  2 done4 , maximum relative shift = 1.488e-02
4 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
2 did  comm_.min(converged) convergedRemote: 1
2 final convergedRemote: 1
2 Update: x^(k+1) = x^k - deltax^k 
2 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
2 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
2 Newton iteration  2 done2 , maximum relative shift = 1.488e-02
2 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
1 did  comm_.min(converged) convergedRemote: 1
1 final convergedRemote: 1
1 Update: x^(k+1) = x^k - deltax^k 
1 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
1 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
1 Newton iteration  2 done1 , maximum relative shift = 1.488e-02
1 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
5 did  comm_.min(converged) convergedRemote: 1
5 final convergedRemote: 1
5 Update: x^(k+1) = x^k - deltax^k 
5 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
5 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
5 Newton iteration  2 done5 , maximum relative shift = 1.488e-02
5 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
6 did  comm_.min(converged) convergedRemote: 1
6 final convergedRemote: 1
6 Update: x^(k+1) = x^k - deltax^k 
6 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
6 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
6 Newton iteration  2 done6 , maximum relative shift = 1.488e-02
6 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
2 Solve: M deltax^k = r 
2 Newton::solveLinearSystem : entering the try section 2
2 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
2 solveLinearSystemImpl_, before converged = ls.solve 
2 7 amgbackend::solve isParallel 1
1 Solve: M deltax^k = r 
1 Newton::solveLinearSystem : entering the try section 2
1 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
1 solveLinearSystemImpl_, before converged = ls.solve 
1 7 amgbackend::solve isParallel 1
6 Solve: M deltax^k = r 
6 Newton::solveLinearSystem : entering the try section 2
6 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
6 solveLinearSystemImpl_, before converged = ls.solve 
6 7 amgbackend::solve isParallel 1
5 Solve: M deltax^k = r 
5 Newton::solveLinearSystem : entering the try section 2
5 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
5 solveLinearSystemImpl_, before converged = ls.solve 
5 7 amgbackend::solve isParallel 1
3 Solve: M deltax^k = r 
3 Newton::solveLinearSystem : entering the try section 2
3 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
3 solveLinearSystemImpl_, before converged = ls.solve 
3 7 amgbackend::solve isParallel 1
0 Solve: M deltax^k = r 
0 Newton::solveLinearSystem : entering the try section 2
0 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
0 solveLinearSystemImpl_, before converged = ls.solve 
0 7 amgbackend::solve isParallel 1
4 Solve: M deltax^k = r 
4 Newton::solveLinearSystem : entering the try section 2
4 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
4 solveLinearSystemImpl_, before converged = ls.solve 
4 7 amgbackend::solve isParallel 1
6 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
6 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
5 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
5 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
3 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
3 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
0 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
0 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 7
1 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
1 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
2 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
2 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
4 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
4 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 7
2 7 before Dune::BiCGSTABSolver 
2 Dune::BiCGSTABSolver.apply(x, b, result_) 
2 BiCGSTABSolver::apply 
2 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
6 7 before Dune::BiCGSTABSolver 
6 Dune::BiCGSTABSolver.apply(x, b, result_) 
6 BiCGSTABSolver::apply 
6 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
5 7 before Dune::BiCGSTABSolver 
5 Dune::BiCGSTABSolver.apply(x, b, result_) 
5 BiCGSTABSolver::apply 
5 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
3 7 before Dune::BiCGSTABSolver 
3 Dune::BiCGSTABSolver.apply(x, b, result_) 
3 BiCGSTABSolver::apply 
3 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
4 7 before Dune::BiCGSTABSolver 
4 Dune::BiCGSTABSolver.apply(x, b, result_) 
4 BiCGSTABSolver::apply 
4 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
1 7 before Dune::BiCGSTABSolver 
1 Dune::BiCGSTABSolver.apply(x, b, result_) 
1 BiCGSTABSolver::apply 
1 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
Using a direct coarse solver (SuperLU)
Building hierarchy of 1 levels (inclusive coarse solver) took 0.111385 seconds.
0 7 before Dune::BiCGSTABSolver 
0 Dune::BiCGSTABSolver.apply(x, b, result_) 
0 BiCGSTABSolver::apply 
0 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
1 === BiCGSTABSolver
 Iter          Defect            Rate
    0      8.46881e-12
1  (it = 0.5; it < _maxit; it+=.5) 0.5 250
1  before p = r 
1  before  y = W^-1 * p 
1  before _prec->apply(y,p); 0.5 250
1 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
2 === BiCGSTABSolver
 Iter          Defect            Rate
    0      8.46881e-12
2  (it = 0.5; it < _maxit; it+=.5) 0.5 250
2  before p = r 
2  before  y = W^-1 * p 
2  before _prec->apply(y,p); 0.5 250
2 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
6 === BiCGSTABSolver
 Iter          Defect            Rate
    0      8.46881e-12
6  (it = 0.5; it < _maxit; it+=.5) 0.5 250
6  before p = r 
6  before  y = W^-1 * p 
6  before _prec->apply(y,p); 0.5 250
6 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
5 === BiCGSTABSolver
 Iter          Defect            Rate
    0      8.46881e-12
5  (it = 0.5; it < _maxit; it+=.5) 0.5 250
5  before p = r 
5  before  y = W^-1 * p 
5  before _prec->apply(y,p); 0.5 250
5 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
3 === BiCGSTABSolver
 Iter          Defect            Rate
    0      8.46881e-12
3  (it = 0.5; it < _maxit; it+=.5) 0.5 250
3  before p = r 
3  before  y = W^-1 * p 
3  before _prec->apply(y,p); 0.5 250
3 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
4 === BiCGSTABSolver
 Iter          Defect            Rate
    0      8.46881e-12
4  (it = 0.5; it < _maxit; it+=.5) 0.5 250
4  before p = r 
4  before  y = W^-1 * p 
4  before _prec->apply(y,p); 0.5 250
4 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
0 === BiCGSTABSolver
 Iter          Defect            Rate
    0      8.46881e-12
0  (it = 0.5; it < _maxit; it+=.5) 0.5 250
0  before p = r 
0  before  y = W^-1 * p 
0  before _prec->apply(y,p); 0.5 250
0 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 7
0 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
4 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
4 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
4  before v = A * y 
4  before  alpha = rho_new / < rt, v > 
1 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
2 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
2 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
6 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
6 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
6  before v = A * y 
6  before  alpha = rho_new / < rt, v > 
5 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
5 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
5  before v = A * y 
3 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
3 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
3  before v = A * y 
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
0 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
1 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
2  before v = A * y 
2  before  alpha = rho_new / < rt, v > 
5  before  alpha = rho_new / < rt, v > 
0 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
3  before  alpha = rho_new / < rt, v > 
2  before alpha = rho_new / h; 
2  before apply first correction to x 
2  before r = r - alpha*v 
2  before test stop criteria 
2 istlsolver::printOutput 
  0.5      2.64316e-28      3.12105e-17
2 === rate=9.74096e-34, T=0.125296, TIT=0.250593, IT=0.5
2 finished solveLinearSystemImpl_ 1
2 converged value: 1 7 1 let s go get convergedRemote
2 to  comm_.min(converged) 
3  before alpha = rho_new / h; 
3  before apply first correction to x 
3  before r = r - alpha*v 
3  before test stop criteria 
3 istlsolver::printOutput 
  0.5      2.64316e-28      3.12105e-17
3 === rate=9.74096e-34, T=0.125328, TIT=0.250657, IT=0.5
3 finished solveLinearSystemImpl_ 1
3 converged value: 1 7 1 let s go get convergedRemote
3 to  comm_.min(converged) 
0  before v = A * y 
0  before  alpha = rho_new / < rt, v > 
0  before alpha = rho_new / h; 
0  before apply first correction to x 
0  before r = r - alpha*v 
0  before test stop criteria 
0 istlsolver::printOutput 
  0.5      2.64316e-28      3.12105e-17
0 === rate=9.74096e-34, T=0.0216393, TIT=0.0432787, IT=0.5
4  before alpha = rho_new / h; 
4  before apply first correction to x 
4  before r = r - alpha*v 
4  before test stop criteria 
4 istlsolver::printOutput 
  0.5      2.64316e-28      3.12105e-17
4 === rate=9.74096e-34, T=0.125297, TIT=0.250594, IT=0.5
4 finished solveLinearSystemImpl_ 1
4 converged value: 1 7 1 let s go get convergedRemote
4 to  comm_.min(converged) 
5  before alpha = rho_new / h; 
5  before apply first correction to x 
5  before r = r - alpha*v 
5  before test stop criteria 
5 istlsolver::printOutput 
  0.5      2.64316e-28      3.12105e-17
5 === rate=9.74096e-34, T=0.125328, TIT=0.250655, IT=0.5
5 finished solveLinearSystemImpl_ 1
5 converged value: 1 7 1 let s go get convergedRemote
5 to  comm_.min(converged) 
1  before v = A * y 
1  before  alpha = rho_new / < rt, v > 
1  before alpha = rho_new / h; 
1  before apply first correction to x 
1  before r = r - alpha*v 
1  before test stop criteria 
1 istlsolver::printOutput 
  0.5      2.64316e-28      3.12105e-17
1 === rate=9.74096e-34, T=0.12533, TIT=0.25066, IT=0.5
1 finished solveLinearSystemImpl_ 1
1 converged value: 1 7 1 let s go get convergedRemote
1 to  comm_.min(converged) 
6  before alpha = rho_new / h; 
6  before apply first correction to x 
6  before r = r - alpha*v 
6  before test stop criteria 
6 istlsolver::printOutput 
  0.5      2.64316e-28      3.12105e-17
6 === rate=9.74096e-34, T=0.125297, TIT=0.250593, IT=0.5
6 finished solveLinearSystemImpl_ 1
6 converged value: 1 7 1 let s go get convergedRemote
6 to  comm_.min(converged) 
3 did  comm_.min(converged) convergedRemote: 1
3 final convergedRemote: 1
3 Update: x^(k+1) = x^k - deltax^k 
3 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
3 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
3 Newton iteration  3 done3 , maximum relative shift = 2.537e-03
3 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
0 finished solveLinearSystemImpl_ 1
0 converged value: 1 7 1 let s go get convergedRemote
0 to  comm_.min(converged) 
0 did  comm_.min(converged) convergedRemote: 1
0 final convergedRemote: 1
0 Update: x^(k+1) = x^k - deltax^k 
0 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
0 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
0 Newton iteration  3 done0 , maximum relative shift = 2.537e-03
0 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
4 did  comm_.min(converged) convergedRemote: 1
4 final convergedRemote: 1
4 Update: x^(k+1) = x^k - deltax^k 
4 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
4 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
4 Newton iteration  3 done4 , maximum relative shift = 2.537e-03
4 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
5 did  comm_.min(converged) convergedRemote: 1
5 final convergedRemote: 1
5 Update: x^(k+1) = x^k - deltax^k 
5 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
5 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
5 Newton iteration  3 done5 , maximum relative shift = 2.537e-03
5 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
1 did  comm_.min(converged) convergedRemote: 1
1 final convergedRemote: 1
1 Update: x^(k+1) = x^k - deltax^k 
1 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
1 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
1 Newton iteration  3 done1 , maximum relative shift = 2.537e-03
1 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
2 did  comm_.min(converged) convergedRemote: 1
2 final convergedRemote: 1
2 Update: x^(k+1) = x^k - deltax^k 
2 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
2 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
2 Newton iteration  3 done2 , maximum relative shift = 2.537e-03
2 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
6 did  comm_.min(converged) convergedRemote: 1
6 final convergedRemote: 1
6 Update: x^(k+1) = x^k - deltax^k 
6 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
6 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
6 Newton iteration  3 done6 , maximum relative shift = 2.537e-03
6 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
4 Solve: M deltax^k = r 
4 Newton::solveLinearSystem : entering the try section 3
4 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
4 solveLinearSystemImpl_, before converged = ls.solve 
4 7 amgbackend::solve isParallel 1
5 Solve: M deltax^k = r 
5 Newton::solveLinearSystem : entering the try section 3
5 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
5 solveLinearSystemImpl_, before converged = ls.solve 
5 7 amgbackend::solve isParallel 1
1 Solve: M deltax^k = r 
1 Newton::solveLinearSystem : entering the try section 3
1 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
1 solveLinearSystemImpl_, before converged = ls.solve 
1 7 amgbackend::solve isParallel 1
6 Solve: M deltax^k = r 
6 Newton::solveLinearSystem : entering the try section 3
6 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
6 solveLinearSystemImpl_, before converged = ls.solve 
6 7 amgbackend::solve isParallel 1
2 Solve: M deltax^k = r 
2 Newton::solveLinearSystem : entering the try section 3
2 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
2 solveLinearSystemImpl_, before converged = ls.solve 
2 7 amgbackend::solve isParallel 1
3 Solve: M deltax^k = r 
3 Newton::solveLinearSystem : entering the try section 3
3 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
3 solveLinearSystemImpl_, before converged = ls.solve 
3 7 amgbackend::solve isParallel 1
0 Solve: M deltax^k = r 
0 Newton::solveLinearSystem : entering the try section 3
0 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
0 solveLinearSystemImpl_, before converged = ls.solve 
0 7 amgbackend::solve isParallel 1
0 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
0 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 7
6 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
6 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
2 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
2 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
3 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
3 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
1 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
1 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
4 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
4 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
5 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
5 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 7
5 7 before Dune::BiCGSTABSolver 
5 Dune::BiCGSTABSolver.apply(x, b, result_) 
5 BiCGSTABSolver::apply 
5 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
6 7 before Dune::BiCGSTABSolver 
6 Dune::BiCGSTABSolver.apply(x, b, result_) 
6 BiCGSTABSolver::apply 
6 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
2 7 before Dune::BiCGSTABSolver 
2 Dune::BiCGSTABSolver.apply(x, b, result_) 
2 BiCGSTABSolver::apply 
2 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
3 7 before Dune::BiCGSTABSolver 
3 Dune::BiCGSTABSolver.apply(x, b, result_) 
3 BiCGSTABSolver::apply 
3 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
1 7 before Dune::BiCGSTABSolver 
1 Dune::BiCGSTABSolver.apply(x, b, result_) 
1 BiCGSTABSolver::apply 
1 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
4 7 before Dune::BiCGSTABSolver 
4 Dune::BiCGSTABSolver.apply(x, b, result_) 
4 BiCGSTABSolver::apply 
4 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 7
Using a direct coarse solver (SuperLU)
Building hierarchy of 1 levels (inclusive coarse solver) took 0.112747 seconds.
0 7 before Dune::BiCGSTABSolver 
0 Dune::BiCGSTABSolver.apply(x, b, result_) 
0 BiCGSTABSolver::apply 
0 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
2 === BiCGSTABSolver
 Iter          Defect            Rate
    0      3.27064e-13
2  (it = 0.5; it < _maxit; it+=.5) 0.5 250
2  before p = r 
2  before  y = W^-1 * p 
2  before _prec->apply(y,p); 0.5 250
2 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
3 === BiCGSTABSolver
 Iter          Defect            Rate
    0      3.27064e-13
3  (it = 0.5; it < _maxit; it+=.5) 0.5 250
3  before p = r 
3  before  y = W^-1 * p 
3  before _prec->apply(y,p); 0.5 250
3 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
1 === BiCGSTABSolver
 Iter          Defect            Rate
    0      3.27064e-13
1  (it = 0.5; it < _maxit; it+=.5) 0.5 250
1  before p = r 
1  before  y = W^-1 * p 
1  before _prec->apply(y,p); 0.5 250
1 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
0 === BiCGSTABSolver
 Iter          Defect            Rate
    0      3.27064e-13
0  (it = 0.5; it < _maxit; it+=.5) 0.5 250
0  before p = r 
0  before  y = W^-1 * p 
0  before _prec->apply(y,p); 0.5 250
0 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 7
0 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
4 === BiCGSTABSolver
 Iter          Defect            Rate
    0      3.27064e-13
4  (it = 0.5; it < _maxit; it+=.5) 0.5 250
4  before p = r 
4  before  y = W^-1 * p 
4  before _prec->apply(y,p); 0.5 250
4 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
5 === BiCGSTABSolver
 Iter          Defect            Rate
    0      3.27064e-13
5  (it = 0.5; it < _maxit; it+=.5) 0.5 250
5  before p = r 
5  before  y = W^-1 * p 
5  before _prec->apply(y,p); 0.5 250
5 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
6 === BiCGSTABSolver
 Iter          Defect            Rate
    0      3.27064e-13
6  (it = 0.5; it < _maxit; it+=.5) 0.5 250
6  before p = r 
6  before  y = W^-1 * p 
6  before _prec->apply(y,p); 0.5 250
6 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
3 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
3 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
3  before v = A * y 
6 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
6 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
6  before v = A * y 
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
0 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
4 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
4 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
4  before v = A * y 
5 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
5 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
5  before v = A * y 
1 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
2 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
2 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
4  before  alpha = rho_new / < rt, v > 
5  before  alpha = rho_new / < rt, v > 
1 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
1  before v = A * y 
1  before  alpha = rho_new / < rt, v > 
3  before  alpha = rho_new / < rt, v > 
2  before v = A * y 
2  before  alpha = rho_new / < rt, v > 
6  before  alpha = rho_new / < rt, v > 
0 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
0  before v = A * y 
0  before  alpha = rho_new / < rt, v > 
0  before alpha = rho_new / h; 
0  before apply first correction to x 
0  before r = r - alpha*v 
0  before test stop criteria 
0 istlsolver::printOutput 
  0.5      2.35266e-29      7.19328e-17
0 === rate=5.17432e-33, T=0.0215423, TIT=0.0430847, IT=0.5
2  before alpha = rho_new / h; 
2  before apply first correction to x 
2  before r = r - alpha*v 
2  before test stop criteria 
2 istlsolver::printOutput 
  0.5      2.35266e-29      7.19328e-17
2 === rate=5.17432e-33, T=0.126484, TIT=0.252968, IT=0.5
2 finished solveLinearSystemImpl_ 1
2 converged value: 1 7 1 let s go get convergedRemote
2 to  comm_.min(converged) 
6  before alpha = rho_new / h; 
6  before apply first correction to x 
6  before r = r - alpha*v 
6  before test stop criteria 
6 istlsolver::printOutput 
  0.5      2.35266e-29      7.19328e-17
6 === rate=5.17432e-33, T=0.126485, TIT=0.252971, IT=0.5
6 finished solveLinearSystemImpl_ 1
6 converged value: 1 7 1 let s go get convergedRemote
6 to  comm_.min(converged) 
3  before alpha = rho_new / h; 
3  before apply first correction to x 
3  before r = r - alpha*v 
3  before test stop criteria 
3 istlsolver::printOutput 
  0.5      2.35266e-29      7.19328e-17
3 === rate=5.17432e-33, T=0.12644, TIT=0.252881, IT=0.5
3 finished solveLinearSystemImpl_ 1
3 converged value: 1 7 1 let s go get convergedRemote
3 to  comm_.min(converged) 
4  before alpha = rho_new / h; 
4  before apply first correction to x 
4  before r = r - alpha*v 
4  before test stop criteria 
4 istlsolver::printOutput 
  0.5      2.35266e-29      7.19328e-17
4 === rate=5.17432e-33, T=0.126483, TIT=0.252967, IT=0.5
4 finished solveLinearSystemImpl_ 1
4 converged value: 1 7 1 let s go get convergedRemote
4 to  comm_.min(converged) 
5  before alpha = rho_new / h; 
5  before apply first correction to x 
5  before r = r - alpha*v 
5  before test stop criteria 
5 istlsolver::printOutput 
  0.5      2.35266e-29      7.19328e-17
5 === rate=5.17432e-33, T=0.126439, TIT=0.252879, IT=0.5
5 finished solveLinearSystemImpl_ 1
5 converged value: 1 7 1 let s go get convergedRemote
5 to  comm_.min(converged) 
1  before alpha = rho_new / h; 
1  before apply first correction to x 
1  before r = r - alpha*v 
1  before test stop criteria 
1 istlsolver::printOutput 
  0.5      2.35266e-29      7.19328e-17
1 === rate=5.17432e-33, T=0.126439, TIT=0.252878, IT=0.5
1 finished solveLinearSystemImpl_ 1
1 converged value: 1 7 1 let s go get convergedRemote
1 to  comm_.min(converged) 
5 did  comm_.min(converged) convergedRemote: 1
5 final convergedRemote: 1
5 Update: x^(k+1) = x^k - deltax^k 
5 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
5 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
5 Newton iteration  4 done5 , maximum relative shift = 9.781e-05
5 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
1 did  comm_.min(converged) convergedRemote: 1
1 final convergedRemote: 1
1 Update: x^(k+1) = x^k - deltax^k 
1 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
1 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
1 Newton iteration  4 done1 , maximum relative shift = 9.781e-05
1 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
0 finished solveLinearSystemImpl_ 1
0 converged value: 1 7 1 let s go get convergedRemote
0 to  comm_.min(converged) 
0 did  comm_.min(converged) convergedRemote: 1
0 final convergedRemote: 1
0 Update: x^(k+1) = x^k - deltax^k 
0 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
0 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
0 Newton iteration  4 done0 , maximum relative shift = 9.781e-05
0 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
2 did  comm_.min(converged) convergedRemote: 1
2 final convergedRemote: 1
2 Update: x^(k+1) = x^k - deltax^k 
2 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
2 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
2 Newton iteration  4 done2 , maximum relative shift = 9.781e-05
2 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
6 did  comm_.min(converged) convergedRemote: 1
6 final convergedRemote: 1
6 Update: x^(k+1) = x^k - deltax^k 
6 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
6 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
6 Newton iteration  4 done6 , maximum relative shift = 9.781e-05
6 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
3 did  comm_.min(converged) convergedRemote: 1
3 final convergedRemote: 1
3 Update: x^(k+1) = x^k - deltax^k 
3 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
3 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
3 Newton iteration  4 done3 , maximum relative shift = 9.781e-05
3 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
4 did  comm_.min(converged) convergedRemote: 1
4 final convergedRemote: 1
4 Update: x^(k+1) = x^k - deltax^k 
4 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
4 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
4 Newton iteration  4 done4 , maximum relative shift = 9.781e-05
4 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
3 Solve: M deltax^k = r 
3 Newton::solveLinearSystem : entering the try section 4
3 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
3 solveLinearSystemImpl_, before converged = ls.solve 
3 7 amgbackend::solve isParallel 1
5 Solve: M deltax^k = r 
5 Newton::solveLinearSystem : entering the try section 4
5 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
5 solveLinearSystemImpl_, before converged = ls.solve 
5 7 amgbackend::solve isParallel 1
4 Solve: M deltax^k = r 
4 Newton::solveLinearSystem : entering the try section 4
4 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
4 solveLinearSystemImpl_, before converged = ls.solve 
4 7 amgbackend::solve isParallel 1
1 Solve: M deltax^k = r 
1 Newton::solveLinearSystem : entering the try section 4
1 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
1 solveLinearSystemImpl_, before converged = ls.solve 
1 7 amgbackend::solve isParallel 1
0 Solve: M deltax^k = r 
0 Newton::solveLinearSystem : entering the try section 4
0 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
0 solveLinearSystemImpl_, before converged = ls.solve 
0 7 amgbackend::solve isParallel 1
2 Solve: M deltax^k = r 
2 Newton::solveLinearSystem : entering the try section 4
2 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
2 solveLinearSystemImpl_, before converged = ls.solve 
2 7 amgbackend::solve isParallel 1
6 Solve: M deltax^k = r 
6 Newton::solveLinearSystem : entering the try section 4
6 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
6 solveLinearSystemImpl_, before converged = ls.solve 
6 7 amgbackend::solve isParallel 1
4 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
4 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
1 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
1 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
0 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
0 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 7
2 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
2 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
5 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
5 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
3 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
3 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
6 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
6 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 7
3 7 before Dune::BiCGSTABSolver 
3 Dune::BiCGSTABSolver.apply(x, b, result_) 
3 BiCGSTABSolver::apply 
3 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
4 7 before Dune::BiCGSTABSolver 
4 Dune::BiCGSTABSolver.apply(x, b, result_) 
4 BiCGSTABSolver::apply 
4 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
1 7 before Dune::BiCGSTABSolver 
1 Dune::BiCGSTABSolver.apply(x, b, result_) 
1 BiCGSTABSolver::apply 
1 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
6 7 before Dune::BiCGSTABSolver 
6 Dune::BiCGSTABSolver.apply(x, b, result_) 
6 BiCGSTABSolver::apply 
6 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
2 7 before Dune::BiCGSTABSolver 
2 Dune::BiCGSTABSolver.apply(x, b, result_) 
2 BiCGSTABSolver::apply 
2 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
5 7 before Dune::BiCGSTABSolver 
5 Dune::BiCGSTABSolver.apply(x, b, result_) 
5 BiCGSTABSolver::apply 
5 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 7
3 === BiCGSTABSolver
 Iter          Defect            Rate
    0      1.20268e-14
3  (it = 0.5; it < _maxit; it+=.5) 0.5 250
3  before p = r 
3  before  y = W^-1 * p 
3  before _prec->apply(y,p); 0.5 250
3 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
4 === BiCGSTABSolver
 Iter          Defect            Rate
    0      1.20268e-14
4  (it = 0.5; it < _maxit; it+=.5) 0.5 250
4  before p = r 
4  before  y = W^-1 * p 
4  before _prec->apply(y,p); 0.5 250
4 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
1 === BiCGSTABSolver
 Iter          Defect            Rate
    0      1.20268e-14
1  (it = 0.5; it < _maxit; it+=.5) 0.5 250
1  before p = r 
1  before  y = W^-1 * p 
1  before _prec->apply(y,p); 0.5 250
1 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
6 === BiCGSTABSolver
 Iter          Defect            Rate
    0      1.20268e-14
6  (it = 0.5; it < _maxit; it+=.5) 0.5 250
6  before p = r 
6  before  y = W^-1 * p 
6  before _prec->apply(y,p); 0.5 250
6 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
2 === BiCGSTABSolver
 Iter          Defect            Rate
    0      1.20268e-14
2  (it = 0.5; it < _maxit; it+=.5) 0.5 250
2  before p = r 
2  before  y = W^-1 * p 
2  before _prec->apply(y,p); 0.5 250
2 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
Using a direct coarse solver (SuperLU)
Building hierarchy of 1 levels (inclusive coarse solver) took 0.112679 seconds.
0 7 before Dune::BiCGSTABSolver 
0 Dune::BiCGSTABSolver.apply(x, b, result_) 
0 BiCGSTABSolver::apply 
0 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
0 === BiCGSTABSolver
 Iter          Defect            Rate
    0      1.20268e-14
0  (it = 0.5; it < _maxit; it+=.5) 0.5 250
0  before p = r 
0  before  y = W^-1 * p 
0  before _prec->apply(y,p); 0.5 250
0 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 7
5 === BiCGSTABSolver
 Iter          Defect            Rate
    0      1.20268e-14
5  (it = 0.5; it < _maxit; it+=.5) 0.5 250
5  before p = r 
5  before  y = W^-1 * p 
5  before _prec->apply(y,p); 0.5 250
5 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 7
0 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
6 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
6 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
6  before v = A * y 
6  before  alpha = rho_new / < rt, v > 
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
0 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
0 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
0  before v = A * y 
0  before  alpha = rho_new / < rt, v > 
5 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
5 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
5  before v = A * y 
5  before  alpha = rho_new / < rt, v > 
2 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
2 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
2  before v = A * y 
2  before  alpha = rho_new / < rt, v > 
3 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
3 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
3  before v = A * y 
3  before  alpha = rho_new / < rt, v > 
4 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
4 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
4  before v = A * y 
4  before  alpha = rho_new / < rt, v > 
1 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
1 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
1  before v = A * y 
1  before  alpha = rho_new / < rt, v > 
1  before alpha = rho_new / h; 
1  before apply first correction to x 
1  before r = r - alpha*v 
1  before test stop criteria 
0  before alpha = rho_new / h; 
0  before apply first correction to x 
0  before r = r - alpha*v 
0  before test stop criteria 
0 istlsolver::printOutput 
5  before alpha = rho_new / h; 
5  before apply first correction to x 
5  before r = r - alpha*v 
5  before test stop criteria 
2  before alpha = rho_new / h; 
2  before apply first correction to x 
2  before r = r - alpha*v 
2  before test stop criteria 
2 istlsolver::printOutput 
  0.5      4.93246e-32      4.10121e-18
2 === rate=1.68199e-35, T=0.123321, TIT=0.246642, IT=0.5
3  before alpha = rho_new / h; 
3  before apply first correction to x 
3  before r = r - alpha*v 
3  before test stop criteria 
4  before alpha = rho_new / h; 
4  before apply first correction to x 
4  before r = r - alpha*v 
4  before test stop criteria 
4 istlsolver::printOutput 
  0.5      4.93246e-32      4.10121e-18
4 === rate=1.68199e-35, T=0.123301, TIT=0.246602, IT=0.5
6  before alpha = rho_new / h; 
6  before apply first correction to x 
6  before r = r - alpha*v 
6  before test stop criteria 
6 istlsolver::printOutput 
  0.5      4.93246e-32      4.10121e-18
6 === rate=1.68199e-35, T=0.123321, TIT=0.246641, IT=0.5
6 finished solveLinearSystemImpl_ 1
6 converged value: 1 7 1 let s go get convergedRemote
6 to  comm_.min(converged) 
1 istlsolver::printOutput 
  0.5      4.93246e-32      4.10121e-18
1 === rate=1.68199e-35, T=0.123417, TIT=0.246835, IT=0.5
1 finished solveLinearSystemImpl_ 1
1 converged value: 1 7 1 let s go get convergedRemote
1 to  comm_.min(converged) 
  0.5      4.93246e-32      4.10121e-18
0 === rate=1.68199e-35, T=0.0185407, TIT=0.0370814, IT=0.5
5 istlsolver::printOutput 
  0.5      4.93246e-32      4.10121e-18
5 === rate=1.68199e-35, T=0.123417, TIT=0.246835, IT=0.5
5 finished solveLinearSystemImpl_ 1
5 converged value: 1 7 1 let s go get convergedRemote
5 to  comm_.min(converged) 
2 finished solveLinearSystemImpl_ 1
2 converged value: 1 7 1 let s go get convergedRemote
2 to  comm_.min(converged) 
3 istlsolver::printOutput 
  0.5      4.93246e-32      4.10121e-18
3 === rate=1.68199e-35, T=0.123414, TIT=0.246829, IT=0.5
3 finished solveLinearSystemImpl_ 1
3 converged value: 1 7 1 let s go get convergedRemote
3 to  comm_.min(converged) 
4 finished solveLinearSystemImpl_ 1
4 converged value: 1 7 1 let s go get convergedRemote
4 to  comm_.min(converged) 
2 did  comm_.min(converged) convergedRemote: 1
2 final convergedRemote: 1
2 Update: x^(k+1) = x^k - deltax^k 
2 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
2 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
2 Newton iteration  5 done2 , maximum relative shift = 4.505e-06
2 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
3 did  comm_.min(converged) convergedRemote: 1
3 final convergedRemote: 1
3 Update: x^(k+1) = x^k - deltax^k 
3 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
3 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
3 Newton iteration  5 done3 , maximum relative shift = 4.505e-06
3 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
4 did  comm_.min(converged) convergedRemote: 1
4 final convergedRemote: 1
4 Update: x^(k+1) = x^k - deltax^k 
4 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
4 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
4 Newton iteration  5 done4 , maximum relative shift = 4.505e-06
4 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
6 did  comm_.min(converged) convergedRemote: 1
6 final convergedRemote: 1
6 Update: x^(k+1) = x^k - deltax^k 
6 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
6 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
6 Newton iteration  5 done6 , maximum relative shift = 4.505e-06
6 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
1 did  comm_.min(converged) convergedRemote: 1
1 final convergedRemote: 1
1 Update: x^(k+1) = x^k - deltax^k 
1 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
1 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
1 Newton iteration  5 done1 , maximum relative shift = 4.505e-06
1 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
0 finished solveLinearSystemImpl_ 1
0 converged value: 1 7 1 let s go get convergedRemote
0 to  comm_.min(converged) 
0 did  comm_.min(converged) convergedRemote: 1
0 final convergedRemote: 1
0 Update: x^(k+1) = x^k - deltax^k 
0 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
0 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
0 Newton iteration  5 done0 , maximum relative shift = 4.505e-06
0 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
5 did  comm_.min(converged) convergedRemote: 1
5 final convergedRemote: 1
5 Update: x^(k+1) = x^k - deltax^k 
5 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
5 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
5 Newton iteration  5 done5 , maximum relative shift = 4.505e-06
5 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
2 Solve: M deltax^k = r 
2 Newton::solveLinearSystem : entering the try section 5
2 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
2 solveLinearSystemImpl_, before converged = ls.solve 
2 7 amgbackend::solve isParallel 1
5 Solve: M deltax^k = r 
5 Newton::solveLinearSystem : entering the try section 5
5 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
5 solveLinearSystemImpl_, before converged = ls.solve 
5 7 amgbackend::solve isParallel 1
4 Solve: M deltax^k = r 
4 Newton::solveLinearSystem : entering the try section 5
4 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
4 solveLinearSystemImpl_, before converged = ls.solve 
4 7 amgbackend::solve isParallel 1
6 Solve: M deltax^k = r 
6 Newton::solveLinearSystem : entering the try section 5
6 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
6 solveLinearSystemImpl_, before converged = ls.solve 
6 7 amgbackend::solve isParallel 1
6 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
1 Solve: M deltax^k = r 
1 Newton::solveLinearSystem : entering the try section 5
1 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
1 solveLinearSystemImpl_, before converged = ls.solve 
1 7 amgbackend::solve isParallel 1
1 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
1 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
0 Solve: M deltax^k = r 
0 Newton::solveLinearSystem : entering the try section 5
0 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
0 solveLinearSystemImpl_, before converged = ls.solve 
0 7 amgbackend::solve isParallel 1
3 Solve: M deltax^k = r 
3 Newton::solveLinearSystem : entering the try section 5
3 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
3 solveLinearSystemImpl_, before converged = ls.solve 
3 7 amgbackend::solve isParallel 1
3 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
3 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
5 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
5 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
4 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
4 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
6 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
0 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
0 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
2 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
2 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 7
5 7 before Dune::BiCGSTABSolver 
5 Dune::BiCGSTABSolver.apply(x, b, result_) 
5 BiCGSTABSolver::apply 
5 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
4 7 before Dune::BiCGSTABSolver 
4 Dune::BiCGSTABSolver.apply(x, b, result_) 
4 BiCGSTABSolver::apply 
4 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
6 7 before Dune::BiCGSTABSolver 
6 Dune::BiCGSTABSolver.apply(x, b, result_) 
6 BiCGSTABSolver::apply 
6 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
1 7 before Dune::BiCGSTABSolver 
1 Dune::BiCGSTABSolver.apply(x, b, result_) 
1 BiCGSTABSolver::apply 
1 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
2 7 before Dune::BiCGSTABSolver 
2 Dune::BiCGSTABSolver.apply(x, b, result_) 
2 BiCGSTABSolver::apply 
2 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
3 7 before Dune::BiCGSTABSolver 
3 Dune::BiCGSTABSolver.apply(x, b, result_) 
3 BiCGSTABSolver::apply 
3 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
Using a direct coarse solver (SuperLU)
Building hierarchy of 1 levels (inclusive coarse solver) took 0.112139 seconds.
0 7 before Dune::BiCGSTABSolver 
0 Dune::BiCGSTABSolver.apply(x, b, result_) 
0 BiCGSTABSolver::apply 
0 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
6 === BiCGSTABSolver
 Iter          Defect            Rate
    0      2.44831e-15
6  (it = 0.5; it < _maxit; it+=.5) 0.5 250
6  before p = r 
6  before  y = W^-1 * p 
6  before _prec->apply(y,p); 0.5 250
6 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
1 === BiCGSTABSolver
 Iter          Defect            Rate
    0      2.44831e-15
1  (it = 0.5; it < _maxit; it+=.5) 0.5 250
1  before p = r 
1  before  y = W^-1 * p 
1  before _prec->apply(y,p); 0.5 250
1 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
0 === BiCGSTABSolver
 Iter          Defect            Rate
    0      2.44831e-15
0  (it = 0.5; it < _maxit; it+=.5) 0.5 250
0  before p = r 
0  before  y = W^-1 * p 
0  before _prec->apply(y,p); 0.5 250
0 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 7
0 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
2 === BiCGSTABSolver
 Iter          Defect            Rate
    0      2.44831e-15
2  (it = 0.5; it < _maxit; it+=.5) 0.5 250
2  before p = r 
2  before  y = W^-1 * p 
2  before _prec->apply(y,p); 0.5 250
2 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
3 === BiCGSTABSolver
 Iter          Defect            Rate
    0      2.44831e-15
3  (it = 0.5; it < _maxit; it+=.5) 0.5 250
3  before p = r 
3  before  y = W^-1 * p 
3  before _prec->apply(y,p); 0.5 250
3 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
5 === BiCGSTABSolver
 Iter          Defect            Rate
    0      2.44831e-15
5  (it = 0.5; it < _maxit; it+=.5) 0.5 250
5  before p = r 
5  before  y = W^-1 * p 
5  before _prec->apply(y,p); 0.5 250
5 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
4 === BiCGSTABSolver
 Iter          Defect            Rate
    0      2.44831e-15
4  (it = 0.5; it < _maxit; it+=.5) 0.5 250
4  before p = r 
4  before  y = W^-1 * p 
4  before _prec->apply(y,p); 0.5 250
4 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
3 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
3 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
3  before v = A * y 
5 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
5 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
5  before v = A * y 
1 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
6 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
6 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
6  before v = A * y 
4 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
4 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
4  before v = A * y 
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
0 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
2 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
2 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
5  before  alpha = rho_new / < rt, v > 
1 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
1  before v = A * y 
2  before v = A * y 
2  before  alpha = rho_new / < rt, v > 
4  before  alpha = rho_new / < rt, v > 
0 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
0  before v = A * y 
0  before  alpha = rho_new / < rt, v > 
6  before  alpha = rho_new / < rt, v > 
3  before  alpha = rho_new / < rt, v > 
2  before alpha = rho_new / h; 
2  before apply first correction to x 
2  before r = r - alpha*v 
2  before test stop criteria 
2 istlsolver::printOutput 
  0.5      1.22615e-31      5.00815e-17
2 === rate=2.50816e-33, T=0.126351, TIT=0.252701, IT=0.5
2 finished solveLinearSystemImpl_ 1
2 converged value: 1 7 1 let s go get convergedRemote
2 to  comm_.min(converged) 
4  before alpha = rho_new / h; 
4  before apply first correction to x 
4  before r = r - alpha*v 
4  before test stop criteria 
4 istlsolver::printOutput 
  0.5      1.22615e-31      5.00815e-17
4 === rate=2.50816e-33, T=0.126349, TIT=0.252698, IT=0.5
4 finished solveLinearSystemImpl_ 1
4 converged value: 1 7 1 let s go get convergedRemote
4 to  comm_.min(converged) 
0  before alpha = rho_new / h; 
0  before apply first correction to x 
0  before r = r - alpha*v 
0  before test stop criteria 
0 istlsolver::printOutput 
  0.5      1.22615e-31      5.00815e-17
0 === rate=2.50816e-33, T=0.0221525, TIT=0.044305, IT=0.5
6  before alpha = rho_new / h; 
6  before apply first correction to x 
6  before r = r - alpha*v 
6  before test stop criteria 
6 istlsolver::printOutput 
  0.5      1.22615e-31      5.00815e-17
6 === rate=2.50816e-33, T=0.126348, TIT=0.252696, IT=0.5
6 finished solveLinearSystemImpl_ 1
6 converged value: 1 7 1 let s go get convergedRemote
6 to  comm_.min(converged) 
1  before  alpha = rho_new / < rt, v > 
1  before alpha = rho_new / h; 
1  before apply first correction to x 
1  before r = r - alpha*v 
1  before test stop criteria 
1 istlsolver::printOutput 
  0.5      1.22615e-31      5.00815e-17
1 === rate=2.50816e-33, T=0.126462, TIT=0.252924, IT=0.5
1 finished solveLinearSystemImpl_ 1
1 converged value: 1 7 1 let s go get convergedRemote
1 to  comm_.min(converged) 
5  before alpha = rho_new / h; 
5  before apply first correction to x 
5  before r = r - alpha*v 
5  before test stop criteria 
5 istlsolver::printOutput 
  0.5      1.22615e-31      5.00815e-17
5 === rate=2.50816e-33, T=0.126462, TIT=0.252925, IT=0.5
5 finished solveLinearSystemImpl_ 1
5 converged value: 1 7 1 let s go get convergedRemote
5 to  comm_.min(converged) 
3  before alpha = rho_new / h; 
3  before apply first correction to x 
3  before r = r - alpha*v 
3  before test stop criteria 
3 istlsolver::printOutput 
  0.5      1.22615e-31      5.00815e-17
3 === rate=2.50816e-33, T=0.126463, TIT=0.252927, IT=0.5
3 finished solveLinearSystemImpl_ 1
3 converged value: 1 7 1 let s go get convergedRemote
3 to  comm_.min(converged) 
5 did  comm_.min(converged) convergedRemote: 1
5 final convergedRemote: 1
5 Update: x^(k+1) = x^k - deltax^k 
5 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
5 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
5 Newton iteration  6 done5 , maximum relative shift = 9.174e-07
5 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
2 did  comm_.min(converged) convergedRemote: 1
2 final convergedRemote: 1
2 Update: x^(k+1) = x^k - deltax^k 
2 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
2 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
2 Newton iteration  6 done2 , maximum relative shift = 9.174e-07
2 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
4 did  comm_.min(converged) convergedRemote: 1
4 final convergedRemote: 1
4 Update: x^(k+1) = x^k - deltax^k 
4 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
4 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
4 Newton iteration  6 done4 , maximum relative shift = 9.174e-07
4 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
0 finished solveLinearSystemImpl_ 1
0 converged value: 1 7 1 let s go get convergedRemote
0 to  comm_.min(converged) 
0 did  comm_.min(converged) convergedRemote: 1
0 final convergedRemote: 1
0 Update: x^(k+1) = x^k - deltax^k 
0 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
0 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
0 Newton iteration  6 done0 , maximum relative shift = 9.174e-07
0 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
6 did  comm_.min(converged) convergedRemote: 1
6 final convergedRemote: 1
6 Update: x^(k+1) = x^k - deltax^k 
6 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
6 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
6 Newton iteration  6 done6 , maximum relative shift = 9.174e-07
6 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
1 did  comm_.min(converged) convergedRemote: 1
1 final convergedRemote: 1
1 Update: x^(k+1) = x^k - deltax^k 
1 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
1 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
1 Newton iteration  6 done1 , maximum relative shift = 9.174e-07
1 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
3 did  comm_.min(converged) convergedRemote: 1
3 final convergedRemote: 1
3 Update: x^(k+1) = x^k - deltax^k 
3 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
3 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
3 Newton iteration  6 done3 , maximum relative shift = 9.174e-07
3 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
2 Solve: M deltax^k = r 
2 Newton::solveLinearSystem : entering the try section 6
2 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
2 solveLinearSystemImpl_, before converged = ls.solve 
2 7 amgbackend::solve isParallel 1
4 Solve: M deltax^k = r 
4 Newton::solveLinearSystem : entering the try section 6
4 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
4 solveLinearSystemImpl_, before converged = ls.solve 
4 7 amgbackend::solve isParallel 1
0 Solve: M deltax^k = r 
0 Newton::solveLinearSystem : entering the try section 6
0 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
0 solveLinearSystemImpl_, before converged = ls.solve 
0 7 amgbackend::solve isParallel 1
6 Solve: M deltax^k = r 
6 Newton::solveLinearSystem : entering the try section 6
6 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
6 solveLinearSystemImpl_, before converged = ls.solve 
6 7 amgbackend::solve isParallel 1
3 Solve: M deltax^k = r 
3 Newton::solveLinearSystem : entering the try section 6
3 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
3 solveLinearSystemImpl_, before converged = ls.solve 
3 7 amgbackend::solve isParallel 1
1 Solve: M deltax^k = r 
1 Newton::solveLinearSystem : entering the try section 6
1 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
1 solveLinearSystemImpl_, before converged = ls.solve 
1 7 amgbackend::solve isParallel 1
5 Solve: M deltax^k = r 
5 Newton::solveLinearSystem : entering the try section 6
5 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
5 solveLinearSystemImpl_, before converged = ls.solve 
5 7 amgbackend::solve isParallel 1
6 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
6 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
2 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
2 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
4 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
4 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
0 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
0 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 7
5 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
5 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
3 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
3 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
1 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
1 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 7
1 7 before Dune::BiCGSTABSolver 
1 Dune::BiCGSTABSolver.apply(x, b, result_) 
1 BiCGSTABSolver::apply 
1 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
6 7 before Dune::BiCGSTABSolver 
6 Dune::BiCGSTABSolver.apply(x, b, result_) 
6 BiCGSTABSolver::apply 
6 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
2 7 before Dune::BiCGSTABSolver 
2 Dune::BiCGSTABSolver.apply(x, b, result_) 
2 BiCGSTABSolver::apply 
2 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
4 7 before Dune::BiCGSTABSolver 
4 Dune::BiCGSTABSolver.apply(x, b, result_) 
4 BiCGSTABSolver::apply 
4 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
5 7 before Dune::BiCGSTABSolver 
5 Dune::BiCGSTABSolver.apply(x, b, result_) 
5 BiCGSTABSolver::apply 
5 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
3 7 before Dune::BiCGSTABSolver 
3 Dune::BiCGSTABSolver.apply(x, b, result_) 
3 BiCGSTABSolver::apply 
3 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
Using a direct coarse solver (SuperLU)
Building hierarchy of 1 levels (inclusive coarse solver) took 0.112553 seconds.
0 7 before Dune::BiCGSTABSolver 
0 Dune::BiCGSTABSolver.apply(x, b, result_) 
0 BiCGSTABSolver::apply 
0 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
0 === BiCGSTABSolver
 Iter          Defect            Rate
    0      4.98644e-16
0  (it = 0.5; it < _maxit; it+=.5) 0.5 250
0  before p = r 
0  before  y = W^-1 * p 
0  before _prec->apply(y,p); 0.5 250
0 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
2 === BiCGSTABSolver
 Iter          Defect            Rate
    0      4.98644e-16
2  (it = 0.5; it < _maxit; it+=.5) 0.5 250
2  before p = r 
2  before  y = W^-1 * p 
2  before _prec->apply(y,p); 0.5 250
2 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 7
0 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
4 === BiCGSTABSolver
 Iter          Defect            Rate
    0      4.98644e-16
4  (it = 0.5; it < _maxit; it+=.5) 0.5 250
4  before p = r 
4  before  y = W^-1 * p 
4  before _prec->apply(y,p); 0.5 250
4 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
5 === BiCGSTABSolver
 Iter          Defect            Rate
    0      4.98644e-16
5  (it = 0.5; it < _maxit; it+=.5) 0.5 250
5  before p = r 
5  before  y = W^-1 * p 
5  before _prec->apply(y,p); 0.5 250
5 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
3 === BiCGSTABSolver
 Iter          Defect            Rate
    0      4.98644e-16
3  (it = 0.5; it < _maxit; it+=.5) 0.5 250
3  before p = r 
3  before  y = W^-1 * p 
3  before _prec->apply(y,p); 0.5 250
3 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
1 === BiCGSTABSolver
 Iter          Defect            Rate
    0      4.98644e-16
1  (it = 0.5; it < _maxit; it+=.5) 0.5 250
1  before p = r 
1  before  y = W^-1 * p 
1  before _prec->apply(y,p); 0.5 250
1 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
6 === BiCGSTABSolver
 Iter          Defect            Rate
    0      4.98644e-16
6  (it = 0.5; it < _maxit; it+=.5) 0.5 250
6  before p = r 
6  before  y = W^-1 * p 
6  before _prec->apply(y,p); 0.5 250
6 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
4 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
4 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
4  before v = A * y 
5 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
5 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
5  before v = A * y 
3 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
3 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
3  before v = A * y 
1 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
2 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
2 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
6 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
6 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
6  before v = A * y 
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
0 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
3  before  alpha = rho_new / < rt, v > 
1 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
0 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
6  before  alpha = rho_new / < rt, v > 
2  before v = A * y 
4  before  alpha = rho_new / < rt, v > 
5  before  alpha = rho_new / < rt, v > 
0  before v = A * y 
0  before  alpha = rho_new / < rt, v > 
0  before alpha = rho_new / h; 
0  before apply first correction to x 
0  before r = r - alpha*v 
0  before test stop criteria 
0 istlsolver::printOutput 
  0.5       1.6751e-34       3.3593e-19
0 === rate=1.12849e-37, T=0.0183984, TIT=0.0367968, IT=0.5
6  before alpha = rho_new / h; 
6  before apply first correction to x 
6  before r = r - alpha*v 
6  before test stop criteria 
6 istlsolver::printOutput 
  0.5       1.6751e-34       3.3593e-19
6 === rate=1.12849e-37, T=0.12302, TIT=0.246041, IT=0.5
6 finished solveLinearSystemImpl_ 1
6 converged value: 1 7 1 let s go get convergedRemote
6 to  comm_.min(converged) 
2  before  alpha = rho_new / < rt, v > 
2  before alpha = rho_new / h; 
2  before apply first correction to x 
2  before r = r - alpha*v 
2  before test stop criteria 
2 istlsolver::printOutput 
  0.5       1.6751e-34       3.3593e-19
2 === rate=1.12849e-37, T=0.123017, TIT=0.246034, IT=0.5
2 finished solveLinearSystemImpl_ 1
2 converged value: 1 7 1 let s go get convergedRemote
2 to  comm_.min(converged) 
4  before alpha = rho_new / h; 
4  before apply first correction to x 
4  before r = r - alpha*v 
4  before test stop criteria 
4 istlsolver::printOutput 
  0.5       1.6751e-34       3.3593e-19
4 === rate=1.12849e-37, T=0.123019, TIT=0.246038, IT=0.5
4 finished solveLinearSystemImpl_ 1
4 converged value: 1 7 1 let s go get convergedRemote
4 to  comm_.min(converged) 
1  before v = A * y 
1  before  alpha = rho_new / < rt, v > 
1  before alpha = rho_new / h; 
1  before apply first correction to x 
1  before r = r - alpha*v 
1  before test stop criteria 
1 istlsolver::printOutput 
  0.5       1.6751e-34       3.3593e-19
1 === rate=1.12849e-37, T=0.123035, TIT=0.246069, IT=0.5
1 finished solveLinearSystemImpl_ 1
1 converged value: 1 7 1 let s go get convergedRemote
1 to  comm_.min(converged) 
3  before alpha = rho_new / h; 
3  before apply first correction to x 
3  before r = r - alpha*v 
3  before test stop criteria 
3 istlsolver::printOutput 
  0.5       1.6751e-34       3.3593e-19
3 === rate=1.12849e-37, T=0.12304, TIT=0.246081, IT=0.5
3 finished solveLinearSystemImpl_ 1
3 converged value: 1 7 1 let s go get convergedRemote
3 to  comm_.min(converged) 
5  before alpha = rho_new / h; 
5  before apply first correction to x 
5  before r = r - alpha*v 
5  before test stop criteria 
5 istlsolver::printOutput 
  0.5       1.6751e-34       3.3593e-19
5 === rate=1.12849e-37, T=0.12304, TIT=0.24608, IT=0.5
5 finished solveLinearSystemImpl_ 1
5 converged value: 1 7 1 let s go get convergedRemote
5 to  comm_.min(converged) 
6 did  comm_.min(converged) convergedRemote: 1
6 final convergedRemote: 1
6 Update: x^(k+1) = x^k - deltax^k 
6 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
6 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
6 Newton iteration  7 done6 , maximum relative shift = 1.869e-07
6 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
2 did  comm_.min(converged) convergedRemote: 1
2 final convergedRemote: 1
2 Update: x^(k+1) = x^k - deltax^k 
2 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
2 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
4 did  comm_.min(converged) convergedRemote: 1
4 final convergedRemote: 1
4 Update: x^(k+1) = x^k - deltax^k 
4 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
4 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
4 Newton iteration  7 done4 , maximum relative shift = 1.869e-07
4 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
1 did  comm_.min(converged) convergedRemote: 1
1 final convergedRemote: 1
1 Update: x^(k+1) = x^k - deltax^k 
1 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
1 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
1 Newton iteration  7 done1 , maximum relative shift = 1.869e-07
1 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
5 did  comm_.min(converged) convergedRemote: 1
5 final convergedRemote: 1
5 Update: x^(k+1) = x^k - deltax^k 
5 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
5 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
5 Newton iteration  7 done5 , maximum relative shift = 1.869e-07
5 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
3 did  comm_.min(converged) convergedRemote: 1
3 final convergedRemote: 1
3 Update: x^(k+1) = x^k - deltax^k 
3 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
3 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
3 Newton iteration  7 done3 , maximum relative shift = 1.869e-07
3 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
0 finished solveLinearSystemImpl_ 1
0 converged value: 1 7 1 let s go get convergedRemote
0 to  comm_.min(converged) 
0 did  comm_.min(converged) convergedRemote: 1
0 final convergedRemote: 1
0 Update: x^(k+1) = x^k - deltax^k 
0 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
0 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
0 Newton iteration  7 done0 , maximum relative shift = 1.869e-07
0 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
2 Newton iteration  7 done2 , maximum relative shift = 1.869e-07
2 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
5 Solve: M deltax^k = r 
5 Newton::solveLinearSystem : entering the try section 7
5 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
5 solveLinearSystemImpl_, before converged = ls.solve 
5 7 amgbackend::solve isParallel 1
3 Solve: M deltax^k = r 
3 Newton::solveLinearSystem : entering the try section 7
3 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
3 solveLinearSystemImpl_, before converged = ls.solve 
3 7 amgbackend::solve isParallel 1
1 Solve: M deltax^k = r 
1 Newton::solveLinearSystem : entering the try section 7
1 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
1 solveLinearSystemImpl_, before converged = ls.solve 
1 7 amgbackend::solve isParallel 1
6 Solve: M deltax^k = r 
6 Newton::solveLinearSystem : entering the try section 7
6 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
6 solveLinearSystemImpl_, before converged = ls.solve 
6 7 amgbackend::solve isParallel 1
0 Solve: M deltax^k = r 
0 Newton::solveLinearSystem : entering the try section 7
0 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
0 solveLinearSystemImpl_, before converged = ls.solve 
0 7 amgbackend::solve isParallel 1
4 Solve: M deltax^k = r 
4 Newton::solveLinearSystem : entering the try section 7
4 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
4 solveLinearSystemImpl_, before converged = ls.solve 
4 7 amgbackend::solve isParallel 1
2 Solve: M deltax^k = r 
2 Newton::solveLinearSystem : entering the try section 7
2 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
2 solveLinearSystemImpl_, before converged = ls.solve 
2 7 amgbackend::solve isParallel 1
4 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
4 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
5 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
5 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
3 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
3 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
1 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
1 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
6 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
6 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
0 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
0 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 7
2 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
2 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 7
4 7 before Dune::BiCGSTABSolver 
4 Dune::BiCGSTABSolver.apply(x, b, result_) 
4 BiCGSTABSolver::apply 
4 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
5 7 before Dune::BiCGSTABSolver 
5 Dune::BiCGSTABSolver.apply(x, b, result_) 
5 BiCGSTABSolver::apply 
5 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
3 7 before Dune::BiCGSTABSolver 
3 Dune::BiCGSTABSolver.apply(x, b, result_) 
3 BiCGSTABSolver::apply 
3 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
1 7 before Dune::BiCGSTABSolver 
1 Dune::BiCGSTABSolver.apply(x, b, result_) 
1 BiCGSTABSolver::apply 
1 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
6 7 before Dune::BiCGSTABSolver 
6 Dune::BiCGSTABSolver.apply(x, b, result_) 
6 BiCGSTABSolver::apply 
6 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 7
2 7 before Dune::BiCGSTABSolver 
2 Dune::BiCGSTABSolver.apply(x, b, result_) 
2 BiCGSTABSolver::apply 
2 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
Using a direct coarse solver (SuperLU)
Building hierarchy of 1 levels (inclusive coarse solver) took 0.112073 seconds.
0 7 before Dune::BiCGSTABSolver 
0 Dune::BiCGSTABSolver.apply(x, b, result_) 
0 BiCGSTABSolver::apply 
0 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
0 === BiCGSTABSolver
 Iter          Defect            Rate
    0      1.01643e-16
0  (it = 0.5; it < _maxit; it+=.5) 0.5 250
0  before p = r 
0  before  y = W^-1 * p 
0  before _prec->apply(y,p); 0.5 250
0 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 7
0 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
2 === BiCGSTABSolver
 Iter          Defect            Rate
    0      1.01643e-16
2  (it = 0.5; it < _maxit; it+=.5) 0.5 250
2  before p = r 
2  before  y = W^-1 * p 
2  before _prec->apply(y,p); 0.5 250
2 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
6 === BiCGSTABSolver
 Iter          Defect            Rate
    0      1.01643e-16
6  (it = 0.5; it < _maxit; it+=.5) 0.5 250
6  before p = r 
6  before  y = W^-1 * p 
6  before _prec->apply(y,p); 0.5 250
6 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
4 === BiCGSTABSolver
 Iter          Defect            Rate
    0      1.01643e-16
4  (it = 0.5; it < _maxit; it+=.5) 0.5 250
4  before p = r 
4  before  y = W^-1 * p 
4  before _prec->apply(y,p); 0.5 250
4 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
5 === BiCGSTABSolver
 Iter          Defect            Rate
    0      1.01643e-16
5  (it = 0.5; it < _maxit; it+=.5) 0.5 250
5  before p = r 
5  before  y = W^-1 * p 
5  before _prec->apply(y,p); 0.5 250
5 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
3 === BiCGSTABSolver
 Iter          Defect            Rate
    0      1.01643e-16
3  (it = 0.5; it < _maxit; it+=.5) 0.5 250
3  before p = r 
3  before  y = W^-1 * p 
3  before _prec->apply(y,p); 0.5 250
3 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
1 === BiCGSTABSolver
 Iter          Defect            Rate
    0      1.01643e-16
1  (it = 0.5; it < _maxit; it+=.5) 0.5 250
1  before p = r 
1  before  y = W^-1 * p 
1  before _prec->apply(y,p); 0.5 250
1 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
1 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
0 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
2 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
2 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
6 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
6 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
6  before v = A * y 
4 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
4 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
4  before v = A * y 
5 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
5 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
5  before v = A * y 
3 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
3 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
3  before v = A * y 
4  before  alpha = rho_new / < rt, v > 
5  before  alpha = rho_new / < rt, v > 
3  before  alpha = rho_new / < rt, v > 
1 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
0 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
2  before v = A * y 
6  before  alpha = rho_new / < rt, v > 
3  before alpha = rho_new / h; 
3  before apply first correction to x 
3  before r = r - alpha*v 
3  before test stop criteria 
3 istlsolver::printOutput 
  0.5      1.05614e-33      1.03907e-17
3 === rate=1.07966e-34, T=0.12281, TIT=0.245621, IT=0.5
3 finished solveLinearSystemImpl_ 1
3 converged value: 1 7 1 let s go get convergedRemote
3 to  comm_.min(converged) 
1  before v = A * y 
1  before  alpha = rho_new / < rt, v > 
1  before alpha = rho_new / h; 
1  before apply first correction to x 
1  before r = r - alpha*v 
1  before test stop criteria 
1 istlsolver::printOutput 
  0.5      1.05614e-33      1.03907e-17
1 === rate=1.07966e-34, T=0.122812, TIT=0.245624, IT=0.5
1 finished solveLinearSystemImpl_ 1
1 converged value: 1 7 1 let s go get convergedRemote
1 to  comm_.min(converged) 
0  before v = A * y 
0  before  alpha = rho_new / < rt, v > 
0  before alpha = rho_new / h; 
0  before apply first correction to x 
0  before r = r - alpha*v 
0  before test stop criteria 
0 istlsolver::printOutput 
  0.5      1.05614e-33      1.03907e-17
0 === rate=1.07966e-34, T=0.0186476, TIT=0.0372952, IT=0.5
2  before  alpha = rho_new / < rt, v > 
2  before alpha = rho_new / h; 
2  before apply first correction to x 
2  before r = r - alpha*v 
2  before test stop criteria 
2 istlsolver::printOutput 
  0.5      1.05614e-33      1.03907e-17
2 === rate=1.07966e-34, T=0.122872, TIT=0.245744, IT=0.5
2 finished solveLinearSystemImpl_ 1
2 converged value: 1 7 1 let s go get convergedRemote
2 to  comm_.min(converged) 
5  before alpha = rho_new / h; 
5  before apply first correction to x 
5  before r = r - alpha*v 
5  before test stop criteria 
5 istlsolver::printOutput 
  0.5      1.05614e-33      1.03907e-17
5 === rate=1.07966e-34, T=0.122811, TIT=0.245623, IT=0.5
5 finished solveLinearSystemImpl_ 1
5 converged value: 1 7 1 let s go get convergedRemote
5 to  comm_.min(converged) 
4  before alpha = rho_new / h; 
4  before apply first correction to x 
4  before r = r - alpha*v 
4  before test stop criteria 
4 istlsolver::printOutput 
  0.5      1.05614e-33      1.03907e-17
4 === rate=1.07966e-34, T=0.122873, TIT=0.245746, IT=0.5
4 finished solveLinearSystemImpl_ 1
4 converged value: 1 7 1 let s go get convergedRemote
4 to  comm_.min(converged) 
6  before alpha = rho_new / h; 
6  before apply first correction to x 
6  before r = r - alpha*v 
6  before test stop criteria 
6 istlsolver::printOutput 
  0.5      1.05614e-33      1.03907e-17
6 === rate=1.07966e-34, T=0.122873, TIT=0.245746, IT=0.5
6 finished solveLinearSystemImpl_ 1
6 converged value: 1 7 1 let s go get convergedRemote
6 to  comm_.min(converged) 
0 finished solveLinearSystemImpl_ 1
0 converged value: 1 7 1 let s go get convergedRemote
0 to  comm_.min(converged) 
0 did  comm_.min(converged) convergedRemote: 1
0 final convergedRemote: 1
0 Update: x^(k+1) = x^k - deltax^k 
0 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
0 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
0 Newton iteration  8 done0 , maximum relative shift = 3.048e-08
0 Assemble/solve/update time: 0.351239(24.7995%)/1.06489(75.1879%)/0.000178631(0.0126124%)
0 gridVariables->advanceTimeStep
0 before, nonLinearSolver->suggestTimeStepSize, current time: 673.767 simTime 57600
0 before assembler->setPreviousSolution, ddt: 22.0103
0 nonLinearSolver->solve
0 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
2 did  comm_.min(converged) convergedRemote: 1
2 final convergedRemote: 1
2 Update: x^(k+1) = x^k - deltax^k 
2 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
2 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
2 Newton iteration  8 done2 , maximum relative shift = 3.048e-08
2 Assemble/solve/update time: 0.352015(24.84%)/1.06494(75.1479%)/0.000170612(0.0120393%)
2 gridVariables->advanceTimeStep
2 before, nonLinearSolver->suggestTimeStepSize, current time: 673.767 simTime 57600
2 before assembler->setPreviousSolution, ddt: 22.0103
2 nonLinearSolver->solve
2 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
5 did  comm_.min(converged) convergedRemote: 1
5 final convergedRemote: 1
5 Update: x^(k+1) = x^k - deltax^k 
5 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
5 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
5 Newton iteration  8 done5 , maximum relative shift = 3.048e-08
5 Assemble/solve/update time: 0.352776(24.8813%)/1.06491(75.1084%)/0.000145301(0.0102481%)
5 gridVariables->advanceTimeStep
5 before, nonLinearSolver->suggestTimeStepSize, current time: 673.767 simTime 57600
5 before assembler->setPreviousSolution, ddt: 22.0103
5 nonLinearSolver->solve
5 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
6 did  comm_.min(converged) convergedRemote: 1
6 final convergedRemote: 1
6 Update: x^(k+1) = x^k - deltax^k 
6 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
6 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
6 Newton iteration  8 done6 , maximum relative shift = 3.048e-08
6 Assemble/solve/update time: 0.352721(24.8784%)/1.06494(75.1135%)/0.000116011(0.00818258%)
6 gridVariables->advanceTimeStep
6 before, nonLinearSolver->suggestTimeStepSize, current time: 673.767 simTime 57600
6 before assembler->setPreviousSolution, ddt: 22.0103
6 nonLinearSolver->solve
6 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
4 did  comm_.min(converged) convergedRemote: 1
4 final convergedRemote: 1
4 Update: x^(k+1) = x^k - deltax^k 
4 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
4 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
4 Newton iteration  8 done4 , maximum relative shift = 3.048e-08
4 Assemble/solve/update time: 0.352513(24.8671%)/1.06492(75.1218%)/0.000156802(0.0110612%)
4 gridVariables->advanceTimeStep
4 before, nonLinearSolver->suggestTimeStepSize, current time: 673.767 simTime 57600
4 before assembler->setPreviousSolution, ddt: 22.0103
4 nonLinearSolver->solve
4 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
3 did  comm_.min(converged) convergedRemote: 1
3 final convergedRemote: 1
3 Update: x^(k+1) = x^k - deltax^k 
3 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
3 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
3 Newton iteration  8 done3 , maximum relative shift = 3.048e-08
3 Assemble/solve/update time: 0.352726(24.878%)/1.06494(75.1111%)/0.000154011(0.0108625%)
3 gridVariables->advanceTimeStep
3 before, nonLinearSolver->suggestTimeStepSize, current time: 673.767 simTime 57600
3 before assembler->setPreviousSolution, ddt: 22.0103
3 nonLinearSolver->solve
3 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
1 did  comm_.min(converged) convergedRemote: 1
1 final convergedRemote: 1
1 Update: x^(k+1) = x^k - deltax^k 
1 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
1 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
1 Newton iteration  8 done1 , maximum relative shift = 3.048e-08
1 Assemble/solve/update time: 0.352757(24.8802%)/1.06492(75.1091%)/0.000151891(0.010713%)
1 gridVariables->advanceTimeStep
1 before, nonLinearSolver->suggestTimeStepSize, current time: 673.767 simTime 57600
1 before assembler->setPreviousSolution, ddt: 22.0103
1 nonLinearSolver->solve
1 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
0 Solve: M deltax^k = r 
0 Newton::solveLinearSystem : entering the try section 0
0to  comm_.sum(norm2) 
0did  comm_.sum(norm2) 
0 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
0 solveLinearSystemImpl_, before converged = ls.solve 
0 7 amgbackend::solve isParallel 1
2 Solve: M deltax^k = r 
2 Newton::solveLinearSystem : entering the try section 0
2to  comm_.sum(norm2) 
2did  comm_.sum(norm2) 
2 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
2 solveLinearSystemImpl_, before converged = ls.solve 
2 7 amgbackend::solve isParallel 1
1 Solve: M deltax^k = r 
1 Newton::solveLinearSystem : entering the try section 0
1to  comm_.sum(norm2) 
1did  comm_.sum(norm2) 
1 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
1 solveLinearSystemImpl_, before converged = ls.solve 
1 7 amgbackend::solve isParallel 1
6 Solve: M deltax^k = r 
6 Newton::solveLinearSystem : entering the try section 0
6to  comm_.sum(norm2) 
6did  comm_.sum(norm2) 
6 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
6 solveLinearSystemImpl_, before converged = ls.solve 
6 7 amgbackend::solve isParallel 1
4 Solve: M deltax^k = r 
4 Newton::solveLinearSystem : entering the try section 0
4to  comm_.sum(norm2) 
4did  comm_.sum(norm2) 
4 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
4 solveLinearSystemImpl_, before converged = ls.solve 
4 7 amgbackend::solve isParallel 1
3 Solve: M deltax^k = r 
3 Newton::solveLinearSystem : entering the try section 0
3to  comm_.sum(norm2) 
3did  comm_.sum(norm2) 
3 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
3 solveLinearSystemImpl_, before converged = ls.solve 
3 7 amgbackend::solve isParallel 1
5 Solve: M deltax^k = r 
5 Newton::solveLinearSystem : entering the try section 0
5to  comm_.sum(norm2) 
5did  comm_.sum(norm2) 
5 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
5 solveLinearSystemImpl_, before converged = ls.solve 
5 7 amgbackend::solve isParallel 1
1 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
1 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
6 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
6 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
4 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
4 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
3 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
3 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
2 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
2 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
0 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
0 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 7
5 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
5 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 7
6 7 before Dune::BiCGSTABSolver 
6 Dune::BiCGSTABSolver.apply(x, b, result_) 
6 BiCGSTABSolver::apply 
6 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
4 7 before Dune::BiCGSTABSolver 
4 Dune::BiCGSTABSolver.apply(x, b, result_) 
4 BiCGSTABSolver::apply 
4 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
3 7 before Dune::BiCGSTABSolver 
3 Dune::BiCGSTABSolver.apply(x, b, result_) 
3 BiCGSTABSolver::apply 
3 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
2 7 before Dune::BiCGSTABSolver 
2 Dune::BiCGSTABSolver.apply(x, b, result_) 
2 BiCGSTABSolver::apply 
2 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 7
5 7 before Dune::BiCGSTABSolver 
5 Dune::BiCGSTABSolver.apply(x, b, result_) 
5 BiCGSTABSolver::apply 
5 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
1 7 before Dune::BiCGSTABSolver 
1 Dune::BiCGSTABSolver.apply(x, b, result_) 
1 BiCGSTABSolver::apply 
1 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
Using a direct coarse solver (SuperLU)
Building hierarchy of 1 levels (inclusive coarse solver) took 0.11329 seconds.
0 7 before Dune::BiCGSTABSolver 
0 Dune::BiCGSTABSolver.apply(x, b, result_) 
0 BiCGSTABSolver::apply 
0 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
0 === BiCGSTABSolver
 Iter          Defect            Rate
    0       3.1618e-09
0  (it = 0.5; it < _maxit; it+=.5) 0.5 250
3 === BiCGSTABSolver
 Iter          Defect            Rate
    0       3.1618e-09
3  (it = 0.5; it < _maxit; it+=.5) 0.5 250
3  before p = r 
3  before  y = W^-1 * p 
3  before _prec->apply(y,p); 0.5 250
3 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
1 === BiCGSTABSolver
 Iter          Defect            Rate
    0       3.1618e-09
1  (it = 0.5; it < _maxit; it+=.5) 0.5 250
1  before p = r 
1  before  y = W^-1 * p 
1  before _prec->apply(y,p); 0.5 250
1 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
2 === BiCGSTABSolver
 Iter          Defect            Rate
    0       3.1618e-09
2  (it = 0.5; it < _maxit; it+=.5) 0.5 250
2  before p = r 
2  before  y = W^-1 * p 
2  before _prec->apply(y,p); 0.5 250
2 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
6 === BiCGSTABSolver
 Iter          Defect            Rate
    0       3.1618e-09
6  (it = 0.5; it < _maxit; it+=.5) 0.5 250
6  before p = r 
6  before  y = W^-1 * p 
6  before _prec->apply(y,p); 0.5 250
6 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
4 === BiCGSTABSolver
 Iter          Defect            Rate
    0       3.1618e-09
4  (it = 0.5; it < _maxit; it+=.5) 0.5 250
4  before p = r 
4  before  y = W^-1 * p 
4  before _prec->apply(y,p); 0.5 250
4 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
5 === BiCGSTABSolver
 Iter          Defect            Rate
    0       3.1618e-09
5  (it = 0.5; it < _maxit; it+=.5) 0.5 250
5  before p = r 
5  before  y = W^-1 * p 
5  before _prec->apply(y,p); 0.5 250
5 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
0  before p = r 
0  before  y = W^-1 * p 
0  before _prec->apply(y,p); 0.5 250
0 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 7
0 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
5 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
5 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
5  before v = A * y 
3 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
3 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
3  before v = A * y 
1 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
2 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
2 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
6 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
6 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
6  before v = A * y 
4 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
4 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
4  before v = A * y 
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
0 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
6  before  alpha = rho_new / < rt, v > 
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
0 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
0  before v = A * y 
0  before  alpha = rho_new / < rt, v > 
4  before  alpha = rho_new / < rt, v > 
5  before  alpha = rho_new / < rt, v > 
3  before  alpha = rho_new / < rt, v > 
1 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
1  before v = A * y 
1  before  alpha = rho_new / < rt, v > 
2  before v = A * y 
2  before  alpha = rho_new / < rt, v > 
0  before alpha = rho_new / h; 
0  before apply first correction to x 
0  before r = r - alpha*v 
0  before test stop criteria 
0 istlsolver::printOutput 
  0.5      4.65484e-25      1.47221e-16
0 === rate=2.1674e-32, T=0.0187092, TIT=0.0374184, IT=0.5
4  before alpha = rho_new / h; 
4  before apply first correction to x 
4  before r = r - alpha*v 
4  before test stop criteria 
4 istlsolver::printOutput 
  0.5      4.65484e-25      1.47221e-16
4 === rate=2.1674e-32, T=0.124036, TIT=0.248071, IT=0.5
4 finished solveLinearSystemImpl_ 1
4 converged value: 1 7 1 let s go get convergedRemote
4 to  comm_.min(converged) 
5  before alpha = rho_new / h; 
5  before apply first correction to x 
5  before r = r - alpha*v 
5  before test stop criteria 
5 istlsolver::printOutput 
  0.5      4.65484e-25      1.47221e-16
5 === rate=2.1674e-32, T=0.124012, TIT=0.248024, IT=0.5
5 finished solveLinearSystemImpl_ 1
5 converged value: 1 7 1 let s go get convergedRemote
5 to  comm_.min(converged) 
3  before alpha = rho_new / h; 
3  before apply first correction to x 
3  before r = r - alpha*v 
3  before test stop criteria 
3 istlsolver::printOutput 
  0.5      4.65484e-25      1.47221e-16
3 === rate=2.1674e-32, T=0.124012, TIT=0.248025, IT=0.5
3 finished solveLinearSystemImpl_ 1
3 converged value: 1 7 1 let s go get convergedRemote
3 to  comm_.min(converged) 
1  before alpha = rho_new / h; 
1  before apply first correction to x 
1  before r = r - alpha*v 
1  before test stop criteria 
1 istlsolver::printOutput 
  0.5      4.65484e-25      1.47221e-16
1 === rate=2.1674e-32, T=0.124015, TIT=0.24803, IT=0.5
1 finished solveLinearSystemImpl_ 1
1 converged value: 1 7 1 let s go get convergedRemote
1 to  comm_.min(converged) 
6  before alpha = rho_new / h; 
6  before apply first correction to x 
6  before r = r - alpha*v 
6  before test stop criteria 
6 istlsolver::printOutput 
  0.5      4.65484e-25      1.47221e-16
6 === rate=2.1674e-32, T=0.124038, TIT=0.248077, IT=0.5
6 finished solveLinearSystemImpl_ 1
6 converged value: 1 7 1 let s go get convergedRemote
6 to  comm_.min(converged) 
2  before alpha = rho_new / h; 
2  before apply first correction to x 
2  before r = r - alpha*v 
2  before test stop criteria 
2 istlsolver::printOutput 
  0.5      4.65484e-25      1.47221e-16
2 === rate=2.1674e-32, T=0.124037, TIT=0.248073, IT=0.5
2 finished solveLinearSystemImpl_ 1
2 converged value: 1 7 1 let s go get convergedRemote
2 to  comm_.min(converged) 
2 did  comm_.min(converged) convergedRemote: 1
2 final convergedRemote: 1
2 Update: x^(k+1) = x^k - deltax^k 
2 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
2 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
2 Newton iteration  1 done2 , maximum relative shift = 1.049e-01
2 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
6 did  comm_.min(converged) convergedRemote: 1
6 final convergedRemote: 1
6 Update: x^(k+1) = x^k - deltax^k 
6 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
6 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
6 Newton iteration  1 done6 , maximum relative shift = 1.049e-01
6 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
0 finished solveLinearSystemImpl_ 1
0 converged value: 1 7 1 let s go get convergedRemote
0 to  comm_.min(converged) 
0 did  comm_.min(converged) convergedRemote: 1
0 final convergedRemote: 1
0 Update: x^(k+1) = x^k - deltax^k 
0 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
0 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
0 Newton iteration  1 done0 , maximum relative shift = 1.049e-01
0 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
4 did  comm_.min(converged) convergedRemote: 1
4 final convergedRemote: 1
4 Update: x^(k+1) = x^k - deltax^k 
4 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
4 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
4 Newton iteration  1 done4 , maximum relative shift = 1.049e-01
4 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
5 did  comm_.min(converged) convergedRemote: 1
5 final convergedRemote: 1
5 Update: x^(k+1) = x^k - deltax^k 
5 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
5 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
5 Newton iteration  1 done5 , maximum relative shift = 1.049e-01
5 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
3 did  comm_.min(converged) convergedRemote: 1
3 final convergedRemote: 1
3 Update: x^(k+1) = x^k - deltax^k 
3 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
3 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
3 Newton iteration  1 done3 , maximum relative shift = 1.049e-01
3 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
1 did  comm_.min(converged) convergedRemote: 1
1 final convergedRemote: 1
1 Update: x^(k+1) = x^k - deltax^k 
1 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
1 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
1 Newton iteration  1 done1 , maximum relative shift = 1.049e-01
1 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
4 Solve: M deltax^k = r 
4 Newton::solveLinearSystem : entering the try section 1
4 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
4 solveLinearSystemImpl_, before converged = ls.solve 
4 7 amgbackend::solve isParallel 1
5 Solve: M deltax^k = r 
5 Newton::solveLinearSystem : entering the try section 1
5 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
5 solveLinearSystemImpl_, before converged = ls.solve 
5 7 amgbackend::solve isParallel 1
3 Solve: M deltax^k = r 
3 Newton::solveLinearSystem : entering the try section 1
3 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
3 solveLinearSystemImpl_, before converged = ls.solve 
3 7 amgbackend::solve isParallel 1
1 Solve: M deltax^k = r 
1 Newton::solveLinearSystem : entering the try section 1
1 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
1 solveLinearSystemImpl_, before converged = ls.solve 
1 7 amgbackend::solve isParallel 1
2 Solve: M deltax^k = r 
2 Newton::solveLinearSystem : entering the try section 1
2 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
2 solveLinearSystemImpl_, before converged = ls.solve 
2 7 amgbackend::solve isParallel 1
6 Solve: M deltax^k = r 
6 Newton::solveLinearSystem : entering the try section 1
6 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
6 solveLinearSystemImpl_, before converged = ls.solve 
6 7 amgbackend::solve isParallel 1
0 Solve: M deltax^k = r 
0 Newton::solveLinearSystem : entering the try section 1
0 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
0 solveLinearSystemImpl_, before converged = ls.solve 
0 7 amgbackend::solve isParallel 1
0 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
0 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 7
1 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
1 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
2 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
2 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
6 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
6 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
3 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
3 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
4 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
4 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
5 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
5 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 7
6 7 before Dune::BiCGSTABSolver 
6 Dune::BiCGSTABSolver.apply(x, b, result_) 
6 BiCGSTABSolver::apply 
6 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
3 7 before Dune::BiCGSTABSolver 
3 Dune::BiCGSTABSolver.apply(x, b, result_) 
3 BiCGSTABSolver::apply 
3 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
4 7 before Dune::BiCGSTABSolver 
4 Dune::BiCGSTABSolver.apply(x, b, result_) 
4 BiCGSTABSolver::apply 
4 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 7
5 7 before Dune::BiCGSTABSolver 
5 Dune::BiCGSTABSolver.apply(x, b, result_) 
5 BiCGSTABSolver::apply 
5 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
1 7 before Dune::BiCGSTABSolver 
1 Dune::BiCGSTABSolver.apply(x, b, result_) 
1 BiCGSTABSolver::apply 
1 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
2 7 before Dune::BiCGSTABSolver 
2 Dune::BiCGSTABSolver.apply(x, b, result_) 
2 BiCGSTABSolver::apply 
2 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
Using a direct coarse solver (SuperLU)
Building hierarchy of 1 levels (inclusive coarse solver) took 0.111975 seconds.
0 7 before Dune::BiCGSTABSolver 
0 Dune::BiCGSTABSolver.apply(x, b, result_) 
0 BiCGSTABSolver::apply 
0 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
0 === BiCGSTABSolver
 Iter          Defect            Rate
    0      1.37437e-11
0  (it = 0.5; it < _maxit; it+=.5) 0.5 250
0  before p = r 
0  before  y = W^-1 * p 
0  before _prec->apply(y,p); 0.5 250
0 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 7
0 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
2 === BiCGSTABSolver
 Iter          Defect            Rate
    0      1.37437e-11
2  (it = 0.5; it < _maxit; it+=.5) 0.5 250
2  before p = r 
2  before  y = W^-1 * p 
2  before _prec->apply(y,p); 0.5 250
2 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
5 === BiCGSTABSolver
 Iter          Defect            Rate
    0      1.37437e-11
5  (it = 0.5; it < _maxit; it+=.5) 0.5 250
5  before p = r 
5  before  y = W^-1 * p 
5  before _prec->apply(y,p); 0.5 250
5 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
1 === BiCGSTABSolver
 Iter          Defect            Rate
    0      1.37437e-11
1  (it = 0.5; it < _maxit; it+=.5) 0.5 250
1  before p = r 
1  before  y = W^-1 * p 
1  before _prec->apply(y,p); 0.5 250
1 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
4 === BiCGSTABSolver
 Iter          Defect            Rate
    0      1.37437e-11
4  (it = 0.5; it < _maxit; it+=.5) 0.5 250
4  before p = r 
4  before  y = W^-1 * p 
4  before _prec->apply(y,p); 0.5 250
4 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
6 === BiCGSTABSolver
 Iter          Defect            Rate
    0      1.37437e-11
6  (it = 0.5; it < _maxit; it+=.5) 0.5 250
6  before p = r 
6  before  y = W^-1 * p 
6  before _prec->apply(y,p); 0.5 250
6 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
3 === BiCGSTABSolver
 Iter          Defect            Rate
    0      1.37437e-11
3  (it = 0.5; it < _maxit; it+=.5) 0.5 250
3  before p = r 
3  before  y = W^-1 * p 
3  before _prec->apply(y,p); 0.5 250
3 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
6 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
6 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
6  before v = A * y 
3 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
3 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
3  before v = A * y 
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
0 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
2 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
2 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
5 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
5 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
5  before v = A * y 
1 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
4 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
4 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
4  before v = A * y 
0 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
0  before v = A * y 
0  before  alpha = rho_new / < rt, v > 
2  before v = A * y 
2  before  alpha = rho_new / < rt, v > 
5  before  alpha = rho_new / < rt, v > 
1 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
1  before v = A * y 
6  before  alpha = rho_new / < rt, v > 
4  before  alpha = rho_new / < rt, v > 
3  before  alpha = rho_new / < rt, v > 
1  before  alpha = rho_new / < rt, v > 
1  before alpha = rho_new / h; 
1  before apply first correction to x 
1  before r = r - alpha*v 
1  before test stop criteria 
1 istlsolver::printOutput 
  0.5      1.30548e-27      9.49877e-17
1 === rate=9.02266e-33, T=0.125974, TIT=0.251948, IT=0.5
1 finished solveLinearSystemImpl_ 1
1 converged value: 1 7 1 let s go get convergedRemote
1 to  comm_.min(converged) 
3  before alpha = rho_new / h; 
3  before apply first correction to x 
3  before r = r - alpha*v 
3  before test stop criteria 
3 istlsolver::printOutput 
  0.5      1.30548e-27      9.49877e-17
3 === rate=9.02266e-33, T=0.125972, TIT=0.251943, IT=0.5
3 finished solveLinearSystemImpl_ 1
3 converged value: 1 7 1 let s go get convergedRemote
3 to  comm_.min(converged) 
4  before alpha = rho_new / h; 
4  before apply first correction to x 
4  before r = r - alpha*v 
4  before test stop criteria 
4 istlsolver::printOutput 
  0.5      1.30548e-27      9.49877e-17
4 === rate=9.02266e-33, T=0.125977, TIT=0.251954, IT=0.5
4 finished solveLinearSystemImpl_ 1
4 converged value: 1 7 1 let s go get convergedRemote
4 to  comm_.min(converged) 
6  before alpha = rho_new / h; 
6  before apply first correction to x 
6  before r = r - alpha*v 
6  before test stop criteria 
6 istlsolver::printOutput 
  0.5      1.30548e-27      9.49877e-17
6 === rate=9.02266e-33, T=0.125972, TIT=0.251943, IT=0.5
6 finished solveLinearSystemImpl_ 1
6 converged value: 1 7 1 let s go get convergedRemote
6 to  comm_.min(converged) 
0  before alpha = rho_new / h; 
0  before apply first correction to x 
0  before r = r - alpha*v 
0  before test stop criteria 
0 istlsolver::printOutput 
  0.5      1.30548e-27      9.49877e-17
0 === rate=9.02266e-33, T=0.0216877, TIT=0.0433755, IT=0.5
2  before alpha = rho_new / h; 
2  before apply first correction to x 
2  before r = r - alpha*v 
2  before test stop criteria 
2 istlsolver::printOutput 
  0.5      1.30548e-27      9.49877e-17
2 === rate=9.02266e-33, T=0.125977, TIT=0.251954, IT=0.5
2 finished solveLinearSystemImpl_ 1
2 converged value: 1 7 1 let s go get convergedRemote
2 to  comm_.min(converged) 
5  before alpha = rho_new / h; 
5  before apply first correction to x 
5  before r = r - alpha*v 
5  before test stop criteria 
5 istlsolver::printOutput 
  0.5      1.30548e-27      9.49877e-17
5 === rate=9.02266e-33, T=0.12597, TIT=0.25194, IT=0.5
5 finished solveLinearSystemImpl_ 1
5 converged value: 1 7 1 let s go get convergedRemote
5 to  comm_.min(converged) 
5 did  comm_.min(converged) convergedRemote: 1
5 final convergedRemote: 1
5 Update: x^(k+1) = x^k - deltax^k 
5 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
5 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
5 Newton iteration  2 done5 , maximum relative shift = 3.941e-03
5 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
3 did  comm_.min(converged) convergedRemote: 1
3 final convergedRemote: 1
3 Update: x^(k+1) = x^k - deltax^k 
3 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
3 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
3 Newton iteration  2 done3 , maximum relative shift = 3.941e-03
3 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
4 did  comm_.min(converged) convergedRemote: 1
4 final convergedRemote: 1
4 Update: x^(k+1) = x^k - deltax^k 
4 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
4 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
4 Newton iteration  2 done4 , maximum relative shift = 3.941e-03
4 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
6 did  comm_.min(converged) convergedRemote: 1
6 final convergedRemote: 1
6 Update: x^(k+1) = x^k - deltax^k 
6 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
6 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
6 Newton iteration  2 done6 , maximum relative shift = 3.941e-03
6 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
0 finished solveLinearSystemImpl_ 1
0 converged value: 1 7 1 let s go get convergedRemote
0 to  comm_.min(converged) 
0 did  comm_.min(converged) convergedRemote: 1
0 final convergedRemote: 1
0 Update: x^(k+1) = x^k - deltax^k 
0 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
0 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
0 Newton iteration  2 done0 , maximum relative shift = 3.941e-03
0 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
2 did  comm_.min(converged) convergedRemote: 1
2 final convergedRemote: 1
2 Update: x^(k+1) = x^k - deltax^k 
2 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
2 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
2 Newton iteration  2 done2 , maximum relative shift = 3.941e-03
2 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
1 did  comm_.min(converged) convergedRemote: 1
1 final convergedRemote: 1
1 Update: x^(k+1) = x^k - deltax^k 
1 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
1 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
1 Newton iteration  2 done1 , maximum relative shift = 3.941e-03
1 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
6 Solve: M deltax^k = r 
6 Newton::solveLinearSystem : entering the try section 2
6 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
6 solveLinearSystemImpl_, before converged = ls.solve 
6 7 amgbackend::solve isParallel 1
0 Solve: M deltax^k = r 
0 Newton::solveLinearSystem : entering the try section 2
0 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
0 solveLinearSystemImpl_, before converged = ls.solve 
0 7 amgbackend::solve isParallel 1
2 Solve: M deltax^k = r 
2 Newton::solveLinearSystem : entering the try section 2
2 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
2 solveLinearSystemImpl_, before converged = ls.solve 
2 7 amgbackend::solve isParallel 1
1 Solve: M deltax^k = r 
1 Newton::solveLinearSystem : entering the try section 2
1 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
1 solveLinearSystemImpl_, before converged = ls.solve 
1 7 amgbackend::solve isParallel 1
5 Solve: M deltax^k = r 
5 Newton::solveLinearSystem : entering the try section 2
5 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
5 solveLinearSystemImpl_, before converged = ls.solve 
5 7 amgbackend::solve isParallel 1
3 Solve: M deltax^k = r 
3 Newton::solveLinearSystem : entering the try section 2
3 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
3 solveLinearSystemImpl_, before converged = ls.solve 
3 7 amgbackend::solve isParallel 1
4 Solve: M deltax^k = r 
4 Newton::solveLinearSystem : entering the try section 2
4 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
4 solveLinearSystemImpl_, before converged = ls.solve 
4 7 amgbackend::solve isParallel 1
2 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
2 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
6 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
6 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
0 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
0 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 7
4 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
4 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
1 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
1 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
5 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
5 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
3 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
3 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 7
3 7 before Dune::BiCGSTABSolver 
3 Dune::BiCGSTABSolver.apply(x, b, result_) 
3 BiCGSTABSolver::apply 
3 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
2 7 before Dune::BiCGSTABSolver 
2 Dune::BiCGSTABSolver.apply(x, b, result_) 
2 BiCGSTABSolver::apply 
2 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
6 7 before Dune::BiCGSTABSolver 
6 Dune::BiCGSTABSolver.apply(x, b, result_) 
6 BiCGSTABSolver::apply 
6 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 7
4 7 before Dune::BiCGSTABSolver 
4 Dune::BiCGSTABSolver.apply(x, b, result_) 
4 BiCGSTABSolver::apply 
4 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
1 7 before Dune::BiCGSTABSolver 
1 Dune::BiCGSTABSolver.apply(x, b, result_) 
1 BiCGSTABSolver::apply 
1 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
5 7 before Dune::BiCGSTABSolver 
5 Dune::BiCGSTABSolver.apply(x, b, result_) 
5 BiCGSTABSolver::apply 
5 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
Using a direct coarse solver (SuperLU)
Building hierarchy of 1 levels (inclusive coarse solver) took 0.112223 seconds.
0 7 before Dune::BiCGSTABSolver 
0 Dune::BiCGSTABSolver.apply(x, b, result_) 
0 BiCGSTABSolver::apply 
0 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
6 === BiCGSTABSolver
 Iter          Defect            Rate
    0      4.72529e-13
6  (it = 0.5; it < _maxit; it+=.5) 0.5 250
6  before p = r 
6  before  y = W^-1 * p 
6  before _prec->apply(y,p); 0.5 250
6 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
3 === BiCGSTABSolver
 Iter          Defect            Rate
    0      4.72529e-13
3  (it = 0.5; it < _maxit; it+=.5) 0.5 250
3  before p = r 
3  before  y = W^-1 * p 
3  before _prec->apply(y,p); 0.5 250
3 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
2 === BiCGSTABSolver
 Iter          Defect            Rate
    0      4.72529e-13
2  (it = 0.5; it < _maxit; it+=.5) 0.5 250
2  before p = r 
2  before  y = W^-1 * p 
2  before _prec->apply(y,p); 0.5 250
2 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
0 === BiCGSTABSolver
 Iter          Defect            Rate
    0      4.72529e-13
0  (it = 0.5; it < _maxit; it+=.5) 0.5 250
0  before p = r 
0  before  y = W^-1 * p 
0  before _prec->apply(y,p); 0.5 250
0 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 7
5 === BiCGSTABSolver
 Iter          Defect            Rate
    0      4.72529e-13
5  (it = 0.5; it < _maxit; it+=.5) 0.5 250
5  before p = r 
5  before  y = W^-1 * p 
5  before _prec->apply(y,p); 0.5 250
5 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
4 === BiCGSTABSolver
 Iter          Defect            Rate
    0      4.72529e-13
4  (it = 0.5; it < _maxit; it+=.5) 0.5 250
4  before p = r 
4  before  y = W^-1 * p 
4  before _prec->apply(y,p); 0.5 250
4 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
1 === BiCGSTABSolver
 Iter          Defect            Rate
    0      4.72529e-13
1  (it = 0.5; it < _maxit; it+=.5) 0.5 250
1  before p = r 
1  before  y = W^-1 * p 
1  before _prec->apply(y,p); 0.5 250
1 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
0 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
2 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
2 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
2  before v = A * y 
2  before  alpha = rho_new / < rt, v > 
6 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
6 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
6  before v = A * y 
6  before  alpha = rho_new / < rt, v > 
3 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
3 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
3  before v = A * y 
3  before  alpha = rho_new / < rt, v > 
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
0 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
0 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
0  before v = A * y 
0  before  alpha = rho_new / < rt, v > 
1 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
1 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
1  before v = A * y 
5 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
5 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
5  before v = A * y 
5  before  alpha = rho_new / < rt, v > 
4 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
4 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
4  before v = A * y 
4  before  alpha = rho_new / < rt, v > 
3  before alpha = rho_new / h; 
3  before apply first correction to x 
3  before r = r - alpha*v 
3  before test stop criteria 
3 istlsolver::printOutput 
  0.5      6.84474e-30      1.44853e-17
3 === rate=2.09825e-34, T=0.122905, TIT=0.24581, IT=0.5
3 finished solveLinearSystemImpl_ 1
3 converged value: 1 7 1 let s go get convergedRemote
3 to  comm_.min(converged) 
2  before alpha = rho_new / h; 
2  before apply first correction to x 
2  before r = r - alpha*v 
2  before test stop criteria 
6  before alpha = rho_new / h; 
6  before apply first correction to x 
6  before r = r - alpha*v 
6  before test stop criteria 
4  before alpha = rho_new / h; 
4  before apply first correction to x 
4  before r = r - alpha*v 
4  before test stop criteria 
1  before  alpha = rho_new / < rt, v > 
1  before alpha = rho_new / h; 
1  before apply first correction to x 
1  before r = r - alpha*v 
1  before test stop criteria 
1 istlsolver::printOutput 
  0.5      6.84474e-30      1.44853e-17
1 === rate=2.09825e-34, T=0.122903, TIT=0.245806, IT=0.5
1 finished solveLinearSystemImpl_ 1
1 converged value: 1 7 1 let s go get convergedRemote
1 to  comm_.min(converged) 
5  before alpha = rho_new / h; 
5  before apply first correction to x 
5  before r = r - alpha*v 
5  before test stop criteria 
5 istlsolver::printOutput 
  0.5      6.84474e-30      1.44853e-17
5 === rate=2.09825e-34, T=0.122903, TIT=0.245807, IT=0.5
5 finished solveLinearSystemImpl_ 1
5 converged value: 1 7 1 let s go get convergedRemote
5 to  comm_.min(converged) 
0  before alpha = rho_new / h; 
0  before apply first correction to x 
0  before r = r - alpha*v 
0  before test stop criteria 
0 istlsolver::printOutput 
  0.5      6.84474e-30      1.44853e-17
0 === rate=2.09825e-34, T=0.0186138, TIT=0.0372277, IT=0.5
0 finished solveLinearSystemImpl_ 1
0 converged value: 1 7 1 let s go get convergedRemote
0 to  comm_.min(converged) 
0 did  comm_.min(converged) convergedRemote: 1
0 final convergedRemote: 1
0 Update: x^(k+1) = x^k - deltax^k 
0 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
0 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
0 Newton iteration  3 done0 , maximum relative shift = 1.210e-04
4 istlsolver::printOutput 
  0.5      6.84474e-30      1.44853e-17
4 === rate=2.09825e-34, T=0.122974, TIT=0.245949, IT=0.5
4 finished solveLinearSystemImpl_ 1
4 converged value: 1 7 1 let s go get convergedRemote
4 to  comm_.min(converged) 
4 did  comm_.min(converged) convergedRemote: 1
4 final convergedRemote: 1
4 Update: x^(k+1) = x^k - deltax^k 
4 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
4 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
4 Newton iteration  3 done4 , maximum relative shift = 1.210e-04
4 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
1 did  comm_.min(converged) convergedRemote: 1
1 final convergedRemote: 1
1 Update: x^(k+1) = x^k - deltax^k 
1 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
1 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
1 Newton iteration  3 done1 , maximum relative shift = 1.210e-04
1 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
6 istlsolver::printOutput 
  0.5      6.84474e-30      1.44853e-17
6 === rate=2.09825e-34, T=0.122983, TIT=0.245967, IT=0.5
6 finished solveLinearSystemImpl_ 1
6 converged value: 1 7 1 let s go get convergedRemote
6 to  comm_.min(converged) 
6 did  comm_.min(converged) convergedRemote: 1
6 final convergedRemote: 1
6 Update: x^(k+1) = x^k - deltax^k 
6 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
6 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
6 Newton iteration  3 done6 , maximum relative shift = 1.210e-04
6 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
3 did  comm_.min(converged) convergedRemote: 1
3 final convergedRemote: 1
3 Update: x^(k+1) = x^k - deltax^k 
3 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
3 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
3 Newton iteration  3 done3 , maximum relative shift = 1.210e-04
3 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
2 istlsolver::printOutput 
  0.5      6.84474e-30      1.44853e-17
2 === rate=2.09825e-34, T=0.122984, TIT=0.245968, IT=0.5
2 finished solveLinearSystemImpl_ 1
2 converged value: 1 7 1 let s go get convergedRemote
2 to  comm_.min(converged) 
2 did  comm_.min(converged) convergedRemote: 1
2 final convergedRemote: 1
2 Update: x^(k+1) = x^k - deltax^k 
2 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
2 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
2 Newton iteration  3 done2 , maximum relative shift = 1.210e-04
2 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
5 did  comm_.min(converged) convergedRemote: 1
5 final convergedRemote: 1
5 Update: x^(k+1) = x^k - deltax^k 
5 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
5 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
5 Newton iteration  3 done5 , maximum relative shift = 1.210e-04
5 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
0 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
0 Solve: M deltax^k = r 
0 Newton::solveLinearSystem : entering the try section 3
0 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
0 solveLinearSystemImpl_, before converged = ls.solve 
0 7 amgbackend::solve isParallel 1
0 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
0 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
2 Solve: M deltax^k = r 
2 Newton::solveLinearSystem : entering the try section 3
2 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
2 solveLinearSystemImpl_, before converged = ls.solve 
2 7 amgbackend::solve isParallel 1
2 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
2 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
5 Solve: M deltax^k = r 
5 Newton::solveLinearSystem : entering the try section 3
5 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
5 solveLinearSystemImpl_, before converged = ls.solve 
5 7 amgbackend::solve isParallel 1
5 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
5 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
4 Solve: M deltax^k = r 
4 Newton::solveLinearSystem : entering the try section 3
4 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
4 solveLinearSystemImpl_, before converged = ls.solve 
4 7 amgbackend::solve isParallel 1
4 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
4 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
1 Solve: M deltax^k = r 
1 Newton::solveLinearSystem : entering the try section 3
1 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
1 solveLinearSystemImpl_, before converged = ls.solve 
1 7 amgbackend::solve isParallel 1
1 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
1 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
6 Solve: M deltax^k = r 
6 Newton::solveLinearSystem : entering the try section 3
6 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
6 solveLinearSystemImpl_, before converged = ls.solve 
6 7 amgbackend::solve isParallel 1
6 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
6 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
3 Solve: M deltax^k = r 
3 Newton::solveLinearSystem : entering the try section 3
3 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
3 solveLinearSystemImpl_, before converged = ls.solve 
3 7 amgbackend::solve isParallel 1
3 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
3 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 7
3 7 before Dune::BiCGSTABSolver 
3 Dune::BiCGSTABSolver.apply(x, b, result_) 
3 BiCGSTABSolver::apply 
3 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
2 7 before Dune::BiCGSTABSolver 
2 Dune::BiCGSTABSolver.apply(x, b, result_) 
2 BiCGSTABSolver::apply 
2 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
5 7 before Dune::BiCGSTABSolver 
5 Dune::BiCGSTABSolver.apply(x, b, result_) 
5 BiCGSTABSolver::apply 
5 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
4 7 before Dune::BiCGSTABSolver 
4 Dune::BiCGSTABSolver.apply(x, b, result_) 
4 BiCGSTABSolver::apply 
4 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
1 7 before Dune::BiCGSTABSolver 
1 Dune::BiCGSTABSolver.apply(x, b, result_) 
1 BiCGSTABSolver::apply 
1 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
6 7 before Dune::BiCGSTABSolver 
6 Dune::BiCGSTABSolver.apply(x, b, result_) 
6 BiCGSTABSolver::apply 
6 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 7
Using a direct coarse solver (SuperLU)
Building hierarchy of 1 levels (inclusive coarse solver) took 0.112082 seconds.
0 7 before Dune::BiCGSTABSolver 
0 Dune::BiCGSTABSolver.apply(x, b, result_) 
0 BiCGSTABSolver::apply 
0 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
0 === BiCGSTABSolver
 Iter          Defect            Rate
    0        6.293e-14
0  (it = 0.5; it < _maxit; it+=.5) 0.5 250
1 === BiCGSTABSolver
 Iter          Defect            Rate
    0        6.293e-14
1  (it = 0.5; it < _maxit; it+=.5) 0.5 250
1  before p = r 
1  before  y = W^-1 * p 
1  before _prec->apply(y,p); 0.5 250
1 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
0  before p = r 
0  before  y = W^-1 * p 
0  before _prec->apply(y,p); 0.5 250
0 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 7
0 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
6 === BiCGSTABSolver
 Iter          Defect            Rate
    0        6.293e-14
6  (it = 0.5; it < _maxit; it+=.5) 0.5 250
6  before p = r 
6  before  y = W^-1 * p 
6  before _prec->apply(y,p); 0.5 250
6 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
3 === BiCGSTABSolver
 Iter          Defect            Rate
    0        6.293e-14
3  (it = 0.5; it < _maxit; it+=.5) 0.5 250
3  before p = r 
3  before  y = W^-1 * p 
3  before _prec->apply(y,p); 0.5 250
3 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
2 === BiCGSTABSolver
 Iter          Defect            Rate
    0        6.293e-14
2  (it = 0.5; it < _maxit; it+=.5) 0.5 250
2  before p = r 
2  before  y = W^-1 * p 
2  before _prec->apply(y,p); 0.5 250
2 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
5 === BiCGSTABSolver
 Iter          Defect            Rate
    0        6.293e-14
5  (it = 0.5; it < _maxit; it+=.5) 0.5 250
5  before p = r 
5  before  y = W^-1 * p 
5  before _prec->apply(y,p); 0.5 250
5 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
4 === BiCGSTABSolver
 Iter          Defect            Rate
    0        6.293e-14
4  (it = 0.5; it < _maxit; it+=.5) 0.5 250
4  before p = r 
4  before  y = W^-1 * p 
4  before _prec->apply(y,p); 0.5 250
4 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
0 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
6 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
6 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
6  before v = A * y 
3 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
3 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
3  before v = A * y 
2 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
2 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
5 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
5 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
5  before v = A * y 
1 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
4 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
4 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
4  before v = A * y 
1 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
1  before v = A * y 
0 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
0  before v = A * y 
6  before  alpha = rho_new / < rt, v > 
3  before  alpha = rho_new / < rt, v > 
2  before v = A * y 
5  before  alpha = rho_new / < rt, v > 
4  before  alpha = rho_new / < rt, v > 
1  before  alpha = rho_new / < rt, v > 
1  before alpha = rho_new / h; 
1  before apply first correction to x 
1  before r = r - alpha*v 
1  before test stop criteria 
1 istlsolver::printOutput 
  0.5      1.35227e-30      2.14885e-17
1 === rate=4.61757e-34, T=0.123037, TIT=0.246073, IT=0.5
1 finished solveLinearSystemImpl_ 1
1 converged value: 1 7 1 let s go get convergedRemote
1 to  comm_.min(converged) 
1 did  comm_.min(converged) convergedRemote: 1
1 final convergedRemote: 1
1 Update: x^(k+1) = x^k - deltax^k 
1 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
1 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
1 Newton iteration  4 done1 , maximum relative shift = 1.936e-05
1 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
0  before  alpha = rho_new / < rt, v > 
0  before alpha = rho_new / h; 
0  before apply first correction to x 
0  before r = r - alpha*v 
0  before test stop criteria 
0 istlsolver::printOutput 
  0.5      1.35227e-30      2.14885e-17
0 === rate=4.61757e-34, T=0.0187352, TIT=0.0374705, IT=0.5
0 finished solveLinearSystemImpl_ 1
0 converged value: 1 7 1 let s go get convergedRemote
0 to  comm_.min(converged) 
0 did  comm_.min(converged) convergedRemote: 1
0 final convergedRemote: 1
0 Update: x^(k+1) = x^k - deltax^k 
0 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
0 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
0 Newton iteration  4 done0 , maximum relative shift = 1.936e-05
0 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
6  before alpha = rho_new / h; 
6  before apply first correction to x 
6  before r = r - alpha*v 
6  before test stop criteria 
6 istlsolver::printOutput 
  0.5      1.35227e-30      2.14885e-17
6 === rate=4.61757e-34, T=0.122989, TIT=0.245979, IT=0.5
6 finished solveLinearSystemImpl_ 1
6 converged value: 1 7 1 let s go get convergedRemote
6 to  comm_.min(converged) 
6 did  comm_.min(converged) convergedRemote: 1
6 final convergedRemote: 1
6 Update: x^(k+1) = x^k - deltax^k 
6 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
6 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
6 Newton iteration  4 done6 , maximum relative shift = 1.936e-05
6 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
3  before alpha = rho_new / h; 
3  before apply first correction to x 
3  before r = r - alpha*v 
3  before test stop criteria 
3 istlsolver::printOutput 
  0.5      1.35227e-30      2.14885e-17
3 === rate=4.61757e-34, T=0.123037, TIT=0.246074, IT=0.5
3 finished solveLinearSystemImpl_ 1
3 converged value: 1 7 1 let s go get convergedRemote
3 to  comm_.min(converged) 
3 did  comm_.min(converged) convergedRemote: 1
3 final convergedRemote: 1
3 Update: x^(k+1) = x^k - deltax^k 
3 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
3 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
3 Newton iteration  4 done3 , maximum relative shift = 1.936e-05
3 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
2  before  alpha = rho_new / < rt, v > 
2  before alpha = rho_new / h; 
2  before apply first correction to x 
2  before r = r - alpha*v 
2  before test stop criteria 
2 istlsolver::printOutput 
  0.5      1.35227e-30      2.14885e-17
2 === rate=4.61757e-34, T=0.122989, TIT=0.245978, IT=0.5
2 finished solveLinearSystemImpl_ 1
2 converged value: 1 7 1 let s go get convergedRemote
2 to  comm_.min(converged) 
2 did  comm_.min(converged) convergedRemote: 1
2 final convergedRemote: 1
2 Update: x^(k+1) = x^k - deltax^k 
2 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
2 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
2 Newton iteration  4 done2 , maximum relative shift = 1.936e-05
2 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
4  before alpha = rho_new / h; 
4  before apply first correction to x 
4  before r = r - alpha*v 
4  before test stop criteria 
4 istlsolver::printOutput 
  0.5      1.35227e-30      2.14885e-17
4 === rate=4.61757e-34, T=0.12299, TIT=0.24598, IT=0.5
4 finished solveLinearSystemImpl_ 1
4 converged value: 1 7 1 let s go get convergedRemote
4 to  comm_.min(converged) 
4 did  comm_.min(converged) convergedRemote: 1
4 final convergedRemote: 1
4 Update: x^(k+1) = x^k - deltax^k 
4 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
4 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
4 Newton iteration  4 done4 , maximum relative shift = 1.936e-05
4 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
5  before alpha = rho_new / h; 
5  before apply first correction to x 
5  before r = r - alpha*v 
5  before test stop criteria 
5 istlsolver::printOutput 
  0.5      1.35227e-30      2.14885e-17
5 === rate=4.61757e-34, T=0.123036, TIT=0.246072, IT=0.5
5 finished solveLinearSystemImpl_ 1
5 converged value: 1 7 1 let s go get convergedRemote
5 to  comm_.min(converged) 
5 did  comm_.min(converged) convergedRemote: 1
5 final convergedRemote: 1
5 Update: x^(k+1) = x^k - deltax^k 
5 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
5 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
5 Newton iteration  4 done5 , maximum relative shift = 1.936e-05
5 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
6 Solve: M deltax^k = r 
6 Newton::solveLinearSystem : entering the try section 4
6 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
6 solveLinearSystemImpl_, before converged = ls.solve 
6 7 amgbackend::solve isParallel 1
3 Solve: M deltax^k = r 
3 Newton::solveLinearSystem : entering the try section 4
3 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
3 solveLinearSystemImpl_, before converged = ls.solve 
3 7 amgbackend::solve isParallel 1
5 Solve: M deltax^k = r 
5 Newton::solveLinearSystem : entering the try section 4
5 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
5 solveLinearSystemImpl_, before converged = ls.solve 
5 7 amgbackend::solve isParallel 1
4 Solve: M deltax^k = r 
4 Newton::solveLinearSystem : entering the try section 4
4 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
4 solveLinearSystemImpl_, before converged = ls.solve 
4 7 amgbackend::solve isParallel 1
2 Solve: M deltax^k = r 
2 Newton::solveLinearSystem : entering the try section 4
2 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
2 solveLinearSystemImpl_, before converged = ls.solve 
2 7 amgbackend::solve isParallel 1
1 Solve: M deltax^k = r 
1 Newton::solveLinearSystem : entering the try section 4
1 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
1 solveLinearSystemImpl_, before converged = ls.solve 
1 7 amgbackend::solve isParallel 1
0 Solve: M deltax^k = r 
0 Newton::solveLinearSystem : entering the try section 4
0 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
0 solveLinearSystemImpl_, before converged = ls.solve 
0 7 amgbackend::solve isParallel 1
6 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
6 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
0 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
0 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 7
5 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
5 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
4 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
4 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
2 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
2 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
1 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
1 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
3 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
3 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 7
3 7 before Dune::BiCGSTABSolver 
3 Dune::BiCGSTABSolver.apply(x, b, result_) 
3 BiCGSTABSolver::apply 
3 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
5 7 before Dune::BiCGSTABSolver 
5 Dune::BiCGSTABSolver.apply(x, b, result_) 
5 BiCGSTABSolver::apply 
5 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
4 7 before Dune::BiCGSTABSolver 
4 Dune::BiCGSTABSolver.apply(x, b, result_) 
4 BiCGSTABSolver::apply 
4 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
2 7 before Dune::BiCGSTABSolver 
2 Dune::BiCGSTABSolver.apply(x, b, result_) 
2 BiCGSTABSolver::apply 
2 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
1 7 before Dune::BiCGSTABSolver 
1 Dune::BiCGSTABSolver.apply(x, b, result_) 
1 BiCGSTABSolver::apply 
1 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
6 7 before Dune::BiCGSTABSolver 
6 Dune::BiCGSTABSolver.apply(x, b, result_) 
6 BiCGSTABSolver::apply 
6 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 7
Using a direct coarse solver (SuperLU)
Building hierarchy of 1 levels (inclusive coarse solver) took 0.112717 seconds.
0 7 before Dune::BiCGSTABSolver 
0 Dune::BiCGSTABSolver.apply(x, b, result_) 
0 BiCGSTABSolver::apply 
0 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
3 === BiCGSTABSolver
 Iter          Defect            Rate
    0      2.49451e-15
3  (it = 0.5; it < _maxit; it+=.5) 0.5 250
3  before p = r 
3  before  y = W^-1 * p 
3  before _prec->apply(y,p); 0.5 250
3 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
5 === BiCGSTABSolver
 Iter          Defect            Rate
    0      2.49451e-15
5  (it = 0.5; it < _maxit; it+=.5) 0.5 250
5  before p = r 
5  before  y = W^-1 * p 
5  before _prec->apply(y,p); 0.5 250
5 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
4 === BiCGSTABSolver
 Iter          Defect            Rate
    0      2.49451e-15
4  (it = 0.5; it < _maxit; it+=.5) 0.5 250
4  before p = r 
4  before  y = W^-1 * p 
4  before _prec->apply(y,p); 0.5 250
4 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
2 === BiCGSTABSolver
 Iter          Defect            Rate
    0      2.49451e-15
2  (it = 0.5; it < _maxit; it+=.5) 0.5 250
2  before p = r 
2  before  y = W^-1 * p 
2  before _prec->apply(y,p); 0.5 250
2 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
1 === BiCGSTABSolver
 Iter          Defect            Rate
    0      2.49451e-15
1  (it = 0.5; it < _maxit; it+=.5) 0.5 250
1  before p = r 
1  before  y = W^-1 * p 
1  before _prec->apply(y,p); 0.5 250
1 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
0 === BiCGSTABSolver
 Iter          Defect            Rate
    0      2.49451e-15
0  (it = 0.5; it < _maxit; it+=.5) 0.5 250
0  before p = r 
0  before  y = W^-1 * p 
0  before _prec->apply(y,p); 0.5 250
0 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 7
0 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
6 === BiCGSTABSolver
 Iter          Defect            Rate
    0      2.49451e-15
6  (it = 0.5; it < _maxit; it+=.5) 0.5 250
6  before p = r 
6  before  y = W^-1 * p 
6  before _prec->apply(y,p); 0.5 250
6 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
1 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
3 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
3 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
3  before v = A * y 
5 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
5 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
5  before v = A * y 
4 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
4 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
4  before v = A * y 
2 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
2 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
6 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
6 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
6  before v = A * y 
5  before  alpha = rho_new / < rt, v > 
4  before  alpha = rho_new / < rt, v > 
6  before  alpha = rho_new / < rt, v > 
0 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
3  before  alpha = rho_new / < rt, v > 
3  before alpha = rho_new / h; 
3  before apply first correction to x 
3  before r = r - alpha*v 
3  before test stop criteria 
3 istlsolver::printOutput 
  0.5      4.19805e-32      1.68292e-17
3 === rate=2.83221e-34, T=0.124288, TIT=0.248576, IT=0.5
3 finished solveLinearSystemImpl_ 1
3 converged value: 1 7 1 let s go get convergedRemote
3 to  comm_.min(converged) 
2  before v = A * y 
2  before  alpha = rho_new / < rt, v > 
2  before alpha = rho_new / h; 
2  before apply first correction to x 
2  before r = r - alpha*v 
2  before test stop criteria 
2 istlsolver::printOutput 
  0.5      4.19805e-32      1.68292e-17
2 === rate=2.83221e-34, T=0.124247, TIT=0.248494, IT=0.5
2 finished solveLinearSystemImpl_ 1
2 converged value: 1 7 1 let s go get convergedRemote
2 to  comm_.min(converged) 
6  before alpha = rho_new / h; 
6  before apply first correction to x 
6  before r = r - alpha*v 
6  before test stop criteria 
6 istlsolver::printOutput 
  0.5      4.19805e-32      1.68292e-17
6 === rate=2.83221e-34, T=0.124247, TIT=0.248494, IT=0.5
6 finished solveLinearSystemImpl_ 1
6 converged value: 1 7 1 let s go get convergedRemote
6 to  comm_.min(converged) 
1 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
1  before v = A * y 
1  before  alpha = rho_new / < rt, v > 
1  before alpha = rho_new / h; 
1  before apply first correction to x 
1  before r = r - alpha*v 
1  before test stop criteria 
1 istlsolver::printOutput 
  0.5      4.19805e-32      1.68292e-17
1 === rate=2.83221e-34, T=0.12429, TIT=0.24858, IT=0.5
1 finished solveLinearSystemImpl_ 1
1 converged value: 1 7 1 let s go get convergedRemote
1 to  comm_.min(converged) 
0 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
0  before v = A * y 
0  before  alpha = rho_new / < rt, v > 
0  before alpha = rho_new / h; 
0  before apply first correction to x 
0  before r = r - alpha*v 
0  before test stop criteria 
0 istlsolver::printOutput 
  0.5      4.19805e-32      1.68292e-17
0 === rate=2.83221e-34, T=0.0192872, TIT=0.0385745, IT=0.5
4  before alpha = rho_new / h; 
4  before apply first correction to x 
4  before r = r - alpha*v 
4  before test stop criteria 
4 istlsolver::printOutput 
  0.5      4.19805e-32      1.68292e-17
4 === rate=2.83221e-34, T=0.124247, TIT=0.248494, IT=0.5
4 finished solveLinearSystemImpl_ 1
4 converged value: 1 7 1 let s go get convergedRemote
4 to  comm_.min(converged) 
5  before alpha = rho_new / h; 
5  before apply first correction to x 
5  before r = r - alpha*v 
5  before test stop criteria 
5 istlsolver::printOutput 
  0.5      4.19805e-32      1.68292e-17
5 === rate=2.83221e-34, T=0.124286, TIT=0.248573, IT=0.5
5 finished solveLinearSystemImpl_ 1
5 converged value: 1 7 1 let s go get convergedRemote
5 to  comm_.min(converged) 
5 did  comm_.min(converged) convergedRemote: 1
5 final convergedRemote: 1
5 Update: x^(k+1) = x^k - deltax^k 
5 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
5 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
5 Newton iteration  5 done5 , maximum relative shift = 6.398e-07
5 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
3 did  comm_.min(converged) convergedRemote: 1
3 final convergedRemote: 1
3 Update: x^(k+1) = x^k - deltax^k 
3 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
3 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
3 Newton iteration  5 done3 , maximum relative shift = 6.398e-07
3 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
2 did  comm_.min(converged) convergedRemote: 1
2 final convergedRemote: 1
2 Update: x^(k+1) = x^k - deltax^k 
2 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
2 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
2 Newton iteration  5 done2 , maximum relative shift = 6.398e-07
2 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
6 did  comm_.min(converged) convergedRemote: 1
6 final convergedRemote: 1
6 Update: x^(k+1) = x^k - deltax^k 
6 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
6 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
6 Newton iteration  5 done6 , maximum relative shift = 6.398e-07
6 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
1 did  comm_.min(converged) convergedRemote: 1
1 final convergedRemote: 1
1 Update: x^(k+1) = x^k - deltax^k 
1 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
1 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
1 Newton iteration  5 done1 , maximum relative shift = 6.398e-07
1 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
0 finished solveLinearSystemImpl_ 1
0 converged value: 1 7 1 let s go get convergedRemote
0 to  comm_.min(converged) 
0 did  comm_.min(converged) convergedRemote: 1
0 final convergedRemote: 1
0 Update: x^(k+1) = x^k - deltax^k 
0 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
0 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
0 Newton iteration  5 done0 , maximum relative shift = 6.398e-07
0 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
4 did  comm_.min(converged) convergedRemote: 1
4 final convergedRemote: 1
4 Update: x^(k+1) = x^k - deltax^k 
4 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
4 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
4 Newton iteration  5 done4 , maximum relative shift = 6.398e-07
4 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
5 Solve: M deltax^k = r 
5 Newton::solveLinearSystem : entering the try section 5
5 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
5 solveLinearSystemImpl_, before converged = ls.solve 
5 7 amgbackend::solve isParallel 1
3 Solve: M deltax^k = r 
3 Newton::solveLinearSystem : entering the try section 5
3 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
3 solveLinearSystemImpl_, before converged = ls.solve 
3 7 amgbackend::solve isParallel 1
2 Solve: M deltax^k = r 
2 Newton::solveLinearSystem : entering the try section 5
2 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
2 solveLinearSystemImpl_, before converged = ls.solve 
2 7 amgbackend::solve isParallel 1
6 Solve: M deltax^k = r 
6 Newton::solveLinearSystem : entering the try section 5
6 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
6 solveLinearSystemImpl_, before converged = ls.solve 
6 7 amgbackend::solve isParallel 1
1 Solve: M deltax^k = r 
1 Newton::solveLinearSystem : entering the try section 5
1 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
1 solveLinearSystemImpl_, before converged = ls.solve 
1 7 amgbackend::solve isParallel 1
0 Solve: M deltax^k = r 
0 Newton::solveLinearSystem : entering the try section 5
0 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
0 solveLinearSystemImpl_, before converged = ls.solve 
0 7 amgbackend::solve isParallel 1
4 Solve: M deltax^k = r 
4 Newton::solveLinearSystem : entering the try section 5
4 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
4 solveLinearSystemImpl_, before converged = ls.solve 
4 7 amgbackend::solve isParallel 1
4 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
4 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
0 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
0 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 7
5 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
5 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
3 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
3 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
2 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
2 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
6 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
6 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
1 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
1 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 7
4 7 before Dune::BiCGSTABSolver 
4 Dune::BiCGSTABSolver.apply(x, b, result_) 
4 BiCGSTABSolver::apply 
4 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
1 7 before Dune::BiCGSTABSolver 
1 Dune::BiCGSTABSolver.apply(x, b, result_) 
1 BiCGSTABSolver::apply 
1 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
5 7 before Dune::BiCGSTABSolver 
5 Dune::BiCGSTABSolver.apply(x, b, result_) 
5 BiCGSTABSolver::apply 
5 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
3 7 before Dune::BiCGSTABSolver 
3 Dune::BiCGSTABSolver.apply(x, b, result_) 
3 BiCGSTABSolver::apply 
3 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
2 7 before Dune::BiCGSTABSolver 
2 Dune::BiCGSTABSolver.apply(x, b, result_) 
2 BiCGSTABSolver::apply 
2 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
6 7 before Dune::BiCGSTABSolver 
6 Dune::BiCGSTABSolver.apply(x, b, result_) 
6 BiCGSTABSolver::apply 
6 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
Using a direct coarse solver (SuperLU)
Building hierarchy of 1 levels (inclusive coarse solver) took 0.109977 seconds.
0 7 before Dune::BiCGSTABSolver 
0 Dune::BiCGSTABSolver.apply(x, b, result_) 
0 BiCGSTABSolver::apply 
0 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
0 === BiCGSTABSolver
 Iter          Defect            Rate
    0      3.32826e-16
0  (it = 0.5; it < _maxit; it+=.5) 0.5 250
6 === BiCGSTABSolver
 Iter          Defect            Rate
    0      3.32826e-16
6  (it = 0.5; it < _maxit; it+=.5) 0.5 250
4 === BiCGSTABSolver
 Iter          Defect            Rate
    0      3.32826e-16
4  (it = 0.5; it < _maxit; it+=.5) 0.5 250
1 === BiCGSTABSolver
 Iter          Defect            Rate
    0      3.32826e-16
1  (it = 0.5; it < _maxit; it+=.5) 0.5 250
5 === BiCGSTABSolver
 Iter          Defect            Rate
    0      3.32826e-16
5  (it = 0.5; it < _maxit; it+=.5) 0.5 250
3 === BiCGSTABSolver
 Iter          Defect            Rate
    0      3.32826e-16
3  (it = 0.5; it < _maxit; it+=.5) 0.5 250
6  before p = r 
6  before  y = W^-1 * p 
6  before _prec->apply(y,p); 0.5 250
6 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
4  before p = r 
4  before  y = W^-1 * p 
4  before _prec->apply(y,p); 0.5 250
4 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
1  before p = r 
1  before  y = W^-1 * p 
1  before _prec->apply(y,p); 0.5 250
1 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
5  before p = r 
5  before  y = W^-1 * p 
5  before _prec->apply(y,p); 0.5 250
5 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
2 === BiCGSTABSolver
 Iter          Defect            Rate
    0      3.32826e-16
2  (it = 0.5; it < _maxit; it+=.5) 0.5 250
2  before p = r 
2  before  y = W^-1 * p 
2  before _prec->apply(y,p); 0.5 250
3  before p = r 
3  before  y = W^-1 * p 
3  before _prec->apply(y,p); 0.5 250
3 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
2 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
0  before p = r 
0  before  y = W^-1 * p 
0  before _prec->apply(y,p); 0.5 250
0 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 7
0 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
2 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
2 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
5 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
5 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
5  before v = A * y 
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
0 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
3 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
3 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
3  before v = A * y 
6 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
6 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
6  before v = A * y 
6  before  alpha = rho_new / < rt, v > 
4 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
4 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
4  before v = A * y 
1 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
5  before  alpha = rho_new / < rt, v > 
0 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
3  before  alpha = rho_new / < rt, v > 
4  before  alpha = rho_new / < rt, v > 
1 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
6  before alpha = rho_new / h; 
6  before apply first correction to x 
6  before r = r - alpha*v 
6  before test stop criteria 
6 istlsolver::printOutput 
  0.5      1.81108e-34      5.44154e-19
6 === rate=2.96103e-37, T=0.12544, TIT=0.250881, IT=0.5
6 finished solveLinearSystemImpl_ 1
6 converged value: 1 7 1 let s go get convergedRemote
6 to  comm_.min(converged) 
3  before alpha = rho_new / h; 
3  before apply first correction to x 
3  before r = r - alpha*v 
3  before test stop criteria 
3 istlsolver::printOutput 
  0.5      1.81108e-34      5.44154e-19
3 === rate=2.96103e-37, T=0.125522, TIT=0.251043, IT=0.5
1  before v = A * y 
1  before  alpha = rho_new / < rt, v > 
1  before alpha = rho_new / h; 
1  before apply first correction to x 
1  before r = r - alpha*v 
1  before test stop criteria 
1 istlsolver::printOutput 
  0.5      1.81108e-34      5.44154e-19
1 === rate=2.96103e-37, T=0.125387, TIT=0.250774, IT=0.5
1 finished solveLinearSystemImpl_ 1
1 converged value: 1 7 1 let s go get convergedRemote
2  before v = A * y 
2  before  alpha = rho_new / < rt, v > 
2  before alpha = rho_new / h; 
2  before apply first correction to x 
2  before r = r - alpha*v 
2  before test stop criteria 
2 istlsolver::printOutput 
  0.5      1.81108e-34      5.44154e-19
2 === rate=2.96103e-37, T=0.125444, TIT=0.250888, IT=0.5
2 finished solveLinearSystemImpl_ 1
2 converged value: 1 7 1 let s go get convergedRemote
2 to  comm_.min(converged) 
4  before alpha = rho_new / h; 
4  before apply first correction to x 
4  before r = r - alpha*v 
4  before test stop criteria 
4 istlsolver::printOutput 
  0.5      1.81108e-34      5.44154e-19
4 === rate=2.96103e-37, T=0.12544, TIT=0.250881, IT=0.5
4 finished solveLinearSystemImpl_ 1
4 converged value: 1 7 1 let s go get convergedRemote
4 to  comm_.min(converged) 
5  before alpha = rho_new / h; 
5  before apply first correction to x 
5  before r = r - alpha*v 
5  before test stop criteria 
5 istlsolver::printOutput 
  0.5      1.81108e-34      5.44154e-19
5 === rate=2.96103e-37, T=0.125519, TIT=0.251037, IT=0.5
0  before v = A * y 
0  before  alpha = rho_new / < rt, v > 
0  before alpha = rho_new / h; 
0  before apply first correction to x 
0  before r = r - alpha*v 
0  before test stop criteria 
0 istlsolver::printOutput 
  0.5      1.81108e-34      5.44154e-19
0 === rate=2.96103e-37, T=0.0232798, TIT=0.0465596, IT=0.5
1 to  comm_.min(converged) 
5 finished solveLinearSystemImpl_ 1
5 converged value: 1 7 1 let s go get convergedRemote
5 to  comm_.min(converged) 
3 finished solveLinearSystemImpl_ 1
3 converged value: 1 7 1 let s go get convergedRemote
3 to  comm_.min(converged) 
6 did  comm_.min(converged) convergedRemote: 1
6 final convergedRemote: 1
6 Update: x^(k+1) = x^k - deltax^k 
6 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
6 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
6 Newton iteration  6 done6 , maximum relative shift = 8.536e-08
6 Assemble/solve/update time: 0.264345(24.8614%)/0.798838(75.1298%)/9.347e-05(0.00879075%)
6 gridVariables->advanceTimeStep
6 before, nonLinearSolver->suggestTimeStepSize, current time: 693.767 simTime 57600
6 before assembler->setPreviousSolution, ddt: 26.6667
6 nonLinearSolver->solve
6 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
3 did  comm_.min(converged) convergedRemote: 1
3 final convergedRemote: 1
3 Update: x^(k+1) = x^k - deltax^k 
3 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
3 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
3 Newton iteration  6 done3 , maximum relative shift = 8.536e-08
3 Assemble/solve/update time: 0.264351(24.8614%)/0.798837(75.128%)/0.00011261(0.0105906%)
3 gridVariables->advanceTimeStep
3 before, nonLinearSolver->suggestTimeStepSize, current time: 693.767 simTime 57600
3 before assembler->setPreviousSolution, ddt: 26.6667
3 nonLinearSolver->solve
3 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
0 finished solveLinearSystemImpl_ 1
0 converged value: 1 7 1 let s go get convergedRemote
0 to  comm_.min(converged) 
0 did  comm_.min(converged) convergedRemote: 1
0 final convergedRemote: 1
0 Update: x^(k+1) = x^k - deltax^k 
0 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
0 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
0 Newton iteration  6 done0 , maximum relative shift = 8.536e-08
0 Assemble/solve/update time: 0.263577(24.8069%)/0.798798(75.1801%)/0.00013855(0.0130398%)
0 gridVariables->advanceTimeStep
0 before, nonLinearSolver->suggestTimeStepSize, current time: 693.767 simTime 57600
0 before assembler->setPreviousSolution, ddt: 26.6667
0 nonLinearSolver->solve
0 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
2 did  comm_.min(converged) convergedRemote: 1
2 final convergedRemote: 1
2 Update: x^(k+1) = x^k - deltax^k 
2 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
2 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
2 Newton iteration  6 done2 , maximum relative shift = 8.536e-08
2 Assemble/solve/update time: 0.264321(24.8589%)/0.79884(75.1293%)/0.0001256(0.0118124%)
2 gridVariables->advanceTimeStep
2 before, nonLinearSolver->suggestTimeStepSize, current time: 693.767 simTime 57600
2 before assembler->setPreviousSolution, ddt: 26.6667
2 nonLinearSolver->solve
4 did  comm_.min(converged) convergedRemote: 1
4 final convergedRemote: 1
4 Update: x^(k+1) = x^k - deltax^k 
4 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
4 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
4 Newton iteration  6 done4 , maximum relative shift = 8.536e-08
4 Assemble/solve/update time: 0.264253(24.8547%)/0.798817(75.1339%)/0.00012163(0.0114401%)
4 gridVariables->advanceTimeStep
4 before, nonLinearSolver->suggestTimeStepSize, current time: 693.767 simTime 57600
4 before assembler->setPreviousSolution, ddt: 26.6667
4 nonLinearSolver->solve
4 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
1 did  comm_.min(converged) convergedRemote: 1
1 final convergedRemote: 1
1 Update: x^(k+1) = x^k - deltax^k 
1 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
1 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
1 Newton iteration  6 done1 , maximum relative shift = 8.536e-08
1 Assemble/solve/update time: 0.264365(24.8627%)/0.798818(75.1263%)/0.00011671(0.0109762%)
1 gridVariables->advanceTimeStep
1 before, nonLinearSolver->suggestTimeStepSize, current time: 693.767 simTime 57600
1 before assembler->setPreviousSolution, ddt: 26.6667
1 nonLinearSolver->solve
1 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
5 did  comm_.min(converged) convergedRemote: 1
5 final convergedRemote: 1
5 Update: x^(k+1) = x^k - deltax^k 
5 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
5 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
5 Newton iteration  6 done5 , maximum relative shift = 8.536e-08
5 Assemble/solve/update time: 0.264379(24.8637%)/0.798817(75.1253%)/0.00011687(0.0109911%)
5 gridVariables->advanceTimeStep
5 before, nonLinearSolver->suggestTimeStepSize, current time: 693.767 simTime 57600
5 before assembler->setPreviousSolution, ddt: 26.6667
5 nonLinearSolver->solve
5 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
2 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
2 Solve: M deltax^k = r 
2 Newton::solveLinearSystem : entering the try section 0
2to  comm_.sum(norm2) 
2did  comm_.sum(norm2) 
2 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
2 solveLinearSystemImpl_, before converged = ls.solve 
2 7 amgbackend::solve isParallel 1
3 Solve: M deltax^k = r 
3 Newton::solveLinearSystem : entering the try section 0
3to  comm_.sum(norm2) 
3did  comm_.sum(norm2) 
3 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
3 solveLinearSystemImpl_, before converged = ls.solve 
3 7 amgbackend::solve isParallel 1
0 Solve: M deltax^k = r 
0 Newton::solveLinearSystem : entering the try section 0
0to  comm_.sum(norm2) 
0did  comm_.sum(norm2) 
0 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
0 solveLinearSystemImpl_, before converged = ls.solve 
0 7 amgbackend::solve isParallel 1
5 Solve: M deltax^k = r 
5 Newton::solveLinearSystem : entering the try section 0
5to  comm_.sum(norm2) 
5did  comm_.sum(norm2) 
5 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
5 solveLinearSystemImpl_, before converged = ls.solve 
5 7 amgbackend::solve isParallel 1
4 Solve: M deltax^k = r 
4 Newton::solveLinearSystem : entering the try section 0
4to  comm_.sum(norm2) 
4did  comm_.sum(norm2) 
4 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
4 solveLinearSystemImpl_, before converged = ls.solve 
4 7 amgbackend::solve isParallel 1
1 Solve: M deltax^k = r 
1 Newton::solveLinearSystem : entering the try section 0
1to  comm_.sum(norm2) 
1did  comm_.sum(norm2) 
1 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
1 solveLinearSystemImpl_, before converged = ls.solve 
1 7 amgbackend::solve isParallel 1
6 Solve: M deltax^k = r 
6 Newton::solveLinearSystem : entering the try section 0
6to  comm_.sum(norm2) 
6did  comm_.sum(norm2) 
6 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
6 solveLinearSystemImpl_, before converged = ls.solve 
6 7 amgbackend::solve isParallel 1
2 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
2 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
3 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
3 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
0 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
0 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 7
5 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
5 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
4 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
4 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
1 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
1 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
6 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
6 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 7
5 7 before Dune::BiCGSTABSolver 
5 Dune::BiCGSTABSolver.apply(x, b, result_) 
5 BiCGSTABSolver::apply 
5 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
4 7 before Dune::BiCGSTABSolver 
4 Dune::BiCGSTABSolver.apply(x, b, result_) 
4 BiCGSTABSolver::apply 
4 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 7
1 7 before Dune::BiCGSTABSolver 
1 Dune::BiCGSTABSolver.apply(x, b, result_) 
1 BiCGSTABSolver::apply 
1 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
2 7 before Dune::BiCGSTABSolver 
2 Dune::BiCGSTABSolver.apply(x, b, result_) 
2 BiCGSTABSolver::apply 
2 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
3 7 before Dune::BiCGSTABSolver 
3 Dune::BiCGSTABSolver.apply(x, b, result_) 
3 BiCGSTABSolver::apply 
3 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
6 7 before Dune::BiCGSTABSolver 
6 Dune::BiCGSTABSolver.apply(x, b, result_) 
6 BiCGSTABSolver::apply 
6 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
Using a direct coarse solver (SuperLU)
Building hierarchy of 1 levels (inclusive coarse solver) took 0.110289 seconds.
0 7 before Dune::BiCGSTABSolver 
0 Dune::BiCGSTABSolver.apply(x, b, result_) 
0 BiCGSTABSolver::apply 
0 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
0 === BiCGSTABSolver
 Iter          Defect            Rate
    0      3.16163e-09
0  (it = 0.5; it < _maxit; it+=.5) 0.5 250
0  before p = r 
0  before  y = W^-1 * p 
0  before _prec->apply(y,p); 0.5 250
0 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 7
6 === BiCGSTABSolver
 Iter          Defect            Rate
    0      3.16163e-09
6  (it = 0.5; it < _maxit; it+=.5) 0.5 250
6  before p = r 
6  before  y = W^-1 * p 
6  before _prec->apply(y,p); 0.5 250
6 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
1 === BiCGSTABSolver
 Iter          Defect            Rate
    0      3.16163e-09
1  (it = 0.5; it < _maxit; it+=.5) 0.5 250
1  before p = r 
1  before  y = W^-1 * p 
1  before _prec->apply(y,p); 0.5 250
1 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
2 === BiCGSTABSolver
 Iter          Defect            Rate
    0      3.16163e-09
2  (it = 0.5; it < _maxit; it+=.5) 0.5 250
2  before p = r 
2  before  y = W^-1 * p 
2  before _prec->apply(y,p); 0.5 250
2 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
3 === BiCGSTABSolver
 Iter          Defect            Rate
    0      3.16163e-09
3  (it = 0.5; it < _maxit; it+=.5) 0.5 250
3  before p = r 
3  before  y = W^-1 * p 
3  before _prec->apply(y,p); 0.5 250
3 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
4 === BiCGSTABSolver
 Iter          Defect            Rate
    0      3.16163e-09
4  (it = 0.5; it < _maxit; it+=.5) 0.5 250
4  before p = r 
4  before  y = W^-1 * p 
4  before _prec->apply(y,p); 0.5 250
4 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
5 === BiCGSTABSolver
 Iter          Defect            Rate
    0      3.16163e-09
5  (it = 0.5; it < _maxit; it+=.5) 0.5 250
5  before p = r 
5  before  y = W^-1 * p 
5  before _prec->apply(y,p); 0.5 250
5 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 7
0 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
0 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
5 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
5 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
5  before v = A * y 
6 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
6 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
6  before v = A * y 
1 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
2 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
2 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
3 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
3 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
3  before v = A * y 
4 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
4 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
4  before v = A * y 
2  before v = A * y 
2  before  alpha = rho_new / < rt, v > 
3  before  alpha = rho_new / < rt, v > 
4  before  alpha = rho_new / < rt, v > 
0 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
0  before v = A * y 
0  before  alpha = rho_new / < rt, v > 
5  before  alpha = rho_new / < rt, v > 
6  before  alpha = rho_new / < rt, v > 
1 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
1  before v = A * y 
6  before alpha = rho_new / h; 
6  before apply first correction to x 
6  before r = r - alpha*v 
6  before test stop criteria 
6 istlsolver::printOutput 
  0.5      4.85911e-25       1.5369e-16
6 === rate=2.36207e-32, T=0.123175, TIT=0.246351, IT=0.5
6 finished solveLinearSystemImpl_ 1
6 converged value: 1 7 1 let s go get convergedRemote
6 to  comm_.min(converged) 
3  before alpha = rho_new / h; 
3  before apply first correction to x 
3  before r = r - alpha*v 
3  before test stop criteria 
3 istlsolver::printOutput 
  0.5      4.85911e-25       1.5369e-16
3 === rate=2.36207e-32, T=0.123304, TIT=0.246608, IT=0.5
3 finished solveLinearSystemImpl_ 1
3 converged value: 1 7 1 let s go get convergedRemote
3 to  comm_.min(converged) 
2  before alpha = rho_new / h; 
2  before apply first correction to x 
2  before r = r - alpha*v 
2  before test stop criteria 
2 istlsolver::printOutput 
  0.5      4.85911e-25       1.5369e-16
2 === rate=2.36207e-32, T=0.12318, TIT=0.24636, IT=0.5
2 finished solveLinearSystemImpl_ 1
2 converged value: 1 7 1 let s go get convergedRemote
2 to  comm_.min(converged) 
1  before  alpha = rho_new / < rt, v > 
1  before alpha = rho_new / h; 
1  before apply first correction to x 
1  before r = r - alpha*v 
1  before test stop criteria 
1 istlsolver::printOutput 
  0.5      4.85911e-25       1.5369e-16
1 === rate=2.36207e-32, T=0.123307, TIT=0.246613, IT=0.5
1 finished solveLinearSystemImpl_ 1
1 converged value: 1 7 1 let s go get convergedRemote
1 to  comm_.min(converged) 
4  before alpha = rho_new / h; 
4  before apply first correction to x 
4  before r = r - alpha*v 
4  before test stop criteria 
4 istlsolver::printOutput 
  0.5      4.85911e-25       1.5369e-16
4 === rate=2.36207e-32, T=0.123175, TIT=0.24635, IT=0.5
4 finished solveLinearSystemImpl_ 1
4 converged value: 1 7 1 let s go get convergedRemote
4 to  comm_.min(converged) 
0  before alpha = rho_new / h; 
0  before apply first correction to x 
0  before r = r - alpha*v 
0  before test stop criteria 
0 istlsolver::printOutput 
  0.5      4.85911e-25       1.5369e-16
0 === rate=2.36207e-32, T=0.0207029, TIT=0.0414058, IT=0.5
5  before alpha = rho_new / h; 
5  before apply first correction to x 
5  before r = r - alpha*v 
5  before test stop criteria 
5 istlsolver::printOutput 
  0.5      4.85911e-25       1.5369e-16
5 === rate=2.36207e-32, T=0.123303, TIT=0.246607, IT=0.5
5 finished solveLinearSystemImpl_ 1
5 converged value: 1 7 1 let s go get convergedRemote
5 to  comm_.min(converged) 
6 did  comm_.min(converged) convergedRemote: 1
6 final convergedRemote: 1
6 Update: x^(k+1) = x^k - deltax^k 
6 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
6 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
6 Newton iteration  1 done6 , maximum relative shift = 1.018e-01
6 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
3 did  comm_.min(converged) convergedRemote: 1
3 final convergedRemote: 1
3 Update: x^(k+1) = x^k - deltax^k 
3 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
3 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
3 Newton iteration  1 done3 , maximum relative shift = 1.018e-01
3 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
5 did  comm_.min(converged) convergedRemote: 1
5 final convergedRemote: 1
5 Update: x^(k+1) = x^k - deltax^k 
5 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
5 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
5 Newton iteration  1 done5 , maximum relative shift = 1.018e-01
5 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
1 did  comm_.min(converged) convergedRemote: 1
1 final convergedRemote: 1
1 Update: x^(k+1) = x^k - deltax^k 
1 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
1 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
1 Newton iteration  1 done1 , maximum relative shift = 1.018e-01
1 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
4 did  comm_.min(converged) convergedRemote: 1
4 final convergedRemote: 1
4 Update: x^(k+1) = x^k - deltax^k 
4 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
4 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
4 Newton iteration  1 done4 , maximum relative shift = 1.018e-01
4 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
0 finished solveLinearSystemImpl_ 1
0 converged value: 1 7 1 let s go get convergedRemote
0 to  comm_.min(converged) 
0 did  comm_.min(converged) convergedRemote: 1
0 final convergedRemote: 1
0 Update: x^(k+1) = x^k - deltax^k 
0 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
0 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
0 Newton iteration  1 done0 , maximum relative shift = 1.018e-01
0 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
2 did  comm_.min(converged) convergedRemote: 1
2 final convergedRemote: 1
2 Update: x^(k+1) = x^k - deltax^k 
2 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
2 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
2 Newton iteration  1 done2 , maximum relative shift = 1.018e-01
2 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
5 Solve: M deltax^k = r 
5 Newton::solveLinearSystem : entering the try section 1
5 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
5 solveLinearSystemImpl_, before converged = ls.solve 
5 7 amgbackend::solve isParallel 1
1 Solve: M deltax^k = r 
1 Newton::solveLinearSystem : entering the try section 1
1 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
1 solveLinearSystemImpl_, before converged = ls.solve 
1 7 amgbackend::solve isParallel 1
4 Solve: M deltax^k = r 
4 Newton::solveLinearSystem : entering the try section 1
4 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
4 solveLinearSystemImpl_, before converged = ls.solve 
4 7 amgbackend::solve isParallel 1
0 Solve: M deltax^k = r 
0 Newton::solveLinearSystem : entering the try section 1
0 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
0 solveLinearSystemImpl_, before converged = ls.solve 
0 7 amgbackend::solve isParallel 1
3 Solve: M deltax^k = r 
3 Newton::solveLinearSystem : entering the try section 1
3 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
3 solveLinearSystemImpl_, before converged = ls.solve 
3 7 amgbackend::solve isParallel 1
6 Solve: M deltax^k = r 
6 Newton::solveLinearSystem : entering the try section 1
6 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
6 solveLinearSystemImpl_, before converged = ls.solve 
6 7 amgbackend::solve isParallel 1
2 Solve: M deltax^k = r 
2 Newton::solveLinearSystem : entering the try section 1
2 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
2 solveLinearSystemImpl_, before converged = ls.solve 
2 7 amgbackend::solve isParallel 1
0 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
0 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 7
3 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
3 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
2 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
2 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
6 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
6 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
5 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
5 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
1 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
1 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
4 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
4 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 7
4 7 before Dune::BiCGSTABSolver 
4 Dune::BiCGSTABSolver.apply(x, b, result_) 
4 BiCGSTABSolver::apply 
4 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 7
2 7 before Dune::BiCGSTABSolver 
2 Dune::BiCGSTABSolver.apply(x, b, result_) 
2 BiCGSTABSolver::apply 
2 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
6 7 before Dune::BiCGSTABSolver 
6 Dune::BiCGSTABSolver.apply(x, b, result_) 
6 BiCGSTABSolver::apply 
6 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
5 7 before Dune::BiCGSTABSolver 
5 Dune::BiCGSTABSolver.apply(x, b, result_) 
5 BiCGSTABSolver::apply 
5 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
1 7 before Dune::BiCGSTABSolver 
1 Dune::BiCGSTABSolver.apply(x, b, result_) 
1 BiCGSTABSolver::apply 
1 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
3 7 before Dune::BiCGSTABSolver 
3 Dune::BiCGSTABSolver.apply(x, b, result_) 
3 BiCGSTABSolver::apply 
3 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
Using a direct coarse solver (SuperLU)
Building hierarchy of 1 levels (inclusive coarse solver) took 0.111415 seconds.
0 7 before Dune::BiCGSTABSolver 
0 Dune::BiCGSTABSolver.apply(x, b, result_) 
0 BiCGSTABSolver::apply 
0 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
3 === BiCGSTABSolver
 Iter          Defect            Rate
    0      1.43603e-11
3  (it = 0.5; it < _maxit; it+=.5) 0.5 250
3  before p = r 
3  before  y = W^-1 * p 
3  before _prec->apply(y,p); 0.5 250
3 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
2 === BiCGSTABSolver
 Iter          Defect            Rate
    0      1.43603e-11
2  (it = 0.5; it < _maxit; it+=.5) 0.5 250
2  before p = r 
2  before  y = W^-1 * p 
2  before _prec->apply(y,p); 0.5 250
2 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
6 === BiCGSTABSolver
 Iter          Defect            Rate
    0      1.43603e-11
6  (it = 0.5; it < _maxit; it+=.5) 0.5 250
6  before p = r 
6  before  y = W^-1 * p 
6  before _prec->apply(y,p); 0.5 250
6 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
5 === BiCGSTABSolver
 Iter          Defect            Rate
    0      1.43603e-11
5  (it = 0.5; it < _maxit; it+=.5) 0.5 250
5  before p = r 
5  before  y = W^-1 * p 
5  before _prec->apply(y,p); 0.5 250
5 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
1 === BiCGSTABSolver
 Iter          Defect            Rate
    0      1.43603e-11
1  (it = 0.5; it < _maxit; it+=.5) 0.5 250
1  before p = r 
1  before  y = W^-1 * p 
1  before _prec->apply(y,p); 0.5 250
1 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
4 === BiCGSTABSolver
 Iter          Defect            Rate
    0      1.43603e-11
4  (it = 0.5; it < _maxit; it+=.5) 0.5 250
4  before p = r 
4  before  y = W^-1 * p 
4  before _prec->apply(y,p); 0.5 250
4 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
0 === BiCGSTABSolver
 Iter          Defect            Rate
    0      1.43603e-11
0  (it = 0.5; it < _maxit; it+=.5) 0.5 250
0  before p = r 
0  before  y = W^-1 * p 
0  before _prec->apply(y,p); 0.5 250
0 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 7
0 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
4 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
4 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
4  before v = A * y 
3 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
3 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
3  before v = A * y 
2 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
2 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
6 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
6 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
6  before v = A * y 
5 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
5 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
5  before v = A * y 
1 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
0 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
1 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
1  before v = A * y 
4  before  alpha = rho_new / < rt, v > 
3  before  alpha = rho_new / < rt, v > 
2  before v = A * y 
6  before  alpha = rho_new / < rt, v > 
5  before  alpha = rho_new / < rt, v > 
0 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
0  before v = A * y 
0  before  alpha = rho_new / < rt, v > 
0  before alpha = rho_new / h; 
0  before apply first correction to x 
0  before r = r - alpha*v 
0  before test stop criteria 
0 istlsolver::printOutput 
  0.5      1.65165e-27      1.15015e-16
0 === rate=1.32284e-32, T=0.0198859, TIT=0.0397718, IT=0.5
5  before alpha = rho_new / h; 
5  before apply first correction to x 
5  before r = r - alpha*v 
5  before test stop criteria 
5 istlsolver::printOutput 
  0.5      1.65165e-27      1.15015e-16
5 === rate=1.32284e-32, T=0.123309, TIT=0.246619, IT=0.5
5 finished solveLinearSystemImpl_ 1
5 converged value: 1 7 1 let s go get convergedRemote
5 to  comm_.min(converged) 
1  before  alpha = rho_new / < rt, v > 
1  before alpha = rho_new / h; 
1  before apply first correction to x 
1  before r = r - alpha*v 
1  before test stop criteria 
1 istlsolver::printOutput 
  0.5      1.65165e-27      1.15015e-16
1 === rate=1.32284e-32, T=0.123309, TIT=0.246618, IT=0.5
1 finished solveLinearSystemImpl_ 1
1 converged value: 1 7 1 let s go get convergedRemote
1 to  comm_.min(converged) 
4  before alpha = rho_new / h; 
4  before apply first correction to x 
4  before r = r - alpha*v 
4  before test stop criteria 
4 istlsolver::printOutput 
  0.5      1.65165e-27      1.15015e-16
4 === rate=1.32284e-32, T=0.123458, TIT=0.246917, IT=0.5
4 finished solveLinearSystemImpl_ 1
4 converged value: 1 7 1 let s go get convergedRemote
4 to  comm_.min(converged) 
3  before alpha = rho_new / h; 
3  before apply first correction to x 
3  before r = r - alpha*v 
3  before test stop criteria 
3 istlsolver::printOutput 
  0.5      1.65165e-27      1.15015e-16
3 === rate=1.32284e-32, T=0.123309, TIT=0.246618, IT=0.5
3 finished solveLinearSystemImpl_ 1
3 converged value: 1 7 1 let s go get convergedRemote
3 to  comm_.min(converged) 
2  before  alpha = rho_new / < rt, v > 
2  before alpha = rho_new / h; 
2  before apply first correction to x 
2  before r = r - alpha*v 
2  before test stop criteria 
2 istlsolver::printOutput 
  0.5      1.65165e-27      1.15015e-16
2 === rate=1.32284e-32, T=0.123459, TIT=0.246919, IT=0.5
2 finished solveLinearSystemImpl_ 1
2 converged value: 1 7 1 let s go get convergedRemote
2 to  comm_.min(converged) 
6  before alpha = rho_new / h; 
6  before apply first correction to x 
6  before r = r - alpha*v 
6  before test stop criteria 
6 istlsolver::printOutput 
  0.5      1.65165e-27      1.15015e-16
6 === rate=1.32284e-32, T=0.123461, TIT=0.246923, IT=0.5
6 finished solveLinearSystemImpl_ 1
6 converged value: 1 7 1 let s go get convergedRemote
6 to  comm_.min(converged) 
0 finished solveLinearSystemImpl_ 1
0 converged value: 1 7 1 let s go get convergedRemote
0 to  comm_.min(converged) 
0 did  comm_.min(converged) convergedRemote: 1
0 final convergedRemote: 1
0 Update: x^(k+1) = x^k - deltax^k 
0 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
0 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
0 Newton iteration  2 done0 , maximum relative shift = 4.138e-03
0 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
5 did  comm_.min(converged) convergedRemote: 1
5 final convergedRemote: 1
5 Update: x^(k+1) = x^k - deltax^k 
5 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
5 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
5 Newton iteration  2 done5 , maximum relative shift = 4.138e-03
5 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
1 did  comm_.min(converged) convergedRemote: 1
1 final convergedRemote: 1
1 Update: x^(k+1) = x^k - deltax^k 
1 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
1 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
1 Newton iteration  2 done1 , maximum relative shift = 4.138e-03
1 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
4 did  comm_.min(converged) convergedRemote: 1
4 final convergedRemote: 1
4 Update: x^(k+1) = x^k - deltax^k 
4 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
4 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
4 Newton iteration  2 done4 , maximum relative shift = 4.138e-03
4 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
3 did  comm_.min(converged) convergedRemote: 1
3 final convergedRemote: 1
3 Update: x^(k+1) = x^k - deltax^k 
3 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
3 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
3 Newton iteration  2 done3 , maximum relative shift = 4.138e-03
3 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
2 did  comm_.min(converged) convergedRemote: 1
2 final convergedRemote: 1
2 Update: x^(k+1) = x^k - deltax^k 
2 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
2 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
2 Newton iteration  2 done2 , maximum relative shift = 4.138e-03
2 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
6 did  comm_.min(converged) convergedRemote: 1
6 final convergedRemote: 1
6 Update: x^(k+1) = x^k - deltax^k 
6 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
6 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
6 Newton iteration  2 done6 , maximum relative shift = 4.138e-03
6 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
5 Solve: M deltax^k = r 
5 Newton::solveLinearSystem : entering the try section 2
5 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
5 solveLinearSystemImpl_, before converged = ls.solve 
5 7 amgbackend::solve isParallel 1
1 Solve: M deltax^k = r 
1 Newton::solveLinearSystem : entering the try section 2
1 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
1 solveLinearSystemImpl_, before converged = ls.solve 
1 7 amgbackend::solve isParallel 1
4 Solve: M deltax^k = r 
4 Newton::solveLinearSystem : entering the try section 2
4 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
4 solveLinearSystemImpl_, before converged = ls.solve 
4 7 amgbackend::solve isParallel 1
3 Solve: M deltax^k = r 
3 Newton::solveLinearSystem : entering the try section 2
3 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
3 solveLinearSystemImpl_, before converged = ls.solve 
3 7 amgbackend::solve isParallel 1
6 Solve: M deltax^k = r 
6 Newton::solveLinearSystem : entering the try section 2
6 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
6 solveLinearSystemImpl_, before converged = ls.solve 
6 7 amgbackend::solve isParallel 1
2 Solve: M deltax^k = r 
2 Newton::solveLinearSystem : entering the try section 2
2 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
2 solveLinearSystemImpl_, before converged = ls.solve 
2 7 amgbackend::solve isParallel 1
0 Solve: M deltax^k = r 
0 Newton::solveLinearSystem : entering the try section 2
0 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
0 solveLinearSystemImpl_, before converged = ls.solve 
0 7 amgbackend::solve isParallel 1
6 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
6 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
2 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
2 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
3 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
3 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
5 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
5 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
1 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
1 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
4 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
4 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
0 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
0 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 7
1 7 before Dune::BiCGSTABSolver 
1 Dune::BiCGSTABSolver.apply(x, b, result_) 
1 BiCGSTABSolver::apply 
1 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 7
4 7 before Dune::BiCGSTABSolver 
4 Dune::BiCGSTABSolver.apply(x, b, result_) 
4 BiCGSTABSolver::apply 
4 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
6 7 before Dune::BiCGSTABSolver 
6 Dune::BiCGSTABSolver.apply(x, b, result_) 
6 BiCGSTABSolver::apply 
6 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
2 7 before Dune::BiCGSTABSolver 
2 Dune::BiCGSTABSolver.apply(x, b, result_) 
2 BiCGSTABSolver::apply 
2 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
3 7 before Dune::BiCGSTABSolver 
3 Dune::BiCGSTABSolver.apply(x, b, result_) 
3 BiCGSTABSolver::apply 
3 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
5 7 before Dune::BiCGSTABSolver 
5 Dune::BiCGSTABSolver.apply(x, b, result_) 
5 BiCGSTABSolver::apply 
5 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
Using a direct coarse solver (SuperLU)
Building hierarchy of 1 levels (inclusive coarse solver) took 0.110186 seconds.
0 7 before Dune::BiCGSTABSolver 
0 Dune::BiCGSTABSolver.apply(x, b, result_) 
0 BiCGSTABSolver::apply 
0 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
0 === BiCGSTABSolver
 Iter          Defect            Rate
    0       1.6437e-12
0  (it = 0.5; it < _maxit; it+=.5) 0.5 250
0  before p = r 
0  before  y = W^-1 * p 
0  before _prec->apply(y,p); 0.5 250
0 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
5 === BiCGSTABSolver
 Iter          Defect            Rate
    0       1.6437e-12
5  (it = 0.5; it < _maxit; it+=.5) 0.5 250
5  before p = r 
5  before  y = W^-1 * p 
5  before _prec->apply(y,p); 0.5 250
5 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
6 === BiCGSTABSolver
 Iter          Defect            Rate
    0       1.6437e-12
6  (it = 0.5; it < _maxit; it+=.5) 0.5 250
6  before p = r 
6  before  y = W^-1 * p 
6  before _prec->apply(y,p); 0.5 250
6 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
2 === BiCGSTABSolver
 Iter          Defect            Rate
    0       1.6437e-12
2  (it = 0.5; it < _maxit; it+=.5) 0.5 250
2  before p = r 
2  before  y = W^-1 * p 
2  before _prec->apply(y,p); 0.5 250
2 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
1 === BiCGSTABSolver
 Iter          Defect            Rate
    0       1.6437e-12
1  (it = 0.5; it < _maxit; it+=.5) 0.5 250
1  before p = r 
1  before  y = W^-1 * p 
1  before _prec->apply(y,p); 0.5 250
1 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
3 === BiCGSTABSolver
 Iter          Defect            Rate
    0       1.6437e-12
3  (it = 0.5; it < _maxit; it+=.5) 0.5 250
3  before p = r 
3  before  y = W^-1 * p 
3  before _prec->apply(y,p); 0.5 250
3 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
4 === BiCGSTABSolver
 Iter          Defect            Rate
    0       1.6437e-12
4  (it = 0.5; it < _maxit; it+=.5) 0.5 250
4  before p = r 
4  before  y = W^-1 * p 
4  before _prec->apply(y,p); 0.5 250
4 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 7
0 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
5 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
5 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
5  before v = A * y 
6 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
6 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
6  before v = A * y 
2 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
2 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
3 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
3 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
3  before v = A * y 
1 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
4 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
4 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
4  before v = A * y 
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
0 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
0 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
0  before v = A * y 
3  before  alpha = rho_new / < rt, v > 
1 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
4  before  alpha = rho_new / < rt, v > 
2  before v = A * y 
5  before  alpha = rho_new / < rt, v > 
6  before  alpha = rho_new / < rt, v > 
3  before alpha = rho_new / h; 
3  before apply first correction to x 
3  before r = r - alpha*v 
3  before test stop criteria 
3 istlsolver::printOutput 
  0.5      3.94912e-29      2.40257e-17
3 === rate=5.77236e-34, T=0.122275, TIT=0.244551, IT=0.5
3 finished solveLinearSystemImpl_ 1
3 converged value: 1 7 1 let s go get convergedRemote
3 to  comm_.min(converged) 
1  before v = A * y 
1  before  alpha = rho_new / < rt, v > 
1  before alpha = rho_new / h; 
1  before apply first correction to x 
1  before r = r - alpha*v 
1  before test stop criteria 
1 istlsolver::printOutput 
  0.5      3.94912e-29      2.40257e-17
1 === rate=5.77236e-34, T=0.122276, TIT=0.244553, IT=0.5
1 finished solveLinearSystemImpl_ 1
1 converged value: 1 7 1 let s go get convergedRemote
1 to  comm_.min(converged) 
4  before alpha = rho_new / h; 
4  before apply first correction to x 
4  before r = r - alpha*v 
4  before test stop criteria 
4 istlsolver::printOutput 
  0.5      3.94912e-29      2.40257e-17
4 === rate=5.77236e-34, T=0.122167, TIT=0.244334, IT=0.5
4 finished solveLinearSystemImpl_ 1
4 converged value: 1 7 1 let s go get convergedRemote
4 to  comm_.min(converged) 
2  before  alpha = rho_new / < rt, v > 
2  before alpha = rho_new / h; 
2  before apply first correction to x 
2  before r = r - alpha*v 
2  before test stop criteria 
2 istlsolver::printOutput 
  0.5      3.94912e-29      2.40257e-17
2 === rate=5.77236e-34, T=0.122173, TIT=0.244347, IT=0.5
2 finished solveLinearSystemImpl_ 1
2 converged value: 1 7 1 let s go get convergedRemote
2 to  comm_.min(converged) 
5  before alpha = rho_new / h; 
5  before apply first correction to x 
5  before r = r - alpha*v 
5  before test stop criteria 
5 istlsolver::printOutput 
  0.5      3.94912e-29      2.40257e-17
5 === rate=5.77236e-34, T=0.122279, TIT=0.244559, IT=0.5
5 finished solveLinearSystemImpl_ 1
5 converged value: 1 7 1 let s go get convergedRemote
5 to  comm_.min(converged) 
6  before alpha = rho_new / h; 
6  before apply first correction to x 
6  before r = r - alpha*v 
6  before test stop criteria 
6 istlsolver::printOutput 
  0.5      3.94912e-29      2.40257e-17
6 === rate=5.77236e-34, T=0.122171, TIT=0.244341, IT=0.5
6 finished solveLinearSystemImpl_ 1
6 converged value: 1 7 1 let s go get convergedRemote
6 to  comm_.min(converged) 
0  before  alpha = rho_new / < rt, v > 
0  before alpha = rho_new / h; 
0  before apply first correction to x 
0  before r = r - alpha*v 
0  before test stop criteria 
0 istlsolver::printOutput 
  0.5      3.94912e-29      2.40257e-17
0 === rate=5.77236e-34, T=0.0199349, TIT=0.0398698, IT=0.5
6 did  comm_.min(converged) convergedRemote: 1
6 final convergedRemote: 1
6 Update: x^(k+1) = x^k - deltax^k 
6 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
6 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
6 Newton iteration  3 done6 , maximum relative shift = 4.101e-04
6 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
5 did  comm_.min(converged) convergedRemote: 1
5 final convergedRemote: 1
5 Update: x^(k+1) = x^k - deltax^k 
5 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
5 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
5 Newton iteration  3 done5 , maximum relative shift = 4.101e-04
5 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
3 did  comm_.min(converged) convergedRemote: 1
3 final convergedRemote: 1
3 Update: x^(k+1) = x^k - deltax^k 
3 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
3 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
3 Newton iteration  3 done3 , maximum relative shift = 4.101e-04
3 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
1 did  comm_.min(converged) convergedRemote: 1
1 final convergedRemote: 1
1 Update: x^(k+1) = x^k - deltax^k 
1 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
1 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
1 Newton iteration  3 done1 , maximum relative shift = 4.101e-04
1 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
4 did  comm_.min(converged) convergedRemote: 1
4 final convergedRemote: 1
4 Update: x^(k+1) = x^k - deltax^k 
4 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
4 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
4 Newton iteration  3 done4 , maximum relative shift = 4.101e-04
4 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
2 did  comm_.min(converged) convergedRemote: 1
2 final convergedRemote: 1
2 Update: x^(k+1) = x^k - deltax^k 
2 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
2 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
2 Newton iteration  3 done2 , maximum relative shift = 4.101e-04
2 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
0 finished solveLinearSystemImpl_ 1
0 converged value: 1 7 1 let s go get convergedRemote
0 to  comm_.min(converged) 
0 did  comm_.min(converged) convergedRemote: 1
0 final convergedRemote: 1
0 Update: x^(k+1) = x^k - deltax^k 
0 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
0 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
0 Newton iteration  3 done0 , maximum relative shift = 4.101e-04
0 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
2 Solve: M deltax^k = r 
2 Newton::solveLinearSystem : entering the try section 3
2 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
2 solveLinearSystemImpl_, before converged = ls.solve 
2 7 amgbackend::solve isParallel 1
6 Solve: M deltax^k = r 
6 Newton::solveLinearSystem : entering the try section 3
6 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
6 solveLinearSystemImpl_, before converged = ls.solve 
6 7 amgbackend::solve isParallel 1
5 Solve: M deltax^k = r 
5 Newton::solveLinearSystem : entering the try section 3
5 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
5 solveLinearSystemImpl_, before converged = ls.solve 
5 7 amgbackend::solve isParallel 1
3 Solve: M deltax^k = r 
3 Newton::solveLinearSystem : entering the try section 3
3 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
3 solveLinearSystemImpl_, before converged = ls.solve 
3 7 amgbackend::solve isParallel 1
1 Solve: M deltax^k = r 
1 Newton::solveLinearSystem : entering the try section 3
1 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
1 solveLinearSystemImpl_, before converged = ls.solve 
1 7 amgbackend::solve isParallel 1
4 Solve: M deltax^k = r 
4 Newton::solveLinearSystem : entering the try section 3
4 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
4 solveLinearSystemImpl_, before converged = ls.solve 
4 7 amgbackend::solve isParallel 1
4 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
0 Solve: M deltax^k = r 
0 Newton::solveLinearSystem : entering the try section 3
0 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
0 solveLinearSystemImpl_, before converged = ls.solve 
0 7 amgbackend::solve isParallel 1
3 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
3 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
1 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
1 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
0 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
0 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
4 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
2 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
2 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
6 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
6 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
5 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
5 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 7
2 7 before Dune::BiCGSTABSolver 
2 Dune::BiCGSTABSolver.apply(x, b, result_) 
2 BiCGSTABSolver::apply 
2 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
6 7 before Dune::BiCGSTABSolver 
6 Dune::BiCGSTABSolver.apply(x, b, result_) 
6 BiCGSTABSolver::apply 
6 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
1 7 before Dune::BiCGSTABSolver 
1 Dune::BiCGSTABSolver.apply(x, b, result_) 
1 BiCGSTABSolver::apply 
1 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
3 7 before Dune::BiCGSTABSolver 
3 Dune::BiCGSTABSolver.apply(x, b, result_) 
3 BiCGSTABSolver::apply 
3 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 7
5 7 before Dune::BiCGSTABSolver 
5 Dune::BiCGSTABSolver.apply(x, b, result_) 
5 BiCGSTABSolver::apply 
5 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
4 7 before Dune::BiCGSTABSolver 
4 Dune::BiCGSTABSolver.apply(x, b, result_) 
4 BiCGSTABSolver::apply 
4 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
Using a direct coarse solver (SuperLU)
Building hierarchy of 1 levels (inclusive coarse solver) took 0.111452 seconds.
0 7 before Dune::BiCGSTABSolver 
0 Dune::BiCGSTABSolver.apply(x, b, result_) 
0 BiCGSTABSolver::apply 
0 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
5 === BiCGSTABSolver
 Iter          Defect            Rate
    0      1.15126e-13
5  (it = 0.5; it < _maxit; it+=.5) 0.5 250
5  before p = r 
5  before  y = W^-1 * p 
5  before _prec->apply(y,p); 0.5 250
5 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
3 === BiCGSTABSolver
 Iter          Defect            Rate
    0      1.15126e-13
3  (it = 0.5; it < _maxit; it+=.5) 0.5 250
3  before p = r 
3  before  y = W^-1 * p 
3  before _prec->apply(y,p); 0.5 250
3 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
4 === BiCGSTABSolver
 Iter          Defect            Rate
    0      1.15126e-13
4  (it = 0.5; it < _maxit; it+=.5) 0.5 250
4  before p = r 
4  before  y = W^-1 * p 
4  before _prec->apply(y,p); 0.5 250
4 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
6 === BiCGSTABSolver
 Iter          Defect            Rate
    0      1.15126e-13
6  (it = 0.5; it < _maxit; it+=.5) 0.5 250
6  before p = r 
6  before  y = W^-1 * p 
6  before _prec->apply(y,p); 0.5 250
6 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
1 === BiCGSTABSolver
 Iter          Defect            Rate
    0      1.15126e-13
1  (it = 0.5; it < _maxit; it+=.5) 0.5 250
1  before p = r 
1  before  y = W^-1 * p 
1  before _prec->apply(y,p); 0.5 250
1 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
0 === BiCGSTABSolver
 Iter          Defect            Rate
    0      1.15126e-13
0  (it = 0.5; it < _maxit; it+=.5) 0.5 250
0  before p = r 
0  before  y = W^-1 * p 
0  before _prec->apply(y,p); 0.5 250
0 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 7
0 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
2 === BiCGSTABSolver
 Iter          Defect            Rate
    0      1.15126e-13
2  (it = 0.5; it < _maxit; it+=.5) 0.5 250
2  before p = r 
2  before  y = W^-1 * p 
2  before _prec->apply(y,p); 0.5 250
2 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
0 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
1 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
5 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
5 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
5  before v = A * y 
3 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
3 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
3  before v = A * y 
4 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
4 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
4  before v = A * y 
4  before  alpha = rho_new / < rt, v > 
6 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
6 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
6  before v = A * y 
6  before  alpha = rho_new / < rt, v > 
2 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
2 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
3  before  alpha = rho_new / < rt, v > 
2  before v = A * y 
2  before  alpha = rho_new / < rt, v > 
0 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
0  before v = A * y 
0  before  alpha = rho_new / < rt, v > 
1 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
1  before v = A * y 
5  before  alpha = rho_new / < rt, v > 
6  before alpha = rho_new / h; 
6  before apply first correction to x 
6  before r = r - alpha*v 
6  before test stop criteria 
6 istlsolver::printOutput 
  0.5      9.88609e-32      8.58717e-19
6 === rate=7.37396e-37, T=0.122017, TIT=0.244034, IT=0.5
6 finished solveLinearSystemImpl_ 1
6 converged value: 1 7 1 let s go get convergedRemote
6 to  comm_.min(converged) 
2  before alpha = rho_new / h; 
2  before apply first correction to x 
2  before r = r - alpha*v 
2  before test stop criteria 
2 istlsolver::printOutput 
  0.5      9.88609e-32      8.58717e-19
2 === rate=7.37396e-37, T=0.122134, TIT=0.244268, IT=0.5
2 finished solveLinearSystemImpl_ 1
2 converged value: 1 7 1 let s go get convergedRemote
2 to  comm_.min(converged) 
0  before alpha = rho_new / h; 
0  before apply first correction to x 
0  before r = r - alpha*v 
0  before test stop criteria 
0 istlsolver::printOutput 
  0.5      9.88609e-32      8.58717e-19
0 === rate=7.37396e-37, T=0.0183978, TIT=0.0367955, IT=0.5
1  before  alpha = rho_new / < rt, v > 
1  before alpha = rho_new / h; 
1  before apply first correction to x 
1  before r = r - alpha*v 
1  before test stop criteria 
1 istlsolver::printOutput 
  0.5      9.88609e-32      8.58717e-19
1 === rate=7.37396e-37, T=0.122019, TIT=0.244039, IT=0.5
1 finished solveLinearSystemImpl_ 1
1 converged value: 1 7 1 let s go get convergedRemote
1 to  comm_.min(converged) 
3  before alpha = rho_new / h; 
3  before apply first correction to x 
3  before r = r - alpha*v 
3  before test stop criteria 
3 istlsolver::printOutput 
  0.5      9.88609e-32      8.58717e-19
3 === rate=7.37396e-37, T=0.12202, TIT=0.244041, IT=0.5
3 finished solveLinearSystemImpl_ 1
3 converged value: 1 7 1 let s go get convergedRemote
3 to  comm_.min(converged) 
4  before alpha = rho_new / h; 
4  before apply first correction to x 
4  before r = r - alpha*v 
4  before test stop criteria 
4 istlsolver::printOutput 
  0.5      9.88609e-32      8.58717e-19
4 === rate=7.37396e-37, T=0.122134, TIT=0.244269, IT=0.5
4 finished solveLinearSystemImpl_ 1
4 converged value: 1 7 1 let s go get convergedRemote
4 to  comm_.min(converged) 
5  before alpha = rho_new / h; 
5  before apply first correction to x 
5  before r = r - alpha*v 
5  before test stop criteria 
5 istlsolver::printOutput 
  0.5      9.88609e-32      8.58717e-19
5 === rate=7.37396e-37, T=0.12202, TIT=0.24404, IT=0.5
5 finished solveLinearSystemImpl_ 1
5 converged value: 1 7 1 let s go get convergedRemote
5 to  comm_.min(converged) 
3 did  comm_.min(converged) convergedRemote: 1
3 final convergedRemote: 1
3 Update: x^(k+1) = x^k - deltax^k 
3 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
3 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
3 Newton iteration  4 done3 , maximum relative shift = 3.447e-05
3 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
5 did  comm_.min(converged) convergedRemote: 1
5 final convergedRemote: 1
5 Update: x^(k+1) = x^k - deltax^k 
5 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
5 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
5 Newton iteration  4 done5 , maximum relative shift = 3.447e-05
5 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
4 did  comm_.min(converged) convergedRemote: 1
4 final convergedRemote: 1
4 Update: x^(k+1) = x^k - deltax^k 
4 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
4 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
4 Newton iteration  4 done4 , maximum relative shift = 3.447e-05
4 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
6 did  comm_.min(converged) convergedRemote: 1
6 final convergedRemote: 1
6 Update: x^(k+1) = x^k - deltax^k 
6 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
6 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
6 Newton iteration  4 done6 , maximum relative shift = 3.447e-05
6 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
2 did  comm_.min(converged) convergedRemote: 1
2 final convergedRemote: 1
2 Update: x^(k+1) = x^k - deltax^k 
2 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
2 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
2 Newton iteration  4 done2 , maximum relative shift = 3.447e-05
2 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
0 finished solveLinearSystemImpl_ 1
0 converged value: 1 7 1 let s go get convergedRemote
0 to  comm_.min(converged) 
0 did  comm_.min(converged) convergedRemote: 1
0 final convergedRemote: 1
0 Update: x^(k+1) = x^k - deltax^k 
0 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
0 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
0 Newton iteration  4 done0 , maximum relative shift = 3.447e-05
0 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
1 did  comm_.min(converged) convergedRemote: 1
1 final convergedRemote: 1
1 Update: x^(k+1) = x^k - deltax^k 
1 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
1 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
1 Newton iteration  4 done1 , maximum relative shift = 3.447e-05
1 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
6 Solve: M deltax^k = r 
6 Newton::solveLinearSystem : entering the try section 4
6 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
6 solveLinearSystemImpl_, before converged = ls.solve 
6 7 amgbackend::solve isParallel 1
2 Solve: M deltax^k = r 
2 Newton::solveLinearSystem : entering the try section 4
2 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
2 solveLinearSystemImpl_, before converged = ls.solve 
2 7 amgbackend::solve isParallel 1
0 Solve: M deltax^k = r 
0 Newton::solveLinearSystem : entering the try section 4
0 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
0 solveLinearSystemImpl_, before converged = ls.solve 
0 7 amgbackend::solve isParallel 1
3 Solve: M deltax^k = r 
3 Newton::solveLinearSystem : entering the try section 4
3 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
3 solveLinearSystemImpl_, before converged = ls.solve 
3 7 amgbackend::solve isParallel 1
1 Solve: M deltax^k = r 
1 Newton::solveLinearSystem : entering the try section 4
1 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
1 solveLinearSystemImpl_, before converged = ls.solve 
1 7 amgbackend::solve isParallel 1
5 Solve: M deltax^k = r 
5 Newton::solveLinearSystem : entering the try section 4
5 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
5 solveLinearSystemImpl_, before converged = ls.solve 
5 7 amgbackend::solve isParallel 1
4 Solve: M deltax^k = r 
4 Newton::solveLinearSystem : entering the try section 4
4 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
4 solveLinearSystemImpl_, before converged = ls.solve 
4 7 amgbackend::solve isParallel 1
5 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
5 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
3 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
3 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
6 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
6 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
2 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
2 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
0 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
0 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
4 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
4 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
1 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
1 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 7
2 7 before Dune::BiCGSTABSolver 
2 Dune::BiCGSTABSolver.apply(x, b, result_) 
2 BiCGSTABSolver::apply 
2 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
1 7 before Dune::BiCGSTABSolver 
1 Dune::BiCGSTABSolver.apply(x, b, result_) 
1 BiCGSTABSolver::apply 
1 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
5 7 before Dune::BiCGSTABSolver 
5 Dune::BiCGSTABSolver.apply(x, b, result_) 
5 BiCGSTABSolver::apply 
5 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
3 7 before Dune::BiCGSTABSolver 
3 Dune::BiCGSTABSolver.apply(x, b, result_) 
3 BiCGSTABSolver::apply 
3 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
6 7 before Dune::BiCGSTABSolver 
6 Dune::BiCGSTABSolver.apply(x, b, result_) 
6 BiCGSTABSolver::apply 
6 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
4 7 before Dune::BiCGSTABSolver 
4 Dune::BiCGSTABSolver.apply(x, b, result_) 
4 BiCGSTABSolver::apply 
4 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
Using a direct coarse solver (SuperLU)
Building hierarchy of 1 levels (inclusive coarse solver) took 0.110823 seconds.
0 7 before Dune::BiCGSTABSolver 
0 Dune::BiCGSTABSolver.apply(x, b, result_) 
0 BiCGSTABSolver::apply 
0 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
0 === BiCGSTABSolver
 Iter          Defect            Rate
    0      1.33536e-14
0  (it = 0.5; it < _maxit; it+=.5) 0.5 250
4 === BiCGSTABSolver
 Iter          Defect            Rate
    0      1.33536e-14
4  (it = 0.5; it < _maxit; it+=.5) 0.5 250
4  before p = r 
4  before  y = W^-1 * p 
4  before _prec->apply(y,p); 0.5 250
4 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
2 === BiCGSTABSolver
 Iter          Defect            Rate
    0      1.33536e-14
2  (it = 0.5; it < _maxit; it+=.5) 0.5 250
2  before p = r 
2  before  y = W^-1 * p 
2  before _prec->apply(y,p); 0.5 250
2 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
1 === BiCGSTABSolver
 Iter          Defect            Rate
    0      1.33536e-14
1  (it = 0.5; it < _maxit; it+=.5) 0.5 250
1  before p = r 
1  before  y = W^-1 * p 
1  before _prec->apply(y,p); 0.5 250
1 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
5 === BiCGSTABSolver
 Iter          Defect            Rate
    0      1.33536e-14
5  (it = 0.5; it < _maxit; it+=.5) 0.5 250
5  before p = r 
5  before  y = W^-1 * p 
5  before _prec->apply(y,p); 0.5 250
5 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
3 === BiCGSTABSolver
 Iter          Defect            Rate
    0      1.33536e-14
3  (it = 0.5; it < _maxit; it+=.5) 0.5 250
3  before p = r 
3  before  y = W^-1 * p 
3  before _prec->apply(y,p); 0.5 250
3 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
6 === BiCGSTABSolver
 Iter          Defect            Rate
    0      1.33536e-14
6  (it = 0.5; it < _maxit; it+=.5) 0.5 250
6  before p = r 
6  before  y = W^-1 * p 
6  before _prec->apply(y,p); 0.5 250
6 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
0  before p = r 
0  before  y = W^-1 * p 
0  before _prec->apply(y,p); 0.5 250
0 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 7
0 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
4 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
4 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
4  before v = A * y 
2 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
2 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
1 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
5 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
5 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
5  before v = A * y 
3 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
3 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
3  before v = A * y 
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
0 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
6 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
6 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
6  before v = A * y 
0 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
3  before  alpha = rho_new / < rt, v > 
4  before  alpha = rho_new / < rt, v > 
2  before v = A * y 
1 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
5  before  alpha = rho_new / < rt, v > 
6  before  alpha = rho_new / < rt, v > 
6  before alpha = rho_new / h; 
6  before apply first correction to x 
6  before r = r - alpha*v 
6  before test stop criteria 
6 istlsolver::printOutput 
  0.5      1.38739e-30      1.03896e-16
6 === rate=1.07943e-32, T=0.121122, TIT=0.242244, IT=0.5
6 finished solveLinearSystemImpl_ 1
6 converged value: 1 7 1 let s go get convergedRemote
6 to  comm_.min(converged) 
5  before alpha = rho_new / h; 
5  before apply first correction to x 
5  before r = r - alpha*v 
5  before test stop criteria 
5 istlsolver::printOutput 
  0.5      1.38739e-30      1.03896e-16
5 === rate=1.07943e-32, T=0.121086, TIT=0.242171, IT=0.5
5 finished solveLinearSystemImpl_ 1
5 converged value: 1 7 1 let s go get convergedRemote
5 to  comm_.min(converged) 
0  before v = A * y 
0  before  alpha = rho_new / < rt, v > 
0  before alpha = rho_new / h; 
0  before apply first correction to x 
0  before r = r - alpha*v 
0  before test stop criteria 
0 istlsolver::printOutput 
  0.5      1.38739e-30      1.03896e-16
0 === rate=1.07943e-32, T=0.0184529, TIT=0.0369058, IT=0.5
3  before alpha = rho_new / h; 
3  before apply first correction to x 
3  before r = r - alpha*v 
3  before test stop criteria 
3 istlsolver::printOutput 
  0.5      1.38739e-30      1.03896e-16
3 === rate=1.07943e-32, T=0.121087, TIT=0.242174, IT=0.5
3 finished solveLinearSystemImpl_ 1
3 converged value: 1 7 1 let s go get convergedRemote
3 to  comm_.min(converged) 
4  before alpha = rho_new / h; 
4  before apply first correction to x 
4  before r = r - alpha*v 
4  before test stop criteria 
4 istlsolver::printOutput 
  0.5      1.38739e-30      1.03896e-16
4 === rate=1.07943e-32, T=0.121121, TIT=0.242243, IT=0.5
4 finished solveLinearSystemImpl_ 1
4 converged value: 1 7 1 let s go get convergedRemote
4 to  comm_.min(converged) 
2  before  alpha = rho_new / < rt, v > 
2  before alpha = rho_new / h; 
2  before apply first correction to x 
2  before r = r - alpha*v 
2  before test stop criteria 
2 istlsolver::printOutput 
  0.5      1.38739e-30      1.03896e-16
2 === rate=1.07943e-32, T=0.12112, TIT=0.24224, IT=0.5
2 finished solveLinearSystemImpl_ 1
2 converged value: 1 7 1 let s go get convergedRemote
2 to  comm_.min(converged) 
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
1  before v = A * y 
1  before  alpha = rho_new / < rt, v > 
1  before alpha = rho_new / h; 
1  before apply first correction to x 
1  before r = r - alpha*v 
1  before test stop criteria 
1 istlsolver::printOutput 
  0.5      1.38739e-30      1.03896e-16
1 === rate=1.07943e-32, T=0.121086, TIT=0.242172, IT=0.5
1 finished solveLinearSystemImpl_ 1
1 converged value: 1 7 1 let s go get convergedRemote
1 to  comm_.min(converged) 
2 did  comm_.min(converged) convergedRemote: 1
2 final convergedRemote: 1
2 Update: x^(k+1) = x^k - deltax^k 
2 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
2 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
2 Newton iteration  5 done2 , maximum relative shift = 3.333e-06
2 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
1 did  comm_.min(converged) convergedRemote: 1
1 final convergedRemote: 1
1 Update: x^(k+1) = x^k - deltax^k 
1 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
1 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
1 Newton iteration  5 done1 , maximum relative shift = 3.333e-06
1 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
6 did  comm_.min(converged) convergedRemote: 1
6 final convergedRemote: 1
6 Update: x^(k+1) = x^k - deltax^k 
6 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
6 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
6 Newton iteration  5 done6 , maximum relative shift = 3.333e-06
6 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
5 did  comm_.min(converged) convergedRemote: 1
5 final convergedRemote: 1
5 Update: x^(k+1) = x^k - deltax^k 
5 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
5 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
5 Newton iteration  5 done5 , maximum relative shift = 3.333e-06
5 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
0 finished solveLinearSystemImpl_ 1
0 converged value: 1 7 1 let s go get convergedRemote
0 to  comm_.min(converged) 
0 did  comm_.min(converged) convergedRemote: 1
0 final convergedRemote: 1
0 Update: x^(k+1) = x^k - deltax^k 
0 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
0 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
0 Newton iteration  5 done0 , maximum relative shift = 3.333e-06
0 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
3 did  comm_.min(converged) convergedRemote: 1
3 final convergedRemote: 1
3 Update: x^(k+1) = x^k - deltax^k 
3 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
3 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
3 Newton iteration  5 done3 , maximum relative shift = 3.333e-06
3 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
4 did  comm_.min(converged) convergedRemote: 1
4 final convergedRemote: 1
4 Update: x^(k+1) = x^k - deltax^k 
4 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
4 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
4 Newton iteration  5 done4 , maximum relative shift = 3.333e-06
4 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
4 Solve: M deltax^k = r 
4 Newton::solveLinearSystem : entering the try section 5
4 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
4 solveLinearSystemImpl_, before converged = ls.solve 
4 7 amgbackend::solve isParallel 1
1 Solve: M deltax^k = r 
1 Newton::solveLinearSystem : entering the try section 5
1 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
1 solveLinearSystemImpl_, before converged = ls.solve 
1 7 amgbackend::solve isParallel 1
6 Solve: M deltax^k = r 
6 Newton::solveLinearSystem : entering the try section 5
6 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
6 solveLinearSystemImpl_, before converged = ls.solve 
6 7 amgbackend::solve isParallel 1
5 Solve: M deltax^k = r 
5 Newton::solveLinearSystem : entering the try section 5
5 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
5 solveLinearSystemImpl_, before converged = ls.solve 
5 7 amgbackend::solve isParallel 1
0 Solve: M deltax^k = r 
0 Newton::solveLinearSystem : entering the try section 5
0 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
0 solveLinearSystemImpl_, before converged = ls.solve 
0 7 amgbackend::solve isParallel 1
3 Solve: M deltax^k = r 
3 Newton::solveLinearSystem : entering the try section 5
3 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
3 solveLinearSystemImpl_, before converged = ls.solve 
3 7 amgbackend::solve isParallel 1
2 Solve: M deltax^k = r 
2 Newton::solveLinearSystem : entering the try section 5
2 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
2 solveLinearSystemImpl_, before converged = ls.solve 
2 7 amgbackend::solve isParallel 1
3 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
3 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
2 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
2 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
4 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
4 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
1 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
1 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
6 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
6 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
5 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
5 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
0 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
0 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 7
6 7 before Dune::BiCGSTABSolver 
6 Dune::BiCGSTABSolver.apply(x, b, result_) 
6 BiCGSTABSolver::apply 
6 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
5 7 before Dune::BiCGSTABSolver 
5 Dune::BiCGSTABSolver.apply(x, b, result_) 
5 BiCGSTABSolver::apply 
5 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
3 7 before Dune::BiCGSTABSolver 
3 Dune::BiCGSTABSolver.apply(x, b, result_) 
3 BiCGSTABSolver::apply 
3 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 7
2 7 before Dune::BiCGSTABSolver 
2 Dune::BiCGSTABSolver.apply(x, b, result_) 
2 BiCGSTABSolver::apply 
2 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
4 7 before Dune::BiCGSTABSolver 
4 Dune::BiCGSTABSolver.apply(x, b, result_) 
4 BiCGSTABSolver::apply 
4 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
1 7 before Dune::BiCGSTABSolver 
1 Dune::BiCGSTABSolver.apply(x, b, result_) 
1 BiCGSTABSolver::apply 
1 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
Using a direct coarse solver (SuperLU)
Building hierarchy of 1 levels (inclusive coarse solver) took 0.110421 seconds.
0 7 before Dune::BiCGSTABSolver 
0 Dune::BiCGSTABSolver.apply(x, b, result_) 
0 BiCGSTABSolver::apply 
0 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
0 === BiCGSTABSolver
 Iter          Defect            Rate
    0      9.31401e-16
0  (it = 0.5; it < _maxit; it+=.5) 0.5 250
0  before p = r 
0  before  y = W^-1 * p 
0  before _prec->apply(y,p); 0.5 250
0 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 7
0 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
1 === BiCGSTABSolver
 Iter          Defect            Rate
    0      9.31401e-16
1  (it = 0.5; it < _maxit; it+=.5) 0.5 250
1  before p = r 
1  before  y = W^-1 * p 
1  before _prec->apply(y,p); 0.5 250
1 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
2 === BiCGSTABSolver
 Iter          Defect            Rate
    0      9.31401e-16
2  (it = 0.5; it < _maxit; it+=.5) 0.5 250
2  before p = r 
2  before  y = W^-1 * p 
2  before _prec->apply(y,p); 0.5 250
2 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
4 === BiCGSTABSolver
 Iter          Defect            Rate
    0      9.31401e-16
4  (it = 0.5; it < _maxit; it+=.5) 0.5 250
4  before p = r 
4  before  y = W^-1 * p 
4  before _prec->apply(y,p); 0.5 250
4 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
3 === BiCGSTABSolver
 Iter          Defect            Rate
    0      9.31401e-16
3  (it = 0.5; it < _maxit; it+=.5) 0.5 250
3  before p = r 
3  before  y = W^-1 * p 
3  before _prec->apply(y,p); 0.5 250
3 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
6 === BiCGSTABSolver
 Iter          Defect            Rate
    0      9.31401e-16
6  (it = 0.5; it < _maxit; it+=.5) 0.5 250
6  before p = r 
6  before  y = W^-1 * p 
6  before _prec->apply(y,p); 0.5 250
6 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
5 === BiCGSTABSolver
 Iter          Defect            Rate
    0      9.31401e-16
5  (it = 0.5; it < _maxit; it+=.5) 0.5 250
5  before p = r 
5  before  y = W^-1 * p 
5  before _prec->apply(y,p); 0.5 250
5 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
0 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
1 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
2 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
2 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
4 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
4 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
4  before v = A * y 
3 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
3 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
3  before v = A * y 
6 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
6 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
6  before v = A * y 
5 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
5 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
5  before v = A * y 
5  before  alpha = rho_new / < rt, v > 
6  before  alpha = rho_new / < rt, v > 
0 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
1 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
2  before v = A * y 
2  before  alpha = rho_new / < rt, v > 
4  before  alpha = rho_new / < rt, v > 
3  before  alpha = rho_new / < rt, v > 
5  before alpha = rho_new / h; 
5  before apply first correction to x 
5  before r = r - alpha*v 
5  before test stop criteria 
5 istlsolver::printOutput 
  0.5      3.04891e-34      3.27346e-19
5 === rate=1.07156e-37, T=0.120821, TIT=0.241642, IT=0.5
5 finished solveLinearSystemImpl_ 1
5 converged value: 1 7 1 let s go get convergedRemote
5 to  comm_.min(converged) 
6  before alpha = rho_new / h; 
6  before apply first correction to x 
6  before r = r - alpha*v 
6  before test stop criteria 
6 istlsolver::printOutput 
  0.5      3.04891e-34      3.27346e-19
6 === rate=1.07156e-37, T=0.120756, TIT=0.241511, IT=0.5
6 finished solveLinearSystemImpl_ 1
6 converged value: 1 7 1 let s go get convergedRemote
6 to  comm_.min(converged) 
0  before v = A * y 
0  before  alpha = rho_new / < rt, v > 
0  before alpha = rho_new / h; 
0  before apply first correction to x 
0  before r = r - alpha*v 
0  before test stop criteria 
0 istlsolver::printOutput 
  0.5      3.04891e-34      3.27346e-19
0 === rate=1.07156e-37, T=0.0182758, TIT=0.0365517, IT=0.5
1  before v = A * y 
1  before  alpha = rho_new / < rt, v > 
1  before alpha = rho_new / h; 
1  before apply first correction to x 
1  before r = r - alpha*v 
1  before test stop criteria 
1 istlsolver::printOutput 
  0.5      3.04891e-34      3.27346e-19
1 === rate=1.07156e-37, T=0.120822, TIT=0.241643, IT=0.5
1 finished solveLinearSystemImpl_ 1
1 converged value: 1 7 1 let s go get convergedRemote
1 to  comm_.min(converged) 
2  before alpha = rho_new / h; 
2  before apply first correction to x 
2  before r = r - alpha*v 
2  before test stop criteria 
2 istlsolver::printOutput 
  0.5      3.04891e-34      3.27346e-19
2 === rate=1.07156e-37, T=0.120759, TIT=0.241517, IT=0.5
2 finished solveLinearSystemImpl_ 1
2 converged value: 1 7 1 let s go get convergedRemote
2 to  comm_.min(converged) 
4  before alpha = rho_new / h; 
4  before apply first correction to x 
4  before r = r - alpha*v 
4  before test stop criteria 
4 istlsolver::printOutput 
  0.5      3.04891e-34      3.27346e-19
4 === rate=1.07156e-37, T=0.12084, TIT=0.241679, IT=0.5
4 finished solveLinearSystemImpl_ 1
4 converged value: 1 7 1 let s go get convergedRemote
4 to  comm_.min(converged) 
3  before alpha = rho_new / h; 
3  before apply first correction to x 
3  before r = r - alpha*v 
3  before test stop criteria 
3 istlsolver::printOutput 
  0.5      3.04891e-34      3.27346e-19
3 === rate=1.07156e-37, T=0.120822, TIT=0.241643, IT=0.5
3 finished solveLinearSystemImpl_ 1
3 converged value: 1 7 1 let s go get convergedRemote
3 to  comm_.min(converged) 
4 did  comm_.min(converged) convergedRemote: 1
4 final convergedRemote: 1
4 Update: x^(k+1) = x^k - deltax^k 
4 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
4 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
4 Newton iteration  6 done4 , maximum relative shift = 2.325e-07
4 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
5 did  comm_.min(converged) convergedRemote: 1
5 final convergedRemote: 1
5 Update: x^(k+1) = x^k - deltax^k 
5 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
5 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
5 Newton iteration  6 done5 , maximum relative shift = 2.325e-07
5 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
6 did  comm_.min(converged) convergedRemote: 1
6 final convergedRemote: 1
6 Update: x^(k+1) = x^k - deltax^k 
6 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
6 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
6 Newton iteration  6 done6 , maximum relative shift = 2.325e-07
6 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
0 finished solveLinearSystemImpl_ 1
0 converged value: 1 7 1 let s go get convergedRemote
0 to  comm_.min(converged) 
0 did  comm_.min(converged) convergedRemote: 1
0 final convergedRemote: 1
0 Update: x^(k+1) = x^k - deltax^k 
0 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
0 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
0 Newton iteration  6 done0 , maximum relative shift = 2.325e-07
0 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
1 did  comm_.min(converged) convergedRemote: 1
1 final convergedRemote: 1
1 Update: x^(k+1) = x^k - deltax^k 
1 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
1 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
1 Newton iteration  6 done1 , maximum relative shift = 2.325e-07
1 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
2 did  comm_.min(converged) convergedRemote: 1
2 final convergedRemote: 1
2 Update: x^(k+1) = x^k - deltax^k 
2 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
2 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
2 Newton iteration  6 done2 , maximum relative shift = 2.325e-07
2 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
3 did  comm_.min(converged) convergedRemote: 1
3 final convergedRemote: 1
3 Update: x^(k+1) = x^k - deltax^k 
3 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
3 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
3 Newton iteration  6 done3 , maximum relative shift = 2.325e-07
3 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
3 Solve: M deltax^k = r 
3 Newton::solveLinearSystem : entering the try section 6
3 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
3 solveLinearSystemImpl_, before converged = ls.solve 
3 7 amgbackend::solve isParallel 1
2 Solve: M deltax^k = r 
2 Newton::solveLinearSystem : entering the try section 6
2 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
2 solveLinearSystemImpl_, before converged = ls.solve 
2 7 amgbackend::solve isParallel 1
4 Solve: M deltax^k = r 
4 Newton::solveLinearSystem : entering the try section 6
4 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
4 solveLinearSystemImpl_, before converged = ls.solve 
4 7 amgbackend::solve isParallel 1
5 Solve: M deltax^k = r 
5 Newton::solveLinearSystem : entering the try section 6
5 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
5 solveLinearSystemImpl_, before converged = ls.solve 
5 7 amgbackend::solve isParallel 1
6 Solve: M deltax^k = r 
6 Newton::solveLinearSystem : entering the try section 6
6 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
6 solveLinearSystemImpl_, before converged = ls.solve 
6 7 amgbackend::solve isParallel 1
0 Solve: M deltax^k = r 
0 Newton::solveLinearSystem : entering the try section 6
0 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
0 solveLinearSystemImpl_, before converged = ls.solve 
0 7 amgbackend::solve isParallel 1
1 Solve: M deltax^k = r 
1 Newton::solveLinearSystem : entering the try section 6
1 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
1 solveLinearSystemImpl_, before converged = ls.solve 
1 7 amgbackend::solve isParallel 1
0 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
0 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 7
1 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
1 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
3 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
3 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
2 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
2 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
4 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
4 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
5 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
5 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
6 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
6 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 7
1 7 before Dune::BiCGSTABSolver 
1 Dune::BiCGSTABSolver.apply(x, b, result_) 
1 BiCGSTABSolver::apply 
1 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
3 7 before Dune::BiCGSTABSolver 
3 Dune::BiCGSTABSolver.apply(x, b, result_) 
3 BiCGSTABSolver::apply 
3 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
2 7 before Dune::BiCGSTABSolver 
2 Dune::BiCGSTABSolver.apply(x, b, result_) 
2 BiCGSTABSolver::apply 
2 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
4 7 before Dune::BiCGSTABSolver 
4 Dune::BiCGSTABSolver.apply(x, b, result_) 
4 BiCGSTABSolver::apply 
4 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
5 7 before Dune::BiCGSTABSolver 
5 Dune::BiCGSTABSolver.apply(x, b, result_) 
5 BiCGSTABSolver::apply 
5 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
6 7 before Dune::BiCGSTABSolver 
6 Dune::BiCGSTABSolver.apply(x, b, result_) 
6 BiCGSTABSolver::apply 
6 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
Using a direct coarse solver (SuperLU)
Building hierarchy of 1 levels (inclusive coarse solver) took 0.110295 seconds.
0 7 before Dune::BiCGSTABSolver 
0 Dune::BiCGSTABSolver.apply(x, b, result_) 
0 BiCGSTABSolver::apply 
0 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
0 === BiCGSTABSolver
 Iter          Defect            Rate
    0      6.49512e-17
0  (it = 0.5; it < _maxit; it+=.5) 0.5 250
0  before p = r 
0  before  y = W^-1 * p 
0  before _prec->apply(y,p); 0.5 250
0 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 7
0 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
6 === BiCGSTABSolver
 Iter          Defect            Rate
    0      6.49512e-17
6  (it = 0.5; it < _maxit; it+=.5) 0.5 250
6  before p = r 
6  before  y = W^-1 * p 
6  before _prec->apply(y,p); 0.5 250
6 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
1 === BiCGSTABSolver
 Iter          Defect            Rate
    0      6.49512e-17
1  (it = 0.5; it < _maxit; it+=.5) 0.5 250
1  before p = r 
1  before  y = W^-1 * p 
1  before _prec->apply(y,p); 0.5 250
1 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
3 === BiCGSTABSolver
 Iter          Defect            Rate
    0      6.49512e-17
3  (it = 0.5; it < _maxit; it+=.5) 0.5 250
3  before p = r 
3  before  y = W^-1 * p 
3  before _prec->apply(y,p); 0.5 250
3 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
2 === BiCGSTABSolver
 Iter          Defect            Rate
    0      6.49512e-17
2  (it = 0.5; it < _maxit; it+=.5) 0.5 250
2  before p = r 
2  before  y = W^-1 * p 
2  before _prec->apply(y,p); 0.5 250
2 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
4 === BiCGSTABSolver
 Iter          Defect            Rate
    0      6.49512e-17
4  (it = 0.5; it < _maxit; it+=.5) 0.5 250
4  before p = r 
4  before  y = W^-1 * p 
4  before _prec->apply(y,p); 0.5 250
4 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
5 === BiCGSTABSolver
 Iter          Defect            Rate
    0      6.49512e-17
5  (it = 0.5; it < _maxit; it+=.5) 0.5 250
5  before p = r 
5  before  y = W^-1 * p 
5  before _prec->apply(y,p); 0.5 250
5 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
0 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
0 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
0  before v = A * y 
0  before  alpha = rho_new / < rt, v > 
6 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
6 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
6  before v = A * y 
6  before  alpha = rho_new / < rt, v > 
1 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
1 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
1  before v = A * y 
1  before  alpha = rho_new / < rt, v > 
3 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
3 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
3  before v = A * y 
3  before  alpha = rho_new / < rt, v > 
2 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
2 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
2  before v = A * y 
2  before  alpha = rho_new / < rt, v > 
4 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
4 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
4  before v = A * y 
4  before  alpha = rho_new / < rt, v > 
5 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
5 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
5  before v = A * y 
5  before  alpha = rho_new / < rt, v > 
3  before alpha = rho_new / h; 
3  before apply first correction to x 
3  before r = r - alpha*v 
3  before test stop criteria 
2  before alpha = rho_new / h; 
2  before apply first correction to x 
2  before r = r - alpha*v 
2  before test stop criteria 
5  before alpha = rho_new / h; 
5  before apply first correction to x 
5  before r = r - alpha*v 
5  before test stop criteria 
0  before alpha = rho_new / h; 
0  before apply first correction to x 
0  before r = r - alpha*v 
0  before test stop criteria 
0 istlsolver::printOutput 
  0.5      1.53266e-34       2.3597e-18
0 === rate=5.5682e-36, T=0.0184415, TIT=0.0368831, IT=0.5
0 finished solveLinearSystemImpl_ 1
0 converged value: 1 7 1 let s go get convergedRemote
0 to  comm_.min(converged) 
0 did  comm_.min(converged) convergedRemote: 1
0 final convergedRemote: 1
0 Update: x^(k+1) = x^k - deltax^k 
0 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
0 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
0 Newton iteration  7 done0 , maximum relative shift = 1.621e-08
0 Assemble/solve/update time: 0.307691(25.1332%)/0.916403(74.8547%)/0.000147352(0.0120362%)
6  before alpha = rho_new / h; 
6  before apply first correction to x 
6  before r = r - alpha*v 
6  before test stop criteria 
6 istlsolver::printOutput 
  0.5      1.53266e-34       2.3597e-18
6 === rate=5.5682e-36, T=0.120759, TIT=0.241519, IT=0.5
6 finished solveLinearSystemImpl_ 1
6 converged value: 1 7 1 let s go get convergedRemote
6 to  comm_.min(converged) 
6 did  comm_.min(converged) convergedRemote: 1
6 final convergedRemote: 1
6 Update: x^(k+1) = x^k - deltax^k 
6 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
6 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
6 Newton iteration  7 done6 , maximum relative shift = 1.621e-08
6 Assemble/solve/update time: 0.308291(25.1698%)/0.91645(74.8216%)/0.000105941(0.00864932%)
6 gridVariables->advanceTimeStep
6 before, nonLinearSolver->suggestTimeStepSize, current time: 713.767 simTime 57600
6 before assembler->setPreviousSolution, ddt: 25
6 nonLinearSolver->solve
6 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
1  before alpha = rho_new / h; 
1  before apply first correction to x 
1  before r = r - alpha*v 
1  before test stop criteria 
1 istlsolver::printOutput 
  0.5      1.53266e-34       2.3597e-18
1 === rate=5.5682e-36, T=0.120829, TIT=0.241658, IT=0.5
1 finished solveLinearSystemImpl_ 1
1 converged value: 1 7 1 let s go get convergedRemote
1 to  comm_.min(converged) 
1 did  comm_.min(converged) convergedRemote: 1
1 final convergedRemote: 1
1 Update: x^(k+1) = x^k - deltax^k 
1 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
1 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
1 Newton iteration  7 done1 , maximum relative shift = 1.621e-08
1 Assemble/solve/update time: 0.308299(25.1702%)/0.916428(74.8192%)/0.000129611(0.0105817%)
1 gridVariables->advanceTimeStep
1 before, nonLinearSolver->suggestTimeStepSize, current time: 713.767 simTime 57600
1 before assembler->setPreviousSolution, ddt: 25
1 nonLinearSolver->solve
1 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
2 istlsolver::printOutput 
  0.5      1.53266e-34       2.3597e-18
2 === rate=5.5682e-36, T=0.120759, TIT=0.241518, IT=0.5
2 finished solveLinearSystemImpl_ 1
2 converged value: 1 7 1 let s go get convergedRemote
2 to  comm_.min(converged) 
2 did  comm_.min(converged) convergedRemote: 1
2 final convergedRemote: 1
2 Update: x^(k+1) = x^k - deltax^k 
2 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
2 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
2 Newton iteration  7 done2 , maximum relative shift = 1.621e-08
2 Assemble/solve/update time: 0.307737(25.1352%)/0.916441(74.8527%)/0.000147922(0.0120819%)
2 gridVariables->advanceTimeStep
2 before, nonLinearSolver->suggestTimeStepSize, current time: 713.767 simTime 57600
2 before assembler->setPreviousSolution, ddt: 25
3 istlsolver::printOutput 
  0.5      1.53266e-34       2.3597e-18
3 === rate=5.5682e-36, T=0.120995, TIT=0.241989, IT=0.5
3 finished solveLinearSystemImpl_ 1
3 converged value: 1 7 1 let s go get convergedRemote
3 to  comm_.min(converged) 
3 did  comm_.min(converged) convergedRemote: 1
3 final convergedRemote: 1
3 Update: x^(k+1) = x^k - deltax^k 
3 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
3 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
3 Newton iteration  7 done3 , maximum relative shift = 1.621e-08
3 Assemble/solve/update time: 0.308371(25.1739%)/0.916448(74.8145%)/0.000141102(0.0115189%)
3 gridVariables->advanceTimeStep
3 before, nonLinearSolver->suggestTimeStepSize, current time: 713.767 simTime 57600
3 before assembler->setPreviousSolution, ddt: 25
3 nonLinearSolver->solve
3 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
5 istlsolver::printOutput 
  0.5      1.53266e-34       2.3597e-18
5 === rate=5.5682e-36, T=0.120997, TIT=0.241993, IT=0.5
5 finished solveLinearSystemImpl_ 1
5 converged value: 1 7 1 let s go get convergedRemote
5 to  comm_.min(converged) 
5 did  comm_.min(converged) convergedRemote: 1
5 final convergedRemote: 1
5 Update: x^(k+1) = x^k - deltax^k 
5 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
5 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
5 Newton iteration  7 done5 , maximum relative shift = 1.621e-08
5 Assemble/solve/update time: 0.308308(25.1709%)/0.916423(74.8187%)/0.000127791(0.0104331%)
5 gridVariables->advanceTimeStep
5 before, nonLinearSolver->suggestTimeStepSize, current time: 713.767 simTime 57600
5 before assembler->setPreviousSolution, ddt: 25
5 nonLinearSolver->solve
5 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
4  before alpha = rho_new / h; 
4  before apply first correction to x 
4  before r = r - alpha*v 
4  before test stop criteria 
4 istlsolver::printOutput 
  0.5      1.53266e-34       2.3597e-18
4 === rate=5.5682e-36, T=0.120759, TIT=0.241518, IT=0.5
4 finished solveLinearSystemImpl_ 1
4 converged value: 1 7 1 let s go get convergedRemote
4 to  comm_.min(converged) 
4 did  comm_.min(converged) convergedRemote: 1
4 final convergedRemote: 1
4 Update: x^(k+1) = x^k - deltax^k 
4 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
4 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
4 Newton iteration  7 done4 , maximum relative shift = 1.621e-08
4 Assemble/solve/update time: 0.308308(25.1709%)/0.91643(74.8193%)/0.000119911(0.00978979%)
4 gridVariables->advanceTimeStep
4 before, nonLinearSolver->suggestTimeStepSize, current time: 713.767 simTime 57600
4 before assembler->setPreviousSolution, ddt: 25
4 nonLinearSolver->solve
4 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
2 nonLinearSolver->solve
2 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
0 gridVariables->advanceTimeStep
0 before, nonLinearSolver->suggestTimeStepSize, current time: 713.767 simTime 57600
0 before assembler->setPreviousSolution, ddt: 25
0 nonLinearSolver->solve
0 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
5 Solve: M deltax^k = r 
5 Newton::solveLinearSystem : entering the try section 0
5to  comm_.sum(norm2) 
5did  comm_.sum(norm2) 
5 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
5 solveLinearSystemImpl_, before converged = ls.solve 
5 7 amgbackend::solve isParallel 1
3 Solve: M deltax^k = r 
3 Newton::solveLinearSystem : entering the try section 0
3to  comm_.sum(norm2) 
3did  comm_.sum(norm2) 
3 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
3 solveLinearSystemImpl_, before converged = ls.solve 
3 7 amgbackend::solve isParallel 1
2 Solve: M deltax^k = r 
2 Newton::solveLinearSystem : entering the try section 0
2to  comm_.sum(norm2) 
2did  comm_.sum(norm2) 
2 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
2 solveLinearSystemImpl_, before converged = ls.solve 
2 7 amgbackend::solve isParallel 1
6 Solve: M deltax^k = r 
6 Newton::solveLinearSystem : entering the try section 0
6to  comm_.sum(norm2) 
6did  comm_.sum(norm2) 
6 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
6 solveLinearSystemImpl_, before converged = ls.solve 
6 7 amgbackend::solve isParallel 1
1 Solve: M deltax^k = r 
1 Newton::solveLinearSystem : entering the try section 0
1to  comm_.sum(norm2) 
1did  comm_.sum(norm2) 
1 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
1 solveLinearSystemImpl_, before converged = ls.solve 
1 7 amgbackend::solve isParallel 1
4 Solve: M deltax^k = r 
4 Newton::solveLinearSystem : entering the try section 0
4to  comm_.sum(norm2) 
4did  comm_.sum(norm2) 
4 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
4 solveLinearSystemImpl_, before converged = ls.solve 
4 7 amgbackend::solve isParallel 1
0 Solve: M deltax^k = r 
0 Newton::solveLinearSystem : entering the try section 0
0to  comm_.sum(norm2) 
0did  comm_.sum(norm2) 
0 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
0 solveLinearSystemImpl_, before converged = ls.solve 
0 7 amgbackend::solve isParallel 1
2 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
2 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
6 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
6 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
1 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
1 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
0 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
0 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 7
4 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
4 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
5 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
5 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
3 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
3 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 7
2 7 before Dune::BiCGSTABSolver 
2 Dune::BiCGSTABSolver.apply(x, b, result_) 
2 BiCGSTABSolver::apply 
2 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
6 7 before Dune::BiCGSTABSolver 
6 Dune::BiCGSTABSolver.apply(x, b, result_) 
6 BiCGSTABSolver::apply 
6 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 7
3 7 before Dune::BiCGSTABSolver 
3 Dune::BiCGSTABSolver.apply(x, b, result_) 
3 BiCGSTABSolver::apply 
3 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
4 7 before Dune::BiCGSTABSolver 
4 Dune::BiCGSTABSolver.apply(x, b, result_) 
4 BiCGSTABSolver::apply 
4 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
5 7 before Dune::BiCGSTABSolver 
5 Dune::BiCGSTABSolver.apply(x, b, result_) 
5 BiCGSTABSolver::apply 
5 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
1 7 before Dune::BiCGSTABSolver 
1 Dune::BiCGSTABSolver.apply(x, b, result_) 
1 BiCGSTABSolver::apply 
1 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
Using a direct coarse solver (SuperLU)
Building hierarchy of 1 levels (inclusive coarse solver) took 0.110969 seconds.
0 7 before Dune::BiCGSTABSolver 
0 Dune::BiCGSTABSolver.apply(x, b, result_) 
0 BiCGSTABSolver::apply 
0 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
5 === BiCGSTABSolver
 Iter          Defect            Rate
    0      3.16145e-09
5  (it = 0.5; it < _maxit; it+=.5) 0.5 250
5  before p = r 
5  before  y = W^-1 * p 
5  before _prec->apply(y,p); 0.5 250
5 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
6 === BiCGSTABSolver
 Iter          Defect            Rate
    0      3.16145e-09
6  (it = 0.5; it < _maxit; it+=.5) 0.5 250
6  before p = r 
6  before  y = W^-1 * p 
6  before _prec->apply(y,p); 0.5 250
6 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
2 === BiCGSTABSolver
 Iter          Defect            Rate
    0      3.16145e-09
2  (it = 0.5; it < _maxit; it+=.5) 0.5 250
2  before p = r 
2  before  y = W^-1 * p 
2  before _prec->apply(y,p); 0.5 250
2 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
0 === BiCGSTABSolver
 Iter          Defect            Rate
    0      3.16145e-09
0  (it = 0.5; it < _maxit; it+=.5) 0.5 250
0  before p = r 
0  before  y = W^-1 * p 
0  before _prec->apply(y,p); 0.5 250
0 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 7
0 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
3 === BiCGSTABSolver
 Iter          Defect            Rate
    0      3.16145e-09
3  (it = 0.5; it < _maxit; it+=.5) 0.5 250
3  before p = r 
3  before  y = W^-1 * p 
3  before _prec->apply(y,p); 0.5 250
3 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
1 === BiCGSTABSolver
 Iter          Defect            Rate
    0      3.16145e-09
1  (it = 0.5; it < _maxit; it+=.5) 0.5 250
1  before p = r 
1  before  y = W^-1 * p 
1  before _prec->apply(y,p); 0.5 250
1 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
4 === BiCGSTABSolver
 Iter          Defect            Rate
    0      3.16145e-09
4  (it = 0.5; it < _maxit; it+=.5) 0.5 250
4  before p = r 
4  before  y = W^-1 * p 
4  before _prec->apply(y,p); 0.5 250
4 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
6 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
6 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
6  before v = A * y 
6  before  alpha = rho_new / < rt, v > 
4 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
4 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
4  before v = A * y 
4  before  alpha = rho_new / < rt, v > 
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
0 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
3 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
3 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
3  before v = A * y 
1 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
2 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
2 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
5 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
5 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
5  before v = A * y 
1 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
2  before v = A * y 
5  before  alpha = rho_new / < rt, v > 
0 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
3  before  alpha = rho_new / < rt, v > 
0  before v = A * y 
0  before  alpha = rho_new / < rt, v > 
0  before alpha = rho_new / h; 
0  before apply first correction to x 
0  before r = r - alpha*v 
0  before test stop criteria 
0 istlsolver::printOutput 
  0.5      8.55095e-25      2.70475e-16
0 === rate=7.31569e-32, T=0.0182562, TIT=0.0365124, IT=0.5
2  before  alpha = rho_new / < rt, v > 
2  before alpha = rho_new / h; 
2  before apply first correction to x 
2  before r = r - alpha*v 
2  before test stop criteria 
2 istlsolver::printOutput 
  0.5      8.55095e-25      2.70475e-16
2 === rate=7.31569e-32, T=0.12145, TIT=0.242899, IT=0.5
2 finished solveLinearSystemImpl_ 1
2 converged value: 1 7 1 let s go get convergedRemote
2 to  comm_.min(converged) 
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
1  before v = A * y 
1  before  alpha = rho_new / < rt, v > 
1  before alpha = rho_new / h; 
1  before apply first correction to x 
1  before r = r - alpha*v 
1  before test stop criteria 
1 istlsolver::printOutput 
  0.5      8.55095e-25      2.70475e-16
1 === rate=7.31569e-32, T=0.121533, TIT=0.243067, IT=0.5
1 finished solveLinearSystemImpl_ 1
1 converged value: 1 7 1 let s go get convergedRemote
1 to  comm_.min(converged) 
6  before alpha = rho_new / h; 
6  before apply first correction to x 
6  before r = r - alpha*v 
6  before test stop criteria 
6 istlsolver::printOutput 
  0.5      8.55095e-25      2.70475e-16
6 === rate=7.31569e-32, T=0.12145, TIT=0.2429, IT=0.5
6 finished solveLinearSystemImpl_ 1
6 converged value: 1 7 1 let s go get convergedRemote
6 to  comm_.min(converged) 
3  before alpha = rho_new / h; 
3  before apply first correction to x 
3  before r = r - alpha*v 
3  before test stop criteria 
3 istlsolver::printOutput 
  0.5      8.55095e-25      2.70475e-16
3 === rate=7.31569e-32, T=0.121531, TIT=0.243061, IT=0.5
3 finished solveLinearSystemImpl_ 1
3 converged value: 1 7 1 let s go get convergedRemote
3 to  comm_.min(converged) 
4  before alpha = rho_new / h; 
4  before apply first correction to x 
4  before r = r - alpha*v 
4  before test stop criteria 
4 istlsolver::printOutput 
  0.5      8.55095e-25      2.70475e-16
4 === rate=7.31569e-32, T=0.121451, TIT=0.242902, IT=0.5
4 finished solveLinearSystemImpl_ 1
4 converged value: 1 7 1 let s go get convergedRemote
4 to  comm_.min(converged) 
5  before alpha = rho_new / h; 
5  before apply first correction to x 
5  before r = r - alpha*v 
5  before test stop criteria 
5 istlsolver::printOutput 
  0.5      8.55095e-25      2.70475e-16
5 === rate=7.31569e-32, T=0.121532, TIT=0.243063, IT=0.5
5 finished solveLinearSystemImpl_ 1
5 converged value: 1 7 1 let s go get convergedRemote
5 to  comm_.min(converged) 
2 did  comm_.min(converged) convergedRemote: 1
2 final convergedRemote: 1
2 Update: x^(k+1) = x^k - deltax^k 
2 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
2 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
2 Newton iteration  1 done2 , maximum relative shift = 8.334e-02
2 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
1 did  comm_.min(converged) convergedRemote: 1
1 final convergedRemote: 1
1 Update: x^(k+1) = x^k - deltax^k 
1 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
1 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
1 Newton iteration  1 done1 , maximum relative shift = 8.334e-02
1 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
5 did  comm_.min(converged) convergedRemote: 1
5 final convergedRemote: 1
5 Update: x^(k+1) = x^k - deltax^k 
5 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
5 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
5 Newton iteration  1 done5 , maximum relative shift = 8.334e-02
5 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
3 did  comm_.min(converged) convergedRemote: 1
3 final convergedRemote: 1
3 Update: x^(k+1) = x^k - deltax^k 
3 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
3 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
3 Newton iteration  1 done3 , maximum relative shift = 8.334e-02
3 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
4 did  comm_.min(converged) convergedRemote: 1
4 final convergedRemote: 1
4 Update: x^(k+1) = x^k - deltax^k 
4 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
4 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
4 Newton iteration  1 done4 , maximum relative shift = 8.334e-02
4 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
6 did  comm_.min(converged) convergedRemote: 1
6 final convergedRemote: 1
6 Update: x^(k+1) = x^k - deltax^k 
6 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
6 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
6 Newton iteration  1 done6 , maximum relative shift = 8.334e-02
6 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
0 finished solveLinearSystemImpl_ 1
0 converged value: 1 7 1 let s go get convergedRemote
0 to  comm_.min(converged) 
0 did  comm_.min(converged) convergedRemote: 1
0 final convergedRemote: 1
0 Update: x^(k+1) = x^k - deltax^k 
0 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
0 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
0 Newton iteration  1 done0 , maximum relative shift = 8.334e-02
0 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
5 Solve: M deltax^k = r 
5 Newton::solveLinearSystem : entering the try section 1
5 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
5 solveLinearSystemImpl_, before converged = ls.solve 
5 7 amgbackend::solve isParallel 1
3 Solve: M deltax^k = r 
3 Newton::solveLinearSystem : entering the try section 1
3 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
3 solveLinearSystemImpl_, before converged = ls.solve 
3 7 amgbackend::solve isParallel 1
4 Solve: M deltax^k = r 
4 Newton::solveLinearSystem : entering the try section 1
4 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
4 solveLinearSystemImpl_, before converged = ls.solve 
4 7 amgbackend::solve isParallel 1
6 Solve: M deltax^k = r 
6 Newton::solveLinearSystem : entering the try section 1
6 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
6 solveLinearSystemImpl_, before converged = ls.solve 
6 7 amgbackend::solve isParallel 1
1 Solve: M deltax^k = r 
1 Newton::solveLinearSystem : entering the try section 1
1 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
1 solveLinearSystemImpl_, before converged = ls.solve 
1 7 amgbackend::solve isParallel 1
2 Solve: M deltax^k = r 
2 Newton::solveLinearSystem : entering the try section 1
2 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
2 solveLinearSystemImpl_, before converged = ls.solve 
2 7 amgbackend::solve isParallel 1
0 Solve: M deltax^k = r 
0 Newton::solveLinearSystem : entering the try section 1
0 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
0 solveLinearSystemImpl_, before converged = ls.solve 
0 7 amgbackend::solve isParallel 1
6 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
6 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
1 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
1 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
0 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
0 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 7
2 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
2 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
5 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
5 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
3 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
3 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
4 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
4 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 7
2 7 before Dune::BiCGSTABSolver 
2 Dune::BiCGSTABSolver.apply(x, b, result_) 
2 BiCGSTABSolver::apply 
2 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
5 7 before Dune::BiCGSTABSolver 
5 Dune::BiCGSTABSolver.apply(x, b, result_) 
5 BiCGSTABSolver::apply 
5 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
3 7 before Dune::BiCGSTABSolver 
3 Dune::BiCGSTABSolver.apply(x, b, result_) 
3 BiCGSTABSolver::apply 
3 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
1 7 before Dune::BiCGSTABSolver 
1 Dune::BiCGSTABSolver.apply(x, b, result_) 
1 BiCGSTABSolver::apply 
1 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
6 7 before Dune::BiCGSTABSolver 
6 Dune::BiCGSTABSolver.apply(x, b, result_) 
6 BiCGSTABSolver::apply 
6 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 7
4 7 before Dune::BiCGSTABSolver 
4 Dune::BiCGSTABSolver.apply(x, b, result_) 
4 BiCGSTABSolver::apply 
4 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
Using a direct coarse solver (SuperLU)
Building hierarchy of 1 levels (inclusive coarse solver) took 0.110694 seconds.
0 7 before Dune::BiCGSTABSolver 
0 Dune::BiCGSTABSolver.apply(x, b, result_) 
0 BiCGSTABSolver::apply 
0 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
3 === BiCGSTABSolver
 Iter          Defect            Rate
    0      2.68686e-11
3  (it = 0.5; it < _maxit; it+=.5) 0.5 250
3  before p = r 
3  before  y = W^-1 * p 
3  before _prec->apply(y,p); 0.5 250
3 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
1 === BiCGSTABSolver
 Iter          Defect            Rate
    0      2.68686e-11
1  (it = 0.5; it < _maxit; it+=.5) 0.5 250
1  before p = r 
1  before  y = W^-1 * p 
1  before _prec->apply(y,p); 0.5 250
1 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
0 === BiCGSTABSolver
 Iter          Defect            Rate
    0      2.68686e-11
0  (it = 0.5; it < _maxit; it+=.5) 0.5 250
0  before p = r 
0  before  y = W^-1 * p 
0  before _prec->apply(y,p); 0.5 250
0 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 7
0 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
4 === BiCGSTABSolver
 Iter          Defect            Rate
    0      2.68686e-11
4  (it = 0.5; it < _maxit; it+=.5) 0.5 250
4  before p = r 
4  before  y = W^-1 * p 
4  before _prec->apply(y,p); 0.5 250
4 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
6 === BiCGSTABSolver
 Iter          Defect            Rate
    0      2.68686e-11
6  (it = 0.5; it < _maxit; it+=.5) 0.5 250
6  before p = r 
6  before  y = W^-1 * p 
6  before _prec->apply(y,p); 0.5 250
6 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
2 === BiCGSTABSolver
 Iter          Defect            Rate
    0      2.68686e-11
2  (it = 0.5; it < _maxit; it+=.5) 0.5 250
2  before p = r 
2  before  y = W^-1 * p 
2  before _prec->apply(y,p); 0.5 250
2 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
5 === BiCGSTABSolver
 Iter          Defect            Rate
    0      2.68686e-11
5  (it = 0.5; it < _maxit; it+=.5) 0.5 250
5  before p = r 
5  before  y = W^-1 * p 
5  before _prec->apply(y,p); 0.5 250
5 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
6 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
6 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
6  before v = A * y 
6  before  alpha = rho_new / < rt, v > 
2 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
2 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
2  before v = A * y 
2  before  alpha = rho_new / < rt, v > 
1 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
1 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
1  before v = A * y 
3 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
3 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
3  before v = A * y 
3  before  alpha = rho_new / < rt, v > 
5 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
5 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
5  before v = A * y 
5  before  alpha = rho_new / < rt, v > 
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
0 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
0 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
0  before v = A * y 
4 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
4 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
4  before v = A * y 
4  before  alpha = rho_new / < rt, v > 
4  before alpha = rho_new / h; 
4  before apply first correction to x 
4  before r = r - alpha*v 
4  before test stop criteria 
0  before  alpha = rho_new / < rt, v > 
0  before alpha = rho_new / h; 
0  before apply first correction to x 
0  before r = r - alpha*v 
0  before test stop criteria 
0 istlsolver::printOutput 
1  before  alpha = rho_new / < rt, v > 
1  before alpha = rho_new / h; 
1  before apply first correction to x 
1  before r = r - alpha*v 
1  before test stop criteria 
5  before alpha = rho_new / h; 
5  before apply first correction to x 
5  before r = r - alpha*v 
5  before test stop criteria 
5 istlsolver::printOutput 
  0.5      8.62043e-28      3.20837e-17
5 === rate=1.02936e-33, T=0.124902, TIT=0.249803, IT=0.5
5 finished solveLinearSystemImpl_ 1
5 converged value: 1 7 1 let s go get convergedRemote
5 to  comm_.min(converged) 
4 istlsolver::printOutput 
  0.5      8.62043e-28      3.20837e-17
4 === rate=1.02936e-33, T=0.124755, TIT=0.249509, IT=0.5
4 finished solveLinearSystemImpl_ 1
4 converged value: 1 7 1 let s go get convergedRemote
4 to  comm_.min(converged) 
3  before alpha = rho_new / h; 
3  before apply first correction to x 
3  before r = r - alpha*v 
3  before test stop criteria 
3 istlsolver::printOutput 
  0.5      8.62043e-28      3.20837e-17
3 === rate=1.02936e-33, T=0.124903, TIT=0.249805, IT=0.5
3 finished solveLinearSystemImpl_ 1
3 converged value: 1 7 1 let s go get convergedRemote
3 to  comm_.min(converged) 
6  before alpha = rho_new / h; 
6  before apply first correction to x 
6  before r = r - alpha*v 
6  before test stop criteria 
6 istlsolver::printOutput 
  0.5      8.62043e-28      3.20837e-17
6 === rate=1.02936e-33, T=0.124754, TIT=0.249508, IT=0.5
6 finished solveLinearSystemImpl_ 1
6 converged value: 1 7 1 let s go get convergedRemote
6 to  comm_.min(converged) 
2  before alpha = rho_new / h; 
2  before apply first correction to x 
2  before r = r - alpha*v 
2  before test stop criteria 
2 istlsolver::printOutput 
  0.5      8.62043e-28      3.20837e-17
2 === rate=1.02936e-33, T=0.124755, TIT=0.24951, IT=0.5
2 finished solveLinearSystemImpl_ 1
2 converged value: 1 7 1 let s go get convergedRemote
2 to  comm_.min(converged) 
  0.5      8.62043e-28      3.20837e-17
0 === rate=1.02936e-33, T=0.0216765, TIT=0.0433529, IT=0.5
1 istlsolver::printOutput 
  0.5      8.62043e-28      3.20837e-17
1 === rate=1.02936e-33, T=0.124902, TIT=0.249804, IT=0.5
1 finished solveLinearSystemImpl_ 1
1 converged value: 1 7 1 let s go get convergedRemote
1 to  comm_.min(converged) 
5 did  comm_.min(converged) convergedRemote: 1
5 final convergedRemote: 1
5 Update: x^(k+1) = x^k - deltax^k 
5 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
5 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
5 Newton iteration  2 done5 , maximum relative shift = 5.919e-03
5 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
4 did  comm_.min(converged) convergedRemote: 1
4 final convergedRemote: 1
4 Update: x^(k+1) = x^k - deltax^k 
4 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
4 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
4 Newton iteration  2 done4 , maximum relative shift = 5.919e-03
4 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
3 did  comm_.min(converged) convergedRemote: 1
3 final convergedRemote: 1
3 Update: x^(k+1) = x^k - deltax^k 
3 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
3 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
3 Newton iteration  2 done3 , maximum relative shift = 5.919e-03
3 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
6 did  comm_.min(converged) convergedRemote: 1
6 final convergedRemote: 1
6 Update: x^(k+1) = x^k - deltax^k 
6 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
6 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
6 Newton iteration  2 done6 , maximum relative shift = 5.919e-03
6 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
2 did  comm_.min(converged) convergedRemote: 1
2 final convergedRemote: 1
2 Update: x^(k+1) = x^k - deltax^k 
2 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
2 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
2 Newton iteration  2 done2 , maximum relative shift = 5.919e-03
2 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
1 did  comm_.min(converged) convergedRemote: 1
1 final convergedRemote: 1
1 Update: x^(k+1) = x^k - deltax^k 
1 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
1 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
1 Newton iteration  2 done1 , maximum relative shift = 5.919e-03
1 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
0 finished solveLinearSystemImpl_ 1
0 converged value: 1 7 1 let s go get convergedRemote
0 to  comm_.min(converged) 
0 did  comm_.min(converged) convergedRemote: 1
0 final convergedRemote: 1
0 Update: x^(k+1) = x^k - deltax^k 
0 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
0 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
0 Newton iteration  2 done0 , maximum relative shift = 5.919e-03
0 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
0 Solve: M deltax^k = r 
0 Newton::solveLinearSystem : entering the try section 2
0 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
0 solveLinearSystemImpl_, before converged = ls.solve 
0 7 amgbackend::solve isParallel 1
1 Solve: M deltax^k = r 
1 Newton::solveLinearSystem : entering the try section 2
1 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
1 solveLinearSystemImpl_, before converged = ls.solve 
1 7 amgbackend::solve isParallel 1
2 Solve: M deltax^k = r 
2 Newton::solveLinearSystem : entering the try section 2
2 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
2 solveLinearSystemImpl_, before converged = ls.solve 
2 7 amgbackend::solve isParallel 1
5 Solve: M deltax^k = r 
5 Newton::solveLinearSystem : entering the try section 2
5 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
5 solveLinearSystemImpl_, before converged = ls.solve 
5 7 amgbackend::solve isParallel 1
4 Solve: M deltax^k = r 
4 Newton::solveLinearSystem : entering the try section 2
4 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
4 solveLinearSystemImpl_, before converged = ls.solve 
4 7 amgbackend::solve isParallel 1
3 Solve: M deltax^k = r 
3 Newton::solveLinearSystem : entering the try section 2
3 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
3 solveLinearSystemImpl_, before converged = ls.solve 
3 7 amgbackend::solve isParallel 1
6 Solve: M deltax^k = r 
6 Newton::solveLinearSystem : entering the try section 2
6 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
6 solveLinearSystemImpl_, before converged = ls.solve 
6 7 amgbackend::solve isParallel 1
3 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
3 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
6 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
6 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
0 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
0 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 7
1 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
1 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
2 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
2 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
5 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
5 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
4 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
4 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 7
3 7 before Dune::BiCGSTABSolver 
3 Dune::BiCGSTABSolver.apply(x, b, result_) 
3 BiCGSTABSolver::apply 
3 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 7
6 7 before Dune::BiCGSTABSolver 
6 Dune::BiCGSTABSolver.apply(x, b, result_) 
6 BiCGSTABSolver::apply 
6 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
4 7 before Dune::BiCGSTABSolver 
4 Dune::BiCGSTABSolver.apply(x, b, result_) 
4 BiCGSTABSolver::apply 
4 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
1 7 before Dune::BiCGSTABSolver 
1 Dune::BiCGSTABSolver.apply(x, b, result_) 
1 BiCGSTABSolver::apply 
1 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
2 7 before Dune::BiCGSTABSolver 
2 Dune::BiCGSTABSolver.apply(x, b, result_) 
2 BiCGSTABSolver::apply 
2 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
5 7 before Dune::BiCGSTABSolver 
5 Dune::BiCGSTABSolver.apply(x, b, result_) 
5 BiCGSTABSolver::apply 
5 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
Using a direct coarse solver (SuperLU)
Building hierarchy of 1 levels (inclusive coarse solver) took 0.111148 seconds.
0 7 before Dune::BiCGSTABSolver 
0 Dune::BiCGSTABSolver.apply(x, b, result_) 
0 BiCGSTABSolver::apply 
0 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
1 === BiCGSTABSolver
 Iter          Defect            Rate
    0       3.2959e-13
1  (it = 0.5; it < _maxit; it+=.5) 0.5 250
1  before p = r 
1  before  y = W^-1 * p 
1  before _prec->apply(y,p); 0.5 250
1 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
2 === BiCGSTABSolver
 Iter          Defect            Rate
    0       3.2959e-13
2  (it = 0.5; it < _maxit; it+=.5) 0.5 250
2  before p = r 
2  before  y = W^-1 * p 
2  before _prec->apply(y,p); 0.5 250
2 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
3 === BiCGSTABSolver
 Iter          Defect            Rate
    0       3.2959e-13
3  (it = 0.5; it < _maxit; it+=.5) 0.5 250
3  before p = r 
3  before  y = W^-1 * p 
3  before _prec->apply(y,p); 0.5 250
3 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
0 === BiCGSTABSolver
 Iter          Defect            Rate
    0       3.2959e-13
0  (it = 0.5; it < _maxit; it+=.5) 0.5 250
0  before p = r 
0  before  y = W^-1 * p 
0  before _prec->apply(y,p); 0.5 250
0 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 7
0 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
5 === BiCGSTABSolver
 Iter          Defect            Rate
    0       3.2959e-13
5  (it = 0.5; it < _maxit; it+=.5) 0.5 250
5  before p = r 
5  before  y = W^-1 * p 
5  before _prec->apply(y,p); 0.5 250
5 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
6 === BiCGSTABSolver
 Iter          Defect            Rate
    0       3.2959e-13
6  (it = 0.5; it < _maxit; it+=.5) 0.5 250
6  before p = r 
6  before  y = W^-1 * p 
6  before _prec->apply(y,p); 0.5 250
6 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
4 === BiCGSTABSolver
 Iter          Defect            Rate
    0       3.2959e-13
4  (it = 0.5; it < _maxit; it+=.5) 0.5 250
4  before p = r 
4  before  y = W^-1 * p 
4  before _prec->apply(y,p); 0.5 250
4 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
6 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
6 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
6  before v = A * y 
6  before  alpha = rho_new / < rt, v > 
3 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
3 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
3  before v = A * y 
1 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
2 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
2 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
4 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
4 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
4  before v = A * y 
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
0 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
5 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
5 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
5  before v = A * y 
3  before  alpha = rho_new / < rt, v > 
1 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
5  before  alpha = rho_new / < rt, v > 
4  before  alpha = rho_new / < rt, v > 
0 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
0  before v = A * y 
2  before v = A * y 
2  before  alpha = rho_new / < rt, v > 
3  before alpha = rho_new / h; 
3  before apply first correction to x 
3  before r = r - alpha*v 
3  before test stop criteria 
3 istlsolver::printOutput 
  0.5      5.04954e-29      1.53206e-16
3 === rate=2.34722e-32, T=0.121557, TIT=0.243114, IT=0.5
3 finished solveLinearSystemImpl_ 1
3 converged value: 1 7 1 let s go get convergedRemote
3 to  comm_.min(converged) 
2  before alpha = rho_new / h; 
2  before apply first correction to x 
2  before r = r - alpha*v 
2  before test stop criteria 
2 istlsolver::printOutput 
  0.5      5.04954e-29      1.53206e-16
2 === rate=2.34722e-32, T=0.121532, TIT=0.243065, IT=0.5
2 finished solveLinearSystemImpl_ 1
2 converged value: 1 7 1 let s go get convergedRemote
2 to  comm_.min(converged) 
5  before alpha = rho_new / h; 
5  before apply first correction to x 
5  before r = r - alpha*v 
5  before test stop criteria 
5 istlsolver::printOutput 
  0.5      5.04954e-29      1.53206e-16
5 === rate=2.34722e-32, T=0.121556, TIT=0.243112, IT=0.5
5 finished solveLinearSystemImpl_ 1
5 converged value: 1 7 1 let s go get convergedRemote
5 to  comm_.min(converged) 
4  before alpha = rho_new / h; 
4  before apply first correction to x 
4  before r = r - alpha*v 
4  before test stop criteria 
4 istlsolver::printOutput 
  0.5      5.04954e-29      1.53206e-16
4 === rate=2.34722e-32, T=0.121533, TIT=0.243065, IT=0.5
4 finished solveLinearSystemImpl_ 1
4 converged value: 1 7 1 let s go get convergedRemote
4 to  comm_.min(converged) 
0  before  alpha = rho_new / < rt, v > 
0  before alpha = rho_new / h; 
0  before apply first correction to x 
0  before r = r - alpha*v 
0  before test stop criteria 
0 istlsolver::printOutput 
  0.5      5.04954e-29      1.53206e-16
0 === rate=2.34722e-32, T=0.0179342, TIT=0.0358683, IT=0.5
6  before alpha = rho_new / h; 
6  before apply first correction to x 
6  before r = r - alpha*v 
6  before test stop criteria 
6 istlsolver::printOutput 
  0.5      5.04954e-29      1.53206e-16
6 === rate=2.34722e-32, T=0.121532, TIT=0.243065, IT=0.5
6 finished solveLinearSystemImpl_ 1
6 converged value: 1 7 1 let s go get convergedRemote
6 to  comm_.min(converged) 
1  before v = A * y 
1  before  alpha = rho_new / < rt, v > 
1  before alpha = rho_new / h; 
1  before apply first correction to x 
1  before r = r - alpha*v 
1  before test stop criteria 
1 istlsolver::printOutput 
  0.5      5.04954e-29      1.53206e-16
1 === rate=2.34722e-32, T=0.121461, TIT=0.242923, IT=0.5
1 finished solveLinearSystemImpl_ 1
1 converged value: 1 7 1 let s go get convergedRemote
1 to  comm_.min(converged) 
1 did  comm_.min(converged) convergedRemote: 1
1 final convergedRemote: 1
1 Update: x^(k+1) = x^k - deltax^k 
1 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
1 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
1 Newton iteration  3 done1 , maximum relative shift = 6.234e-05
1 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
2 did  comm_.min(converged) convergedRemote: 1
2 final convergedRemote: 1
2 Update: x^(k+1) = x^k - deltax^k 
2 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
2 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
2 Newton iteration  3 done2 , maximum relative shift = 6.234e-05
2 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
5 did  comm_.min(converged) convergedRemote: 1
5 final convergedRemote: 1
5 Update: x^(k+1) = x^k - deltax^k 
5 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
5 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
5 Newton iteration  3 done5 , maximum relative shift = 6.234e-05
5 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
4 did  comm_.min(converged) convergedRemote: 1
4 final convergedRemote: 1
4 Update: x^(k+1) = x^k - deltax^k 
4 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
4 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
4 Newton iteration  3 done4 , maximum relative shift = 6.234e-05
4 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
0 finished solveLinearSystemImpl_ 1
0 converged value: 1 7 1 let s go get convergedRemote
0 to  comm_.min(converged) 
0 did  comm_.min(converged) convergedRemote: 1
0 final convergedRemote: 1
0 Update: x^(k+1) = x^k - deltax^k 
0 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
0 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
0 Newton iteration  3 done0 , maximum relative shift = 6.234e-05
0 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
6 did  comm_.min(converged) convergedRemote: 1
6 final convergedRemote: 1
6 Update: x^(k+1) = x^k - deltax^k 
6 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
6 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
6 Newton iteration  3 done6 , maximum relative shift = 6.234e-05
6 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
3 did  comm_.min(converged) convergedRemote: 1
3 final convergedRemote: 1
3 Update: x^(k+1) = x^k - deltax^k 
3 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
3 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
3 Newton iteration  3 done3 , maximum relative shift = 6.234e-05
3 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
1 Solve: M deltax^k = r 
1 Newton::solveLinearSystem : entering the try section 3
1 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
1 solveLinearSystemImpl_, before converged = ls.solve 
1 7 amgbackend::solve isParallel 1
2 Solve: M deltax^k = r 
2 Newton::solveLinearSystem : entering the try section 3
2 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
2 solveLinearSystemImpl_, before converged = ls.solve 
2 7 amgbackend::solve isParallel 1
5 Solve: M deltax^k = r 
5 Newton::solveLinearSystem : entering the try section 3
5 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
5 solveLinearSystemImpl_, before converged = ls.solve 
5 7 amgbackend::solve isParallel 1
4 Solve: M deltax^k = r 
4 Newton::solveLinearSystem : entering the try section 3
4 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
4 solveLinearSystemImpl_, before converged = ls.solve 
4 7 amgbackend::solve isParallel 1
6 Solve: M deltax^k = r 
6 Newton::solveLinearSystem : entering the try section 3
6 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
6 solveLinearSystemImpl_, before converged = ls.solve 
6 7 amgbackend::solve isParallel 1
3 Solve: M deltax^k = r 
3 Newton::solveLinearSystem : entering the try section 3
3 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
3 solveLinearSystemImpl_, before converged = ls.solve 
3 7 amgbackend::solve isParallel 1
4 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
4 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
3 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
3 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
6 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
6 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
1 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
1 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
2 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
2 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
5 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
5 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
0 Solve: M deltax^k = r 
0 Newton::solveLinearSystem : entering the try section 3
0 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
0 solveLinearSystemImpl_, before converged = ls.solve 
0 7 amgbackend::solve isParallel 1
0 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
0 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 7
6 7 before Dune::BiCGSTABSolver 
6 Dune::BiCGSTABSolver.apply(x, b, result_) 
6 BiCGSTABSolver::apply 
6 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
1 7 before Dune::BiCGSTABSolver 
1 Dune::BiCGSTABSolver.apply(x, b, result_) 
1 BiCGSTABSolver::apply 
1 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
2 7 before Dune::BiCGSTABSolver 
2 Dune::BiCGSTABSolver.apply(x, b, result_) 
2 BiCGSTABSolver::apply 
2 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
5 7 before Dune::BiCGSTABSolver 
5 Dune::BiCGSTABSolver.apply(x, b, result_) 
5 BiCGSTABSolver::apply 
5 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
4 7 before Dune::BiCGSTABSolver 
4 Dune::BiCGSTABSolver.apply(x, b, result_) 
4 BiCGSTABSolver::apply 
4 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 7
3 7 before Dune::BiCGSTABSolver 
3 Dune::BiCGSTABSolver.apply(x, b, result_) 
3 BiCGSTABSolver::apply 
3 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
Using a direct coarse solver (SuperLU)
Building hierarchy of 1 levels (inclusive coarse solver) took 0.109901 seconds.
0 7 before Dune::BiCGSTABSolver 
0 Dune::BiCGSTABSolver.apply(x, b, result_) 
0 BiCGSTABSolver::apply 
0 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
1 === BiCGSTABSolver
 Iter          Defect            Rate
    0      4.46325e-15
1  (it = 0.5; it < _maxit; it+=.5) 0.5 250
1  before p = r 
1  before  y = W^-1 * p 
1  before _prec->apply(y,p); 0.5 250
1 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
2 === BiCGSTABSolver
 Iter          Defect            Rate
    0      4.46325e-15
2  (it = 0.5; it < _maxit; it+=.5) 0.5 250
2  before p = r 
2  before  y = W^-1 * p 
2  before _prec->apply(y,p); 0.5 250
2 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
5 === BiCGSTABSolver
 Iter          Defect            Rate
    0      4.46325e-15
5  (it = 0.5; it < _maxit; it+=.5) 0.5 250
5  before p = r 
5  before  y = W^-1 * p 
5  before _prec->apply(y,p); 0.5 250
5 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
0 === BiCGSTABSolver
 Iter          Defect            Rate
    0      4.46325e-15
0  (it = 0.5; it < _maxit; it+=.5) 0.5 250
0  before p = r 
0  before  y = W^-1 * p 
0  before _prec->apply(y,p); 0.5 250
0 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 7
0 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
3 === BiCGSTABSolver
 Iter          Defect            Rate
    0      4.46325e-15
3  (it = 0.5; it < _maxit; it+=.5) 0.5 250
3  before p = r 
3  before  y = W^-1 * p 
3  before _prec->apply(y,p); 0.5 250
3 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
4 === BiCGSTABSolver
 Iter          Defect            Rate
    0      4.46325e-15
4  (it = 0.5; it < _maxit; it+=.5) 0.5 250
4  before p = r 
4  before  y = W^-1 * p 
4  before _prec->apply(y,p); 0.5 250
4 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
6 === BiCGSTABSolver
 Iter          Defect            Rate
    0      4.46325e-15
6  (it = 0.5; it < _maxit; it+=.5) 0.5 250
6  before p = r 
6  before  y = W^-1 * p 
6  before _prec->apply(y,p); 0.5 250
6 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
6 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
6 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
6  before v = A * y 
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
0 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
3 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
3 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
3  before v = A * y 
4 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
4 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
4  before v = A * y 
5 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
5 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
5  before v = A * y 
1 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
2 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
2 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
6  before  alpha = rho_new / < rt, v > 
0 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
0  before v = A * y 
3  before  alpha = rho_new / < rt, v > 
4  before  alpha = rho_new / < rt, v > 
5  before  alpha = rho_new / < rt, v > 
1 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
2  before v = A * y 
3  before alpha = rho_new / h; 
3  before apply first correction to x 
3  before r = r - alpha*v 
3  before test stop criteria 
3 istlsolver::printOutput 
  0.5      1.87738e-33      4.20631e-19
3 === rate=1.76931e-37, T=0.12085, TIT=0.241701, IT=0.5
3 finished solveLinearSystemImpl_ 1
3 converged value: 1 7 1 let s go get convergedRemote
3 to  comm_.min(converged) 
4  before alpha = rho_new / h; 
4  before apply first correction to x 
4  before r = r - alpha*v 
4  before test stop criteria 
4 istlsolver::printOutput 
  0.5      1.87738e-33      4.20631e-19
4 === rate=1.76931e-37, T=0.120791, TIT=0.241581, IT=0.5
4 finished solveLinearSystemImpl_ 1
4 converged value: 1 7 1 let s go get convergedRemote
4 to  comm_.min(converged) 
5  before alpha = rho_new / h; 
5  before apply first correction to x 
5  before r = r - alpha*v 
5  before test stop criteria 
5 istlsolver::printOutput 
  0.5      1.87738e-33      4.20631e-19
5 === rate=1.76931e-37, T=0.120851, TIT=0.241701, IT=0.5
5 finished solveLinearSystemImpl_ 1
5 converged value: 1 7 1 let s go get convergedRemote
5 to  comm_.min(converged) 
2  before  alpha = rho_new / < rt, v > 
2  before alpha = rho_new / h; 
2  before apply first correction to x 
2  before r = r - alpha*v 
2  before test stop criteria 
2 istlsolver::printOutput 
  0.5      1.87738e-33      4.20631e-19
2 === rate=1.76931e-37, T=0.120786, TIT=0.241572, IT=0.5
2 finished solveLinearSystemImpl_ 1
2 converged value: 1 7 1 let s go get convergedRemote
2 to  comm_.min(converged) 
1  before v = A * y 
1  before  alpha = rho_new / < rt, v > 
1  before alpha = rho_new / h; 
1  before apply first correction to x 
1  before r = r - alpha*v 
1  before test stop criteria 
1 istlsolver::printOutput 
  0.5      1.87738e-33      4.20631e-19
1 === rate=1.76931e-37, T=0.12085, TIT=0.2417, IT=0.5
1 finished solveLinearSystemImpl_ 1
1 converged value: 1 7 1 let s go get convergedRemote
1 to  comm_.min(converged) 
6  before alpha = rho_new / h; 
6  before apply first correction to x 
6  before r = r - alpha*v 
6  before test stop criteria 
6 istlsolver::printOutput 
  0.5      1.87738e-33      4.20631e-19
6 === rate=1.76931e-37, T=0.120791, TIT=0.241582, IT=0.5
6 finished solveLinearSystemImpl_ 1
6 converged value: 1 7 1 let s go get convergedRemote
6 to  comm_.min(converged) 
0  before  alpha = rho_new / < rt, v > 
0  before alpha = rho_new / h; 
0  before apply first correction to x 
0  before r = r - alpha*v 
0  before test stop criteria 
0 istlsolver::printOutput 
  0.5      1.87738e-33      4.20631e-19
0 === rate=1.76931e-37, T=0.0185032, TIT=0.0370064, IT=0.5
5 did  comm_.min(converged) convergedRemote: 1
5 final convergedRemote: 1
5 Update: x^(k+1) = x^k - deltax^k 
5 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
5 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
5 Newton iteration  4 done5 , maximum relative shift = 3.816e-07
5 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
3 did  comm_.min(converged) convergedRemote: 1
3 final convergedRemote: 1
3 Update: x^(k+1) = x^k - deltax^k 
3 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
3 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
3 Newton iteration  4 done3 , maximum relative shift = 3.816e-07
3 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
4 did  comm_.min(converged) convergedRemote: 1
4 final convergedRemote: 1
4 Update: x^(k+1) = x^k - deltax^k 
4 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
4 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
4 Newton iteration  4 done4 , maximum relative shift = 3.816e-07
4 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
0 finished solveLinearSystemImpl_ 1
0 converged value: 1 7 1 let s go get convergedRemote
0 to  comm_.min(converged) 
0 did  comm_.min(converged) convergedRemote: 1
0 final convergedRemote: 1
0 Update: x^(k+1) = x^k - deltax^k 
0 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
0 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
0 Newton iteration  4 done0 , maximum relative shift = 3.816e-07
0 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
2 did  comm_.min(converged) convergedRemote: 1
2 final convergedRemote: 1
2 Update: x^(k+1) = x^k - deltax^k 
2 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
2 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
2 Newton iteration  4 done2 , maximum relative shift = 3.816e-07
2 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
1 did  comm_.min(converged) convergedRemote: 1
1 final convergedRemote: 1
1 Update: x^(k+1) = x^k - deltax^k 
1 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
1 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
1 Newton iteration  4 done1 , maximum relative shift = 3.816e-07
1 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
6 did  comm_.min(converged) convergedRemote: 1
6 final convergedRemote: 1
6 Update: x^(k+1) = x^k - deltax^k 
6 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
6 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
6 Newton iteration  4 done6 , maximum relative shift = 3.816e-07
6 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
1 Solve: M deltax^k = r 
1 Newton::solveLinearSystem : entering the try section 4
1 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
1 solveLinearSystemImpl_, before converged = ls.solve 
1 7 amgbackend::solve isParallel 1
4 Solve: M deltax^k = r 
4 Newton::solveLinearSystem : entering the try section 4
4 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
4 solveLinearSystemImpl_, before converged = ls.solve 
4 7 amgbackend::solve isParallel 1
5 Solve: M deltax^k = r 
5 Newton::solveLinearSystem : entering the try section 4
5 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
5 solveLinearSystemImpl_, before converged = ls.solve 
5 7 amgbackend::solve isParallel 1
3 Solve: M deltax^k = r 
3 Newton::solveLinearSystem : entering the try section 4
3 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
3 solveLinearSystemImpl_, before converged = ls.solve 
3 7 amgbackend::solve isParallel 1
6 Solve: M deltax^k = r 
6 Newton::solveLinearSystem : entering the try section 4
6 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
6 solveLinearSystemImpl_, before converged = ls.solve 
6 7 amgbackend::solve isParallel 1
0 Solve: M deltax^k = r 
0 Newton::solveLinearSystem : entering the try section 4
0 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
0 solveLinearSystemImpl_, before converged = ls.solve 
0 7 amgbackend::solve isParallel 1
2 Solve: M deltax^k = r 
2 Newton::solveLinearSystem : entering the try section 4
2 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
2 solveLinearSystemImpl_, before converged = ls.solve 
2 7 amgbackend::solve isParallel 1
4 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
4 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
5 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
5 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
2 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
2 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
6 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
6 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
0 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
0 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 7
3 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
3 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
1 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
1 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 7
2 7 before Dune::BiCGSTABSolver 
2 Dune::BiCGSTABSolver.apply(x, b, result_) 
2 BiCGSTABSolver::apply 
2 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
6 7 before Dune::BiCGSTABSolver 
6 Dune::BiCGSTABSolver.apply(x, b, result_) 
6 BiCGSTABSolver::apply 
6 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
1 7 before Dune::BiCGSTABSolver 
1 Dune::BiCGSTABSolver.apply(x, b, result_) 
1 BiCGSTABSolver::apply 
1 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
3 7 before Dune::BiCGSTABSolver 
3 Dune::BiCGSTABSolver.apply(x, b, result_) 
3 BiCGSTABSolver::apply 
3 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
5 7 before Dune::BiCGSTABSolver 
5 Dune::BiCGSTABSolver.apply(x, b, result_) 
5 BiCGSTABSolver::apply 
5 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
4 7 before Dune::BiCGSTABSolver 
4 Dune::BiCGSTABSolver.apply(x, b, result_) 
4 BiCGSTABSolver::apply 
4 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 7
Using a direct coarse solver (SuperLU)
Building hierarchy of 1 levels (inclusive coarse solver) took 0.111614 seconds.
0 7 before Dune::BiCGSTABSolver 
0 Dune::BiCGSTABSolver.apply(x, b, result_) 
0 BiCGSTABSolver::apply 
0 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
2 === BiCGSTABSolver
 Iter          Defect            Rate
    0      8.43254e-17
2  (it = 0.5; it < _maxit; it+=.5) 0.5 250
2  before p = r 
2  before  y = W^-1 * p 
2  before _prec->apply(y,p); 0.5 250
2 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
6 === BiCGSTABSolver
 Iter          Defect            Rate
    0      8.43254e-17
6  (it = 0.5; it < _maxit; it+=.5) 0.5 250
6  before p = r 
6  before  y = W^-1 * p 
6  before _prec->apply(y,p); 0.5 250
6 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
1 === BiCGSTABSolver
 Iter          Defect            Rate
    0      8.43254e-17
1  (it = 0.5; it < _maxit; it+=.5) 0.5 250
1  before p = r 
1  before  y = W^-1 * p 
1  before _prec->apply(y,p); 0.5 250
1 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
3 === BiCGSTABSolver
 Iter          Defect            Rate
    0      8.43254e-17
3  (it = 0.5; it < _maxit; it+=.5) 0.5 250
3  before p = r 
3  before  y = W^-1 * p 
3  before _prec->apply(y,p); 0.5 250
3 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
5 === BiCGSTABSolver
 Iter          Defect            Rate
    0      8.43254e-17
5  (it = 0.5; it < _maxit; it+=.5) 0.5 250
5  before p = r 
5  before  y = W^-1 * p 
5  before _prec->apply(y,p); 0.5 250
5 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
0 === BiCGSTABSolver
 Iter          Defect            Rate
    0      8.43254e-17
0  (it = 0.5; it < _maxit; it+=.5) 0.5 250
0  before p = r 
0  before  y = W^-1 * p 
0  before _prec->apply(y,p); 0.5 250
0 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 7
0 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
4 === BiCGSTABSolver
 Iter          Defect            Rate
    0      8.43254e-17
4  (it = 0.5; it < _maxit; it+=.5) 0.5 250
4  before p = r 
4  before  y = W^-1 * p 
4  before _prec->apply(y,p); 0.5 250
4 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
5 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
5 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
5  before v = A * y 
2 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
2 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
6 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
6 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
6  before v = A * y 
1 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
3 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
3 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
3  before v = A * y 
4 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
4 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
4  before v = A * y 
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
0 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
5  before  alpha = rho_new / < rt, v > 
2  before v = A * y 
6  before  alpha = rho_new / < rt, v > 
1 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
1  before v = A * y 
0 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
4  before  alpha = rho_new / < rt, v > 
3  before  alpha = rho_new / < rt, v > 
1  before  alpha = rho_new / < rt, v > 
1  before alpha = rho_new / h; 
1  before apply first correction to x 
1  before r = r - alpha*v 
1  before test stop criteria 
1 istlsolver::printOutput 
  0.5      6.62899e-33       7.8612e-17
1 === rate=6.17984e-33, T=0.121418, TIT=0.242835, IT=0.5
1 finished solveLinearSystemImpl_ 1
1 converged value: 1 7 1 let s go get convergedRemote
1 to  comm_.min(converged) 
5  before alpha = rho_new / h; 
5  before apply first correction to x 
5  before r = r - alpha*v 
5  before test stop criteria 
5 istlsolver::printOutput 
  0.5      6.62899e-33       7.8612e-17
5 === rate=6.17984e-33, T=0.121417, TIT=0.242834, IT=0.5
5 finished solveLinearSystemImpl_ 1
5 converged value: 1 7 1 let s go get convergedRemote
5 to  comm_.min(converged) 
2  before  alpha = rho_new / < rt, v > 
2  before alpha = rho_new / h; 
2  before apply first correction to x 
2  before r = r - alpha*v 
2  before test stop criteria 
2 istlsolver::printOutput 
  0.5      6.62899e-33       7.8612e-17
2 === rate=6.17984e-33, T=0.121428, TIT=0.242855, IT=0.5
2 finished solveLinearSystemImpl_ 1
2 converged value: 1 7 1 let s go get convergedRemote
2 to  comm_.min(converged) 
6  before alpha = rho_new / h; 
6  before apply first correction to x 
6  before r = r - alpha*v 
6  before test stop criteria 
6 istlsolver::printOutput 
  0.5      6.62899e-33       7.8612e-17
6 === rate=6.17984e-33, T=0.12143, TIT=0.242859, IT=0.5
6 finished solveLinearSystemImpl_ 1
6 converged value: 1 7 1 let s go get convergedRemote
6 to  comm_.min(converged) 
3  before alpha = rho_new / h; 
3  before apply first correction to x 
3  before r = r - alpha*v 
3  before test stop criteria 
3 istlsolver::printOutput 
  0.5      6.62899e-33       7.8612e-17
3 === rate=6.17984e-33, T=0.121419, TIT=0.242837, IT=0.5
3 finished solveLinearSystemImpl_ 1
3 converged value: 1 7 1 let s go get convergedRemote
3 to  comm_.min(converged) 
0  before v = A * y 
0  before  alpha = rho_new / < rt, v > 
0  before alpha = rho_new / h; 
0  before apply first correction to x 
0  before r = r - alpha*v 
0  before test stop criteria 
0 istlsolver::printOutput 
  0.5      6.62899e-33       7.8612e-17
0 === rate=6.17984e-33, T=0.0189769, TIT=0.0379537, IT=0.5
4  before alpha = rho_new / h; 
4  before apply first correction to x 
4  before r = r - alpha*v 
4  before test stop criteria 
4 istlsolver::printOutput 
  0.5      6.62899e-33       7.8612e-17
4 === rate=6.17984e-33, T=0.121429, TIT=0.242858, IT=0.5
4 finished solveLinearSystemImpl_ 1
4 converged value: 1 7 1 let s go get convergedRemote
4 to  comm_.min(converged) 
3 did  comm_.min(converged) convergedRemote: 1
3 final convergedRemote: 1
3 Update: x^(k+1) = x^k - deltax^k 
3 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
3 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
3 Newton iteration  5 done3 , maximum relative shift = 6.326e-09
3 Assemble/solve/update time: 0.220409(25.1334%)/0.65638(74.8474%)/0.000168162(0.0191756%)
3 gridVariables->advanceTimeStep
3 before, nonLinearSolver->suggestTimeStepSize, current time: 733.767 simTime 57600
3 before assembler->setPreviousSolution, ddt: 28.3333
3 nonLinearSolver->solve
3 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
0 finished solveLinearSystemImpl_ 1
0 converged value: 1 7 1 let s go get convergedRemote
0 to  comm_.min(converged) 
0 did  comm_.min(converged) convergedRemote: 1
0 final convergedRemote: 1
0 Update: x^(k+1) = x^k - deltax^k 
0 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
0 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
0 Newton iteration  5 done0 , maximum relative shift = 6.326e-09
0 Assemble/solve/update time: 0.220978(25.216%)/0.655108(74.755%)/0.000253543(0.028932%)
0 gridVariables->advanceTimeStep
0 before, nonLinearSolver->suggestTimeStepSize, current time: 733.767 simTime 57600
0 before assembler->setPreviousSolution, ddt: 28.3333
0 nonLinearSolver->solve
0 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
6 did  comm_.min(converged) convergedRemote: 1
6 final convergedRemote: 1
6 Update: x^(k+1) = x^k - deltax^k 
6 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
6 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
6 Newton iteration  5 done6 , maximum relative shift = 6.326e-09
6 Assemble/solve/update time: 0.22037(25.1327%)/0.656385(74.8594%)/6.9591e-05(0.00793671%)
6 gridVariables->advanceTimeStep
6 before, nonLinearSolver->suggestTimeStepSize, current time: 733.767 simTime 57600
6 before assembler->setPreviousSolution, ddt: 28.3333
6 nonLinearSolver->solve
6 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
1 did  comm_.min(converged) convergedRemote: 1
1 final convergedRemote: 1
1 Update: x^(k+1) = x^k - deltax^k 
1 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
1 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
1 Newton iteration  5 done1 , maximum relative shift = 6.326e-09
1 Assemble/solve/update time: 0.220431(25.1356%)/0.656365(74.845%)/0.000169512(0.0193294%)
1 gridVariables->advanceTimeStep
1 before, nonLinearSolver->suggestTimeStepSize, current time: 733.767 simTime 57600
1 before assembler->setPreviousSolution, ddt: 28.3333
1 nonLinearSolver->solve
1 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
5 did  comm_.min(converged) convergedRemote: 1
5 final convergedRemote: 1
5 Update: x^(k+1) = x^k - deltax^k 
5 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
5 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
5 Newton iteration  5 done5 , maximum relative shift = 6.326e-09
5 Assemble/solve/update time: 0.22044(25.1365%)/0.656364(74.8447%)/0.000164382(0.0187443%)
5 gridVariables->advanceTimeStep
5 before, nonLinearSolver->suggestTimeStepSize, current time: 733.767 simTime 57600
5 before assembler->setPreviousSolution, ddt: 28.3333
5 nonLinearSolver->solve
5 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
2 did  comm_.min(converged) convergedRemote: 1
2 final convergedRemote: 1
2 Update: x^(k+1) = x^k - deltax^k 
2 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
2 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
2 Newton iteration  5 done2 , maximum relative shift = 6.326e-09
2 Assemble/solve/update time: 0.220218(25.1153%)/0.656382(74.8586%)/0.000229323(0.0261537%)
2 gridVariables->advanceTimeStep
2 before, nonLinearSolver->suggestTimeStepSize, current time: 733.767 simTime 57600
2 before assembler->setPreviousSolution, ddt: 28.3333
2 nonLinearSolver->solve
2 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
4 did  comm_.min(converged) convergedRemote: 1
4 final convergedRemote: 1
4 Update: x^(k+1) = x^k - deltax^k 
4 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
4 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
4 Newton iteration  5 done4 , maximum relative shift = 6.326e-09
4 Assemble/solve/update time: 0.220255(25.1189%)/0.656365(74.8551%)/0.000228143(0.0260185%)
4 gridVariables->advanceTimeStep
4 before, nonLinearSolver->suggestTimeStepSize, current time: 733.767 simTime 57600
4 before assembler->setPreviousSolution, ddt: 28.3333
4 nonLinearSolver->solve
4 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
4 Solve: M deltax^k = r 
4 Newton::solveLinearSystem : entering the try section 0
4to  comm_.sum(norm2) 
4did  comm_.sum(norm2) 
4 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
4 solveLinearSystemImpl_, before converged = ls.solve 
4 7 amgbackend::solve isParallel 1
2 Solve: M deltax^k = r 
2 Newton::solveLinearSystem : entering the try section 0
2to  comm_.sum(norm2) 
2did  comm_.sum(norm2) 
2 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
2 solveLinearSystemImpl_, before converged = ls.solve 
2 7 amgbackend::solve isParallel 1
3 Solve: M deltax^k = r 
3 Newton::solveLinearSystem : entering the try section 0
3to  comm_.sum(norm2) 
3did  comm_.sum(norm2) 
3 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
3 solveLinearSystemImpl_, before converged = ls.solve 
3 7 amgbackend::solve isParallel 1
0 Solve: M deltax^k = r 
0 Newton::solveLinearSystem : entering the try section 0
0to  comm_.sum(norm2) 
0did  comm_.sum(norm2) 
0 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
0 solveLinearSystemImpl_, before converged = ls.solve 
0 7 amgbackend::solve isParallel 1
6 Solve: M deltax^k = r 
6 Newton::solveLinearSystem : entering the try section 0
6to  comm_.sum(norm2) 
6did  comm_.sum(norm2) 
6 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
6 solveLinearSystemImpl_, before converged = ls.solve 
6 7 amgbackend::solve isParallel 1
1 Solve: M deltax^k = r 
1 Newton::solveLinearSystem : entering the try section 0
1to  comm_.sum(norm2) 
1did  comm_.sum(norm2) 
1 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
1 solveLinearSystemImpl_, before converged = ls.solve 
1 7 amgbackend::solve isParallel 1
5 Solve: M deltax^k = r 
5 Newton::solveLinearSystem : entering the try section 0
5to  comm_.sum(norm2) 
5did  comm_.sum(norm2) 
5 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
5 solveLinearSystemImpl_, before converged = ls.solve 
5 7 amgbackend::solve isParallel 1
3 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
3 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
0 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
0 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 7
6 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
6 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
1 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
1 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
5 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
5 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
4 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
4 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
2 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
2 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 7
3 7 before Dune::BiCGSTABSolver 
3 Dune::BiCGSTABSolver.apply(x, b, result_) 
3 BiCGSTABSolver::apply 
3 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
2 7 before Dune::BiCGSTABSolver 
2 Dune::BiCGSTABSolver.apply(x, b, result_) 
2 BiCGSTABSolver::apply 
2 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
6 7 before Dune::BiCGSTABSolver 
6 Dune::BiCGSTABSolver.apply(x, b, result_) 
6 BiCGSTABSolver::apply 
6 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 7
5 7 before Dune::BiCGSTABSolver 
5 Dune::BiCGSTABSolver.apply(x, b, result_) 
5 BiCGSTABSolver::apply 
5 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
4 7 before Dune::BiCGSTABSolver 
4 Dune::BiCGSTABSolver.apply(x, b, result_) 
4 BiCGSTABSolver::apply 
4 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
1 7 before Dune::BiCGSTABSolver 
1 Dune::BiCGSTABSolver.apply(x, b, result_) 
1 BiCGSTABSolver::apply 
1 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
Using a direct coarse solver (SuperLU)
Building hierarchy of 1 levels (inclusive coarse solver) took 0.111965 seconds.
0 7 before Dune::BiCGSTABSolver 
0 Dune::BiCGSTABSolver.apply(x, b, result_) 
0 BiCGSTABSolver::apply 
0 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
0 === BiCGSTABSolver
 Iter          Defect            Rate
    0      3.16128e-09
0  (it = 0.5; it < _maxit; it+=.5) 0.5 250
1 === BiCGSTABSolver
 Iter          Defect            Rate
    0      3.16128e-09
1  (it = 0.5; it < _maxit; it+=.5) 0.5 250
1  before p = r 
1  before  y = W^-1 * p 
1  before _prec->apply(y,p); 0.5 250
1 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
5 === BiCGSTABSolver
 Iter          Defect            Rate
    0      3.16128e-09
5  (it = 0.5; it < _maxit; it+=.5) 0.5 250
5  before p = r 
5  before  y = W^-1 * p 
5  before _prec->apply(y,p); 0.5 250
5 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
4 === BiCGSTABSolver
 Iter          Defect            Rate
    0      3.16128e-09
4  (it = 0.5; it < _maxit; it+=.5) 0.5 250
4  before p = r 
4  before  y = W^-1 * p 
4  before _prec->apply(y,p); 0.5 250
4 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
6 === BiCGSTABSolver
 Iter          Defect            Rate
    0      3.16128e-09
6  (it = 0.5; it < _maxit; it+=.5) 0.5 250
6  before p = r 
6  before  y = W^-1 * p 
6  before _prec->apply(y,p); 0.5 250
6 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
3 === BiCGSTABSolver
 Iter          Defect            Rate
    0      3.16128e-09
3  (it = 0.5; it < _maxit; it+=.5) 0.5 250
3  before p = r 
3  before  y = W^-1 * p 
3  before _prec->apply(y,p); 0.5 250
3 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
2 === BiCGSTABSolver
 Iter          Defect            Rate
    0      3.16128e-09
2  (it = 0.5; it < _maxit; it+=.5) 0.5 250
2  before p = r 
2  before  y = W^-1 * p 
2  before _prec->apply(y,p); 0.5 250
2 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
0  before p = r 
0  before  y = W^-1 * p 
0  before _prec->apply(y,p); 0.5 250
0 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 7
0 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
0 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
2 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
2 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
1 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
5 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
5 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
5  before v = A * y 
4 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
4 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
4  before v = A * y 
6 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
6 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
6  before v = A * y 
3 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
3 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
3  before v = A * y 
0 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
0  before v = A * y 
2  before v = A * y 
1 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
5  before  alpha = rho_new / < rt, v > 
4  before  alpha = rho_new / < rt, v > 
6  before  alpha = rho_new / < rt, v > 
3  before  alpha = rho_new / < rt, v > 
0  before  alpha = rho_new / < rt, v > 
0  before alpha = rho_new / h; 
0  before apply first correction to x 
0  before r = r - alpha*v 
0  before test stop criteria 
0 istlsolver::printOutput 
  0.5      4.46739e-25      1.41316e-16
0 === rate=1.99702e-32, T=0.0185719, TIT=0.0371437, IT=0.5
2  before  alpha = rho_new / < rt, v > 
2  before alpha = rho_new / h; 
2  before apply first correction to x 
2  before r = r - alpha*v 
2  before test stop criteria 
2 istlsolver::printOutput 
  0.5      4.46739e-25      1.41316e-16
2 === rate=1.99702e-32, T=0.122701, TIT=0.245401, IT=0.5
2 finished solveLinearSystemImpl_ 1
2 converged value: 1 7 1 let s go get convergedRemote
2 to  comm_.min(converged) 
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
1  before v = A * y 
1  before  alpha = rho_new / < rt, v > 
1  before alpha = rho_new / h; 
1  before apply first correction to x 
1  before r = r - alpha*v 
1  before test stop criteria 
1 istlsolver::printOutput 
  0.5      4.46739e-25      1.41316e-16
1 === rate=1.99702e-32, T=0.122799, TIT=0.245599, IT=0.5
1 finished solveLinearSystemImpl_ 1
1 converged value: 1 7 1 let s go get convergedRemote
1 to  comm_.min(converged) 
5  before alpha = rho_new / h; 
5  before apply first correction to x 
5  before r = r - alpha*v 
5  before test stop criteria 
5 istlsolver::printOutput 
  0.5      4.46739e-25      1.41316e-16
5 === rate=1.99702e-32, T=0.122898, TIT=0.245796, IT=0.5
5 finished solveLinearSystemImpl_ 1
5 converged value: 1 7 1 let s go get convergedRemote
5 to  comm_.min(converged) 
4  before alpha = rho_new / h; 
4  before apply first correction to x 
4  before r = r - alpha*v 
4  before test stop criteria 
4 istlsolver::printOutput 
  0.5      4.46739e-25      1.41316e-16
4 === rate=1.99702e-32, T=0.122701, TIT=0.245403, IT=0.5
4 finished solveLinearSystemImpl_ 1
4 converged value: 1 7 1 let s go get convergedRemote
4 to  comm_.min(converged) 
3  before alpha = rho_new / h; 
3  before apply first correction to x 
3  before r = r - alpha*v 
3  before test stop criteria 
3 istlsolver::printOutput 
  0.5      4.46739e-25      1.41316e-16
3 === rate=1.99702e-32, T=0.122899, TIT=0.245799, IT=0.5
3 finished solveLinearSystemImpl_ 1
3 converged value: 1 7 1 let s go get convergedRemote
3 to  comm_.min(converged) 
6  before alpha = rho_new / h; 
6  before apply first correction to x 
6  before r = r - alpha*v 
6  before test stop criteria 
6 istlsolver::printOutput 
  0.5      4.46739e-25      1.41316e-16
6 === rate=1.99702e-32, T=0.1227, TIT=0.245399, IT=0.5
6 finished solveLinearSystemImpl_ 1
6 converged value: 1 7 1 let s go get convergedRemote
6 to  comm_.min(converged) 
6 did  comm_.min(converged) convergedRemote: 1
6 final convergedRemote: 1
6 Update: x^(k+1) = x^k - deltax^k 
6 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
6 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
6 Newton iteration  1 done6 , maximum relative shift = 9.699e-02
6 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
3 did  comm_.min(converged) convergedRemote: 1
3 final convergedRemote: 1
3 Update: x^(k+1) = x^k - deltax^k 
3 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
3 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
3 Newton iteration  1 done3 , maximum relative shift = 9.699e-02
3 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
4 did  comm_.min(converged) convergedRemote: 1
4 final convergedRemote: 1
4 Update: x^(k+1) = x^k - deltax^k 
4 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
4 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
4 Newton iteration  1 done4 , maximum relative shift = 9.699e-02
4 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
0 finished solveLinearSystemImpl_ 1
0 converged value: 1 7 1 let s go get convergedRemote
0 to  comm_.min(converged) 
0 did  comm_.min(converged) convergedRemote: 1
0 final convergedRemote: 1
0 Update: x^(k+1) = x^k - deltax^k 
0 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
0 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
0 Newton iteration  1 done0 , maximum relative shift = 9.699e-02
0 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
2 did  comm_.min(converged) convergedRemote: 1
2 final convergedRemote: 1
2 Update: x^(k+1) = x^k - deltax^k 
2 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
2 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
2 Newton iteration  1 done2 , maximum relative shift = 9.699e-02
2 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
1 did  comm_.min(converged) convergedRemote: 1
1 final convergedRemote: 1
1 Update: x^(k+1) = x^k - deltax^k 
1 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
1 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
1 Newton iteration  1 done1 , maximum relative shift = 9.699e-02
1 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
5 did  comm_.min(converged) convergedRemote: 1
5 final convergedRemote: 1
5 Update: x^(k+1) = x^k - deltax^k 
5 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
5 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
5 Newton iteration  1 done5 , maximum relative shift = 9.699e-02
5 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
6 Solve: M deltax^k = r 
6 Newton::solveLinearSystem : entering the try section 1
6 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
6 solveLinearSystemImpl_, before converged = ls.solve 
6 7 amgbackend::solve isParallel 1
3 Solve: M deltax^k = r 
3 Newton::solveLinearSystem : entering the try section 1
3 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
3 solveLinearSystemImpl_, before converged = ls.solve 
3 7 amgbackend::solve isParallel 1
4 Solve: M deltax^k = r 
4 Newton::solveLinearSystem : entering the try section 1
4 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
4 solveLinearSystemImpl_, before converged = ls.solve 
4 7 amgbackend::solve isParallel 1
0 Solve: M deltax^k = r 
0 Newton::solveLinearSystem : entering the try section 1
0 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
0 solveLinearSystemImpl_, before converged = ls.solve 
0 7 amgbackend::solve isParallel 1
2 Solve: M deltax^k = r 
2 Newton::solveLinearSystem : entering the try section 1
2 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
2 solveLinearSystemImpl_, before converged = ls.solve 
2 7 amgbackend::solve isParallel 1
1 Solve: M deltax^k = r 
1 Newton::solveLinearSystem : entering the try section 1
1 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
1 solveLinearSystemImpl_, before converged = ls.solve 
1 7 amgbackend::solve isParallel 1
5 Solve: M deltax^k = r 
5 Newton::solveLinearSystem : entering the try section 1
5 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
5 solveLinearSystemImpl_, before converged = ls.solve 
5 7 amgbackend::solve isParallel 1
1 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
1 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
6 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
6 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
3 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
3 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
4 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
4 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
0 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
0 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 7
2 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
2 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
5 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
5 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 7
2 7 before Dune::BiCGSTABSolver 
2 Dune::BiCGSTABSolver.apply(x, b, result_) 
2 BiCGSTABSolver::apply 
2 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
1 7 before Dune::BiCGSTABSolver 
1 Dune::BiCGSTABSolver.apply(x, b, result_) 
1 BiCGSTABSolver::apply 
1 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
6 7 before Dune::BiCGSTABSolver 
6 Dune::BiCGSTABSolver.apply(x, b, result_) 
6 BiCGSTABSolver::apply 
6 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
3 7 before Dune::BiCGSTABSolver 
3 Dune::BiCGSTABSolver.apply(x, b, result_) 
3 BiCGSTABSolver::apply 
3 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
4 7 before Dune::BiCGSTABSolver 
4 Dune::BiCGSTABSolver.apply(x, b, result_) 
4 BiCGSTABSolver::apply 
4 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
5 7 before Dune::BiCGSTABSolver 
5 Dune::BiCGSTABSolver.apply(x, b, result_) 
5 BiCGSTABSolver::apply 
5 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 7
Using a direct coarse solver (SuperLU)
Building hierarchy of 1 levels (inclusive coarse solver) took 0.112325 seconds.
0 7 before Dune::BiCGSTABSolver 
0 Dune::BiCGSTABSolver.apply(x, b, result_) 
0 BiCGSTABSolver::apply 
0 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
4 === BiCGSTABSolver
 Iter          Defect            Rate
    0      6.75396e-11
4  (it = 0.5; it < _maxit; it+=.5) 0.5 250
4  before p = r 
4  before  y = W^-1 * p 
4  before _prec->apply(y,p); 0.5 250
4 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
0 === BiCGSTABSolver
 Iter          Defect            Rate
    0      6.75396e-11
0  (it = 0.5; it < _maxit; it+=.5) 0.5 250
0  before p = r 
0  before  y = W^-1 * p 
0  before _prec->apply(y,p); 0.5 250
0 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 7
0 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
5 === BiCGSTABSolver
 Iter          Defect            Rate
    0      6.75396e-11
5  (it = 0.5; it < _maxit; it+=.5) 0.5 250
5  before p = r 
5  before  y = W^-1 * p 
5  before _prec->apply(y,p); 0.5 250
5 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
2 === BiCGSTABSolver
 Iter          Defect            Rate
    0      6.75396e-11
2  (it = 0.5; it < _maxit; it+=.5) 0.5 250
2  before p = r 
2  before  y = W^-1 * p 
2  before _prec->apply(y,p); 0.5 250
2 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
1 === BiCGSTABSolver
 Iter          Defect            Rate
    0      6.75396e-11
1  (it = 0.5; it < _maxit; it+=.5) 0.5 250
1  before p = r 
1  before  y = W^-1 * p 
1  before _prec->apply(y,p); 0.5 250
1 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
6 === BiCGSTABSolver
 Iter          Defect            Rate
    0      6.75396e-11
6  (it = 0.5; it < _maxit; it+=.5) 0.5 250
6  before p = r 
6  before  y = W^-1 * p 
6  before _prec->apply(y,p); 0.5 250
6 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
3 === BiCGSTABSolver
 Iter          Defect            Rate
    0      6.75396e-11
3  (it = 0.5; it < _maxit; it+=.5) 0.5 250
3  before p = r 
3  before  y = W^-1 * p 
3  before _prec->apply(y,p); 0.5 250
3 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
4 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
4 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
4  before v = A * y 
4  before  alpha = rho_new / < rt, v > 
3 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
3 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
3  before v = A * y 
3  before  alpha = rho_new / < rt, v > 
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
0 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
0 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
5 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
5 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
5  before v = A * y 
5  before  alpha = rho_new / < rt, v > 
2 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
2 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
2  before v = A * y 
2  before  alpha = rho_new / < rt, v > 
1 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
1 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
6 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
6 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
6  before v = A * y 
6  before  alpha = rho_new / < rt, v > 
0  before v = A * y 
1  before v = A * y 
3  before alpha = rho_new / h; 
3  before apply first correction to x 
3  before r = r - alpha*v 
3  before test stop criteria 
3 istlsolver::printOutput 
  0.5      8.77926e-28      1.29987e-17
3 === rate=1.68965e-34, T=0.123104, TIT=0.246208, IT=0.5
3 finished solveLinearSystemImpl_ 1
3 converged value: 1 7 1 let s go get convergedRemote
3 to  comm_.min(converged) 
6  before alpha = rho_new / h; 
6  before apply first correction to x 
6  before r = r - alpha*v 
6  before test stop criteria 
6 istlsolver::printOutput 
  0.5      8.77926e-28      1.29987e-17
6 === rate=1.68965e-34, T=0.123098, TIT=0.246195, IT=0.5
6 finished solveLinearSystemImpl_ 1
6 converged value: 1 7 1 let s go get convergedRemote
6 to  comm_.min(converged) 
5  before alpha = rho_new / h; 
5  before apply first correction to x 
5  before r = r - alpha*v 
5  before test stop criteria 
5 istlsolver::printOutput 
  0.5      8.77926e-28      1.29987e-17
5 === rate=1.68965e-34, T=0.123104, TIT=0.246209, IT=0.5
5 finished solveLinearSystemImpl_ 1
5 converged value: 1 7 1 let s go get convergedRemote
5 to  comm_.min(converged) 
2  before alpha = rho_new / h; 
2  before apply first correction to x 
2  before r = r - alpha*v 
2  before test stop criteria 
2 istlsolver::printOutput 
  0.5      8.77926e-28      1.29987e-17
2 === rate=1.68965e-34, T=0.12309, TIT=0.24618, IT=0.5
2 finished solveLinearSystemImpl_ 1
2 converged value: 1 7 1 let s go get convergedRemote
2 to  comm_.min(converged) 
0  before  alpha = rho_new / < rt, v > 
0  before alpha = rho_new / h; 
0  before apply first correction to x 
0  before r = r - alpha*v 
0  before test stop criteria 
0 istlsolver::printOutput 
  0.5      8.77926e-28      1.29987e-17
0 === rate=1.68965e-34, T=0.0186449, TIT=0.0372898, IT=0.5
4  before alpha = rho_new / h; 
4  before apply first correction to x 
4  before r = r - alpha*v 
4  before test stop criteria 
4 istlsolver::printOutput 
  0.5      8.77926e-28      1.29987e-17
4 === rate=1.68965e-34, T=0.12308, TIT=0.24616, IT=0.5
4 finished solveLinearSystemImpl_ 1
4 converged value: 1 7 1 let s go get convergedRemote
4 to  comm_.min(converged) 
1  before  alpha = rho_new / < rt, v > 
1  before alpha = rho_new / h; 
1  before apply first correction to x 
1  before r = r - alpha*v 
1  before test stop criteria 
1 istlsolver::printOutput 
  0.5      8.77926e-28      1.29987e-17
1 === rate=1.68965e-34, T=0.123107, TIT=0.246213, IT=0.5
1 finished solveLinearSystemImpl_ 1
1 converged value: 1 7 1 let s go get convergedRemote
1 to  comm_.min(converged) 
5 did  comm_.min(converged) convergedRemote: 1
5 final convergedRemote: 1
5 Update: x^(k+1) = x^k - deltax^k 
5 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
5 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
5 Newton iteration  2 done5 , maximum relative shift = 1.378e-02
5 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
2 did  comm_.min(converged) convergedRemote: 1
2 final convergedRemote: 1
2 Update: x^(k+1) = x^k - deltax^k 
2 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
2 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
2 Newton iteration  2 done2 , maximum relative shift = 1.378e-02
2 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
0 finished solveLinearSystemImpl_ 1
0 converged value: 1 7 1 let s go get convergedRemote
0 to  comm_.min(converged) 
0 did  comm_.min(converged) convergedRemote: 1
0 final convergedRemote: 1
0 Update: x^(k+1) = x^k - deltax^k 
0 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
0 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
0 Newton iteration  2 done0 , maximum relative shift = 1.378e-02
0 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
1 did  comm_.min(converged) convergedRemote: 1
1 final convergedRemote: 1
1 Update: x^(k+1) = x^k - deltax^k 
1 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
1 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
1 Newton iteration  2 done1 , maximum relative shift = 1.378e-02
1 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
4 did  comm_.min(converged) convergedRemote: 1
4 final convergedRemote: 1
4 Update: x^(k+1) = x^k - deltax^k 
4 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
4 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
4 Newton iteration  2 done4 , maximum relative shift = 1.378e-02
4 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
3 did  comm_.min(converged) convergedRemote: 1
3 final convergedRemote: 1
3 Update: x^(k+1) = x^k - deltax^k 
3 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
3 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
3 Newton iteration  2 done3 , maximum relative shift = 1.378e-02
3 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
6 did  comm_.min(converged) convergedRemote: 1
6 final convergedRemote: 1
6 Update: x^(k+1) = x^k - deltax^k 
6 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
6 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
6 Newton iteration  2 done6 , maximum relative shift = 1.378e-02
6 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
2 Solve: M deltax^k = r 
2 Newton::solveLinearSystem : entering the try section 2
2 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
2 solveLinearSystemImpl_, before converged = ls.solve 
2 7 amgbackend::solve isParallel 1
6 Solve: M deltax^k = r 
6 Newton::solveLinearSystem : entering the try section 2
6 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
6 solveLinearSystemImpl_, before converged = ls.solve 
6 7 amgbackend::solve isParallel 1
6 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
6 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
1 Solve: M deltax^k = r 
1 Newton::solveLinearSystem : entering the try section 2
1 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
1 solveLinearSystemImpl_, before converged = ls.solve 
1 7 amgbackend::solve isParallel 1
1 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
1 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
4 Solve: M deltax^k = r 
4 Newton::solveLinearSystem : entering the try section 2
4 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
4 solveLinearSystemImpl_, before converged = ls.solve 
4 7 amgbackend::solve isParallel 1
4 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
4 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
3 Solve: M deltax^k = r 
3 Newton::solveLinearSystem : entering the try section 2
3 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
3 solveLinearSystemImpl_, before converged = ls.solve 
3 7 amgbackend::solve isParallel 1
3 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
3 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
0 Solve: M deltax^k = r 
0 Newton::solveLinearSystem : entering the try section 2
0 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
0 solveLinearSystemImpl_, before converged = ls.solve 
0 7 amgbackend::solve isParallel 1
0 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
0 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
5 Solve: M deltax^k = r 
5 Newton::solveLinearSystem : entering the try section 2
5 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
5 solveLinearSystemImpl_, before converged = ls.solve 
5 7 amgbackend::solve isParallel 1
5 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
5 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
2 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
2 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 7
6 7 before Dune::BiCGSTABSolver 
6 Dune::BiCGSTABSolver.apply(x, b, result_) 
6 BiCGSTABSolver::apply 
6 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
2 7 before Dune::BiCGSTABSolver 
2 Dune::BiCGSTABSolver.apply(x, b, result_) 
2 BiCGSTABSolver::apply 
2 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
4 7 before Dune::BiCGSTABSolver 
4 Dune::BiCGSTABSolver.apply(x, b, result_) 
4 BiCGSTABSolver::apply 
4 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
1 7 before Dune::BiCGSTABSolver 
1 Dune::BiCGSTABSolver.apply(x, b, result_) 
1 BiCGSTABSolver::apply 
1 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
3 7 before Dune::BiCGSTABSolver 
3 Dune::BiCGSTABSolver.apply(x, b, result_) 
3 BiCGSTABSolver::apply 
3 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
5 7 before Dune::BiCGSTABSolver 
5 Dune::BiCGSTABSolver.apply(x, b, result_) 
5 BiCGSTABSolver::apply 
5 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
Using a direct coarse solver (SuperLU)
Building hierarchy of 1 levels (inclusive coarse solver) took 0.11196 seconds.
0 7 before Dune::BiCGSTABSolver 
0 Dune::BiCGSTABSolver.apply(x, b, result_) 
0 BiCGSTABSolver::apply 
0 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
0 === BiCGSTABSolver
 Iter          Defect            Rate
    0      5.55991e-12
0  (it = 0.5; it < _maxit; it+=.5) 0.5 250
1 === BiCGSTABSolver
 Iter          Defect            Rate
    0      5.55991e-12
1  (it = 0.5; it < _maxit; it+=.5) 0.5 250
1  before p = r 
1  before  y = W^-1 * p 
1  before _prec->apply(y,p); 0.5 250
1 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
3 === BiCGSTABSolver
 Iter          Defect            Rate
    0      5.55991e-12
3  (it = 0.5; it < _maxit; it+=.5) 0.5 250
3  before p = r 
3  before  y = W^-1 * p 
3  before _prec->apply(y,p); 0.5 250
3 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
0  before p = r 
0  before  y = W^-1 * p 
0  before _prec->apply(y,p); 0.5 250
0 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 7
0 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
5 === BiCGSTABSolver
 Iter          Defect            Rate
    0      5.55991e-12
5  (it = 0.5; it < _maxit; it+=.5) 0.5 250
5  before p = r 
5  before  y = W^-1 * p 
5  before _prec->apply(y,p); 0.5 250
5 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
6 === BiCGSTABSolver
 Iter          Defect            Rate
    0      5.55991e-12
6  (it = 0.5; it < _maxit; it+=.5) 0.5 250
6  before p = r 
6  before  y = W^-1 * p 
6  before _prec->apply(y,p); 0.5 250
6 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
2 === BiCGSTABSolver
 Iter          Defect            Rate
    0      5.55991e-12
2  (it = 0.5; it < _maxit; it+=.5) 0.5 250
2  before p = r 
2  before  y = W^-1 * p 
2  before _prec->apply(y,p); 0.5 250
2 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
4 === BiCGSTABSolver
 Iter          Defect            Rate
    0      5.55991e-12
4  (it = 0.5; it < _maxit; it+=.5) 0.5 250
4  before p = r 
4  before  y = W^-1 * p 
4  before _prec->apply(y,p); 0.5 250
4 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
5 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
5 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
5  before v = A * y 
6 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
6 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
6  before v = A * y 
2 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
2 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
3 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
3 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
3  before v = A * y 
1 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
4 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
4 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
4  before v = A * y 
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
0 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
6  before  alpha = rho_new / < rt, v > 
2  before v = A * y 
3  before  alpha = rho_new / < rt, v > 
0 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
4  before  alpha = rho_new / < rt, v > 
1 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
5  before  alpha = rho_new / < rt, v > 
2  before  alpha = rho_new / < rt, v > 
2  before alpha = rho_new / h; 
2  before apply first correction to x 
2  before r = r - alpha*v 
2  before test stop criteria 
2 istlsolver::printOutput 
  0.5       2.0034e-30      3.60329e-19
2 === rate=1.29837e-37, T=0.125482, TIT=0.250963, IT=0.5
2 finished solveLinearSystemImpl_ 1
2 converged value: 1 7 1 let s go get convergedRemote
2 to  comm_.min(converged) 
5  before alpha = rho_new / h; 
5  before apply first correction to x 
5  before r = r - alpha*v 
5  before test stop criteria 
5 istlsolver::printOutput 
  0.5       2.0034e-30      3.60329e-19
5 === rate=1.29837e-37, T=0.125562, TIT=0.251125, IT=0.5
5 finished solveLinearSystemImpl_ 1
5 converged value: 1 7 1 let s go get convergedRemote
5 to  comm_.min(converged) 
0  before v = A * y 
0  before  alpha = rho_new / < rt, v > 
0  before alpha = rho_new / h; 
0  before apply first correction to x 
0  before r = r - alpha*v 
0  before test stop criteria 
0 istlsolver::printOutput 
  0.5       2.0034e-30      3.60329e-19
0 === rate=1.29837e-37, T=0.0216565, TIT=0.0433129, IT=0.5
4  before alpha = rho_new / h; 
4  before apply first correction to x 
4  before r = r - alpha*v 
4  before test stop criteria 
4 istlsolver::printOutput 
  0.5       2.0034e-30      3.60329e-19
4 === rate=1.29837e-37, T=0.125483, TIT=0.250966, IT=0.5
4 finished solveLinearSystemImpl_ 1
4 converged value: 1 7 1 let s go get convergedRemote
4 to  comm_.min(converged) 
1  before v = A * y 
1  before  alpha = rho_new / < rt, v > 
1  before alpha = rho_new / h; 
1  before apply first correction to x 
1  before r = r - alpha*v 
1  before test stop criteria 
1 istlsolver::printOutput 
  0.5       2.0034e-30      3.60329e-19
1 === rate=1.29837e-37, T=0.125564, TIT=0.251127, IT=0.5
1 finished solveLinearSystemImpl_ 1
1 converged value: 1 7 1 let s go get convergedRemote
1 to  comm_.min(converged) 
3  before alpha = rho_new / h; 
3  before apply first correction to x 
3  before r = r - alpha*v 
3  before test stop criteria 
3 istlsolver::printOutput 
  0.5       2.0034e-30      3.60329e-19
3 === rate=1.29837e-37, T=0.125562, TIT=0.251124, IT=0.5
3 finished solveLinearSystemImpl_ 1
3 converged value: 1 7 1 let s go get convergedRemote
3 to  comm_.min(converged) 
6  before alpha = rho_new / h; 
6  before apply first correction to x 
6  before r = r - alpha*v 
6  before test stop criteria 
6 istlsolver::printOutput 
  0.5       2.0034e-30      3.60329e-19
6 === rate=1.29837e-37, T=0.125482, TIT=0.250965, IT=0.5
6 finished solveLinearSystemImpl_ 1
6 converged value: 1 7 1 let s go get convergedRemote
6 to  comm_.min(converged) 
5 did  comm_.min(converged) convergedRemote: 1
5 final convergedRemote: 1
5 Update: x^(k+1) = x^k - deltax^k 
5 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
5 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
5 Newton iteration  3 done5 , maximum relative shift = 1.138e-03
5 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
0 finished solveLinearSystemImpl_ 1
0 converged value: 1 7 1 let s go get convergedRemote
0 to  comm_.min(converged) 
0 did  comm_.min(converged) convergedRemote: 1
0 final convergedRemote: 1
0 Update: x^(k+1) = x^k - deltax^k 
0 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
0 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
0 Newton iteration  3 done0 , maximum relative shift = 1.138e-03
0 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
4 did  comm_.min(converged) convergedRemote: 1
4 final convergedRemote: 1
4 Update: x^(k+1) = x^k - deltax^k 
4 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
4 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
4 Newton iteration  3 done4 , maximum relative shift = 1.138e-03
4 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
1 did  comm_.min(converged) convergedRemote: 1
1 final convergedRemote: 1
1 Update: x^(k+1) = x^k - deltax^k 
1 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
1 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
1 Newton iteration  3 done1 , maximum relative shift = 1.138e-03
1 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
3 did  comm_.min(converged) convergedRemote: 1
3 final convergedRemote: 1
3 Update: x^(k+1) = x^k - deltax^k 
3 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
3 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
3 Newton iteration  3 done3 , maximum relative shift = 1.138e-03
3 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
2 did  comm_.min(converged) convergedRemote: 1
2 final convergedRemote: 1
2 Update: x^(k+1) = x^k - deltax^k 
2 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
2 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
2 Newton iteration  3 done2 , maximum relative shift = 1.138e-03
2 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
6 did  comm_.min(converged) convergedRemote: 1
6 final convergedRemote: 1
6 Update: x^(k+1) = x^k - deltax^k 
6 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
6 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
6 Newton iteration  3 done6 , maximum relative shift = 1.138e-03
6 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
6 Solve: M deltax^k = r 
6 Newton::solveLinearSystem : entering the try section 3
6 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
6 solveLinearSystemImpl_, before converged = ls.solve 
6 7 amgbackend::solve isParallel 1
2 Solve: M deltax^k = r 
2 Newton::solveLinearSystem : entering the try section 3
2 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
2 solveLinearSystemImpl_, before converged = ls.solve 
2 7 amgbackend::solve isParallel 1
5 Solve: M deltax^k = r 
5 Newton::solveLinearSystem : entering the try section 3
5 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
5 solveLinearSystemImpl_, before converged = ls.solve 
5 7 amgbackend::solve isParallel 1
0 Solve: M deltax^k = r 
0 Newton::solveLinearSystem : entering the try section 3
0 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
0 solveLinearSystemImpl_, before converged = ls.solve 
0 7 amgbackend::solve isParallel 1
4 Solve: M deltax^k = r 
4 Newton::solveLinearSystem : entering the try section 3
4 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
4 solveLinearSystemImpl_, before converged = ls.solve 
4 7 amgbackend::solve isParallel 1
1 Solve: M deltax^k = r 
1 Newton::solveLinearSystem : entering the try section 3
1 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
1 solveLinearSystemImpl_, before converged = ls.solve 
1 7 amgbackend::solve isParallel 1
3 Solve: M deltax^k = r 
3 Newton::solveLinearSystem : entering the try section 3
3 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
3 solveLinearSystemImpl_, before converged = ls.solve 
3 7 amgbackend::solve isParallel 1
5 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
5 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
0 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
0 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 7
4 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
4 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
1 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
1 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
3 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
3 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
6 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
6 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
2 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
2 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 7
6 7 before Dune::BiCGSTABSolver 
6 Dune::BiCGSTABSolver.apply(x, b, result_) 
6 BiCGSTABSolver::apply 
6 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
1 7 before Dune::BiCGSTABSolver 
1 Dune::BiCGSTABSolver.apply(x, b, result_) 
1 BiCGSTABSolver::apply 
1 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
5 7 before Dune::BiCGSTABSolver 
5 Dune::BiCGSTABSolver.apply(x, b, result_) 
5 BiCGSTABSolver::apply 
5 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
2 7 before Dune::BiCGSTABSolver 
2 Dune::BiCGSTABSolver.apply(x, b, result_) 
2 BiCGSTABSolver::apply 
2 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
4 7 before Dune::BiCGSTABSolver 
4 Dune::BiCGSTABSolver.apply(x, b, result_) 
4 BiCGSTABSolver::apply 
4 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
3 7 before Dune::BiCGSTABSolver 
3 Dune::BiCGSTABSolver.apply(x, b, result_) 
3 BiCGSTABSolver::apply 
3 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
Using a direct coarse solver (SuperLU)
Building hierarchy of 1 levels (inclusive coarse solver) took 0.112186 seconds.
0 7 before Dune::BiCGSTABSolver 
0 Dune::BiCGSTABSolver.apply(x, b, result_) 
0 BiCGSTABSolver::apply 
0 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
0 === BiCGSTABSolver
 Iter          Defect            Rate
    0        5.131e-13
0  (it = 0.5; it < _maxit; it+=.5) 0.5 250
0  before p = r 
0  before  y = W^-1 * p 
0  before _prec->apply(y,p); 0.5 250
0 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 7
0 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
4 === BiCGSTABSolver
 Iter          Defect            Rate
    0        5.131e-13
4  (it = 0.5; it < _maxit; it+=.5) 0.5 250
4  before p = r 
4  before  y = W^-1 * p 
4  before _prec->apply(y,p); 0.5 250
4 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
3 === BiCGSTABSolver
 Iter          Defect            Rate
    0        5.131e-13
3  (it = 0.5; it < _maxit; it+=.5) 0.5 250
3  before p = r 
3  before  y = W^-1 * p 
3  before _prec->apply(y,p); 0.5 250
3 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
6 === BiCGSTABSolver
 Iter          Defect            Rate
    0        5.131e-13
6  (it = 0.5; it < _maxit; it+=.5) 0.5 250
6  before p = r 
6  before  y = W^-1 * p 
6  before _prec->apply(y,p); 0.5 250
6 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
1 === BiCGSTABSolver
 Iter          Defect            Rate
    0        5.131e-13
1  (it = 0.5; it < _maxit; it+=.5) 0.5 250
1  before p = r 
1  before  y = W^-1 * p 
1  before _prec->apply(y,p); 0.5 250
1 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
5 === BiCGSTABSolver
 Iter          Defect            Rate
    0        5.131e-13
5  (it = 0.5; it < _maxit; it+=.5) 0.5 250
5  before p = r 
5  before  y = W^-1 * p 
5  before _prec->apply(y,p); 0.5 250
5 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
2 === BiCGSTABSolver
 Iter          Defect            Rate
    0        5.131e-13
2  (it = 0.5; it < _maxit; it+=.5) 0.5 250
2  before p = r 
2  before  y = W^-1 * p 
2  before _prec->apply(y,p); 0.5 250
2 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
0 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
4 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
4 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
4  before v = A * y 
3 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
3 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
3  before v = A * y 
6 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
6 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
6  before v = A * y 
1 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
5 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
5 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
5  before v = A * y 
2 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
2 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
1 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
2  before v = A * y 
5  before  alpha = rho_new / < rt, v > 
0 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
0  before v = A * y 
4  before  alpha = rho_new / < rt, v > 
3  before  alpha = rho_new / < rt, v > 
6  before  alpha = rho_new / < rt, v > 
0  before  alpha = rho_new / < rt, v > 
0  before alpha = rho_new / h; 
0  before apply first correction to x 
0  before r = r - alpha*v 
0  before test stop criteria 
0 istlsolver::printOutput 
  0.5      1.97074e-32      3.84086e-20
0 === rate=1.47522e-39, T=0.0181213, TIT=0.0362427, IT=0.5
4  before alpha = rho_new / h; 
4  before apply first correction to x 
4  before r = r - alpha*v 
4  before test stop criteria 
4 istlsolver::printOutput 
  0.5      1.97074e-32      3.84086e-20
4 === rate=1.47522e-39, T=0.122299, TIT=0.244599, IT=0.5
4 finished solveLinearSystemImpl_ 1
4 converged value: 1 7 1 let s go get convergedRemote
4 to  comm_.min(converged) 
3  before alpha = rho_new / h; 
3  before apply first correction to x 
3  before r = r - alpha*v 
3  before test stop criteria 
3 istlsolver::printOutput 
  0.5      1.97074e-32      3.84086e-20
3 === rate=1.47522e-39, T=0.122424, TIT=0.244847, IT=0.5
3 finished solveLinearSystemImpl_ 1
3 converged value: 1 7 1 let s go get convergedRemote
3 to  comm_.min(converged) 
1  before v = A * y 
1  before  alpha = rho_new / < rt, v > 
1  before alpha = rho_new / h; 
1  before apply first correction to x 
1  before r = r - alpha*v 
1  before test stop criteria 
1 istlsolver::printOutput 
  0.5      1.97074e-32      3.84086e-20
1 === rate=1.47522e-39, T=0.122423, TIT=0.244847, IT=0.5
1 finished solveLinearSystemImpl_ 1
1 converged value: 1 7 1 let s go get convergedRemote
1 to  comm_.min(converged) 
6  before alpha = rho_new / h; 
6  before apply first correction to x 
6  before r = r - alpha*v 
6  before test stop criteria 
6 istlsolver::printOutput 
  0.5      1.97074e-32      3.84086e-20
6 === rate=1.47522e-39, T=0.122297, TIT=0.244593, IT=0.5
6 finished solveLinearSystemImpl_ 1
6 converged value: 1 7 1 let s go get convergedRemote
6 to  comm_.min(converged) 
2  before  alpha = rho_new / < rt, v > 
2  before alpha = rho_new / h; 
2  before apply first correction to x 
2  before r = r - alpha*v 
2  before test stop criteria 
2 istlsolver::printOutput 
  0.5      1.97074e-32      3.84086e-20
2 === rate=1.47522e-39, T=0.1223, TIT=0.244601, IT=0.5
2 finished solveLinearSystemImpl_ 1
2 converged value: 1 7 1 let s go get convergedRemote
2 to  comm_.min(converged) 
5  before alpha = rho_new / h; 
5  before apply first correction to x 
5  before r = r - alpha*v 
5  before test stop criteria 
5 istlsolver::printOutput 
  0.5      1.97074e-32      3.84086e-20
5 === rate=1.47522e-39, T=0.122423, TIT=0.244847, IT=0.5
5 finished solveLinearSystemImpl_ 1
5 converged value: 1 7 1 let s go get convergedRemote
5 to  comm_.min(converged) 
2 did  comm_.min(converged) convergedRemote: 1
2 final convergedRemote: 1
2 Update: x^(k+1) = x^k - deltax^k 
2 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
2 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
2 Newton iteration  4 done2 , maximum relative shift = 1.052e-04
2 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
1 did  comm_.min(converged) convergedRemote: 1
1 final convergedRemote: 1
1 Update: x^(k+1) = x^k - deltax^k 
1 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
1 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
1 Newton iteration  4 done1 , maximum relative shift = 1.052e-04
1 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
0 finished solveLinearSystemImpl_ 1
0 converged value: 1 7 1 let s go get convergedRemote
0 to  comm_.min(converged) 
0 did  comm_.min(converged) convergedRemote: 1
0 final convergedRemote: 1
0 Update: x^(k+1) = x^k - deltax^k 
0 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
0 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
0 Newton iteration  4 done0 , maximum relative shift = 1.052e-04
0 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
4 did  comm_.min(converged) convergedRemote: 1
4 final convergedRemote: 1
4 Update: x^(k+1) = x^k - deltax^k 
4 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
4 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
4 Newton iteration  4 done4 , maximum relative shift = 1.052e-04
4 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
3 did  comm_.min(converged) convergedRemote: 1
3 final convergedRemote: 1
3 Update: x^(k+1) = x^k - deltax^k 
3 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
3 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
3 Newton iteration  4 done3 , maximum relative shift = 1.052e-04
3 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
5 did  comm_.min(converged) convergedRemote: 1
5 final convergedRemote: 1
5 Update: x^(k+1) = x^k - deltax^k 
5 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
5 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
5 Newton iteration  4 done5 , maximum relative shift = 1.052e-04
5 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
6 did  comm_.min(converged) convergedRemote: 1
6 final convergedRemote: 1
6 Update: x^(k+1) = x^k - deltax^k 
6 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
6 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
6 Newton iteration  4 done6 , maximum relative shift = 1.052e-04
6 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
3 Solve: M deltax^k = r 
3 Newton::solveLinearSystem : entering the try section 4
3 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
3 solveLinearSystemImpl_, before converged = ls.solve 
3 7 amgbackend::solve isParallel 1
2 Solve: M deltax^k = r 
2 Newton::solveLinearSystem : entering the try section 4
2 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
2 solveLinearSystemImpl_, before converged = ls.solve 
2 7 amgbackend::solve isParallel 1
1 Solve: M deltax^k = r 
1 Newton::solveLinearSystem : entering the try section 4
1 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
1 solveLinearSystemImpl_, before converged = ls.solve 
1 7 amgbackend::solve isParallel 1
0 Solve: M deltax^k = r 
0 Newton::solveLinearSystem : entering the try section 4
0 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
0 solveLinearSystemImpl_, before converged = ls.solve 
0 7 amgbackend::solve isParallel 1
4 Solve: M deltax^k = r 
4 Newton::solveLinearSystem : entering the try section 4
4 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
4 solveLinearSystemImpl_, before converged = ls.solve 
4 7 amgbackend::solve isParallel 1
6 Solve: M deltax^k = r 
6 Newton::solveLinearSystem : entering the try section 4
6 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
6 solveLinearSystemImpl_, before converged = ls.solve 
6 7 amgbackend::solve isParallel 1
5 Solve: M deltax^k = r 
5 Newton::solveLinearSystem : entering the try section 4
5 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
5 solveLinearSystemImpl_, before converged = ls.solve 
5 7 amgbackend::solve isParallel 1
4 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
4 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
3 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
3 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
2 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
2 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
1 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
1 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
0 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
0 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 7
5 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
5 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
6 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
6 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 7
1 7 before Dune::BiCGSTABSolver 
1 Dune::BiCGSTABSolver.apply(x, b, result_) 
1 BiCGSTABSolver::apply 
1 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 7
5 7 before Dune::BiCGSTABSolver 
5 Dune::BiCGSTABSolver.apply(x, b, result_) 
5 BiCGSTABSolver::apply 
5 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
6 7 before Dune::BiCGSTABSolver 
6 Dune::BiCGSTABSolver.apply(x, b, result_) 
6 BiCGSTABSolver::apply 
6 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
4 7 before Dune::BiCGSTABSolver 
4 Dune::BiCGSTABSolver.apply(x, b, result_) 
4 BiCGSTABSolver::apply 
4 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
3 7 before Dune::BiCGSTABSolver 
3 Dune::BiCGSTABSolver.apply(x, b, result_) 
3 BiCGSTABSolver::apply 
3 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
2 7 before Dune::BiCGSTABSolver 
2 Dune::BiCGSTABSolver.apply(x, b, result_) 
2 BiCGSTABSolver::apply 
2 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
Using a direct coarse solver (SuperLU)
Building hierarchy of 1 levels (inclusive coarse solver) took 0.112765 seconds.
0 7 before Dune::BiCGSTABSolver 
0 Dune::BiCGSTABSolver.apply(x, b, result_) 
5 === BiCGSTABSolver
 Iter          Defect            Rate
    0      4.78371e-14
5  (it = 0.5; it < _maxit; it+=.5) 0.5 250
5  before p = r 
5  before  y = W^-1 * p 
5  before _prec->apply(y,p); 0.5 250
5 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
6 === BiCGSTABSolver
 Iter          Defect            Rate
    0      4.78371e-14
6  (it = 0.5; it < _maxit; it+=.5) 0.5 250
6  before p = r 
6  before  y = W^-1 * p 
6  before _prec->apply(y,p); 0.5 250
6 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
4 === BiCGSTABSolver
 Iter          Defect            Rate
    0      4.78371e-14
4  (it = 0.5; it < _maxit; it+=.5) 0.5 250
4  before p = r 
4  before  y = W^-1 * p 
4  before _prec->apply(y,p); 0.5 250
4 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
3 === BiCGSTABSolver
 Iter          Defect            Rate
    0      4.78371e-14
3  (it = 0.5; it < _maxit; it+=.5) 0.5 250
3  before p = r 
3  before  y = W^-1 * p 
3  before _prec->apply(y,p); 0.5 250
3 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
1 === BiCGSTABSolver
 Iter          Defect            Rate
    0      4.78371e-14
1  (it = 0.5; it < _maxit; it+=.5) 0.5 250
1  before p = r 
1  before  y = W^-1 * p 
1  before _prec->apply(y,p); 0.5 250
1 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
0 BiCGSTABSolver::apply 
0 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
0 === BiCGSTABSolver
 Iter          Defect            Rate
    0      4.78371e-14
0  (it = 0.5; it < _maxit; it+=.5) 0.5 250
0  before p = r 
0  before  y = W^-1 * p 
0  before _prec->apply(y,p); 0.5 250
0 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 7
0 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
2 === BiCGSTABSolver
 Iter          Defect            Rate
    0      4.78371e-14
2  (it = 0.5; it < _maxit; it+=.5) 0.5 250
2  before p = r 
2  before  y = W^-1 * p 
2  before _prec->apply(y,p); 0.5 250
2 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
4 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
4 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
4  before v = A * y 
3 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
3 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
3  before v = A * y 
2 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
2 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
0 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
1 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
5 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
5 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
5  before v = A * y 
6 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
6 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
6  before v = A * y 
0 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
0  before v = A * y 
1 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
5  before  alpha = rho_new / < rt, v > 
3  before  alpha = rho_new / < rt, v > 
4  before  alpha = rho_new / < rt, v > 
6  before  alpha = rho_new / < rt, v > 
6  before alpha = rho_new / h; 
6  before apply first correction to x 
6  before r = r - alpha*v 
6  before test stop criteria 
6 istlsolver::printOutput 
  0.5      6.31089e-30      1.31925e-16
6 === rate=1.74041e-32, T=0.123336, TIT=0.246672, IT=0.5
6 finished solveLinearSystemImpl_ 1
6 converged value: 1 7 1 let s go get convergedRemote
6 to  comm_.min(converged) 
2  before v = A * y 
2  before  alpha = rho_new / < rt, v > 
2  before alpha = rho_new / h; 
2  before apply first correction to x 
2  before r = r - alpha*v 
2  before test stop criteria 
2 istlsolver::printOutput 
  0.5      6.31089e-30      1.31925e-16
2 === rate=1.74041e-32, T=0.123334, TIT=0.246668, IT=0.5
2 finished solveLinearSystemImpl_ 1
2 converged value: 1 7 1 let s go get convergedRemote
2 to  comm_.min(converged) 
4  before alpha = rho_new / h; 
4  before apply first correction to x 
4  before r = r - alpha*v 
4  before test stop criteria 
4 istlsolver::printOutput 
  0.5      6.31089e-30      1.31925e-16
4 === rate=1.74041e-32, T=0.123336, TIT=0.246672, IT=0.5
4 finished solveLinearSystemImpl_ 1
4 converged value: 1 7 1 let s go get convergedRemote
4 to  comm_.min(converged) 
0  before  alpha = rho_new / < rt, v > 
0  before alpha = rho_new / h; 
0  before apply first correction to x 
0  before r = r - alpha*v 
0  before test stop criteria 
0 istlsolver::printOutput 
  0.5      6.31089e-30      1.31925e-16
0 === rate=1.74041e-32, T=0.0183406, TIT=0.0366812, IT=0.5
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
1  before v = A * y 
1  before  alpha = rho_new / < rt, v > 
1  before alpha = rho_new / h; 
1  before apply first correction to x 
1  before r = r - alpha*v 
1  before test stop criteria 
1 istlsolver::printOutput 
  0.5      6.31089e-30      1.31925e-16
1 === rate=1.74041e-32, T=0.123346, TIT=0.246691, IT=0.5
1 finished solveLinearSystemImpl_ 1
1 converged value: 1 7 1 let s go get convergedRemote
1 to  comm_.min(converged) 
5  before alpha = rho_new / h; 
5  before apply first correction to x 
5  before r = r - alpha*v 
5  before test stop criteria 
5 istlsolver::printOutput 
  0.5      6.31089e-30      1.31925e-16
5 === rate=1.74041e-32, T=0.123346, TIT=0.246692, IT=0.5
5 finished solveLinearSystemImpl_ 1
5 converged value: 1 7 1 let s go get convergedRemote
5 to  comm_.min(converged) 
3  before alpha = rho_new / h; 
3  before apply first correction to x 
3  before r = r - alpha*v 
3  before test stop criteria 
3 istlsolver::printOutput 
  0.5      6.31089e-30      1.31925e-16
3 === rate=1.74041e-32, T=0.123346, TIT=0.246692, IT=0.5
3 finished solveLinearSystemImpl_ 1
3 converged value: 1 7 1 let s go get convergedRemote
3 to  comm_.min(converged) 
2 did  comm_.min(converged) convergedRemote: 1
2 final convergedRemote: 1
2 Update: x^(k+1) = x^k - deltax^k 
2 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
2 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
2 Newton iteration  5 done2 , maximum relative shift = 1.144e-05
2 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
4 did  comm_.min(converged) convergedRemote: 1
4 final convergedRemote: 1
4 Update: x^(k+1) = x^k - deltax^k 
4 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
4 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
4 Newton iteration  5 done4 , maximum relative shift = 1.144e-05
4 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
0 finished solveLinearSystemImpl_ 1
0 converged value: 1 7 1 let s go get convergedRemote
0 to  comm_.min(converged) 
0 did  comm_.min(converged) convergedRemote: 1
0 final convergedRemote: 1
0 Update: x^(k+1) = x^k - deltax^k 
0 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
0 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
0 Newton iteration  5 done0 , maximum relative shift = 1.144e-05
0 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
1 did  comm_.min(converged) convergedRemote: 1
1 final convergedRemote: 1
1 Update: x^(k+1) = x^k - deltax^k 
1 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
1 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
1 Newton iteration  5 done1 , maximum relative shift = 1.144e-05
1 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
5 did  comm_.min(converged) convergedRemote: 1
5 final convergedRemote: 1
5 Update: x^(k+1) = x^k - deltax^k 
5 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
5 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
5 Newton iteration  5 done5 , maximum relative shift = 1.144e-05
5 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
3 did  comm_.min(converged) convergedRemote: 1
3 final convergedRemote: 1
3 Update: x^(k+1) = x^k - deltax^k 
3 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
3 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
3 Newton iteration  5 done3 , maximum relative shift = 1.144e-05
3 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
6 did  comm_.min(converged) convergedRemote: 1
6 final convergedRemote: 1
6 Update: x^(k+1) = x^k - deltax^k 
6 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
6 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
6 Newton iteration  5 done6 , maximum relative shift = 1.144e-05
6 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
3 Solve: M deltax^k = r 
3 Newton::solveLinearSystem : entering the try section 5
3 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
3 solveLinearSystemImpl_, before converged = ls.solve 
3 7 amgbackend::solve isParallel 1
5 Solve: M deltax^k = r 
5 Newton::solveLinearSystem : entering the try section 5
5 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
5 solveLinearSystemImpl_, before converged = ls.solve 
5 7 amgbackend::solve isParallel 1
2 Solve: M deltax^k = r 
2 Newton::solveLinearSystem : entering the try section 5
2 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
2 solveLinearSystemImpl_, before converged = ls.solve 
2 7 amgbackend::solve isParallel 1
4 Solve: M deltax^k = r 
4 Newton::solveLinearSystem : entering the try section 5
4 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
4 solveLinearSystemImpl_, before converged = ls.solve 
4 7 amgbackend::solve isParallel 1
0 Solve: M deltax^k = r 
0 Newton::solveLinearSystem : entering the try section 5
0 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
0 solveLinearSystemImpl_, before converged = ls.solve 
0 7 amgbackend::solve isParallel 1
1 Solve: M deltax^k = r 
1 Newton::solveLinearSystem : entering the try section 5
1 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
1 solveLinearSystemImpl_, before converged = ls.solve 
1 7 amgbackend::solve isParallel 1
6 Solve: M deltax^k = r 
6 Newton::solveLinearSystem : entering the try section 5
6 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
6 solveLinearSystemImpl_, before converged = ls.solve 
6 7 amgbackend::solve isParallel 1
1 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
1 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
3 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
3 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
5 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
5 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
2 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
2 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
4 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
4 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
0 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
0 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 7
6 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
6 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 7
6 7 before Dune::BiCGSTABSolver 
6 Dune::BiCGSTABSolver.apply(x, b, result_) 
6 BiCGSTABSolver::apply 
6 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
1 7 before Dune::BiCGSTABSolver 
1 Dune::BiCGSTABSolver.apply(x, b, result_) 
1 BiCGSTABSolver::apply 
1 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
3 7 before Dune::BiCGSTABSolver 
3 Dune::BiCGSTABSolver.apply(x, b, result_) 
3 BiCGSTABSolver::apply 
3 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
5 7 before Dune::BiCGSTABSolver 
5 Dune::BiCGSTABSolver.apply(x, b, result_) 
5 BiCGSTABSolver::apply 
5 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
2 7 before Dune::BiCGSTABSolver 
2 Dune::BiCGSTABSolver.apply(x, b, result_) 
2 BiCGSTABSolver::apply 
2 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
4 7 before Dune::BiCGSTABSolver 
4 Dune::BiCGSTABSolver.apply(x, b, result_) 
4 BiCGSTABSolver::apply 
4 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
Using a direct coarse solver (SuperLU)
Building hierarchy of 1 levels (inclusive coarse solver) took 0.111326 seconds.
0 7 before Dune::BiCGSTABSolver 
0 Dune::BiCGSTABSolver.apply(x, b, result_) 
0 BiCGSTABSolver::apply 
0 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
2 === BiCGSTABSolver
 Iter          Defect            Rate
    0      2.75113e-15
2  (it = 0.5; it < _maxit; it+=.5) 0.5 250
2  before p = r 
2  before  y = W^-1 * p 
2  before _prec->apply(y,p); 0.5 250
2 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
0 === BiCGSTABSolver
 Iter          Defect            Rate
    0      2.75113e-15
0  (it = 0.5; it < _maxit; it+=.5) 0.5 250
0  before p = r 
0  before  y = W^-1 * p 
0  before _prec->apply(y,p); 0.5 250
0 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 7
0 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
4 === BiCGSTABSolver
 Iter          Defect            Rate
    0      2.75113e-15
4  (it = 0.5; it < _maxit; it+=.5) 0.5 250
4  before p = r 
4  before  y = W^-1 * p 
4  before _prec->apply(y,p); 0.5 250
4 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
6 === BiCGSTABSolver
 Iter          Defect            Rate
    0      2.75113e-15
6  (it = 0.5; it < _maxit; it+=.5) 0.5 250
6  before p = r 
6  before  y = W^-1 * p 
6  before _prec->apply(y,p); 0.5 250
6 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
1 === BiCGSTABSolver
 Iter          Defect            Rate
    0      2.75113e-15
1  (it = 0.5; it < _maxit; it+=.5) 0.5 250
1  before p = r 
1  before  y = W^-1 * p 
1  before _prec->apply(y,p); 0.5 250
1 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
3 === BiCGSTABSolver
 Iter          Defect            Rate
    0      2.75113e-15
3  (it = 0.5; it < _maxit; it+=.5) 0.5 250
3  before p = r 
3  before  y = W^-1 * p 
3  before _prec->apply(y,p); 0.5 250
3 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
5 === BiCGSTABSolver
 Iter          Defect            Rate
    0      2.75113e-15
5  (it = 0.5; it < _maxit; it+=.5) 0.5 250
5  before p = r 
5  before  y = W^-1 * p 
5  before _prec->apply(y,p); 0.5 250
5 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
5 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
5 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
5  before v = A * y 
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
0 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
4 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
4 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
4  before v = A * y 
6 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
6 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
6  before v = A * y 
1 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
3 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
3 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
3  before v = A * y 
2 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
2 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
5  before  alpha = rho_new / < rt, v > 
0 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
4  before  alpha = rho_new / < rt, v > 
6  before  alpha = rho_new / < rt, v > 
1 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
3  before  alpha = rho_new / < rt, v > 
2  before v = A * y 
2  before  alpha = rho_new / < rt, v > 
3  before alpha = rho_new / h; 
3  before apply first correction to x 
3  before r = r - alpha*v 
3  before test stop criteria 
3 istlsolver::printOutput 
  0.5      2.16454e-31      7.86782e-17
3 === rate=6.19026e-33, T=0.121995, TIT=0.24399, IT=0.5
3 finished solveLinearSystemImpl_ 1
3 converged value: 1 7 1 let s go get convergedRemote
3 to  comm_.min(converged) 
5  before alpha = rho_new / h; 
5  before apply first correction to x 
5  before r = r - alpha*v 
5  before test stop criteria 
5 istlsolver::printOutput 
  0.5      2.16454e-31      7.86782e-17
5 === rate=6.19026e-33, T=0.121994, TIT=0.243988, IT=0.5
5 finished solveLinearSystemImpl_ 1
5 converged value: 1 7 1 let s go get convergedRemote
5 to  comm_.min(converged) 
0  before v = A * y 
0  before  alpha = rho_new / < rt, v > 
0  before alpha = rho_new / h; 
0  before apply first correction to x 
0  before r = r - alpha*v 
0  before test stop criteria 
0 istlsolver::printOutput 
  0.5      2.16454e-31      7.86782e-17
0 === rate=6.19026e-33, T=0.0184186, TIT=0.0368371, IT=0.5
4  before alpha = rho_new / h; 
4  before apply first correction to x 
4  before r = r - alpha*v 
4  before test stop criteria 
4 istlsolver::printOutput 
  0.5      2.16454e-31      7.86782e-17
4 === rate=6.19026e-33, T=0.122001, TIT=0.244003, IT=0.5
4 finished solveLinearSystemImpl_ 1
4 converged value: 1 7 1 let s go get convergedRemote
4 to  comm_.min(converged) 
6  before alpha = rho_new / h; 
6  before apply first correction to x 
6  before r = r - alpha*v 
6  before test stop criteria 
6 istlsolver::printOutput 
  0.5      2.16454e-31      7.86782e-17
6 === rate=6.19026e-33, T=0.122007, TIT=0.244014, IT=0.5
6 finished solveLinearSystemImpl_ 1
6 converged value: 1 7 1 let s go get convergedRemote
6 to  comm_.min(converged) 
1  before v = A * y 
1  before  alpha = rho_new / < rt, v > 
1  before alpha = rho_new / h; 
1  before apply first correction to x 
1  before r = r - alpha*v 
1  before test stop criteria 
1 istlsolver::printOutput 
  0.5      2.16454e-31      7.86782e-17
1 === rate=6.19026e-33, T=0.121994, TIT=0.243989, IT=0.5
1 finished solveLinearSystemImpl_ 1
1 converged value: 1 7 1 let s go get convergedRemote
1 to  comm_.min(converged) 
2  before alpha = rho_new / h; 
2  before apply first correction to x 
2  before r = r - alpha*v 
2  before test stop criteria 
2 istlsolver::printOutput 
  0.5      2.16454e-31      7.86782e-17
2 === rate=6.19026e-33, T=0.122002, TIT=0.244004, IT=0.5
2 finished solveLinearSystemImpl_ 1
2 converged value: 1 7 1 let s go get convergedRemote
2 to  comm_.min(converged) 
2 did  comm_.min(converged) convergedRemote: 1
2 final convergedRemote: 1
2 Update: x^(k+1) = x^k - deltax^k 
2 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
2 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
2 Newton iteration  6 done2 , maximum relative shift = 5.639e-07
2 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
1 did  comm_.min(converged) convergedRemote: 1
1 final convergedRemote: 1
1 Update: x^(k+1) = x^k - deltax^k 
1 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
1 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
1 Newton iteration  6 done1 , maximum relative shift = 5.639e-07
1 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
3 did  comm_.min(converged) convergedRemote: 1
3 final convergedRemote: 1
3 Update: x^(k+1) = x^k - deltax^k 
3 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
3 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
3 Newton iteration  6 done3 , maximum relative shift = 5.639e-07
3 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
5 did  comm_.min(converged) convergedRemote: 1
5 final convergedRemote: 1
5 Update: x^(k+1) = x^k - deltax^k 
5 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
5 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
5 Newton iteration  6 done5 , maximum relative shift = 5.639e-07
5 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
0 finished solveLinearSystemImpl_ 1
0 converged value: 1 7 1 let s go get convergedRemote
0 to  comm_.min(converged) 
0 did  comm_.min(converged) convergedRemote: 1
0 final convergedRemote: 1
0 Update: x^(k+1) = x^k - deltax^k 
0 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
0 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
0 Newton iteration  6 done0 , maximum relative shift = 5.639e-07
0 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
4 did  comm_.min(converged) convergedRemote: 1
4 final convergedRemote: 1
4 Update: x^(k+1) = x^k - deltax^k 
4 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
4 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
4 Newton iteration  6 done4 , maximum relative shift = 5.639e-07
4 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
6 did  comm_.min(converged) convergedRemote: 1
6 final convergedRemote: 1
6 Update: x^(k+1) = x^k - deltax^k 
6 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
6 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
6 Newton iteration  6 done6 , maximum relative shift = 5.639e-07
6 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
4 Solve: M deltax^k = r 
4 Newton::solveLinearSystem : entering the try section 6
4 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
4 solveLinearSystemImpl_, before converged = ls.solve 
4 7 amgbackend::solve isParallel 1
6 Solve: M deltax^k = r 
6 Newton::solveLinearSystem : entering the try section 6
6 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
6 solveLinearSystemImpl_, before converged = ls.solve 
6 7 amgbackend::solve isParallel 1
2 Solve: M deltax^k = r 
2 Newton::solveLinearSystem : entering the try section 6
2 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
2 solveLinearSystemImpl_, before converged = ls.solve 
2 7 amgbackend::solve isParallel 1
1 Solve: M deltax^k = r 
1 Newton::solveLinearSystem : entering the try section 6
1 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
1 solveLinearSystemImpl_, before converged = ls.solve 
1 7 amgbackend::solve isParallel 1
3 Solve: M deltax^k = r 
3 Newton::solveLinearSystem : entering the try section 6
3 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
3 solveLinearSystemImpl_, before converged = ls.solve 
3 7 amgbackend::solve isParallel 1
5 Solve: M deltax^k = r 
5 Newton::solveLinearSystem : entering the try section 6
5 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
5 solveLinearSystemImpl_, before converged = ls.solve 
5 7 amgbackend::solve isParallel 1
0 Solve: M deltax^k = r 
0 Newton::solveLinearSystem : entering the try section 6
0 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
0 solveLinearSystemImpl_, before converged = ls.solve 
0 7 amgbackend::solve isParallel 1
2 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
2 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
1 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
1 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
3 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
3 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
5 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
5 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
4 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
4 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
0 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
0 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 7
6 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
6 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 7
5 7 before Dune::BiCGSTABSolver 
5 Dune::BiCGSTABSolver.apply(x, b, result_) 
5 BiCGSTABSolver::apply 
5 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 7
6 7 before Dune::BiCGSTABSolver 
6 Dune::BiCGSTABSolver.apply(x, b, result_) 
6 BiCGSTABSolver::apply 
6 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
4 7 before Dune::BiCGSTABSolver 
4 Dune::BiCGSTABSolver.apply(x, b, result_) 
4 BiCGSTABSolver::apply 
4 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
2 7 before Dune::BiCGSTABSolver 
2 Dune::BiCGSTABSolver.apply(x, b, result_) 
2 BiCGSTABSolver::apply 
2 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
1 7 before Dune::BiCGSTABSolver 
1 Dune::BiCGSTABSolver.apply(x, b, result_) 
1 BiCGSTABSolver::apply 
1 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
3 7 before Dune::BiCGSTABSolver 
3 Dune::BiCGSTABSolver.apply(x, b, result_) 
3 BiCGSTABSolver::apply 
3 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
Using a direct coarse solver (SuperLU)
Building hierarchy of 1 levels (inclusive coarse solver) took 0.111453 seconds.
0 7 before Dune::BiCGSTABSolver 
0 Dune::BiCGSTABSolver.apply(x, b, result_) 
0 BiCGSTABSolver::apply 
0 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
6 === BiCGSTABSolver
 Iter          Defect            Rate
    0      2.56835e-16
6  (it = 0.5; it < _maxit; it+=.5) 0.5 250
6  before p = r 
6  before  y = W^-1 * p 
6  before _prec->apply(y,p); 0.5 250
6 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
3 === BiCGSTABSolver
 Iter          Defect            Rate
    0      2.56835e-16
3  (it = 0.5; it < _maxit; it+=.5) 0.5 250
3  before p = r 
3  before  y = W^-1 * p 
3  before _prec->apply(y,p); 0.5 250
3 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
5 === BiCGSTABSolver
 Iter          Defect            Rate
    0      2.56835e-16
5  (it = 0.5; it < _maxit; it+=.5) 0.5 250
5  before p = r 
5  before  y = W^-1 * p 
5  before _prec->apply(y,p); 0.5 250
5 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
2 === BiCGSTABSolver
 Iter          Defect            Rate
    0      2.56835e-16
2  (it = 0.5; it < _maxit; it+=.5) 0.5 250
2  before p = r 
2  before  y = W^-1 * p 
2  before _prec->apply(y,p); 0.5 250
2 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
1 === BiCGSTABSolver
 Iter          Defect            Rate
    0      2.56835e-16
1  (it = 0.5; it < _maxit; it+=.5) 0.5 250
1  before p = r 
1  before  y = W^-1 * p 
1  before _prec->apply(y,p); 0.5 250
1 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
0 === BiCGSTABSolver
 Iter          Defect            Rate
    0      2.56835e-16
0  (it = 0.5; it < _maxit; it+=.5) 0.5 250
0  before p = r 
0  before  y = W^-1 * p 
0  before _prec->apply(y,p); 0.5 250
0 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 7
0 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
4 === BiCGSTABSolver
 Iter          Defect            Rate
    0      2.56835e-16
4  (it = 0.5; it < _maxit; it+=.5) 0.5 250
4  before p = r 
4  before  y = W^-1 * p 
4  before _prec->apply(y,p); 0.5 250
4 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
6 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
6 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
6  before v = A * y 
3 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
3 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
3  before v = A * y 
5 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
5 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
5  before v = A * y 
2 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
2 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
4 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
4 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
4  before v = A * y 
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
0 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
1 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
6  before  alpha = rho_new / < rt, v > 
3  before  alpha = rho_new / < rt, v > 
5  before  alpha = rho_new / < rt, v > 
1 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
4  before  alpha = rho_new / < rt, v > 
0 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
2  before v = A * y 
1  before v = A * y 
1  before  alpha = rho_new / < rt, v > 
1  before alpha = rho_new / h; 
1  before apply first correction to x 
1  before r = r - alpha*v 
1  before test stop criteria 
1 istlsolver::printOutput 
  0.5      1.61863e-34      6.30222e-19
1 === rate=3.97179e-37, T=0.122474, TIT=0.244949, IT=0.5
1 finished solveLinearSystemImpl_ 1
1 converged value: 1 7 1 let s go get convergedRemote
1 to  comm_.min(converged) 
4  before alpha = rho_new / h; 
4  before apply first correction to x 
4  before r = r - alpha*v 
4  before test stop criteria 
4 istlsolver::printOutput 
  0.5      1.61863e-34      6.30222e-19
4 === rate=3.97179e-37, T=0.122403, TIT=0.244805, IT=0.5
4 finished solveLinearSystemImpl_ 1
4 converged value: 1 7 1 let s go get convergedRemote
4 to  comm_.min(converged) 
0  before v = A * y 
0  before  alpha = rho_new / < rt, v > 
0  before alpha = rho_new / h; 
0  before apply first correction to x 
0  before r = r - alpha*v 
0  before test stop criteria 
0 istlsolver::printOutput 
  0.5      1.61863e-34      6.30222e-19
0 === rate=3.97179e-37, T=0.0185495, TIT=0.037099, IT=0.5
5  before alpha = rho_new / h; 
5  before apply first correction to x 
5  before r = r - alpha*v 
5  before test stop criteria 
5 istlsolver::printOutput 
  0.5      1.61863e-34      6.30222e-19
5 === rate=3.97179e-37, T=0.122473, TIT=0.244946, IT=0.5
5 finished solveLinearSystemImpl_ 1
5 converged value: 1 7 1 let s go get convergedRemote
5 to  comm_.min(converged) 
6  before alpha = rho_new / h; 
6  before apply first correction to x 
6  before r = r - alpha*v 
6  before test stop criteria 
6 istlsolver::printOutput 
  0.5      1.61863e-34      6.30222e-19
6 === rate=3.97179e-37, T=0.122402, TIT=0.244805, IT=0.5
6 finished solveLinearSystemImpl_ 1
6 converged value: 1 7 1 let s go get convergedRemote
6 to  comm_.min(converged) 
3  before alpha = rho_new / h; 
3  before apply first correction to x 
3  before r = r - alpha*v 
3  before test stop criteria 
3 istlsolver::printOutput 
  0.5      1.61863e-34      6.30222e-19
3 === rate=3.97179e-37, T=0.122475, TIT=0.244949, IT=0.5
3 finished solveLinearSystemImpl_ 1
3 converged value: 1 7 1 let s go get convergedRemote
3 to  comm_.min(converged) 
2  before  alpha = rho_new / < rt, v > 
2  before alpha = rho_new / h; 
2  before apply first correction to x 
2  before r = r - alpha*v 
2  before test stop criteria 
2 istlsolver::printOutput 
  0.5      1.61863e-34      6.30222e-19
2 === rate=3.97179e-37, T=0.1224, TIT=0.2448, IT=0.5
2 finished solveLinearSystemImpl_ 1
2 converged value: 1 7 1 let s go get convergedRemote
2 to  comm_.min(converged) 
3 did  comm_.min(converged) convergedRemote: 1
3 final convergedRemote: 1
3 Update: x^(k+1) = x^k - deltax^k 
3 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
3 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
3 Newton iteration  7 done3 , maximum relative shift = 7.366e-08
3 Assemble/solve/update time: 0.307922(24.9934%)/0.923955(74.9957%)/0.000134585(0.010924%)
3 gridVariables->advanceTimeStep
3 before, nonLinearSolver->suggestTimeStepSize, current time: 753.767 simTime 57600
3 before assembler->setPreviousSolution, ddt: 25
3 nonLinearSolver->solve
3 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
1 did  comm_.min(converged) convergedRemote: 1
1 final convergedRemote: 1
1 Update: x^(k+1) = x^k - deltax^k 
1 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
1 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
1 Newton iteration  7 done1 , maximum relative shift = 7.366e-08
1 Assemble/solve/update time: 0.307946(24.9954%)/0.923933(74.9938%)/0.000133724(0.0108541%)
1 gridVariables->advanceTimeStep
1 before, nonLinearSolver->suggestTimeStepSize, current time: 753.767 simTime 57600
1 before assembler->setPreviousSolution, ddt: 25
1 nonLinearSolver->solve
1 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
4 did  comm_.min(converged) convergedRemote: 1
4 final convergedRemote: 1
4 Update: x^(k+1) = x^k - deltax^k 
4 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
4 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
4 Newton iteration  7 done4 , maximum relative shift = 7.366e-08
4 Assemble/solve/update time: 0.307938(24.9949%)/0.923931(74.9942%)/0.000135025(0.0109598%)
4 gridVariables->advanceTimeStep
4 before, nonLinearSolver->suggestTimeStepSize, current time: 753.767 simTime 57600
4 before assembler->setPreviousSolution, ddt: 25
4 nonLinearSolver->solve
4 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
0 finished solveLinearSystemImpl_ 1
0 converged value: 1 7 1 let s go get convergedRemote
0 to  comm_.min(converged) 
0 did  comm_.min(converged) convergedRemote: 1
0 final convergedRemote: 1
0 Update: x^(k+1) = x^k - deltax^k 
0 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
0 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
0 Newton iteration  7 done0 , maximum relative shift = 7.366e-08
0 Assemble/solve/update time: 0.307108(24.9444%)/0.92391(75.0432%)/0.000152895(0.0124187%)
0 gridVariables->advanceTimeStep
5 did  comm_.min(converged) convergedRemote: 1
5 final convergedRemote: 1
5 Update: x^(k+1) = x^k - deltax^k 
5 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
5 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
5 Newton iteration  7 done5 , maximum relative shift = 7.366e-08
5 Assemble/solve/update time: 0.307961(24.9964%)/0.923932(74.9931%)/0.000130555(0.0105968%)
5 gridVariables->advanceTimeStep
5 before, nonLinearSolver->suggestTimeStepSize, current time: 753.767 simTime 57600
5 before assembler->setPreviousSolution, ddt: 25
5 nonLinearSolver->solve
5 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
6 did  comm_.min(converged) convergedRemote: 1
6 final convergedRemote: 1
6 Update: x^(k+1) = x^k - deltax^k 
6 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
6 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
6 Newton iteration  7 done6 , maximum relative shift = 7.366e-08
6 Assemble/solve/update time: 0.307934(24.9949%)/0.923957(74.9971%)/9.8873e-05(0.00802547%)
6 gridVariables->advanceTimeStep
6 before, nonLinearSolver->suggestTimeStepSize, current time: 753.767 simTime 57600
6 before assembler->setPreviousSolution, ddt: 25
6 nonLinearSolver->solve
6 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
2 did  comm_.min(converged) convergedRemote: 1
2 final convergedRemote: 1
2 Update: x^(k+1) = x^k - deltax^k 
2 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
2 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
2 Newton iteration  7 done2 , maximum relative shift = 7.366e-08
2 Assemble/solve/update time: 0.307901(24.9921%)/0.923954(74.9966%)/0.000139245(0.0113024%)
2 gridVariables->advanceTimeStep
2 before, nonLinearSolver->suggestTimeStepSize, current time: 753.767 simTime 57600
2 before assembler->setPreviousSolution, ddt: 25
2 nonLinearSolver->solve
2 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
0 before, nonLinearSolver->suggestTimeStepSize, current time: 753.767 simTime 57600
0 before assembler->setPreviousSolution, ddt: 25
0 nonLinearSolver->solve
0 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
6 Solve: M deltax^k = r 
6 Newton::solveLinearSystem : entering the try section 0
6to  comm_.sum(norm2) 
6did  comm_.sum(norm2) 
6 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
6 solveLinearSystemImpl_, before converged = ls.solve 
6 7 amgbackend::solve isParallel 1
3 Solve: M deltax^k = r 
3 Newton::solveLinearSystem : entering the try section 0
3to  comm_.sum(norm2) 
3did  comm_.sum(norm2) 
3 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
3 solveLinearSystemImpl_, before converged = ls.solve 
3 7 amgbackend::solve isParallel 1
1 Solve: M deltax^k = r 
1 Newton::solveLinearSystem : entering the try section 0
1to  comm_.sum(norm2) 
1did  comm_.sum(norm2) 
1 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
1 solveLinearSystemImpl_, before converged = ls.solve 
1 7 amgbackend::solve isParallel 1
4 Solve: M deltax^k = r 
4 Newton::solveLinearSystem : entering the try section 0
4to  comm_.sum(norm2) 
4did  comm_.sum(norm2) 
4 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
4 solveLinearSystemImpl_, before converged = ls.solve 
4 7 amgbackend::solve isParallel 1
2 Solve: M deltax^k = r 
2 Newton::solveLinearSystem : entering the try section 0
2to  comm_.sum(norm2) 
2did  comm_.sum(norm2) 
2 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
2 solveLinearSystemImpl_, before converged = ls.solve 
2 7 amgbackend::solve isParallel 1
5 Solve: M deltax^k = r 
5 Newton::solveLinearSystem : entering the try section 0
5to  comm_.sum(norm2) 
5did  comm_.sum(norm2) 
5 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
5 solveLinearSystemImpl_, before converged = ls.solve 
5 7 amgbackend::solve isParallel 1
0 Solve: M deltax^k = r 
0 Newton::solveLinearSystem : entering the try section 0
0to  comm_.sum(norm2) 
0did  comm_.sum(norm2) 
0 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
0 solveLinearSystemImpl_, before converged = ls.solve 
0 7 amgbackend::solve isParallel 1
0 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
0 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 7
5 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
5 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
6 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
6 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
3 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
3 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
1 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
1 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
4 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
4 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
2 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
2 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 7
5 7 before Dune::BiCGSTABSolver 
5 Dune::BiCGSTABSolver.apply(x, b, result_) 
5 BiCGSTABSolver::apply 
5 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
6 7 before Dune::BiCGSTABSolver 
6 Dune::BiCGSTABSolver.apply(x, b, result_) 
6 BiCGSTABSolver::apply 
6 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
3 7 before Dune::BiCGSTABSolver 
3 Dune::BiCGSTABSolver.apply(x, b, result_) 
3 BiCGSTABSolver::apply 
3 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
1 7 before Dune::BiCGSTABSolver 
1 Dune::BiCGSTABSolver.apply(x, b, result_) 
1 BiCGSTABSolver::apply 
1 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
4 7 before Dune::BiCGSTABSolver 
4 Dune::BiCGSTABSolver.apply(x, b, result_) 
4 BiCGSTABSolver::apply 
4 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 7
2 7 before Dune::BiCGSTABSolver 
2 Dune::BiCGSTABSolver.apply(x, b, result_) 
2 BiCGSTABSolver::apply 
2 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
Using a direct coarse solver (SuperLU)
Building hierarchy of 1 levels (inclusive coarse solver) took 0.111524 seconds.
0 7 before Dune::BiCGSTABSolver 
0 Dune::BiCGSTABSolver.apply(x, b, result_) 
0 BiCGSTABSolver::apply 
0 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
0 === BiCGSTABSolver
 Iter          Defect            Rate
1 === BiCGSTABSolver
 Iter          Defect            Rate
    0      3.16111e-09
1  (it = 0.5; it < _maxit; it+=.5) 0.5 250
1  before p = r 
1  before  y = W^-1 * p 
1  before _prec->apply(y,p); 0.5 250
1 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
4 === BiCGSTABSolver
 Iter          Defect            Rate
    0      3.16111e-09
4  (it = 0.5; it < _maxit; it+=.5) 0.5 250
4  before p = r 
4  before  y = W^-1 * p 
4  before _prec->apply(y,p); 0.5 250
4 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
2 === BiCGSTABSolver
 Iter          Defect            Rate
    0      3.16111e-09
2  (it = 0.5; it < _maxit; it+=.5) 0.5 250
2  before p = r 
2  before  y = W^-1 * p 
2  before _prec->apply(y,p); 0.5 250
2 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
6 === BiCGSTABSolver
 Iter          Defect            Rate
    0      3.16111e-09
6  (it = 0.5; it < _maxit; it+=.5) 0.5 250
6  before p = r 
6  before  y = W^-1 * p 
6  before _prec->apply(y,p); 0.5 250
6 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
5 === BiCGSTABSolver
 Iter          Defect            Rate
    0      3.16111e-09
5  (it = 0.5; it < _maxit; it+=.5) 0.5 250
5  before p = r 
5  before  y = W^-1 * p 
5  before _prec->apply(y,p); 0.5 250
5 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
3 === BiCGSTABSolver
 Iter          Defect            Rate
    0      3.16111e-09
3  (it = 0.5; it < _maxit; it+=.5) 0.5 250
3  before p = r 
3  before  y = W^-1 * p 
3  before _prec->apply(y,p); 0.5 250
3 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
    0      3.16111e-09
0  (it = 0.5; it < _maxit; it+=.5) 0.5 250
0  before p = r 
0  before  y = W^-1 * p 
0  before _prec->apply(y,p); 0.5 250
0 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 7
0 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
4 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
4 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
4  before v = A * y 
2 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
2 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
6 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
6 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
6  before v = A * y 
5 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
5 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
5  before v = A * y 
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
0 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
3 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
3 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
3  before v = A * y 
1 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
0 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
3  before  alpha = rho_new / < rt, v > 
5  before  alpha = rho_new / < rt, v > 
4  before  alpha = rho_new / < rt, v > 
2  before v = A * y 
2  before  alpha = rho_new / < rt, v > 
6  before  alpha = rho_new / < rt, v > 
1 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
6  before alpha = rho_new / h; 
6  before apply first correction to x 
6  before r = r - alpha*v 
6  before test stop criteria 
6 istlsolver::printOutput 
  0.5      8.32193e-25       2.6326e-16
6 === rate=6.9306e-32, T=0.122237, TIT=0.244473, IT=0.5
6 finished solveLinearSystemImpl_ 1
6 converged value: 1 7 1 let s go get convergedRemote
6 to  comm_.min(converged) 
0  before v = A * y 
0  before  alpha = rho_new / < rt, v > 
0  before alpha = rho_new / h; 
0  before apply first correction to x 
0  before r = r - alpha*v 
0  before test stop criteria 
0 istlsolver::printOutput 
  0.5      8.32193e-25       2.6326e-16
0 === rate=6.9306e-32, T=0.0184745, TIT=0.0369491, IT=0.5
3  before alpha = rho_new / h; 
3  before apply first correction to x 
3  before r = r - alpha*v 
3  before test stop criteria 
3 istlsolver::printOutput 
  0.5      8.32193e-25       2.6326e-16
3 === rate=6.9306e-32, T=0.122319, TIT=0.244637, IT=0.5
3 finished solveLinearSystemImpl_ 1
3 converged value: 1 7 1 let s go get convergedRemote
3 to  comm_.min(converged) 
5  before alpha = rho_new / h; 
5  before apply first correction to x 
5  before r = r - alpha*v 
5  before test stop criteria 
5 istlsolver::printOutput 
  0.5      8.32193e-25       2.6326e-16
5 === rate=6.9306e-32, T=0.122317, TIT=0.244634, IT=0.5
5 finished solveLinearSystemImpl_ 1
5 converged value: 1 7 1 let s go get convergedRemote
5 to  comm_.min(converged) 
4  before alpha = rho_new / h; 
4  before apply first correction to x 
4  before r = r - alpha*v 
4  before test stop criteria 
4 istlsolver::printOutput 
  0.5      8.32193e-25       2.6326e-16
4 === rate=6.9306e-32, T=0.122237, TIT=0.244474, IT=0.5
4 finished solveLinearSystemImpl_ 1
4 converged value: 1 7 1 let s go get convergedRemote
4 to  comm_.min(converged) 
2  before alpha = rho_new / h; 
2  before apply first correction to x 
2  before r = r - alpha*v 
2  before test stop criteria 
2 istlsolver::printOutput 
  0.5      8.32193e-25       2.6326e-16
2 === rate=6.9306e-32, T=0.122237, TIT=0.244474, IT=0.5
2 finished solveLinearSystemImpl_ 1
2 converged value: 1 7 1 let s go get convergedRemote
2 to  comm_.min(converged) 
1  before v = A * y 
1  before  alpha = rho_new / < rt, v > 
1  before alpha = rho_new / h; 
1  before apply first correction to x 
1  before r = r - alpha*v 
1  before test stop criteria 
1 istlsolver::printOutput 
  0.5      8.32193e-25       2.6326e-16
1 === rate=6.9306e-32, T=0.122315, TIT=0.244629, IT=0.5
1 finished solveLinearSystemImpl_ 1
1 converged value: 1 7 1 let s go get convergedRemote
1 to  comm_.min(converged) 
1 did  comm_.min(converged) convergedRemote: 1
1 final convergedRemote: 1
1 Update: x^(k+1) = x^k - deltax^k 
1 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
1 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
1 Newton iteration  1 done1 , maximum relative shift = 7.996e-02
1 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
2 did  comm_.min(converged) convergedRemote: 1
2 final convergedRemote: 1
2 Update: x^(k+1) = x^k - deltax^k 
2 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
2 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
2 Newton iteration  1 done2 , maximum relative shift = 7.996e-02
2 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
6 did  comm_.min(converged) convergedRemote: 1
6 final convergedRemote: 1
6 Update: x^(k+1) = x^k - deltax^k 
6 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
6 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
6 Newton iteration  1 done6 , maximum relative shift = 7.996e-02
6 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
0 finished solveLinearSystemImpl_ 1
0 converged value: 1 7 1 let s go get convergedRemote
0 to  comm_.min(converged) 
0 did  comm_.min(converged) convergedRemote: 1
0 final convergedRemote: 1
0 Update: x^(k+1) = x^k - deltax^k 
0 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
0 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
0 Newton iteration  1 done0 , maximum relative shift = 7.996e-02
0 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
3 did  comm_.min(converged) convergedRemote: 1
3 final convergedRemote: 1
3 Update: x^(k+1) = x^k - deltax^k 
3 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
3 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
3 Newton iteration  1 done3 , maximum relative shift = 7.996e-02
3 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
5 did  comm_.min(converged) convergedRemote: 1
5 final convergedRemote: 1
5 Update: x^(k+1) = x^k - deltax^k 
5 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
5 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
5 Newton iteration  1 done5 , maximum relative shift = 7.996e-02
5 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
4 did  comm_.min(converged) convergedRemote: 1
4 final convergedRemote: 1
4 Update: x^(k+1) = x^k - deltax^k 
4 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
4 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
4 Newton iteration  1 done4 , maximum relative shift = 7.996e-02
4 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
1 Solve: M deltax^k = r 
1 Newton::solveLinearSystem : entering the try section 1
1 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
1 solveLinearSystemImpl_, before converged = ls.solve 
1 7 amgbackend::solve isParallel 1
2 Solve: M deltax^k = r 
2 Newton::solveLinearSystem : entering the try section 1
2 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
2 solveLinearSystemImpl_, before converged = ls.solve 
2 7 amgbackend::solve isParallel 1
6 Solve: M deltax^k = r 
6 Newton::solveLinearSystem : entering the try section 1
6 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
6 solveLinearSystemImpl_, before converged = ls.solve 
6 7 amgbackend::solve isParallel 1
0 Solve: M deltax^k = r 
0 Newton::solveLinearSystem : entering the try section 1
0 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
0 solveLinearSystemImpl_, before converged = ls.solve 
0 7 amgbackend::solve isParallel 1
3 Solve: M deltax^k = r 
3 Newton::solveLinearSystem : entering the try section 1
3 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
3 solveLinearSystemImpl_, before converged = ls.solve 
3 7 amgbackend::solve isParallel 1
5 Solve: M deltax^k = r 
5 Newton::solveLinearSystem : entering the try section 1
5 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
5 solveLinearSystemImpl_, before converged = ls.solve 
5 7 amgbackend::solve isParallel 1
4 Solve: M deltax^k = r 
4 Newton::solveLinearSystem : entering the try section 1
4 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
4 solveLinearSystemImpl_, before converged = ls.solve 
4 7 amgbackend::solve isParallel 1
4 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
4 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
5 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
5 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
1 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
1 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
2 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
2 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
6 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
6 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
0 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
0 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 7
3 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
3 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 7
2 7 before Dune::BiCGSTABSolver 
2 Dune::BiCGSTABSolver.apply(x, b, result_) 
2 BiCGSTABSolver::apply 
2 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
6 7 before Dune::BiCGSTABSolver 
6 Dune::BiCGSTABSolver.apply(x, b, result_) 
6 BiCGSTABSolver::apply 
6 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
3 7 before Dune::BiCGSTABSolver 
3 Dune::BiCGSTABSolver.apply(x, b, result_) 
3 BiCGSTABSolver::apply 
3 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 7
4 7 before Dune::BiCGSTABSolver 
4 Dune::BiCGSTABSolver.apply(x, b, result_) 
4 BiCGSTABSolver::apply 
4 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
5 7 before Dune::BiCGSTABSolver 
5 Dune::BiCGSTABSolver.apply(x, b, result_) 
5 BiCGSTABSolver::apply 
5 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
1 7 before Dune::BiCGSTABSolver 
1 Dune::BiCGSTABSolver.apply(x, b, result_) 
1 BiCGSTABSolver::apply 
1 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
Using a direct coarse solver (SuperLU)
Building hierarchy of 1 levels (inclusive coarse solver) took 0.112135 seconds.
0 7 before Dune::BiCGSTABSolver 
0 Dune::BiCGSTABSolver.apply(x, b, result_) 
0 BiCGSTABSolver::apply 
0 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
0 === BiCGSTABSolver
 Iter          Defect            Rate
    0       2.0335e-11
0  (it = 0.5; it < _maxit; it+=.5) 0.5 250
1 === BiCGSTABSolver
 Iter          Defect            Rate
    0       2.0335e-11
1  (it = 0.5; it < _maxit; it+=.5) 0.5 250
1  before p = r 
1  before  y = W^-1 * p 
1  before _prec->apply(y,p); 0.5 250
1 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
4 === BiCGSTABSolver
 Iter          Defect            Rate
    0       2.0335e-11
4  (it = 0.5; it < _maxit; it+=.5) 0.5 250
4  before p = r 
4  before  y = W^-1 * p 
4  before _prec->apply(y,p); 0.5 250
4 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
5 === BiCGSTABSolver
 Iter          Defect            Rate
    0       2.0335e-11
5  (it = 0.5; it < _maxit; it+=.5) 0.5 250
5  before p = r 
5  before  y = W^-1 * p 
5  before _prec->apply(y,p); 0.5 250
5 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
3 === BiCGSTABSolver
 Iter          Defect            Rate
    0       2.0335e-11
3  (it = 0.5; it < _maxit; it+=.5) 0.5 250
3  before p = r 
3  before  y = W^-1 * p 
3  before _prec->apply(y,p); 0.5 250
3 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
2 === BiCGSTABSolver
 Iter          Defect            Rate
    0       2.0335e-11
2  (it = 0.5; it < _maxit; it+=.5) 0.5 250
2  before p = r 
2  before  y = W^-1 * p 
2  before _prec->apply(y,p); 0.5 250
2 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
6 === BiCGSTABSolver
 Iter          Defect            Rate
    0       2.0335e-11
6  (it = 0.5; it < _maxit; it+=.5) 0.5 250
6  before p = r 
6  before  y = W^-1 * p 
6  before _prec->apply(y,p); 0.5 250
6 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
0  before p = r 
0  before  y = W^-1 * p 
0  before _prec->apply(y,p); 0.5 250
0 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 7
0 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
4 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
4 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
4  before v = A * y 
5 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
5 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
5  before v = A * y 
3 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
3 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
3  before v = A * y 
2 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
2 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
0 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
6 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
6 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
6  before v = A * y 
6  before  alpha = rho_new / < rt, v > 
1 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
0 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
0  before v = A * y 
0  before  alpha = rho_new / < rt, v > 
2  before v = A * y 
2  before  alpha = rho_new / < rt, v > 
4  before  alpha = rho_new / < rt, v > 
5  before  alpha = rho_new / < rt, v > 
3  before  alpha = rho_new / < rt, v > 
1 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
1  before v = A * y 
1  before  alpha = rho_new / < rt, v > 
1  before alpha = rho_new / h; 
1  before apply first correction to x 
1  before r = r - alpha*v 
1  before test stop criteria 
1 istlsolver::printOutput 
  0.5      1.49958e-27      7.37439e-17
1 === rate=5.43817e-33, T=0.122679, TIT=0.245358, IT=0.5
1 finished solveLinearSystemImpl_ 1
1 converged value: 1 7 1 let s go get convergedRemote
1 to  comm_.min(converged) 
3  before alpha = rho_new / h; 
3  before apply first correction to x 
3  before r = r - alpha*v 
3  before test stop criteria 
3 istlsolver::printOutput 
  0.5      1.49958e-27      7.37439e-17
3 === rate=5.43817e-33, T=0.12268, TIT=0.245359, IT=0.5
3 finished solveLinearSystemImpl_ 1
3 converged value: 1 7 1 let s go get convergedRemote
3 to  comm_.min(converged) 
6  before alpha = rho_new / h; 
6  before apply first correction to x 
6  before r = r - alpha*v 
6  before test stop criteria 
6 istlsolver::printOutput 
  0.5      1.49958e-27      7.37439e-17
6 === rate=5.43817e-33, T=0.122636, TIT=0.245272, IT=0.5
6 finished solveLinearSystemImpl_ 1
6 converged value: 1 7 1 let s go get convergedRemote
6 to  comm_.min(converged) 
0  before alpha = rho_new / h; 
0  before apply first correction to x 
0  before r = r - alpha*v 
0  before test stop criteria 
0 istlsolver::printOutput 
  0.5      1.49958e-27      7.37439e-17
0 === rate=5.43817e-33, T=0.0182841, TIT=0.0365683, IT=0.5
2  before alpha = rho_new / h; 
2  before apply first correction to x 
2  before r = r - alpha*v 
2  before test stop criteria 
2 istlsolver::printOutput 
  0.5      1.49958e-27      7.37439e-17
2 === rate=5.43817e-33, T=0.122632, TIT=0.245264, IT=0.5
2 finished solveLinearSystemImpl_ 1
2 converged value: 1 7 1 let s go get convergedRemote
2 to  comm_.min(converged) 
4  before alpha = rho_new / h; 
4  before apply first correction to x 
4  before r = r - alpha*v 
4  before test stop criteria 
4 istlsolver::printOutput 
  0.5      1.49958e-27      7.37439e-17
4 === rate=5.43817e-33, T=0.122642, TIT=0.245283, IT=0.5
4 finished solveLinearSystemImpl_ 1
4 converged value: 1 7 1 let s go get convergedRemote
4 to  comm_.min(converged) 
5  before alpha = rho_new / h; 
5  before apply first correction to x 
5  before r = r - alpha*v 
5  before test stop criteria 
5 istlsolver::printOutput 
  0.5      1.49958e-27      7.37439e-17
5 === rate=5.43817e-33, T=0.122678, TIT=0.245355, IT=0.5
5 finished solveLinearSystemImpl_ 1
5 converged value: 1 7 1 let s go get convergedRemote
5 to  comm_.min(converged) 
3 did  comm_.min(converged) convergedRemote: 1
3 final convergedRemote: 1
3 Update: x^(k+1) = x^k - deltax^k 
3 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
3 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
3 Newton iteration  2 done3 , maximum relative shift = 4.046e-03
3 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
6 did  comm_.min(converged) convergedRemote: 1
6 final convergedRemote: 1
6 Update: x^(k+1) = x^k - deltax^k 
6 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
6 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
6 Newton iteration  2 done6 , maximum relative shift = 4.046e-03
6 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
0 finished solveLinearSystemImpl_ 1
0 converged value: 1 7 1 let s go get convergedRemote
0 to  comm_.min(converged) 
0 did  comm_.min(converged) convergedRemote: 1
0 final convergedRemote: 1
0 Update: x^(k+1) = x^k - deltax^k 
0 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
0 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
0 Newton iteration  2 done0 , maximum relative shift = 4.046e-03
0 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
2 did  comm_.min(converged) convergedRemote: 1
2 final convergedRemote: 1
2 Update: x^(k+1) = x^k - deltax^k 
2 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
2 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
2 Newton iteration  2 done2 , maximum relative shift = 4.046e-03
2 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
4 did  comm_.min(converged) convergedRemote: 1
4 final convergedRemote: 1
4 Update: x^(k+1) = x^k - deltax^k 
4 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
4 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
4 Newton iteration  2 done4 , maximum relative shift = 4.046e-03
4 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
5 did  comm_.min(converged) convergedRemote: 1
5 final convergedRemote: 1
5 Update: x^(k+1) = x^k - deltax^k 
5 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
5 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
5 Newton iteration  2 done5 , maximum relative shift = 4.046e-03
5 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
1 did  comm_.min(converged) convergedRemote: 1
1 final convergedRemote: 1
1 Update: x^(k+1) = x^k - deltax^k 
1 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
1 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
1 Newton iteration  2 done1 , maximum relative shift = 4.046e-03
1 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
6 Solve: M deltax^k = r 
6 Newton::solveLinearSystem : entering the try section 2
6 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
6 solveLinearSystemImpl_, before converged = ls.solve 
6 7 amgbackend::solve isParallel 1
0 Solve: M deltax^k = r 
0 Newton::solveLinearSystem : entering the try section 2
0 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
0 solveLinearSystemImpl_, before converged = ls.solve 
0 7 amgbackend::solve isParallel 1
2 Solve: M deltax^k = r 
2 Newton::solveLinearSystem : entering the try section 2
2 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
2 solveLinearSystemImpl_, before converged = ls.solve 
2 7 amgbackend::solve isParallel 1
1 Solve: M deltax^k = r 
1 Newton::solveLinearSystem : entering the try section 2
1 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
1 solveLinearSystemImpl_, before converged = ls.solve 
1 7 amgbackend::solve isParallel 1
5 Solve: M deltax^k = r 
5 Newton::solveLinearSystem : entering the try section 2
5 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
5 solveLinearSystemImpl_, before converged = ls.solve 
5 7 amgbackend::solve isParallel 1
4 Solve: M deltax^k = r 
4 Newton::solveLinearSystem : entering the try section 2
4 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
4 solveLinearSystemImpl_, before converged = ls.solve 
4 7 amgbackend::solve isParallel 1
3 Solve: M deltax^k = r 
3 Newton::solveLinearSystem : entering the try section 2
3 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
3 solveLinearSystemImpl_, before converged = ls.solve 
3 7 amgbackend::solve isParallel 1
1 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
1 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
5 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
5 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
4 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
4 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
2 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
2 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
6 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
6 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
0 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
0 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 7
3 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
3 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 7
3 7 before Dune::BiCGSTABSolver 
3 Dune::BiCGSTABSolver.apply(x, b, result_) 
3 BiCGSTABSolver::apply 
3 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
1 7 before Dune::BiCGSTABSolver 
1 Dune::BiCGSTABSolver.apply(x, b, result_) 
1 BiCGSTABSolver::apply 
1 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
5 7 before Dune::BiCGSTABSolver 
5 Dune::BiCGSTABSolver.apply(x, b, result_) 
5 BiCGSTABSolver::apply 
5 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
4 7 before Dune::BiCGSTABSolver 
4 Dune::BiCGSTABSolver.apply(x, b, result_) 
4 BiCGSTABSolver::apply 
4 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
2 7 before Dune::BiCGSTABSolver 
2 Dune::BiCGSTABSolver.apply(x, b, result_) 
2 BiCGSTABSolver::apply 
2 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
6 7 before Dune::BiCGSTABSolver 
6 Dune::BiCGSTABSolver.apply(x, b, result_) 
6 BiCGSTABSolver::apply 
6 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 7
Using a direct coarse solver (SuperLU)
Building hierarchy of 1 levels (inclusive coarse solver) took 0.113116 seconds.
0 7 before Dune::BiCGSTABSolver 
0 Dune::BiCGSTABSolver.apply(x, b, result_) 
0 BiCGSTABSolver::apply 
0 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
0 === BiCGSTABSolver
 Iter          Defect            Rate
    0      7.14855e-13
0  (it = 0.5; it < _maxit; it+=.5) 0.5 250
0  before p = r 
0  before  y = W^-1 * p 
0  before _prec->apply(y,p); 0.5 250
0 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 7
0 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
6 === BiCGSTABSolver
 Iter          Defect            Rate
    0      7.14855e-13
6  (it = 0.5; it < _maxit; it+=.5) 0.5 250
6  before p = r 
6  before  y = W^-1 * p 
6  before _prec->apply(y,p); 0.5 250
6 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
3 === BiCGSTABSolver
 Iter          Defect            Rate
    0      7.14855e-13
3  (it = 0.5; it < _maxit; it+=.5) 0.5 250
3  before p = r 
3  before  y = W^-1 * p 
3  before _prec->apply(y,p); 0.5 250
3 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
1 === BiCGSTABSolver
 Iter          Defect            Rate
    0      7.14855e-13
1  (it = 0.5; it < _maxit; it+=.5) 0.5 250
1  before p = r 
1  before  y = W^-1 * p 
1  before _prec->apply(y,p); 0.5 250
1 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
5 === BiCGSTABSolver
 Iter          Defect            Rate
    0      7.14855e-13
5  (it = 0.5; it < _maxit; it+=.5) 0.5 250
5  before p = r 
5  before  y = W^-1 * p 
5  before _prec->apply(y,p); 0.5 250
5 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
4 === BiCGSTABSolver
 Iter          Defect            Rate
    0      7.14855e-13
4  (it = 0.5; it < _maxit; it+=.5) 0.5 250
4  before p = r 
4  before  y = W^-1 * p 
4  before _prec->apply(y,p); 0.5 250
4 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
2 === BiCGSTABSolver
 Iter          Defect            Rate
    0      7.14855e-13
2  (it = 0.5; it < _maxit; it+=.5) 0.5 250
2  before p = r 
2  before  y = W^-1 * p 
2  before _prec->apply(y,p); 0.5 250
2 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
5 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
5 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
5  before v = A * y 
4 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
4 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
4  before v = A * y 
2 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
2 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
0 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
6 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
6 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
6  before v = A * y 
3 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
3 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
3  before v = A * y 
1 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
0 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
6  before  alpha = rho_new / < rt, v > 
3  before  alpha = rho_new / < rt, v > 
4  before  alpha = rho_new / < rt, v > 
5  before  alpha = rho_new / < rt, v > 
1 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
2  before v = A * y 
2  before  alpha = rho_new / < rt, v > 
2  before alpha = rho_new / h; 
2  before apply first correction to x 
2  before r = r - alpha*v 
2  before test stop criteria 
2 istlsolver::printOutput 
  0.5      8.75287e-31      1.22443e-18
2 === rate=1.49922e-36, T=0.126724, TIT=0.253448, IT=0.5
2 finished solveLinearSystemImpl_ 1
2 converged value: 1 7 1 let s go get convergedRemote
2 to  comm_.min(converged) 
5  before alpha = rho_new / h; 
5  before apply first correction to x 
5  before r = r - alpha*v 
5  before test stop criteria 
5 istlsolver::printOutput 
  0.5      8.75287e-31      1.22443e-18
5 === rate=1.49922e-36, T=0.126744, TIT=0.253488, IT=0.5
5 finished solveLinearSystemImpl_ 1
5 converged value: 1 7 1 let s go get convergedRemote
5 to  comm_.min(converged) 
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
0  before v = A * y 
0  before  alpha = rho_new / < rt, v > 
0  before alpha = rho_new / h; 
0  before apply first correction to x 
0  before r = r - alpha*v 
0  before test stop criteria 
0 istlsolver::printOutput 
  0.5      8.75287e-31      1.22443e-18
0 === rate=1.49922e-36, T=0.0217158, TIT=0.0434317, IT=0.5
6  before alpha = rho_new / h; 
6  before apply first correction to x 
6  before r = r - alpha*v 
6  before test stop criteria 
6 istlsolver::printOutput 
  0.5      8.75287e-31      1.22443e-18
6 === rate=1.49922e-36, T=0.126726, TIT=0.253453, IT=0.5
6 finished solveLinearSystemImpl_ 1
6 converged value: 1 7 1 let s go get convergedRemote
6 to  comm_.min(converged) 
3  before alpha = rho_new / h; 
3  before apply first correction to x 
3  before r = r - alpha*v 
3  before test stop criteria 
3 istlsolver::printOutput 
  0.5      8.75287e-31      1.22443e-18
3 === rate=1.49922e-36, T=0.126755, TIT=0.253511, IT=0.5
3 finished solveLinearSystemImpl_ 1
3 converged value: 1 7 1 let s go get convergedRemote
3 to  comm_.min(converged) 
4  before alpha = rho_new / h; 
4  before apply first correction to x 
4  before r = r - alpha*v 
4  before test stop criteria 
4 istlsolver::printOutput 
  0.5      8.75287e-31      1.22443e-18
4 === rate=1.49922e-36, T=0.126724, TIT=0.253449, IT=0.5
4 finished solveLinearSystemImpl_ 1
4 converged value: 1 7 1 let s go get convergedRemote
4 to  comm_.min(converged) 
1  before v = A * y 
1  before  alpha = rho_new / < rt, v > 
1  before alpha = rho_new / h; 
1  before apply first correction to x 
1  before r = r - alpha*v 
1  before test stop criteria 
1 istlsolver::printOutput 
  0.5      8.75287e-31      1.22443e-18
1 === rate=1.49922e-36, T=0.126742, TIT=0.253485, IT=0.5
1 finished solveLinearSystemImpl_ 1
1 converged value: 1 7 1 let s go get convergedRemote
1 to  comm_.min(converged) 
5 did  comm_.min(converged) convergedRemote: 1
5 final convergedRemote: 1
5 Update: x^(k+1) = x^k - deltax^k 
5 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
5 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
5 Newton iteration  3 done5 , maximum relative shift = 1.438e-04
5 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
0 finished solveLinearSystemImpl_ 1
0 converged value: 1 7 1 let s go get convergedRemote
0 to  comm_.min(converged) 
0 did  comm_.min(converged) convergedRemote: 1
0 final convergedRemote: 1
0 Update: x^(k+1) = x^k - deltax^k 
0 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
0 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
0 Newton iteration  3 done0 , maximum relative shift = 1.438e-04
0 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
6 did  comm_.min(converged) convergedRemote: 1
6 final convergedRemote: 1
6 Update: x^(k+1) = x^k - deltax^k 
6 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
6 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
6 Newton iteration  3 done6 , maximum relative shift = 1.438e-04
6 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
3 did  comm_.min(converged) convergedRemote: 1
3 final convergedRemote: 1
3 Update: x^(k+1) = x^k - deltax^k 
3 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
3 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
3 Newton iteration  3 done3 , maximum relative shift = 1.438e-04
3 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
1 did  comm_.min(converged) convergedRemote: 1
1 final convergedRemote: 1
1 Update: x^(k+1) = x^k - deltax^k 
1 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
1 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
1 Newton iteration  3 done1 , maximum relative shift = 1.438e-04
1 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
4 did  comm_.min(converged) convergedRemote: 1
4 final convergedRemote: 1
4 Update: x^(k+1) = x^k - deltax^k 
4 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
4 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
4 Newton iteration  3 done4 , maximum relative shift = 1.438e-04
4 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
2 did  comm_.min(converged) convergedRemote: 1
2 final convergedRemote: 1
2 Update: x^(k+1) = x^k - deltax^k 
2 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
2 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
2 Newton iteration  3 done2 , maximum relative shift = 1.438e-04
2 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
6 Solve: M deltax^k = r 
6 Newton::solveLinearSystem : entering the try section 3
6 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
6 solveLinearSystemImpl_, before converged = ls.solve 
6 7 amgbackend::solve isParallel 1
2 Solve: M deltax^k = r 
2 Newton::solveLinearSystem : entering the try section 3
2 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
2 solveLinearSystemImpl_, before converged = ls.solve 
2 7 amgbackend::solve isParallel 1
1 Solve: M deltax^k = r 
1 Newton::solveLinearSystem : entering the try section 3
1 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
1 solveLinearSystemImpl_, before converged = ls.solve 
1 7 amgbackend::solve isParallel 1
4 Solve: M deltax^k = r 
4 Newton::solveLinearSystem : entering the try section 3
4 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
4 solveLinearSystemImpl_, before converged = ls.solve 
4 7 amgbackend::solve isParallel 1
3 Solve: M deltax^k = r 
3 Newton::solveLinearSystem : entering the try section 3
3 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
3 solveLinearSystemImpl_, before converged = ls.solve 
3 7 amgbackend::solve isParallel 1
5 Solve: M deltax^k = r 
5 Newton::solveLinearSystem : entering the try section 3
5 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
5 solveLinearSystemImpl_, before converged = ls.solve 
5 7 amgbackend::solve isParallel 1
0 Solve: M deltax^k = r 
0 Newton::solveLinearSystem : entering the try section 3
0 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
0 solveLinearSystemImpl_, before converged = ls.solve 
0 7 amgbackend::solve isParallel 1
6 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
6 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
0 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
0 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 7
2 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
2 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
1 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
1 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
4 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
4 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
3 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
3 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
5 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
5 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 7
5 7 before Dune::BiCGSTABSolver 
5 Dune::BiCGSTABSolver.apply(x, b, result_) 
5 BiCGSTABSolver::apply 
5 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
2 7 before Dune::BiCGSTABSolver 
2 Dune::BiCGSTABSolver.apply(x, b, result_) 
2 BiCGSTABSolver::apply 
2 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
1 7 before Dune::BiCGSTABSolver 
1 Dune::BiCGSTABSolver.apply(x, b, result_) 
1 BiCGSTABSolver::apply 
1 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
4 7 before Dune::BiCGSTABSolver 
4 Dune::BiCGSTABSolver.apply(x, b, result_) 
4 BiCGSTABSolver::apply 
4 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
3 7 before Dune::BiCGSTABSolver 
3 Dune::BiCGSTABSolver.apply(x, b, result_) 
3 BiCGSTABSolver::apply 
3 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
6 7 before Dune::BiCGSTABSolver 
6 Dune::BiCGSTABSolver.apply(x, b, result_) 
6 BiCGSTABSolver::apply 
6 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
Using a direct coarse solver (SuperLU)
Building hierarchy of 1 levels (inclusive coarse solver) took 0.112516 seconds.
0 7 before Dune::BiCGSTABSolver 
0 Dune::BiCGSTABSolver.apply(x, b, result_) 
0 BiCGSTABSolver::apply 
0 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
0 === BiCGSTABSolver
 Iter          Defect            Rate
3 === BiCGSTABSolver
 Iter          Defect            Rate
    0      2.75778e-14
3  (it = 0.5; it < _maxit; it+=.5) 0.5 250
3  before p = r 
3  before  y = W^-1 * p 
3  before _prec->apply(y,p); 0.5 250
3 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
    0      2.75778e-14
0  (it = 0.5; it < _maxit; it+=.5) 0.5 250
0  before p = r 
0  before  y = W^-1 * p 
0  before _prec->apply(y,p); 0.5 250
0 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 7
0 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
6 === BiCGSTABSolver
 Iter          Defect            Rate
    0      2.75778e-14
6  (it = 0.5; it < _maxit; it+=.5) 0.5 250
6  before p = r 
6  before  y = W^-1 * p 
6  before _prec->apply(y,p); 0.5 250
6 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
5 === BiCGSTABSolver
 Iter          Defect            Rate
    0      2.75778e-14
5  (it = 0.5; it < _maxit; it+=.5) 0.5 250
5  before p = r 
5  before  y = W^-1 * p 
5  before _prec->apply(y,p); 0.5 250
5 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
2 === BiCGSTABSolver
 Iter          Defect            Rate
    0      2.75778e-14
2  (it = 0.5; it < _maxit; it+=.5) 0.5 250
2  before p = r 
2  before  y = W^-1 * p 
2  before _prec->apply(y,p); 0.5 250
2 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
1 === BiCGSTABSolver
 Iter          Defect            Rate
    0      2.75778e-14
1  (it = 0.5; it < _maxit; it+=.5) 0.5 250
1  before p = r 
1  before  y = W^-1 * p 
1  before _prec->apply(y,p); 0.5 250
1 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
4 === BiCGSTABSolver
 Iter          Defect            Rate
    0      2.75778e-14
4  (it = 0.5; it < _maxit; it+=.5) 0.5 250
4  before p = r 
4  before  y = W^-1 * p 
4  before _prec->apply(y,p); 0.5 250
4 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
6 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
6 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
6  before v = A * y 
5 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
5 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
5  before v = A * y 
2 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
2 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
1 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
3 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
3 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
3  before v = A * y 
4 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
4 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
4  before v = A * y 
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
0 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
4  before  alpha = rho_new / < rt, v > 
3  before  alpha = rho_new / < rt, v > 
6  before  alpha = rho_new / < rt, v > 
5  before  alpha = rho_new / < rt, v > 
0 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
0  before v = A * y 
0  before  alpha = rho_new / < rt, v > 
0  before alpha = rho_new / h; 
0  before apply first correction to x 
0  before r = r - alpha*v 
0  before test stop criteria 
0 istlsolver::printOutput 
  0.5      3.15547e-30      1.14421e-16
0 === rate=1.30921e-32, T=0.0185938, TIT=0.0371876, IT=0.5
4  before alpha = rho_new / h; 
4  before apply first correction to x 
4  before r = r - alpha*v 
4  before test stop criteria 
4 istlsolver::printOutput 
  0.5      3.15547e-30      1.14421e-16
4 === rate=1.30921e-32, T=0.12348, TIT=0.246961, IT=0.5
4 finished solveLinearSystemImpl_ 1
4 converged value: 1 7 1 let s go get convergedRemote
4 to  comm_.min(converged) 
3  before alpha = rho_new / h; 
3  before apply first correction to x 
3  before r = r - alpha*v 
3  before test stop criteria 
3 istlsolver::printOutput 
  0.5      3.15547e-30      1.14421e-16
3 === rate=1.30921e-32, T=0.12339, TIT=0.246779, IT=0.5
3 finished solveLinearSystemImpl_ 1
3 converged value: 1 7 1 let s go get convergedRemote
3 to  comm_.min(converged) 
6  before alpha = rho_new / h; 
6  before apply first correction to x 
6  before r = r - alpha*v 
6  before test stop criteria 
6 istlsolver::printOutput 
  0.5      3.15547e-30      1.14421e-16
6 === rate=1.30921e-32, T=0.123472, TIT=0.246943, IT=0.5
6 finished solveLinearSystemImpl_ 1
6 converged value: 1 7 1 let s go get convergedRemote
6 to  comm_.min(converged) 
2  before v = A * y 
2  before  alpha = rho_new / < rt, v > 
2  before alpha = rho_new / h; 
2  before apply first correction to x 
2  before r = r - alpha*v 
2  before test stop criteria 
2 istlsolver::printOutput 
  0.5      3.15547e-30      1.14421e-16
2 === rate=1.30921e-32, T=0.12348, TIT=0.246959, IT=0.5
2 finished solveLinearSystemImpl_ 1
2 converged value: 1 7 1 let s go get convergedRemote
2 to  comm_.min(converged) 
1 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
1  before v = A * y 
1  before  alpha = rho_new / < rt, v > 
1  before alpha = rho_new / h; 
1  before apply first correction to x 
1  before r = r - alpha*v 
1  before test stop criteria 
1 istlsolver::printOutput 
  0.5      3.15547e-30      1.14421e-16
1 === rate=1.30921e-32, T=0.123388, TIT=0.246776, IT=0.5
1 finished solveLinearSystemImpl_ 1
1 converged value: 1 7 1 let s go get convergedRemote
1 to  comm_.min(converged) 
5  before alpha = rho_new / h; 
5  before apply first correction to x 
5  before r = r - alpha*v 
5  before test stop criteria 
5 istlsolver::printOutput 
  0.5      3.15547e-30      1.14421e-16
5 === rate=1.30921e-32, T=0.123391, TIT=0.246782, IT=0.5
5 finished solveLinearSystemImpl_ 1
5 converged value: 1 7 1 let s go get convergedRemote
5 to  comm_.min(converged) 
3 did  comm_.min(converged) convergedRemote: 1
3 final convergedRemote: 1
3 Update: x^(k+1) = x^k - deltax^k 
3 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
3 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
3 Newton iteration  4 done3 , maximum relative shift = 6.479e-06
3 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
6 did  comm_.min(converged) convergedRemote: 1
6 final convergedRemote: 1
6 Update: x^(k+1) = x^k - deltax^k 
6 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
6 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
6 Newton iteration  4 done6 , maximum relative shift = 6.479e-06
6 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
2 did  comm_.min(converged) convergedRemote: 1
2 final convergedRemote: 1
2 Update: x^(k+1) = x^k - deltax^k 
2 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
2 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
2 Newton iteration  4 done2 , maximum relative shift = 6.479e-06
2 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
5 did  comm_.min(converged) convergedRemote: 1
5 final convergedRemote: 1
5 Update: x^(k+1) = x^k - deltax^k 
5 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
5 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
5 Newton iteration  4 done5 , maximum relative shift = 6.479e-06
5 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
1 did  comm_.min(converged) convergedRemote: 1
1 final convergedRemote: 1
1 Update: x^(k+1) = x^k - deltax^k 
1 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
1 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
1 Newton iteration  4 done1 , maximum relative shift = 6.479e-06
1 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
0 finished solveLinearSystemImpl_ 1
0 converged value: 1 7 1 let s go get convergedRemote
0 to  comm_.min(converged) 
0 did  comm_.min(converged) convergedRemote: 1
0 final convergedRemote: 1
0 Update: x^(k+1) = x^k - deltax^k 
0 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
0 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
4 did  comm_.min(converged) convergedRemote: 1
4 final convergedRemote: 1
4 Update: x^(k+1) = x^k - deltax^k 
4 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
4 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
4 Newton iteration  4 done4 , maximum relative shift = 6.479e-06
4 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
0 Newton iteration  4 done0 , maximum relative shift = 6.479e-06
0 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
2 Solve: M deltax^k = r 
2 Newton::solveLinearSystem : entering the try section 4
2 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
2 solveLinearSystemImpl_, before converged = ls.solve 
2 7 amgbackend::solve isParallel 1
3 Solve: M deltax^k = r 
3 Newton::solveLinearSystem : entering the try section 4
3 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
3 solveLinearSystemImpl_, before converged = ls.solve 
3 7 amgbackend::solve isParallel 1
6 Solve: M deltax^k = r 
6 Newton::solveLinearSystem : entering the try section 4
6 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
6 solveLinearSystemImpl_, before converged = ls.solve 
6 7 amgbackend::solve isParallel 1
0 Solve: M deltax^k = r 
0 Newton::solveLinearSystem : entering the try section 4
0 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
0 solveLinearSystemImpl_, before converged = ls.solve 
0 7 amgbackend::solve isParallel 1
5 Solve: M deltax^k = r 
5 Newton::solveLinearSystem : entering the try section 4
5 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
5 solveLinearSystemImpl_, before converged = ls.solve 
5 7 amgbackend::solve isParallel 1
1 Solve: M deltax^k = r 
1 Newton::solveLinearSystem : entering the try section 4
1 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
1 solveLinearSystemImpl_, before converged = ls.solve 
1 7 amgbackend::solve isParallel 1
4 Solve: M deltax^k = r 
4 Newton::solveLinearSystem : entering the try section 4
4 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
4 solveLinearSystemImpl_, before converged = ls.solve 
4 7 amgbackend::solve isParallel 1
5 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
5 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
1 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
1 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
6 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
6 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
2 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
2 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
3 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
3 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
4 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
4 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
0 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
0 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 7
1 7 before Dune::BiCGSTABSolver 
1 Dune::BiCGSTABSolver.apply(x, b, result_) 
1 BiCGSTABSolver::apply 
1 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
6 7 before Dune::BiCGSTABSolver 
6 Dune::BiCGSTABSolver.apply(x, b, result_) 
6 BiCGSTABSolver::apply 
6 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
2 7 before Dune::BiCGSTABSolver 
2 Dune::BiCGSTABSolver.apply(x, b, result_) 
2 BiCGSTABSolver::apply 
2 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
4 7 before Dune::BiCGSTABSolver 
4 Dune::BiCGSTABSolver.apply(x, b, result_) 
4 BiCGSTABSolver::apply 
4 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
3 7 before Dune::BiCGSTABSolver 
3 Dune::BiCGSTABSolver.apply(x, b, result_) 
3 BiCGSTABSolver::apply 
3 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
5 7 before Dune::BiCGSTABSolver 
5 Dune::BiCGSTABSolver.apply(x, b, result_) 
5 BiCGSTABSolver::apply 
5 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
Using a direct coarse solver (SuperLU)
Building hierarchy of 1 levels (inclusive coarse solver) took 0.113083 seconds.
0 7 before Dune::BiCGSTABSolver 
0 Dune::BiCGSTABSolver.apply(x, b, result_) 
0 BiCGSTABSolver::apply 
0 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
0 === BiCGSTABSolver
 Iter          Defect            Rate
    0      3.34322e-15
0  (it = 0.5; it < _maxit; it+=.5) 0.5 250
2 === BiCGSTABSolver
 Iter          Defect            Rate
    0      3.34322e-15
2  (it = 0.5; it < _maxit; it+=.5) 0.5 250
2  before p = r 
2  before  y = W^-1 * p 
2  before _prec->apply(y,p); 0.5 250
2 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
4 === BiCGSTABSolver
 Iter          Defect            Rate
    0      3.34322e-15
4  (it = 0.5; it < _maxit; it+=.5) 0.5 250
4  before p = r 
4  before  y = W^-1 * p 
4  before _prec->apply(y,p); 0.5 250
4 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
3 === BiCGSTABSolver
 Iter          Defect            Rate
    0      3.34322e-15
3  (it = 0.5; it < _maxit; it+=.5) 0.5 250
3  before p = r 
3  before  y = W^-1 * p 
3  before _prec->apply(y,p); 0.5 250
3 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
5 === BiCGSTABSolver
 Iter          Defect            Rate
    0      3.34322e-15
5  (it = 0.5; it < _maxit; it+=.5) 0.5 250
5  before p = r 
5  before  y = W^-1 * p 
5  before _prec->apply(y,p); 0.5 250
5 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
1 === BiCGSTABSolver
 Iter          Defect            Rate
    0      3.34322e-15
1  (it = 0.5; it < _maxit; it+=.5) 0.5 250
1  before p = r 
1  before  y = W^-1 * p 
1  before _prec->apply(y,p); 0.5 250
1 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
6 === BiCGSTABSolver
 Iter          Defect            Rate
    0      3.34322e-15
6  (it = 0.5; it < _maxit; it+=.5) 0.5 250
6  before p = r 
6  before  y = W^-1 * p 
6  before _prec->apply(y,p); 0.5 250
6 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
0  before p = r 
0  before  y = W^-1 * p 
0  before _prec->apply(y,p); 0.5 250
0 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 7
0 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
6 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
6 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
6  before v = A * y 
6  before  alpha = rho_new / < rt, v > 
2 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
2 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
4 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
4 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
4  before v = A * y 
4  before  alpha = rho_new / < rt, v > 
3 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
3 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
3  before v = A * y 
5 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
5 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
5  before v = A * y 
1 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
0 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
1 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
2  before v = A * y 
2  before  alpha = rho_new / < rt, v > 
3  before  alpha = rho_new / < rt, v > 
5  before  alpha = rho_new / < rt, v > 
0 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
0  before v = A * y 
3  before alpha = rho_new / h; 
3  before apply first correction to x 
3  before r = r - alpha*v 
3  before test stop criteria 
3 istlsolver::printOutput 
  0.5      3.47914e-31      1.04066e-16
3 === rate=1.08296e-32, T=0.123422, TIT=0.246844, IT=0.5
3 finished solveLinearSystemImpl_ 1
3 converged value: 1 7 1 let s go get convergedRemote
3 to  comm_.min(converged) 
0  before  alpha = rho_new / < rt, v > 
0  before alpha = rho_new / h; 
0  before apply first correction to x 
0  before r = r - alpha*v 
0  before test stop criteria 
0 istlsolver::printOutput 
  0.5      3.47914e-31      1.04066e-16
0 === rate=1.08296e-32, T=0.018295, TIT=0.03659, IT=0.5
5  before alpha = rho_new / h; 
5  before apply first correction to x 
5  before r = r - alpha*v 
5  before test stop criteria 
5 istlsolver::printOutput 
  0.5      3.47914e-31      1.04066e-16
5 === rate=1.08296e-32, T=0.123421, TIT=0.246841, IT=0.5
5 finished solveLinearSystemImpl_ 1
5 converged value: 1 7 1 let s go get convergedRemote
5 to  comm_.min(converged) 
6  before alpha = rho_new / h; 
6  before apply first correction to x 
6  before r = r - alpha*v 
6  before test stop criteria 
6 istlsolver::printOutput 
  0.5      3.47914e-31      1.04066e-16
6 === rate=1.08296e-32, T=0.123367, TIT=0.246735, IT=0.5
6 finished solveLinearSystemImpl_ 1
6 converged value: 1 7 1 let s go get convergedRemote
6 to  comm_.min(converged) 
1  before v = A * y 
1  before  alpha = rho_new / < rt, v > 
1  before alpha = rho_new / h; 
1  before apply first correction to x 
1  before r = r - alpha*v 
1  before test stop criteria 
1 istlsolver::printOutput 
  0.5      3.47914e-31      1.04066e-16
1 === rate=1.08296e-32, T=0.123421, TIT=0.246842, IT=0.5
1 finished solveLinearSystemImpl_ 1
1 converged value: 1 7 1 let s go get convergedRemote
1 to  comm_.min(converged) 
4  before alpha = rho_new / h; 
4  before apply first correction to x 
4  before r = r - alpha*v 
4  before test stop criteria 
4 istlsolver::printOutput 
  0.5      3.47914e-31      1.04066e-16
4 === rate=1.08296e-32, T=0.123368, TIT=0.246735, IT=0.5
4 finished solveLinearSystemImpl_ 1
4 converged value: 1 7 1 let s go get convergedRemote
4 to  comm_.min(converged) 
2  before alpha = rho_new / h; 
2  before apply first correction to x 
2  before r = r - alpha*v 
2  before test stop criteria 
2 istlsolver::printOutput 
  0.5      3.47914e-31      1.04066e-16
2 === rate=1.08296e-32, T=0.123369, TIT=0.246738, IT=0.5
2 finished solveLinearSystemImpl_ 1
2 converged value: 1 7 1 let s go get convergedRemote
2 to  comm_.min(converged) 
4 did  comm_.min(converged) convergedRemote: 1
4 final convergedRemote: 1
4 Update: x^(k+1) = x^k - deltax^k 
4 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
4 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
4 Newton iteration  5 done4 , maximum relative shift = 7.855e-07
4 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
3 did  comm_.min(converged) convergedRemote: 1
3 final convergedRemote: 1
3 Update: x^(k+1) = x^k - deltax^k 
3 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
3 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
3 Newton iteration  5 done3 , maximum relative shift = 7.855e-07
3 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
2 did  comm_.min(converged) convergedRemote: 1
2 final convergedRemote: 1
2 Update: x^(k+1) = x^k - deltax^k 
2 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
2 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
2 Newton iteration  5 done2 , maximum relative shift = 7.855e-07
2 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
0 finished solveLinearSystemImpl_ 1
0 converged value: 1 7 1 let s go get convergedRemote
0 to  comm_.min(converged) 
0 did  comm_.min(converged) convergedRemote: 1
0 final convergedRemote: 1
0 Update: x^(k+1) = x^k - deltax^k 
0 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
0 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
0 Newton iteration  5 done0 , maximum relative shift = 7.855e-07
0 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
5 did  comm_.min(converged) convergedRemote: 1
5 final convergedRemote: 1
5 Update: x^(k+1) = x^k - deltax^k 
5 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
5 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
5 Newton iteration  5 done5 , maximum relative shift = 7.855e-07
5 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
6 did  comm_.min(converged) convergedRemote: 1
6 final convergedRemote: 1
6 Update: x^(k+1) = x^k - deltax^k 
6 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
6 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
6 Newton iteration  5 done6 , maximum relative shift = 7.855e-07
6 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
1 did  comm_.min(converged) convergedRemote: 1
1 final convergedRemote: 1
1 Update: x^(k+1) = x^k - deltax^k 
1 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
1 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
1 Newton iteration  5 done1 , maximum relative shift = 7.855e-07
1 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
4 Solve: M deltax^k = r 
4 Newton::solveLinearSystem : entering the try section 5
4 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
4 solveLinearSystemImpl_, before converged = ls.solve 
4 7 amgbackend::solve isParallel 1
1 Solve: M deltax^k = r 
1 Newton::solveLinearSystem : entering the try section 5
1 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
1 solveLinearSystemImpl_, before converged = ls.solve 
1 7 amgbackend::solve isParallel 1
2 Solve: M deltax^k = r 
2 Newton::solveLinearSystem : entering the try section 5
2 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
2 solveLinearSystemImpl_, before converged = ls.solve 
2 7 amgbackend::solve isParallel 1
0 Solve: M deltax^k = r 
0 Newton::solveLinearSystem : entering the try section 5
0 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
0 solveLinearSystemImpl_, before converged = ls.solve 
0 7 amgbackend::solve isParallel 1
5 Solve: M deltax^k = r 
5 Newton::solveLinearSystem : entering the try section 5
5 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
5 solveLinearSystemImpl_, before converged = ls.solve 
5 7 amgbackend::solve isParallel 1
6 Solve: M deltax^k = r 
6 Newton::solveLinearSystem : entering the try section 5
6 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
6 solveLinearSystemImpl_, before converged = ls.solve 
6 7 amgbackend::solve isParallel 1
3 Solve: M deltax^k = r 
3 Newton::solveLinearSystem : entering the try section 5
3 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
3 solveLinearSystemImpl_, before converged = ls.solve 
3 7 amgbackend::solve isParallel 1
2 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
2 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
0 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
0 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 7
5 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
5 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
6 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
6 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
4 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
4 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
3 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
3 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
1 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
1 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 7
6 7 before Dune::BiCGSTABSolver 
6 Dune::BiCGSTABSolver.apply(x, b, result_) 
6 BiCGSTABSolver::apply 
6 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
3 7 before Dune::BiCGSTABSolver 
3 Dune::BiCGSTABSolver.apply(x, b, result_) 
3 BiCGSTABSolver::apply 
3 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
4 7 before Dune::BiCGSTABSolver 
4 Dune::BiCGSTABSolver.apply(x, b, result_) 
4 BiCGSTABSolver::apply 
4 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
2 7 before Dune::BiCGSTABSolver 
2 Dune::BiCGSTABSolver.apply(x, b, result_) 
2 BiCGSTABSolver::apply 
2 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
1 7 before Dune::BiCGSTABSolver 
1 Dune::BiCGSTABSolver.apply(x, b, result_) 
1 BiCGSTABSolver::apply 
1 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
5 7 before Dune::BiCGSTABSolver 
5 Dune::BiCGSTABSolver.apply(x, b, result_) 
5 BiCGSTABSolver::apply 
5 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
Using a direct coarse solver (SuperLU)
Building hierarchy of 1 levels (inclusive coarse solver) took 0.111739 seconds.
0 7 before Dune::BiCGSTABSolver 
0 Dune::BiCGSTABSolver.apply(x, b, result_) 
0 BiCGSTABSolver::apply 
0 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
0 === BiCGSTABSolver
 Iter          Defect            Rate
    0      4.05313e-16
0  (it = 0.5; it < _maxit; it+=.5) 0.5 250
1 === BiCGSTABSolver
 Iter          Defect            Rate
    0      4.05313e-16
1  (it = 0.5; it < _maxit; it+=.5) 0.5 250
1  before p = r 
1  before  y = W^-1 * p 
1  before _prec->apply(y,p); 0.5 250
1 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
5 === BiCGSTABSolver
 Iter          Defect            Rate
    0      4.05313e-16
5  (it = 0.5; it < _maxit; it+=.5) 0.5 250
5  before p = r 
5  before  y = W^-1 * p 
5  before _prec->apply(y,p); 0.5 250
5 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
0  before p = r 
0  before  y = W^-1 * p 
0  before _prec->apply(y,p); 0.5 250
0 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 7
0 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
6 === BiCGSTABSolver
 Iter          Defect            Rate
    0      4.05313e-16
6  (it = 0.5; it < _maxit; it+=.5) 0.5 250
6  before p = r 
6  before  y = W^-1 * p 
6  before _prec->apply(y,p); 0.5 250
6 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
3 === BiCGSTABSolver
 Iter          Defect            Rate
    0      4.05313e-16
3  (it = 0.5; it < _maxit; it+=.5) 0.5 250
3  before p = r 
3  before  y = W^-1 * p 
3  before _prec->apply(y,p); 0.5 250
3 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
4 === BiCGSTABSolver
 Iter          Defect            Rate
    0      4.05313e-16
4  (it = 0.5; it < _maxit; it+=.5) 0.5 250
4  before p = r 
4  before  y = W^-1 * p 
4  before _prec->apply(y,p); 0.5 250
4 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
2 === BiCGSTABSolver
 Iter          Defect            Rate
    0      4.05313e-16
2  (it = 0.5; it < _maxit; it+=.5) 0.5 250
2  before p = r 
2  before  y = W^-1 * p 
2  before _prec->apply(y,p); 0.5 250
2 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
0 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
6 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
6 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
6  before v = A * y 
6  before  alpha = rho_new / < rt, v > 
3 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
3 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
3  before v = A * y 
4 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
4 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
4  before v = A * y 
5 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
5 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
5  before v = A * y 
1 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
2 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
2 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
2  before v = A * y 
1 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
1  before v = A * y 
0 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
0  before v = A * y 
3  before  alpha = rho_new / < rt, v > 
4  before  alpha = rho_new / < rt, v > 
5  before  alpha = rho_new / < rt, v > 
6  before alpha = rho_new / h; 
6  before apply first correction to x 
6  before r = r - alpha*v 
6  before test stop criteria 
6 istlsolver::printOutput 
  0.5      4.30944e-33      1.06324e-17
6 === rate=1.13048e-34, T=0.122408, TIT=0.244816, IT=0.5
6 finished solveLinearSystemImpl_ 1
6 converged value: 1 7 1 let s go get convergedRemote
6 to  comm_.min(converged) 
6 did  comm_.min(converged) convergedRemote: 1
6 final convergedRemote: 1
6 Update: x^(k+1) = x^k - deltax^k 
6 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
6 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
6 Newton iteration  6 done6 , maximum relative shift = 9.523e-08
6 Assemble/solve/update time: 0.264855(25.0008%)/0.794444(74.991%)/8.701e-05(0.00821325%)
6 gridVariables->advanceTimeStep
6 before, nonLinearSolver->suggestTimeStepSize, current time: 773.767 simTime 57600
6 before assembler->setPreviousSolution, ddt: 26.6667
6 nonLinearSolver->solve
6 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
0  before  alpha = rho_new / < rt, v > 
0  before alpha = rho_new / h; 
0  before apply first correction to x 
0  before r = r - alpha*v 
0  before test stop criteria 
0 istlsolver::printOutput 
  0.5      4.30944e-33      1.06324e-17
0 === rate=1.13048e-34, T=0.0186546, TIT=0.0373093, IT=0.5
0 finished solveLinearSystemImpl_ 1
0 converged value: 1 7 1 let s go get convergedRemote
0 to  comm_.min(converged) 
0 did  comm_.min(converged) convergedRemote: 1
0 final convergedRemote: 1
0 Update: x^(k+1) = x^k - deltax^k 
0 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
0 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
0 Newton iteration  6 done0 , maximum relative shift = 9.523e-08
0 Assemble/solve/update time: 0.263818(24.9271%)/0.79441(75.0608%)/0.000128312(0.0121237%)
0 gridVariables->advanceTimeStep
0 before, nonLinearSolver->suggestTimeStepSize, current time: 773.767 simTime 57600
0 before assembler->setPreviousSolution, ddt: 26.6667
0 nonLinearSolver->solve
3  before alpha = rho_new / h; 
3  before apply first correction to x 
3  before r = r - alpha*v 
3  before test stop criteria 
3 istlsolver::printOutput 
  0.5      4.30944e-33      1.06324e-17
3 === rate=1.13048e-34, T=0.122477, TIT=0.244953, IT=0.5
3 finished solveLinearSystemImpl_ 1
3 converged value: 1 7 1 let s go get convergedRemote
3 to  comm_.min(converged) 
3 did  comm_.min(converged) convergedRemote: 1
3 final convergedRemote: 1
3 Update: x^(k+1) = x^k - deltax^k 
3 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
3 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
3 Newton iteration  6 done3 , maximum relative shift = 9.523e-08
3 Assemble/solve/update time: 0.264842(24.9991%)/0.794448(74.99%)/0.000115241(0.0108779%)
3 gridVariables->advanceTimeStep
3 before, nonLinearSolver->suggestTimeStepSize, current time: 773.767 simTime 57600
3 before assembler->setPreviousSolution, ddt: 26.6667
3 nonLinearSolver->solve
3 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
4  before alpha = rho_new / h; 
4  before apply first correction to x 
4  before r = r - alpha*v 
4  before test stop criteria 
4 istlsolver::printOutput 
  0.5      4.30944e-33      1.06324e-17
4 === rate=1.13048e-34, T=0.122407, TIT=0.244814, IT=0.5
4 finished solveLinearSystemImpl_ 1
4 converged value: 1 7 1 let s go get convergedRemote
4 to  comm_.min(converged) 
4 did  comm_.min(converged) convergedRemote: 1
4 final convergedRemote: 1
4 Update: x^(k+1) = x^k - deltax^k 
4 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
4 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
4 Newton iteration  6 done4 , maximum relative shift = 9.523e-08
4 Assemble/solve/update time: 0.264849(25.0003%)/0.794425(74.9891%)/0.000112311(0.0106015%)
4 gridVariables->advanceTimeStep
4 before, nonLinearSolver->suggestTimeStepSize, current time: 773.767 simTime 57600
4 before assembler->setPreviousSolution, ddt: 26.6667
4 nonLinearSolver->solve
4 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
5  before alpha = rho_new / h; 
5  before apply first correction to x 
5  before r = r - alpha*v 
5  before test stop criteria 
5 istlsolver::printOutput 
  0.5      4.30944e-33      1.06324e-17
5 === rate=1.13048e-34, T=0.122463, TIT=0.244925, IT=0.5
5 finished solveLinearSystemImpl_ 1
5 converged value: 1 7 1 let s go get convergedRemote
5 to  comm_.min(converged) 
5 did  comm_.min(converged) convergedRemote: 1
5 final convergedRemote: 1
5 Update: x^(k+1) = x^k - deltax^k 
5 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
5 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
5 Newton iteration  6 done5 , maximum relative shift = 9.523e-08
5 Assemble/solve/update time: 0.26487(25.0017%)/0.794429(74.9879%)/0.000109981(0.0103813%)
5 gridVariables->advanceTimeStep
5 before, nonLinearSolver->suggestTimeStepSize, current time: 773.767 simTime 57600
5 before assembler->setPreviousSolution, ddt: 26.6667
5 nonLinearSolver->solve
5 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
2  before  alpha = rho_new / < rt, v > 
2  before alpha = rho_new / h; 
2  before apply first correction to x 
2  before r = r - alpha*v 
2  before test stop criteria 
2 istlsolver::printOutput 
  0.5      4.30944e-33      1.06324e-17
2 === rate=1.13048e-34, T=0.122406, TIT=0.244812, IT=0.5
2 finished solveLinearSystemImpl_ 1
2 converged value: 1 7 1 let s go get convergedRemote
2 to  comm_.min(converged) 
2 did  comm_.min(converged) convergedRemote: 1
2 final convergedRemote: 1
2 Update: x^(k+1) = x^k - deltax^k 
2 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
2 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
2 Newton iteration  6 done2 , maximum relative shift = 9.523e-08
2 Assemble/solve/update time: 0.264834(24.9986%)/0.794446(74.9906%)/0.000113861(0.0107478%)
2 gridVariables->advanceTimeStep
2 before, nonLinearSolver->suggestTimeStepSize, current time: 773.767 simTime 57600
2 before assembler->setPreviousSolution, ddt: 26.6667
2 nonLinearSolver->solve
2 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
1  before  alpha = rho_new / < rt, v > 
1  before alpha = rho_new / h; 
1  before apply first correction to x 
1  before r = r - alpha*v 
1  before test stop criteria 
1 istlsolver::printOutput 
  0.5      4.30944e-33      1.06324e-17
1 === rate=1.13048e-34, T=0.122465, TIT=0.24493, IT=0.5
1 finished solveLinearSystemImpl_ 1
1 converged value: 1 7 1 let s go get convergedRemote
1 to  comm_.min(converged) 
1 did  comm_.min(converged) convergedRemote: 1
1 final convergedRemote: 1
1 Update: x^(k+1) = x^k - deltax^k 
1 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
1 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
1 Newton iteration  6 done1 , maximum relative shift = 9.523e-08
1 Assemble/solve/update time: 0.264863(25.0012%)/0.79443(74.9886%)/0.000108641(0.0102549%)
1 gridVariables->advanceTimeStep
1 before, nonLinearSolver->suggestTimeStepSize, current time: 773.767 simTime 57600
1 before assembler->setPreviousSolution, ddt: 26.6667
1 nonLinearSolver->solve
1 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
0 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
0 Solve: M deltax^k = r 
0 Newton::solveLinearSystem : entering the try section 0
0to  comm_.sum(norm2) 
0did  comm_.sum(norm2) 
0 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
0 solveLinearSystemImpl_, before converged = ls.solve 
0 7 amgbackend::solve isParallel 1
5 Solve: M deltax^k = r 
5 Newton::solveLinearSystem : entering the try section 0
5to  comm_.sum(norm2) 
5did  comm_.sum(norm2) 
5 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
5 solveLinearSystemImpl_, before converged = ls.solve 
5 7 amgbackend::solve isParallel 1
2 Solve: M deltax^k = r 
2 Newton::solveLinearSystem : entering the try section 0
2to  comm_.sum(norm2) 
2did  comm_.sum(norm2) 
2 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
2 solveLinearSystemImpl_, before converged = ls.solve 
2 7 amgbackend::solve isParallel 1
4 Solve: M deltax^k = r 
4 Newton::solveLinearSystem : entering the try section 0
4to  comm_.sum(norm2) 
4did  comm_.sum(norm2) 
4 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
4 solveLinearSystemImpl_, before converged = ls.solve 
4 7 amgbackend::solve isParallel 1
6 Solve: M deltax^k = r 
6 Newton::solveLinearSystem : entering the try section 0
6to  comm_.sum(norm2) 
6did  comm_.sum(norm2) 
6 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
6 solveLinearSystemImpl_, before converged = ls.solve 
6 7 amgbackend::solve isParallel 1
1 Solve: M deltax^k = r 
1 Newton::solveLinearSystem : entering the try section 0
1to  comm_.sum(norm2) 
1did  comm_.sum(norm2) 
1 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
1 solveLinearSystemImpl_, before converged = ls.solve 
1 7 amgbackend::solve isParallel 1
3 Solve: M deltax^k = r 
3 Newton::solveLinearSystem : entering the try section 0
3to  comm_.sum(norm2) 
3did  comm_.sum(norm2) 
3 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
3 solveLinearSystemImpl_, before converged = ls.solve 
3 7 amgbackend::solve isParallel 1
3 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
3 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
0 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
0 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 7
5 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
5 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
2 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
2 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
4 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
4 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
6 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
6 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
1 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
1 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 7
3 7 before Dune::BiCGSTABSolver 
3 Dune::BiCGSTABSolver.apply(x, b, result_) 
3 BiCGSTABSolver::apply 
3 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
1 7 before Dune::BiCGSTABSolver 
1 Dune::BiCGSTABSolver.apply(x, b, result_) 
1 BiCGSTABSolver::apply 
1 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
5 7 before Dune::BiCGSTABSolver 
5 Dune::BiCGSTABSolver.apply(x, b, result_) 
5 BiCGSTABSolver::apply 
5 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
2 7 before Dune::BiCGSTABSolver 
2 Dune::BiCGSTABSolver.apply(x, b, result_) 
2 BiCGSTABSolver::apply 
2 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
4 7 before Dune::BiCGSTABSolver 
4 Dune::BiCGSTABSolver.apply(x, b, result_) 
4 BiCGSTABSolver::apply 
4 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
6 7 before Dune::BiCGSTABSolver 
6 Dune::BiCGSTABSolver.apply(x, b, result_) 
6 BiCGSTABSolver::apply 
6 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
Using a direct coarse solver (SuperLU)
Building hierarchy of 1 levels (inclusive coarse solver) took 0.113007 seconds.
0 7 before Dune::BiCGSTABSolver 
0 Dune::BiCGSTABSolver.apply(x, b, result_) 
0 BiCGSTABSolver::apply 
0 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
0 === BiCGSTABSolver
 Iter          Defect            Rate
    0      3.16093e-09
0  (it = 0.5; it < _maxit; it+=.5) 0.5 250
0  before p = r 
0  before  y = W^-1 * p 
0  before _prec->apply(y,p); 0.5 250
6 === BiCGSTABSolver
 Iter          Defect            Rate
    0      3.16093e-09
6  (it = 0.5; it < _maxit; it+=.5) 0.5 250
6  before p = r 
6  before  y = W^-1 * p 
6  before _prec->apply(y,p); 0.5 250
6 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
3 === BiCGSTABSolver
 Iter          Defect            Rate
    0      3.16093e-09
3  (it = 0.5; it < _maxit; it+=.5) 0.5 250
3  before p = r 
3  before  y = W^-1 * p 
3  before _prec->apply(y,p); 0.5 250
3 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
1 === BiCGSTABSolver
 Iter          Defect            Rate
    0      3.16093e-09
1  (it = 0.5; it < _maxit; it+=.5) 0.5 250
1  before p = r 
1  before  y = W^-1 * p 
1  before _prec->apply(y,p); 0.5 250
1 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
5 === BiCGSTABSolver
 Iter          Defect            Rate
    0      3.16093e-09
5  (it = 0.5; it < _maxit; it+=.5) 0.5 250
5  before p = r 
5  before  y = W^-1 * p 
5  before _prec->apply(y,p); 0.5 250
5 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
2 === BiCGSTABSolver
 Iter          Defect            Rate
    0      3.16093e-09
2  (it = 0.5; it < _maxit; it+=.5) 0.5 250
2  before p = r 
2  before  y = W^-1 * p 
2  before _prec->apply(y,p); 0.5 250
2 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
4 === BiCGSTABSolver
 Iter          Defect            Rate
    0      3.16093e-09
4  (it = 0.5; it < _maxit; it+=.5) 0.5 250
4  before p = r 
4  before  y = W^-1 * p 
4  before _prec->apply(y,p); 0.5 250
4 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
0 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 7
0 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
4 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
4 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
4  before v = A * y 
6 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
6 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
6  before v = A * y 
3 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
3 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
3  before v = A * y 
1 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
5 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
5 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
5  before v = A * y 
2 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
2 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
0 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
4  before  alpha = rho_new / < rt, v > 
6  before  alpha = rho_new / < rt, v > 
3  before  alpha = rho_new / < rt, v > 
1 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
5  before  alpha = rho_new / < rt, v > 
0 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
0  before v = A * y 
1  before v = A * y 
1  before  alpha = rho_new / < rt, v > 
1  before alpha = rho_new / h; 
1  before apply first correction to x 
1  before r = r - alpha*v 
1  before test stop criteria 
1 istlsolver::printOutput 
  0.5      8.76149e-25      2.77181e-16
1 === rate=7.68291e-32, T=0.123797, TIT=0.247594, IT=0.5
1 finished solveLinearSystemImpl_ 1
1 converged value: 1 7 1 let s go get convergedRemote
1 to  comm_.min(converged) 
0  before  alpha = rho_new / < rt, v > 
0  before alpha = rho_new / h; 
0  before apply first correction to x 
0  before r = r - alpha*v 
0  before test stop criteria 
0 istlsolver::printOutput 
  0.5      8.76149e-25      2.77181e-16
0 === rate=7.68291e-32, T=0.0186901, TIT=0.0373802, IT=0.5
2  before v = A * y 
2  before  alpha = rho_new / < rt, v > 
2  before alpha = rho_new / h; 
2  before apply first correction to x 
2  before r = r - alpha*v 
2  before test stop criteria 
2 istlsolver::printOutput 
  0.5      8.76149e-25      2.77181e-16
2 === rate=7.68291e-32, T=0.123753, TIT=0.247505, IT=0.5
2 finished solveLinearSystemImpl_ 1
2 converged value: 1 7 1 let s go get convergedRemote
2 to  comm_.min(converged) 
5  before alpha = rho_new / h; 
5  before apply first correction to x 
5  before r = r - alpha*v 
5  before test stop criteria 
5 istlsolver::printOutput 
  0.5      8.76149e-25      2.77181e-16
5 === rate=7.68291e-32, T=0.123793, TIT=0.247586, IT=0.5
5 finished solveLinearSystemImpl_ 1
5 converged value: 1 7 1 let s go get convergedRemote
5 to  comm_.min(converged) 
4  before alpha = rho_new / h; 
4  before apply first correction to x 
4  before r = r - alpha*v 
4  before test stop criteria 
4 istlsolver::printOutput 
  0.5      8.76149e-25      2.77181e-16
4 === rate=7.68291e-32, T=0.123754, TIT=0.247507, IT=0.5
4 finished solveLinearSystemImpl_ 1
4 converged value: 1 7 1 let s go get convergedRemote
4 to  comm_.min(converged) 
6  before alpha = rho_new / h; 
6  before apply first correction to x 
6  before r = r - alpha*v 
6  before test stop criteria 
6 istlsolver::printOutput 
  0.5      8.76149e-25      2.77181e-16
6 === rate=7.68291e-32, T=0.123753, TIT=0.247505, IT=0.5
6 finished solveLinearSystemImpl_ 1
6 converged value: 1 7 1 let s go get convergedRemote
6 to  comm_.min(converged) 
3  before alpha = rho_new / h; 
3  before apply first correction to x 
3  before r = r - alpha*v 
3  before test stop criteria 
3 istlsolver::printOutput 
  0.5      8.76149e-25      2.77181e-16
3 === rate=7.68291e-32, T=0.123795, TIT=0.24759, IT=0.5
3 finished solveLinearSystemImpl_ 1
3 converged value: 1 7 1 let s go get convergedRemote
3 to  comm_.min(converged) 
0 finished solveLinearSystemImpl_ 1
0 converged value: 1 7 1 let s go get convergedRemote
0 to  comm_.min(converged) 
0 did  comm_.min(converged) convergedRemote: 1
0 final convergedRemote: 1
0 Update: x^(k+1) = x^k - deltax^k 
0 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
0 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
0 Newton iteration  1 done0 , maximum relative shift = 6.779e-02
0 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
2 did  comm_.min(converged) convergedRemote: 1
2 final convergedRemote: 1
2 Update: x^(k+1) = x^k - deltax^k 
2 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
2 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
2 Newton iteration  1 done2 , maximum relative shift = 6.779e-02
2 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
5 did  comm_.min(converged) convergedRemote: 1
5 final convergedRemote: 1
5 Update: x^(k+1) = x^k - deltax^k 
5 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
5 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
5 Newton iteration  1 done5 , maximum relative shift = 6.779e-02
5 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
4 did  comm_.min(converged) convergedRemote: 1
4 final convergedRemote: 1
4 Update: x^(k+1) = x^k - deltax^k 
4 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
4 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
4 Newton iteration  1 done4 , maximum relative shift = 6.779e-02
4 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
6 did  comm_.min(converged) convergedRemote: 1
6 final convergedRemote: 1
6 Update: x^(k+1) = x^k - deltax^k 
6 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
6 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
6 Newton iteration  1 done6 , maximum relative shift = 6.779e-02
6 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
1 did  comm_.min(converged) convergedRemote: 1
1 final convergedRemote: 1
1 Update: x^(k+1) = x^k - deltax^k 
1 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
1 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
1 Newton iteration  1 done1 , maximum relative shift = 6.779e-02
1 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
3 did  comm_.min(converged) convergedRemote: 1
3 final convergedRemote: 1
3 Update: x^(k+1) = x^k - deltax^k 
3 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
3 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
3 Newton iteration  1 done3 , maximum relative shift = 6.779e-02
3 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
4 Solve: M deltax^k = r 
4 Newton::solveLinearSystem : entering the try section 1
4 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
4 solveLinearSystemImpl_, before converged = ls.solve 
4 7 amgbackend::solve isParallel 1
6 Solve: M deltax^k = r 
6 Newton::solveLinearSystem : entering the try section 1
6 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
6 solveLinearSystemImpl_, before converged = ls.solve 
6 7 amgbackend::solve isParallel 1
3 Solve: M deltax^k = r 
3 Newton::solveLinearSystem : entering the try section 1
3 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
3 solveLinearSystemImpl_, before converged = ls.solve 
3 7 amgbackend::solve isParallel 1
1 Solve: M deltax^k = r 
1 Newton::solveLinearSystem : entering the try section 1
1 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
1 solveLinearSystemImpl_, before converged = ls.solve 
1 7 amgbackend::solve isParallel 1
0 Solve: M deltax^k = r 
0 Newton::solveLinearSystem : entering the try section 1
0 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
0 solveLinearSystemImpl_, before converged = ls.solve 
0 7 amgbackend::solve isParallel 1
2 Solve: M deltax^k = r 
2 Newton::solveLinearSystem : entering the try section 1
2 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
2 solveLinearSystemImpl_, before converged = ls.solve 
2 7 amgbackend::solve isParallel 1
5 Solve: M deltax^k = r 
5 Newton::solveLinearSystem : entering the try section 1
5 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
5 solveLinearSystemImpl_, before converged = ls.solve 
5 7 amgbackend::solve isParallel 1
3 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
3 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
1 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
1 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
0 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
0 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 7
2 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
2 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
6 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
6 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
4 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
4 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
5 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
5 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 7
1 7 before Dune::BiCGSTABSolver 
1 Dune::BiCGSTABSolver.apply(x, b, result_) 
1 BiCGSTABSolver::apply 
1 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
5 7 before Dune::BiCGSTABSolver 
5 Dune::BiCGSTABSolver.apply(x, b, result_) 
5 BiCGSTABSolver::apply 
5 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
2 7 before Dune::BiCGSTABSolver 
2 Dune::BiCGSTABSolver.apply(x, b, result_) 
2 BiCGSTABSolver::apply 
2 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
6 7 before Dune::BiCGSTABSolver 
6 Dune::BiCGSTABSolver.apply(x, b, result_) 
6 BiCGSTABSolver::apply 
6 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
4 7 before Dune::BiCGSTABSolver 
4 Dune::BiCGSTABSolver.apply(x, b, result_) 
4 BiCGSTABSolver::apply 
4 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
3 7 before Dune::BiCGSTABSolver 
3 Dune::BiCGSTABSolver.apply(x, b, result_) 
3 BiCGSTABSolver::apply 
3 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
6 === BiCGSTABSolver
 Iter          Defect            Rate
    0      1.47391e-11
6  (it = 0.5; it < _maxit; it+=.5) 0.5 250
6  before p = r 
6  before  y = W^-1 * p 
6  before _prec->apply(y,p); 0.5 250
6 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
4 === BiCGSTABSolver
 Iter          Defect            Rate
    0      1.47391e-11
4  (it = 0.5; it < _maxit; it+=.5) 0.5 250
4  before p = r 
4  before  y = W^-1 * p 
4  before _prec->apply(y,p); 0.5 250
4 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
Using a direct coarse solver (SuperLU)
Building hierarchy of 1 levels (inclusive coarse solver) took 0.11317 seconds.
0 7 before Dune::BiCGSTABSolver 
0 Dune::BiCGSTABSolver.apply(x, b, result_) 
0 BiCGSTABSolver::apply 
0 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
0 === BiCGSTABSolver
 Iter          Defect            Rate
    0      1.47391e-11
0  (it = 0.5; it < _maxit; it+=.5) 0.5 250
0  before p = r 
0  before  y = W^-1 * p 
0  before _prec->apply(y,p); 0.5 250
0 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 7
1 === BiCGSTABSolver
 Iter          Defect            Rate
    0      1.47391e-11
1  (it = 0.5; it < _maxit; it+=.5) 0.5 250
1  before p = r 
1  before  y = W^-1 * p 
1  before _prec->apply(y,p); 0.5 250
1 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
5 === BiCGSTABSolver
 Iter          Defect            Rate
    0      1.47391e-11
5  (it = 0.5; it < _maxit; it+=.5) 0.5 250
5  before p = r 
5  before  y = W^-1 * p 
5  before _prec->apply(y,p); 0.5 250
5 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
2 === BiCGSTABSolver
 Iter          Defect            Rate
    0      1.47391e-11
2  (it = 0.5; it < _maxit; it+=.5) 0.5 250
2  before p = r 
2  before  y = W^-1 * p 
2  before _prec->apply(y,p); 0.5 250
2 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
3 === BiCGSTABSolver
 Iter          Defect            Rate
    0      1.47391e-11
3  (it = 0.5; it < _maxit; it+=.5) 0.5 250
3  before p = r 
3  before  y = W^-1 * p 
3  before _prec->apply(y,p); 0.5 250
3 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 7
0 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
5 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
5 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
5  before v = A * y 
5  before  alpha = rho_new / < rt, v > 
5  before alpha = rho_new / h; 
5  before apply first correction to x 
5  before r = r - alpha*v 
5  before test stop criteria 
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
0 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
0 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
0  before v = A * y 
0  before  alpha = rho_new / < rt, v > 
0  before alpha = rho_new / h; 
0  before apply first correction to x 
0  before r = r - alpha*v 
0  before test stop criteria 
0 istlsolver::printOutput 
  0.5      2.44797e-27      1.66087e-16
0 === rate=2.75848e-32, T=0.018476, TIT=0.0369519, IT=0.5
2 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
2 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
2  before v = A * y 
2  before  alpha = rho_new / < rt, v > 
2  before alpha = rho_new / h; 
2  before apply first correction to x 
2  before r = r - alpha*v 
2  before test stop criteria 
2 istlsolver::printOutput 
  0.5      2.44797e-27      1.66087e-16
2 === rate=2.75848e-32, T=0.124133, TIT=0.248267, IT=0.5
2 finished solveLinearSystemImpl_ 1
2 converged value: 1 7 1 let s go get convergedRemote
2 to  comm_.min(converged) 
6 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
6 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
6  before v = A * y 
6  before  alpha = rho_new / < rt, v > 
6  before alpha = rho_new / h; 
6  before apply first correction to x 
6  before r = r - alpha*v 
6  before test stop criteria 
6 istlsolver::printOutput 
  0.5      2.44797e-27      1.66087e-16
6 === rate=2.75848e-32, T=0.124135, TIT=0.248271, IT=0.5
6 finished solveLinearSystemImpl_ 1
6 converged value: 1 7 1 let s go get convergedRemote
6 to  comm_.min(converged) 
4 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
4 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
4  before v = A * y 
4  before  alpha = rho_new / < rt, v > 
4  before alpha = rho_new / h; 
4  before apply first correction to x 
4  before r = r - alpha*v 
4  before test stop criteria 
4 istlsolver::printOutput 
  0.5      2.44797e-27      1.66087e-16
4 === rate=2.75848e-32, T=0.124135, TIT=0.24827, IT=0.5
4 finished solveLinearSystemImpl_ 1
4 converged value: 1 7 1 let s go get convergedRemote
4 to  comm_.min(converged) 
3 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
3 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
3  before v = A * y 
3  before  alpha = rho_new / < rt, v > 
3  before alpha = rho_new / h; 
3  before apply first correction to x 
3  before r = r - alpha*v 
3  before test stop criteria 
3 istlsolver::printOutput 
  0.5      2.44797e-27      1.66087e-16
1 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
1 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
1  before v = A * y 
1  before  alpha = rho_new / < rt, v > 
1  before alpha = rho_new / h; 
1  before apply first correction to x 
1  before r = r - alpha*v 
1  before test stop criteria 
1 istlsolver::printOutput 
  0.5      2.44797e-27      1.66087e-16
5 istlsolver::printOutput 
  0.5      2.44797e-27      1.66087e-16
5 === rate=2.75848e-32, T=0.124284, TIT=0.248569, IT=0.5
5 finished solveLinearSystemImpl_ 1
5 converged value: 1 7 1 let s go get convergedRemote
5 to  comm_.min(converged) 
1 === rate=2.75848e-32, T=0.124286, TIT=0.248572, IT=0.5
1 finished solveLinearSystemImpl_ 1
1 converged value: 1 7 1 let s go get convergedRemote
1 to  comm_.min(converged) 
3 === rate=2.75848e-32, T=0.124284, TIT=0.248567, IT=0.5
3 finished solveLinearSystemImpl_ 1
3 converged value: 1 7 1 let s go get convergedRemote
3 to  comm_.min(converged) 
3 did  comm_.min(converged) convergedRemote: 1
3 final convergedRemote: 1
3 Update: x^(k+1) = x^k - deltax^k 
3 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
3 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
3 Newton iteration  2 done3 , maximum relative shift = 3.119e-03
3 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
0 finished solveLinearSystemImpl_ 1
0 converged value: 1 7 1 let s go get convergedRemote
0 to  comm_.min(converged) 
0 did  comm_.min(converged) convergedRemote: 1
0 final convergedRemote: 1
0 Update: x^(k+1) = x^k - deltax^k 
0 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
0 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
0 Newton iteration  2 done0 , maximum relative shift = 3.119e-03
0 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
2 did  comm_.min(converged) convergedRemote: 1
2 final convergedRemote: 1
2 Update: x^(k+1) = x^k - deltax^k 
2 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
2 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
2 Newton iteration  2 done2 , maximum relative shift = 3.119e-03
2 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
6 did  comm_.min(converged) convergedRemote: 1
6 final convergedRemote: 1
6 Update: x^(k+1) = x^k - deltax^k 
6 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
6 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
6 Newton iteration  2 done6 , maximum relative shift = 3.119e-03
6 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
4 did  comm_.min(converged) convergedRemote: 1
4 final convergedRemote: 1
4 Update: x^(k+1) = x^k - deltax^k 
4 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
4 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
4 Newton iteration  2 done4 , maximum relative shift = 3.119e-03
4 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
1 did  comm_.min(converged) convergedRemote: 1
1 final convergedRemote: 1
1 Update: x^(k+1) = x^k - deltax^k 
1 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
1 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
1 Newton iteration  2 done1 , maximum relative shift = 3.119e-03
1 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
5 did  comm_.min(converged) convergedRemote: 1
5 final convergedRemote: 1
5 Update: x^(k+1) = x^k - deltax^k 
5 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
5 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
5 Newton iteration  2 done5 , maximum relative shift = 3.119e-03
5 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
4 Solve: M deltax^k = r 
4 Newton::solveLinearSystem : entering the try section 2
4 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
4 solveLinearSystemImpl_, before converged = ls.solve 
4 7 amgbackend::solve isParallel 1
1 Solve: M deltax^k = r 
1 Newton::solveLinearSystem : entering the try section 2
1 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
1 solveLinearSystemImpl_, before converged = ls.solve 
1 7 amgbackend::solve isParallel 1
5 Solve: M deltax^k = r 
5 Newton::solveLinearSystem : entering the try section 2
5 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
5 solveLinearSystemImpl_, before converged = ls.solve 
5 7 amgbackend::solve isParallel 1
3 Solve: M deltax^k = r 
3 Newton::solveLinearSystem : entering the try section 2
3 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
3 solveLinearSystemImpl_, before converged = ls.solve 
3 7 amgbackend::solve isParallel 1
0 Solve: M deltax^k = r 
0 Newton::solveLinearSystem : entering the try section 2
0 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
0 solveLinearSystemImpl_, before converged = ls.solve 
0 7 amgbackend::solve isParallel 1
2 Solve: M deltax^k = r 
2 Newton::solveLinearSystem : entering the try section 2
2 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
2 solveLinearSystemImpl_, before converged = ls.solve 
2 7 amgbackend::solve isParallel 1
6 Solve: M deltax^k = r 
6 Newton::solveLinearSystem : entering the try section 2
6 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
6 solveLinearSystemImpl_, before converged = ls.solve 
6 7 amgbackend::solve isParallel 1
5 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
5 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
3 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
3 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
0 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
0 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 7
2 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
2 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
1 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
1 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
4 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
4 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
6 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
6 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 7
6 7 before Dune::BiCGSTABSolver 
6 Dune::BiCGSTABSolver.apply(x, b, result_) 
6 BiCGSTABSolver::apply 
6 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
2 7 before Dune::BiCGSTABSolver 
2 Dune::BiCGSTABSolver.apply(x, b, result_) 
2 BiCGSTABSolver::apply 
2 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
1 7 before Dune::BiCGSTABSolver 
1 Dune::BiCGSTABSolver.apply(x, b, result_) 
1 BiCGSTABSolver::apply 
1 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 7
4 7 before Dune::BiCGSTABSolver 
4 Dune::BiCGSTABSolver.apply(x, b, result_) 
4 BiCGSTABSolver::apply 
4 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
5 7 before Dune::BiCGSTABSolver 
5 Dune::BiCGSTABSolver.apply(x, b, result_) 
5 BiCGSTABSolver::apply 
5 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
3 7 before Dune::BiCGSTABSolver 
3 Dune::BiCGSTABSolver.apply(x, b, result_) 
3 BiCGSTABSolver::apply 
3 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
Using a direct coarse solver (SuperLU)
Building hierarchy of 1 levels (inclusive coarse solver) took 0.11292 seconds.
0 7 before Dune::BiCGSTABSolver 
0 Dune::BiCGSTABSolver.apply(x, b, result_) 
0 BiCGSTABSolver::apply 
0 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
6 === BiCGSTABSolver
 Iter          Defect            Rate
    0      2.46536e-12
6  (it = 0.5; it < _maxit; it+=.5) 0.5 250
6  before p = r 
6  before  y = W^-1 * p 
6  before _prec->apply(y,p); 0.5 250
6 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
2 === BiCGSTABSolver
 Iter          Defect            Rate
    0      2.46536e-12
2  (it = 0.5; it < _maxit; it+=.5) 0.5 250
2  before p = r 
2  before  y = W^-1 * p 
2  before _prec->apply(y,p); 0.5 250
2 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
0 === BiCGSTABSolver
 Iter          Defect            Rate
    0      2.46536e-12
0  (it = 0.5; it < _maxit; it+=.5) 0.5 250
0  before p = r 
0  before  y = W^-1 * p 
0  before _prec->apply(y,p); 0.5 250
0 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 7
0 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
3 === BiCGSTABSolver
 Iter          Defect            Rate
    0      2.46536e-12
3  (it = 0.5; it < _maxit; it+=.5) 0.5 250
3  before p = r 
3  before  y = W^-1 * p 
3  before _prec->apply(y,p); 0.5 250
3 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
4 === BiCGSTABSolver
 Iter          Defect            Rate
    0      2.46536e-12
4  (it = 0.5; it < _maxit; it+=.5) 0.5 250
4  before p = r 
4  before  y = W^-1 * p 
4  before _prec->apply(y,p); 0.5 250
4 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
5 === BiCGSTABSolver
 Iter          Defect            Rate
    0      2.46536e-12
5  (it = 0.5; it < _maxit; it+=.5) 0.5 250
5  before p = r 
5  before  y = W^-1 * p 
5  before _prec->apply(y,p); 0.5 250
5 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
1 === BiCGSTABSolver
 Iter          Defect            Rate
    0      2.46536e-12
1  (it = 0.5; it < _maxit; it+=.5) 0.5 250
1  before p = r 
1  before  y = W^-1 * p 
1  before _prec->apply(y,p); 0.5 250
1 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
3 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
3 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
3  before v = A * y 
4 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
4 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
4  before v = A * y 
5 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
5 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
5  before v = A * y 
2 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
2 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
6 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
6 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
6  before v = A * y 
6  before  alpha = rho_new / < rt, v > 
1 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
0 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
4  before  alpha = rho_new / < rt, v > 
5  before  alpha = rho_new / < rt, v > 
2  before v = A * y 
2  before  alpha = rho_new / < rt, v > 
0 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
0  before v = A * y 
0  before  alpha = rho_new / < rt, v > 
1 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
3  before  alpha = rho_new / < rt, v > 
1  before v = A * y 
1  before  alpha = rho_new / < rt, v > 
1  before alpha = rho_new / h; 
1  before apply first correction to x 
1  before r = r - alpha*v 
1  before test stop criteria 
1 istlsolver::printOutput 
  0.5      1.42865e-28      5.79491e-17
1 === rate=3.3581e-33, T=0.123687, TIT=0.247373, IT=0.5
1 finished solveLinearSystemImpl_ 1
1 converged value: 1 7 1 let s go get convergedRemote
1 to  comm_.min(converged) 
2  before alpha = rho_new / h; 
2  before apply first correction to x 
2  before r = r - alpha*v 
2  before test stop criteria 
2 istlsolver::printOutput 
  0.5      1.42865e-28      5.79491e-17
2 === rate=3.3581e-33, T=0.123599, TIT=0.247198, IT=0.5
2 finished solveLinearSystemImpl_ 1
2 converged value: 1 7 1 let s go get convergedRemote
2 to  comm_.min(converged) 
4  before alpha = rho_new / h; 
4  before apply first correction to x 
4  before r = r - alpha*v 
4  before test stop criteria 
4 istlsolver::printOutput 
  0.5      1.42865e-28      5.79491e-17
4 === rate=3.3581e-33, T=0.1236, TIT=0.247201, IT=0.5
4 finished solveLinearSystemImpl_ 1
4 converged value: 1 7 1 let s go get convergedRemote
4 to  comm_.min(converged) 
5  before alpha = rho_new / h; 
5  before apply first correction to x 
5  before r = r - alpha*v 
5  before test stop criteria 
5 istlsolver::printOutput 
  0.5      1.42865e-28      5.79491e-17
5 === rate=3.3581e-33, T=0.123672, TIT=0.247343, IT=0.5
5 finished solveLinearSystemImpl_ 1
5 converged value: 1 7 1 let s go get convergedRemote
5 to  comm_.min(converged) 
3  before alpha = rho_new / h; 
3  before apply first correction to x 
3  before r = r - alpha*v 
3  before test stop criteria 
3 istlsolver::printOutput 
  0.5      1.42865e-28      5.79491e-17
3 === rate=3.3581e-33, T=0.123684, TIT=0.247368, IT=0.5
3 finished solveLinearSystemImpl_ 1
3 converged value: 1 7 1 let s go get convergedRemote
3 to  comm_.min(converged) 
0  before alpha = rho_new / h; 
0  before apply first correction to x 
0  before r = r - alpha*v 
0  before test stop criteria 
0 istlsolver::printOutput 
  0.5      1.42865e-28      5.79491e-17
0 === rate=3.3581e-33, T=0.0185508, TIT=0.0371015, IT=0.5
6  before alpha = rho_new / h; 
6  before apply first correction to x 
6  before r = r - alpha*v 
6  before test stop criteria 
6 istlsolver::printOutput 
  0.5      1.42865e-28      5.79491e-17
6 === rate=3.3581e-33, T=0.123598, TIT=0.247197, IT=0.5
6 finished solveLinearSystemImpl_ 1
6 converged value: 1 7 1 let s go get convergedRemote
6 to  comm_.min(converged) 
3 did  comm_.min(converged) convergedRemote: 1
3 final convergedRemote: 1
3 Update: x^(k+1) = x^k - deltax^k 
3 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
3 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
3 Newton iteration  3 done3 , maximum relative shift = 5.697e-04
3 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
0 finished solveLinearSystemImpl_ 1
0 converged value: 1 7 1 let s go get convergedRemote
0 to  comm_.min(converged) 
0 did  comm_.min(converged) convergedRemote: 1
0 final convergedRemote: 1
0 Update: x^(k+1) = x^k - deltax^k 
0 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
0 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
0 Newton iteration  3 done0 , maximum relative shift = 5.697e-04
0 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
5 did  comm_.min(converged) convergedRemote: 1
5 final convergedRemote: 1
5 Update: x^(k+1) = x^k - deltax^k 
5 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
5 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
5 Newton iteration  3 done5 , maximum relative shift = 5.697e-04
5 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
1 did  comm_.min(converged) convergedRemote: 1
1 final convergedRemote: 1
1 Update: x^(k+1) = x^k - deltax^k 
1 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
1 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
1 Newton iteration  3 done1 , maximum relative shift = 5.697e-04
1 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
2 did  comm_.min(converged) convergedRemote: 1
2 final convergedRemote: 1
2 Update: x^(k+1) = x^k - deltax^k 
2 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
2 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
2 Newton iteration  3 done2 , maximum relative shift = 5.697e-04
2 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
4 did  comm_.min(converged) convergedRemote: 1
4 final convergedRemote: 1
4 Update: x^(k+1) = x^k - deltax^k 
4 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
4 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
4 Newton iteration  3 done4 , maximum relative shift = 5.697e-04
4 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
6 did  comm_.min(converged) convergedRemote: 1
6 final convergedRemote: 1
6 Update: x^(k+1) = x^k - deltax^k 
6 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
6 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
6 Newton iteration  3 done6 , maximum relative shift = 5.697e-04
6 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
0 Solve: M deltax^k = r 
0 Newton::solveLinearSystem : entering the try section 3
0 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
0 solveLinearSystemImpl_, before converged = ls.solve 
0 7 amgbackend::solve isParallel 1
5 Solve: M deltax^k = r 
5 Newton::solveLinearSystem : entering the try section 3
5 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
5 solveLinearSystemImpl_, before converged = ls.solve 
5 7 amgbackend::solve isParallel 1
1 Solve: M deltax^k = r 
1 Newton::solveLinearSystem : entering the try section 3
1 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
1 solveLinearSystemImpl_, before converged = ls.solve 
1 7 amgbackend::solve isParallel 1
2 Solve: M deltax^k = r 
2 Newton::solveLinearSystem : entering the try section 3
2 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
2 solveLinearSystemImpl_, before converged = ls.solve 
2 7 amgbackend::solve isParallel 1
6 Solve: M deltax^k = r 
6 Newton::solveLinearSystem : entering the try section 3
6 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
6 solveLinearSystemImpl_, before converged = ls.solve 
6 7 amgbackend::solve isParallel 1
4 Solve: M deltax^k = r 
4 Newton::solveLinearSystem : entering the try section 3
4 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
4 solveLinearSystemImpl_, before converged = ls.solve 
4 7 amgbackend::solve isParallel 1
3 Solve: M deltax^k = r 
3 Newton::solveLinearSystem : entering the try section 3
3 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
3 solveLinearSystemImpl_, before converged = ls.solve 
3 7 amgbackend::solve isParallel 1
6 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
6 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
4 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
4 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
2 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
2 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
0 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
0 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 7
5 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
5 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
1 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
1 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
3 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
3 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 7
1 7 before Dune::BiCGSTABSolver 
1 Dune::BiCGSTABSolver.apply(x, b, result_) 
1 BiCGSTABSolver::apply 
1 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
6 7 before Dune::BiCGSTABSolver 
6 Dune::BiCGSTABSolver.apply(x, b, result_) 
6 BiCGSTABSolver::apply 
6 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
4 7 before Dune::BiCGSTABSolver 
4 Dune::BiCGSTABSolver.apply(x, b, result_) 
4 BiCGSTABSolver::apply 
4 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
2 7 before Dune::BiCGSTABSolver 
2 Dune::BiCGSTABSolver.apply(x, b, result_) 
2 BiCGSTABSolver::apply 
2 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
3 7 before Dune::BiCGSTABSolver 
3 Dune::BiCGSTABSolver.apply(x, b, result_) 
3 BiCGSTABSolver::apply 
3 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
5 7 before Dune::BiCGSTABSolver 
5 Dune::BiCGSTABSolver.apply(x, b, result_) 
5 BiCGSTABSolver::apply 
5 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
Using a direct coarse solver (SuperLU)
Building hierarchy of 1 levels (inclusive coarse solver) took 0.112086 seconds.
0 7 before Dune::BiCGSTABSolver 
0 Dune::BiCGSTABSolver.apply(x, b, result_) 
0 BiCGSTABSolver::apply 
0 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
0 === BiCGSTABSolver
 Iter          Defect            Rate
    0      4.56184e-13
0  (it = 0.5; it < _maxit; it+=.5) 0.5 250
5 === BiCGSTABSolver
 Iter          Defect            Rate
    0      4.56184e-13
5  (it = 0.5; it < _maxit; it+=.5) 0.5 250
5  before p = r 
5  before  y = W^-1 * p 
5  before _prec->apply(y,p); 0.5 250
5 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
1 === BiCGSTABSolver
 Iter          Defect            Rate
    0      4.56184e-13
1  (it = 0.5; it < _maxit; it+=.5) 0.5 250
1  before p = r 
1  before  y = W^-1 * p 
1  before _prec->apply(y,p); 0.5 250
1 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
6 === BiCGSTABSolver
 Iter          Defect            Rate
    0      4.56184e-13
6  (it = 0.5; it < _maxit; it+=.5) 0.5 250
6  before p = r 
6  before  y = W^-1 * p 
6  before _prec->apply(y,p); 0.5 250
6 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
4 === BiCGSTABSolver
 Iter          Defect            Rate
    0      4.56184e-13
4  (it = 0.5; it < _maxit; it+=.5) 0.5 250
4  before p = r 
4  before  y = W^-1 * p 
4  before _prec->apply(y,p); 0.5 250
4 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
2 === BiCGSTABSolver
 Iter          Defect            Rate
    0      4.56184e-13
2  (it = 0.5; it < _maxit; it+=.5) 0.5 250
2  before p = r 
2  before  y = W^-1 * p 
2  before _prec->apply(y,p); 0.5 250
2 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
3 === BiCGSTABSolver
 Iter          Defect            Rate
    0      4.56184e-13
3  (it = 0.5; it < _maxit; it+=.5) 0.5 250
3  before p = r 
3  before  y = W^-1 * p 
3  before _prec->apply(y,p); 0.5 250
3 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
0  before p = r 
0  before  y = W^-1 * p 
0  before _prec->apply(y,p); 0.5 250
0 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 7
0 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
1 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
6 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
6 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
6  before v = A * y 
6  before  alpha = rho_new / < rt, v > 
4 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
4 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
4  before v = A * y 
2 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
2 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
0 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
3 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
3 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
3  before v = A * y 
5 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
5 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
5  before v = A * y 
5  before  alpha = rho_new / < rt, v > 
0 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
0  before v = A * y 
0  before  alpha = rho_new / < rt, v > 
3  before  alpha = rho_new / < rt, v > 
2  before v = A * y 
2  before  alpha = rho_new / < rt, v > 
1 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
4  before  alpha = rho_new / < rt, v > 
3  before alpha = rho_new / h; 
3  before apply first correction to x 
3  before r = r - alpha*v 
3  before test stop criteria 
3 istlsolver::printOutput 
  0.5      4.11821e-32      9.02751e-20
3 === rate=8.14959e-39, T=0.12609, TIT=0.252181, IT=0.5
3 finished solveLinearSystemImpl_ 1
3 converged value: 1 7 1 let s go get convergedRemote
3 to  comm_.min(converged) 
2  before alpha = rho_new / h; 
2  before apply first correction to x 
2  before r = r - alpha*v 
2  before test stop criteria 
2 istlsolver::printOutput 
  0.5      4.11821e-32      9.02751e-20
2 === rate=8.14959e-39, T=0.125967, TIT=0.251933, IT=0.5
2 finished solveLinearSystemImpl_ 1
2 converged value: 1 7 1 let s go get convergedRemote
2 to  comm_.min(converged) 
6  before alpha = rho_new / h; 
6  before apply first correction to x 
6  before r = r - alpha*v 
6  before test stop criteria 
6 istlsolver::printOutput 
  0.5      4.11821e-32      9.02751e-20
6 === rate=8.14959e-39, T=0.125977, TIT=0.251953, IT=0.5
6 finished solveLinearSystemImpl_ 1
6 converged value: 1 7 1 let s go get convergedRemote
6 to  comm_.min(converged) 
1  before v = A * y 
1  before  alpha = rho_new / < rt, v > 
1  before alpha = rho_new / h; 
1  before apply first correction to x 
1  before r = r - alpha*v 
1  before test stop criteria 
1 istlsolver::printOutput 
  0.5      4.11821e-32      9.02751e-20
1 === rate=8.14959e-39, T=0.126092, TIT=0.252183, IT=0.5
1 finished solveLinearSystemImpl_ 1
1 converged value: 1 7 1 let s go get convergedRemote
1 to  comm_.min(converged) 
4  before alpha = rho_new / h; 
4  before apply first correction to x 
4  before r = r - alpha*v 
4  before test stop criteria 
4 istlsolver::printOutput 
  0.5      4.11821e-32      9.02751e-20
4 === rate=8.14959e-39, T=0.125958, TIT=0.251916, IT=0.5
4 finished solveLinearSystemImpl_ 1
4 converged value: 1 7 1 let s go get convergedRemote
4 to  comm_.min(converged) 
5  before alpha = rho_new / h; 
5  before apply first correction to x 
5  before r = r - alpha*v 
5  before test stop criteria 
5 istlsolver::printOutput 
  0.5      4.11821e-32      9.02751e-20
5 === rate=8.14959e-39, T=0.126091, TIT=0.252182, IT=0.5
5 finished solveLinearSystemImpl_ 1
5 converged value: 1 7 1 let s go get convergedRemote
5 to  comm_.min(converged) 
0  before alpha = rho_new / h; 
0  before apply first correction to x 
0  before r = r - alpha*v 
0  before test stop criteria 
0 istlsolver::printOutput 
  0.5      4.11821e-32      9.02751e-20
0 === rate=8.14959e-39, T=0.0218704, TIT=0.0437408, IT=0.5
1 did  comm_.min(converged) convergedRemote: 1
1 final convergedRemote: 1
1 Update: x^(k+1) = x^k - deltax^k 
1 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
1 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
1 Newton iteration  4 done1 , maximum relative shift = 7.920e-05
1 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
3 did  comm_.min(converged) convergedRemote: 1
3 final convergedRemote: 1
3 Update: x^(k+1) = x^k - deltax^k 
3 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
3 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
3 Newton iteration  4 done3 , maximum relative shift = 7.920e-05
3 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
2 did  comm_.min(converged) convergedRemote: 1
2 final convergedRemote: 1
2 Update: x^(k+1) = x^k - deltax^k 
2 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
2 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
2 Newton iteration  4 done2 , maximum relative shift = 7.920e-05
2 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
6 did  comm_.min(converged) convergedRemote: 1
6 final convergedRemote: 1
6 Update: x^(k+1) = x^k - deltax^k 
6 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
6 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
6 Newton iteration  4 done6 , maximum relative shift = 7.920e-05
6 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
0 finished solveLinearSystemImpl_ 1
0 converged value: 1 7 1 let s go get convergedRemote
0 to  comm_.min(converged) 
0 did  comm_.min(converged) convergedRemote: 1
0 final convergedRemote: 1
0 Update: x^(k+1) = x^k - deltax^k 
0 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
0 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
0 Newton iteration  4 done0 , maximum relative shift = 7.920e-05
0 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
4 did  comm_.min(converged) convergedRemote: 1
4 final convergedRemote: 1
4 Update: x^(k+1) = x^k - deltax^k 
4 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
4 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
4 Newton iteration  4 done4 , maximum relative shift = 7.920e-05
4 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
5 did  comm_.min(converged) convergedRemote: 1
5 final convergedRemote: 1
5 Update: x^(k+1) = x^k - deltax^k 
5 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
5 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
5 Newton iteration  4 done5 , maximum relative shift = 7.920e-05
5 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
0 Solve: M deltax^k = r 
0 Newton::solveLinearSystem : entering the try section 4
0 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
0 solveLinearSystemImpl_, before converged = ls.solve 
0 7 amgbackend::solve isParallel 1
4 Solve: M deltax^k = r 
4 Newton::solveLinearSystem : entering the try section 4
4 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
4 solveLinearSystemImpl_, before converged = ls.solve 
4 7 amgbackend::solve isParallel 1
6 Solve: M deltax^k = r 
6 Newton::solveLinearSystem : entering the try section 4
6 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
6 solveLinearSystemImpl_, before converged = ls.solve 
6 7 amgbackend::solve isParallel 1
1 Solve: M deltax^k = r 
1 Newton::solveLinearSystem : entering the try section 4
1 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
1 solveLinearSystemImpl_, before converged = ls.solve 
1 7 amgbackend::solve isParallel 1
3 Solve: M deltax^k = r 
3 Newton::solveLinearSystem : entering the try section 4
3 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
3 solveLinearSystemImpl_, before converged = ls.solve 
3 7 amgbackend::solve isParallel 1
2 Solve: M deltax^k = r 
2 Newton::solveLinearSystem : entering the try section 4
2 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
2 solveLinearSystemImpl_, before converged = ls.solve 
2 7 amgbackend::solve isParallel 1
5 Solve: M deltax^k = r 
5 Newton::solveLinearSystem : entering the try section 4
5 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
5 solveLinearSystemImpl_, before converged = ls.solve 
5 7 amgbackend::solve isParallel 1
2 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
2 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
0 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
0 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 7
4 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
4 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
6 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
6 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
1 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
1 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
3 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
3 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
5 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
5 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 7
3 7 before Dune::BiCGSTABSolver 
3 Dune::BiCGSTABSolver.apply(x, b, result_) 
3 BiCGSTABSolver::apply 
3 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
2 7 before Dune::BiCGSTABSolver 
2 Dune::BiCGSTABSolver.apply(x, b, result_) 
2 BiCGSTABSolver::apply 
2 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
5 7 before Dune::BiCGSTABSolver 
5 Dune::BiCGSTABSolver.apply(x, b, result_) 
5 BiCGSTABSolver::apply 
5 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
4 7 before Dune::BiCGSTABSolver 
4 Dune::BiCGSTABSolver.apply(x, b, result_) 
4 BiCGSTABSolver::apply 
6 7 before Dune::BiCGSTABSolver 
6 Dune::BiCGSTABSolver.apply(x, b, result_) 
6 BiCGSTABSolver::apply 
6 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
1 7 before Dune::BiCGSTABSolver 
1 Dune::BiCGSTABSolver.apply(x, b, result_) 
1 BiCGSTABSolver::apply 
1 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
4 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
Using a direct coarse solver (SuperLU)
Building hierarchy of 1 levels (inclusive coarse solver) took 0.114163 seconds.
0 7 before Dune::BiCGSTABSolver 
0 Dune::BiCGSTABSolver.apply(x, b, result_) 
0 BiCGSTABSolver::apply 
0 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
4 === BiCGSTABSolver
 Iter          Defect            Rate
    0      5.06577e-14
4  (it = 0.5; it < _maxit; it+=.5) 0.5 250
4  before p = r 
4  before  y = W^-1 * p 
4  before _prec->apply(y,p); 0.5 250
4 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
3 === BiCGSTABSolver
 Iter          Defect            Rate
    0      5.06577e-14
3  (it = 0.5; it < _maxit; it+=.5) 0.5 250
3  before p = r 
3  before  y = W^-1 * p 
3  before _prec->apply(y,p); 0.5 250
3 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
1 === BiCGSTABSolver
 Iter          Defect            Rate
    0      5.06577e-14
1  (it = 0.5; it < _maxit; it+=.5) 0.5 250
1  before p = r 
1  before  y = W^-1 * p 
1  before _prec->apply(y,p); 0.5 250
1 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
5 === BiCGSTABSolver
 Iter          Defect            Rate
    0      5.06577e-14
5  (it = 0.5; it < _maxit; it+=.5) 0.5 250
5  before p = r 
5  before  y = W^-1 * p 
5  before _prec->apply(y,p); 0.5 250
5 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
2 === BiCGSTABSolver
 Iter          Defect            Rate
    0      5.06577e-14
2  (it = 0.5; it < _maxit; it+=.5) 0.5 250
2  before p = r 
2  before  y = W^-1 * p 
2  before _prec->apply(y,p); 0.5 250
2 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
6 === BiCGSTABSolver
 Iter          Defect            Rate
    0      5.06577e-14
6  (it = 0.5; it < _maxit; it+=.5) 0.5 250
6  before p = r 
6  before  y = W^-1 * p 
6  before _prec->apply(y,p); 0.5 250
6 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
0 === BiCGSTABSolver
 Iter          Defect            Rate
    0      5.06577e-14
0  (it = 0.5; it < _maxit; it+=.5) 0.5 250
0  before p = r 
0  before  y = W^-1 * p 
0  before _prec->apply(y,p); 0.5 250
0 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 7
0 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
2 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
2 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
0 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
6 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
6 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
6  before v = A * y 
4 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
4 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
4  before v = A * y 
3 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
3 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
3  before v = A * y 
1 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
5 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
5 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
4  before  alpha = rho_new / < rt, v > 
3  before  alpha = rho_new / < rt, v > 
1 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
1  before v = A * y 
1  before  alpha = rho_new / < rt, v > 
2  before v = A * y 
5  before v = A * y 
5  before  alpha = rho_new / < rt, v > 
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
0 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
0  before v = A * y 
0  before  alpha = rho_new / < rt, v > 
6  before  alpha = rho_new / < rt, v > 
5  before alpha = rho_new / h; 
5  before apply first correction to x 
5  before r = r - alpha*v 
5  before test stop criteria 
5 istlsolver::printOutput 
  0.5      6.86802e-31      1.35577e-17
5 === rate=1.83811e-34, T=0.128225, TIT=0.25645, IT=0.5
5 finished solveLinearSystemImpl_ 1
5 converged value: 1 7 1 let s go get convergedRemote
5 to  comm_.min(converged) 
0  before alpha = rho_new / h; 
0  before apply first correction to x 
0  before r = r - alpha*v 
0  before test stop criteria 
0 istlsolver::printOutput 
  0.5      6.86802e-31      1.35577e-17
0 === rate=1.83811e-34, T=0.0221237, TIT=0.0442474, IT=0.5
2  before  alpha = rho_new / < rt, v > 
2  before alpha = rho_new / h; 
2  before apply first correction to x 
2  before r = r - alpha*v 
2  before test stop criteria 
2 istlsolver::printOutput 
  0.5      6.86802e-31      1.35577e-17
2 === rate=1.83811e-34, T=0.128372, TIT=0.256744, IT=0.5
2 finished solveLinearSystemImpl_ 1
2 converged value: 1 7 1 let s go get convergedRemote
2 to  comm_.min(converged) 
4  before alpha = rho_new / h; 
4  before apply first correction to x 
4  before r = r - alpha*v 
4  before test stop criteria 
4 istlsolver::printOutput 
  0.5      6.86802e-31      1.35577e-17
4 === rate=1.83811e-34, T=0.128255, TIT=0.25651, IT=0.5
4 finished solveLinearSystemImpl_ 1
4 converged value: 1 7 1 let s go get convergedRemote
4 to  comm_.min(converged) 
3  before alpha = rho_new / h; 
3  before apply first correction to x 
3  before r = r - alpha*v 
3  before test stop criteria 
3 istlsolver::printOutput 
  0.5      6.86802e-31      1.35577e-17
3 === rate=1.83811e-34, T=0.128225, TIT=0.256451, IT=0.5
3 finished solveLinearSystemImpl_ 1
3 converged value: 1 7 1 let s go get convergedRemote
3 to  comm_.min(converged) 
1  before alpha = rho_new / h; 
1  before apply first correction to x 
1  before r = r - alpha*v 
1  before test stop criteria 
1 istlsolver::printOutput 
  0.5      6.86802e-31      1.35577e-17
1 === rate=1.83811e-34, T=0.128225, TIT=0.25645, IT=0.5
1 finished solveLinearSystemImpl_ 1
1 converged value: 1 7 1 let s go get convergedRemote
1 to  comm_.min(converged) 
6  before alpha = rho_new / h; 
6  before apply first correction to x 
6  before r = r - alpha*v 
6  before test stop criteria 
6 istlsolver::printOutput 
  0.5      6.86802e-31      1.35577e-17
6 === rate=1.83811e-34, T=0.12826, TIT=0.25652, IT=0.5
6 finished solveLinearSystemImpl_ 1
6 converged value: 1 7 1 let s go get convergedRemote
6 to  comm_.min(converged) 
5 did  comm_.min(converged) convergedRemote: 1
5 final convergedRemote: 1
5 Update: x^(k+1) = x^k - deltax^k 
5 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
5 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
5 Newton iteration  5 done5 , maximum relative shift = 1.005e-05
5 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
0 finished solveLinearSystemImpl_ 1
0 converged value: 1 7 1 let s go get convergedRemote
0 to  comm_.min(converged) 
0 did  comm_.min(converged) convergedRemote: 1
0 final convergedRemote: 1
0 Update: x^(k+1) = x^k - deltax^k 
0 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
0 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
0 Newton iteration  5 done0 , maximum relative shift = 1.005e-05
0 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
2 did  comm_.min(converged) convergedRemote: 1
2 final convergedRemote: 1
2 Update: x^(k+1) = x^k - deltax^k 
2 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
2 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
2 Newton iteration  5 done2 , maximum relative shift = 1.005e-05
2 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
4 did  comm_.min(converged) convergedRemote: 1
4 final convergedRemote: 1
4 Update: x^(k+1) = x^k - deltax^k 
4 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
4 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
4 Newton iteration  5 done4 , maximum relative shift = 1.005e-05
4 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
3 did  comm_.min(converged) convergedRemote: 1
3 final convergedRemote: 1
3 Update: x^(k+1) = x^k - deltax^k 
3 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
3 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
3 Newton iteration  5 done3 , maximum relative shift = 1.005e-05
3 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
6 did  comm_.min(converged) convergedRemote: 1
6 final convergedRemote: 1
6 Update: x^(k+1) = x^k - deltax^k 
6 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
6 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
6 Newton iteration  5 done6 , maximum relative shift = 1.005e-05
6 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
1 did  comm_.min(converged) convergedRemote: 1
1 final convergedRemote: 1
1 Update: x^(k+1) = x^k - deltax^k 
1 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
1 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
1 Newton iteration  5 done1 , maximum relative shift = 1.005e-05
1 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
1 Solve: M deltax^k = r 
1 Newton::solveLinearSystem : entering the try section 5
1 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
1 solveLinearSystemImpl_, before converged = ls.solve 
1 7 amgbackend::solve isParallel 1
6 Solve: M deltax^k = r 
6 Newton::solveLinearSystem : entering the try section 5
6 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
6 solveLinearSystemImpl_, before converged = ls.solve 
6 7 amgbackend::solve isParallel 1
3 Solve: M deltax^k = r 
3 Newton::solveLinearSystem : entering the try section 5
3 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
3 solveLinearSystemImpl_, before converged = ls.solve 
3 7 amgbackend::solve isParallel 1
5 Solve: M deltax^k = r 
5 Newton::solveLinearSystem : entering the try section 5
5 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
5 solveLinearSystemImpl_, before converged = ls.solve 
5 7 amgbackend::solve isParallel 1
0 Solve: M deltax^k = r 
0 Newton::solveLinearSystem : entering the try section 5
0 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
0 solveLinearSystemImpl_, before converged = ls.solve 
0 7 amgbackend::solve isParallel 1
2 Solve: M deltax^k = r 
2 Newton::solveLinearSystem : entering the try section 5
2 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
2 solveLinearSystemImpl_, before converged = ls.solve 
2 7 amgbackend::solve isParallel 1
4 Solve: M deltax^k = r 
4 Newton::solveLinearSystem : entering the try section 5
4 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
4 solveLinearSystemImpl_, before converged = ls.solve 
4 7 amgbackend::solve isParallel 1
2 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
2 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
4 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
4 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
1 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
1 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
6 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
6 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
3 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
3 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
5 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
5 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
0 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
0 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 7
4 7 before Dune::BiCGSTABSolver 
4 Dune::BiCGSTABSolver.apply(x, b, result_) 
4 BiCGSTABSolver::apply 
4 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
1 7 before Dune::BiCGSTABSolver 
1 Dune::BiCGSTABSolver.apply(x, b, result_) 
1 BiCGSTABSolver::apply 
1 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
6 7 before Dune::BiCGSTABSolver 
6 Dune::BiCGSTABSolver.apply(x, b, result_) 
6 BiCGSTABSolver::apply 
6 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
3 7 before Dune::BiCGSTABSolver 
3 Dune::BiCGSTABSolver.apply(x, b, result_) 
3 BiCGSTABSolver::apply 
3 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
5 7 before Dune::BiCGSTABSolver 
5 Dune::BiCGSTABSolver.apply(x, b, result_) 
5 BiCGSTABSolver::apply 
5 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
2 7 before Dune::BiCGSTABSolver 
2 Dune::BiCGSTABSolver.apply(x, b, result_) 
2 BiCGSTABSolver::apply 
2 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
Using a direct coarse solver (SuperLU)
Building hierarchy of 1 levels (inclusive coarse solver) took 0.111459 seconds.
0 7 before Dune::BiCGSTABSolver 
0 Dune::BiCGSTABSolver.apply(x, b, result_) 
0 BiCGSTABSolver::apply 
0 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
2 === BiCGSTABSolver
 Iter          Defect            Rate
    0      8.01943e-16
2  (it = 0.5; it < _maxit; it+=.5) 0.5 250
2  before p = r 
2  before  y = W^-1 * p 
2  before _prec->apply(y,p); 0.5 250
2 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
1 === BiCGSTABSolver
 Iter          Defect            Rate
    0      8.01943e-16
1  (it = 0.5; it < _maxit; it+=.5) 0.5 250
1  before p = r 
1  before  y = W^-1 * p 
1  before _prec->apply(y,p); 0.5 250
1 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
6 === BiCGSTABSolver
 Iter          Defect            Rate
    0      8.01943e-16
6  (it = 0.5; it < _maxit; it+=.5) 0.5 250
6  before p = r 
6  before  y = W^-1 * p 
6  before _prec->apply(y,p); 0.5 250
6 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
3 === BiCGSTABSolver
 Iter          Defect            Rate
    0      8.01943e-16
3  (it = 0.5; it < _maxit; it+=.5) 0.5 250
3  before p = r 
3  before  y = W^-1 * p 
3  before _prec->apply(y,p); 0.5 250
3 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
5 === BiCGSTABSolver
 Iter          Defect            Rate
    0      8.01943e-16
5  (it = 0.5; it < _maxit; it+=.5) 0.5 250
5  before p = r 
5  before  y = W^-1 * p 
5  before _prec->apply(y,p); 0.5 250
5 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
0 === BiCGSTABSolver
 Iter          Defect            Rate
    0      8.01943e-16
0  (it = 0.5; it < _maxit; it+=.5) 0.5 250
0  before p = r 
0  before  y = W^-1 * p 
0  before _prec->apply(y,p); 0.5 250
0 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 7
0 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
4 === BiCGSTABSolver
 Iter          Defect            Rate
    0      8.01943e-16
4  (it = 0.5; it < _maxit; it+=.5) 0.5 250
4  before p = r 
4  before  y = W^-1 * p 
4  before _prec->apply(y,p); 0.5 250
4 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
4 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
4 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
4  before v = A * y 
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
0 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
5 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
5 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
5  before v = A * y 
2 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
2 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
1 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
6 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
6 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
6  before v = A * y 
3 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
3 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
3  before v = A * y 
3  before  alpha = rho_new / < rt, v > 
4  before  alpha = rho_new / < rt, v > 
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
0 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
5  before  alpha = rho_new / < rt, v > 
2  before v = A * y 
1 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
6  before  alpha = rho_new / < rt, v > 
1  before v = A * y 
1  before  alpha = rho_new / < rt, v > 
1  before alpha = rho_new / h; 
1  before apply first correction to x 
1  before r = r - alpha*v 
1  before test stop criteria 
1 istlsolver::printOutput 
  0.5      9.66935e-33      1.20574e-17
1 === rate=1.45381e-34, T=0.12261, TIT=0.24522, IT=0.5
1 finished solveLinearSystemImpl_ 1
1 converged value: 1 7 1 let s go get convergedRemote
1 to  comm_.min(converged) 
6  before alpha = rho_new / h; 
6  before apply first correction to x 
6  before r = r - alpha*v 
6  before test stop criteria 
6 istlsolver::printOutput 
  0.5      9.66935e-33      1.20574e-17
6 === rate=1.45381e-34, T=0.122549, TIT=0.245099, IT=0.5
6 finished solveLinearSystemImpl_ 1
6 converged value: 1 7 1 let s go get convergedRemote
6 to  comm_.min(converged) 
3  before alpha = rho_new / h; 
3  before apply first correction to x 
3  before r = r - alpha*v 
3  before test stop criteria 
3 istlsolver::printOutput 
  0.5      9.66935e-33      1.20574e-17
3 === rate=1.45381e-34, T=0.122605, TIT=0.24521, IT=0.5
3 finished solveLinearSystemImpl_ 1
3 converged value: 1 7 1 let s go get convergedRemote
3 to  comm_.min(converged) 
4  before alpha = rho_new / h; 
4  before apply first correction to x 
4  before r = r - alpha*v 
4  before test stop criteria 
4 istlsolver::printOutput 
  0.5      9.66935e-33      1.20574e-17
4 === rate=1.45381e-34, T=0.122549, TIT=0.245097, IT=0.5
4 finished solveLinearSystemImpl_ 1
4 converged value: 1 7 1 let s go get convergedRemote
4 to  comm_.min(converged) 
0  before v = A * y 
0  before  alpha = rho_new / < rt, v > 
0  before alpha = rho_new / h; 
0  before apply first correction to x 
0  before r = r - alpha*v 
0  before test stop criteria 
0 istlsolver::printOutput 
  0.5      9.66935e-33      1.20574e-17
0 === rate=1.45381e-34, T=0.0186472, TIT=0.0372945, IT=0.5
5  before alpha = rho_new / h; 
5  before apply first correction to x 
5  before r = r - alpha*v 
5  before test stop criteria 
5 istlsolver::printOutput 
  0.5      9.66935e-33      1.20574e-17
5 === rate=1.45381e-34, T=0.122604, TIT=0.245207, IT=0.5
5 finished solveLinearSystemImpl_ 1
5 converged value: 1 7 1 let s go get convergedRemote
5 to  comm_.min(converged) 
2  before  alpha = rho_new / < rt, v > 
2  before alpha = rho_new / h; 
2  before apply first correction to x 
2  before r = r - alpha*v 
2  before test stop criteria 
2 istlsolver::printOutput 
  0.5      9.66935e-33      1.20574e-17
2 === rate=1.45381e-34, T=0.122548, TIT=0.245097, IT=0.5
2 finished solveLinearSystemImpl_ 1
2 converged value: 1 7 1 let s go get convergedRemote
2 to  comm_.min(converged) 
2 did  comm_.min(converged) convergedRemote: 1
2 final convergedRemote: 1
2 Update: x^(k+1) = x^k - deltax^k 
2 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
2 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
2 Newton iteration  6 done2 , maximum relative shift = 1.591e-07
2 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
6 did  comm_.min(converged) convergedRemote: 1
6 final convergedRemote: 1
6 Update: x^(k+1) = x^k - deltax^k 
6 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
6 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
6 Newton iteration  6 done6 , maximum relative shift = 1.591e-07
6 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
3 did  comm_.min(converged) convergedRemote: 1
3 final convergedRemote: 1
3 Update: x^(k+1) = x^k - deltax^k 
3 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
3 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
3 Newton iteration  6 done3 , maximum relative shift = 1.591e-07
3 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
4 did  comm_.min(converged) convergedRemote: 1
4 final convergedRemote: 1
4 Update: x^(k+1) = x^k - deltax^k 
4 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
4 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
4 Newton iteration  6 done4 , maximum relative shift = 1.591e-07
4 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
0 finished solveLinearSystemImpl_ 1
0 converged value: 1 7 1 let s go get convergedRemote
0 to  comm_.min(converged) 
0 did  comm_.min(converged) convergedRemote: 1
0 final convergedRemote: 1
0 Update: x^(k+1) = x^k - deltax^k 
0 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
0 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
0 Newton iteration  6 done0 , maximum relative shift = 1.591e-07
0 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
5 did  comm_.min(converged) convergedRemote: 1
5 final convergedRemote: 1
5 Update: x^(k+1) = x^k - deltax^k 
5 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
5 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
5 Newton iteration  6 done5 , maximum relative shift = 1.591e-07
5 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
1 did  comm_.min(converged) convergedRemote: 1
1 final convergedRemote: 1
1 Update: x^(k+1) = x^k - deltax^k 
1 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
1 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
1 Newton iteration  6 done1 , maximum relative shift = 1.591e-07
1 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
6 Solve: M deltax^k = r 
6 Newton::solveLinearSystem : entering the try section 6
6 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
6 solveLinearSystemImpl_, before converged = ls.solve 
6 7 amgbackend::solve isParallel 1
3 Solve: M deltax^k = r 
3 Newton::solveLinearSystem : entering the try section 6
3 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
3 solveLinearSystemImpl_, before converged = ls.solve 
3 7 amgbackend::solve isParallel 1
4 Solve: M deltax^k = r 
4 Newton::solveLinearSystem : entering the try section 6
4 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
4 solveLinearSystemImpl_, before converged = ls.solve 
4 7 amgbackend::solve isParallel 1
0 Solve: M deltax^k = r 
0 Newton::solveLinearSystem : entering the try section 6
0 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
0 solveLinearSystemImpl_, before converged = ls.solve 
0 7 amgbackend::solve isParallel 1
5 Solve: M deltax^k = r 
5 Newton::solveLinearSystem : entering the try section 6
5 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
5 solveLinearSystemImpl_, before converged = ls.solve 
5 7 amgbackend::solve isParallel 1
1 Solve: M deltax^k = r 
1 Newton::solveLinearSystem : entering the try section 6
1 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
1 solveLinearSystemImpl_, before converged = ls.solve 
1 7 amgbackend::solve isParallel 1
2 Solve: M deltax^k = r 
2 Newton::solveLinearSystem : entering the try section 6
2 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
2 solveLinearSystemImpl_, before converged = ls.solve 
2 7 amgbackend::solve isParallel 1
3 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
3 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
4 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
4 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
0 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
0 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 7
2 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
2 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
1 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
1 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
5 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
5 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
6 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
6 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 7
3 7 before Dune::BiCGSTABSolver 
3 Dune::BiCGSTABSolver.apply(x, b, result_) 
3 BiCGSTABSolver::apply 
3 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
4 7 before Dune::BiCGSTABSolver 
4 Dune::BiCGSTABSolver.apply(x, b, result_) 
4 BiCGSTABSolver::apply 
4 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 7
2 7 before Dune::BiCGSTABSolver 
2 Dune::BiCGSTABSolver.apply(x, b, result_) 
2 BiCGSTABSolver::apply 
2 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
1 7 before Dune::BiCGSTABSolver 
1 Dune::BiCGSTABSolver.apply(x, b, result_) 
1 BiCGSTABSolver::apply 
1 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
5 7 before Dune::BiCGSTABSolver 
5 Dune::BiCGSTABSolver.apply(x, b, result_) 
5 BiCGSTABSolver::apply 
5 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
6 7 before Dune::BiCGSTABSolver 
6 Dune::BiCGSTABSolver.apply(x, b, result_) 
6 BiCGSTABSolver::apply 
6 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
Using a direct coarse solver (SuperLU)
Building hierarchy of 1 levels (inclusive coarse solver) took 0.111795 seconds.
0 7 before Dune::BiCGSTABSolver 
0 Dune::BiCGSTABSolver.apply(x, b, result_) 
0 BiCGSTABSolver::apply 
0 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
0 === BiCGSTABSolver
 Iter          Defect            Rate
2 === BiCGSTABSolver
 Iter          Defect            Rate
    0      1.26684e-17
2  (it = 0.5; it < _maxit; it+=.5) 0.5 250
2  before p = r 
2  before  y = W^-1 * p 
2  before _prec->apply(y,p); 0.5 250
2 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
1 === BiCGSTABSolver
 Iter          Defect            Rate
    0      1.26684e-17
1  (it = 0.5; it < _maxit; it+=.5) 0.5 250
1  before p = r 
1  before  y = W^-1 * p 
1  before _prec->apply(y,p); 0.5 250
1 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
5 === BiCGSTABSolver
 Iter          Defect            Rate
    0      1.26684e-17
5  (it = 0.5; it < _maxit; it+=.5) 0.5 250
5  before p = r 
5  before  y = W^-1 * p 
5  before _prec->apply(y,p); 0.5 250
5 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
4 === BiCGSTABSolver
 Iter          Defect            Rate
    0      1.26684e-17
4  (it = 0.5; it < _maxit; it+=.5) 0.5 250
4  before p = r 
4  before  y = W^-1 * p 
4  before _prec->apply(y,p); 0.5 250
4 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
3 === BiCGSTABSolver
 Iter          Defect            Rate
    0      1.26684e-17
3  (it = 0.5; it < _maxit; it+=.5) 0.5 250
3  before p = r 
3  before  y = W^-1 * p 
3  before _prec->apply(y,p); 0.5 250
3 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
    0      1.26684e-17
0  (it = 0.5; it < _maxit; it+=.5) 0.5 250
0  before p = r 
0  before  y = W^-1 * p 
0  before _prec->apply(y,p); 0.5 250
0 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 7
0 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
6 === BiCGSTABSolver
 Iter          Defect            Rate
    0      1.26684e-17
6  (it = 0.5; it < _maxit; it+=.5) 0.5 250
6  before p = r 
6  before  y = W^-1 * p 
6  before _prec->apply(y,p); 0.5 250
6 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
0 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
3 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
3 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
3  before v = A * y 
2 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
2 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
1 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
5 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
5 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
5  before v = A * y 
4 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
4 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
4  before v = A * y 
6 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
6 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
6  before v = A * y 
6  before  alpha = rho_new / < rt, v > 
2  before v = A * y 
2  before  alpha = rho_new / < rt, v > 
1 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
5  before  alpha = rho_new / < rt, v > 
4  before  alpha = rho_new / < rt, v > 
0 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
0  before v = A * y 
3  before  alpha = rho_new / < rt, v > 
5  before alpha = rho_new / h; 
5  before apply first correction to x 
5  before r = r - alpha*v 
5  before test stop criteria 
5 istlsolver::printOutput 
  0.5      1.54784e-33      1.22181e-16
5 === rate=1.49282e-32, T=0.122231, TIT=0.244462, IT=0.5
5 finished solveLinearSystemImpl_ 1
5 converged value: 1 7 1 let s go get convergedRemote
5 to  comm_.min(converged) 
4  before alpha = rho_new / h; 
4  before apply first correction to x 
4  before r = r - alpha*v 
4  before test stop criteria 
4 istlsolver::printOutput 
  0.5      1.54784e-33      1.22181e-16
4 === rate=1.49282e-32, T=0.122154, TIT=0.244307, IT=0.5
4 finished solveLinearSystemImpl_ 1
4 converged value: 1 7 1 let s go get convergedRemote
4 to  comm_.min(converged) 
0  before  alpha = rho_new / < rt, v > 
0  before alpha = rho_new / h; 
0  before apply first correction to x 
0  before r = r - alpha*v 
0  before test stop criteria 
0 istlsolver::printOutput 
  0.5      1.54784e-33      1.22181e-16
0 === rate=1.49282e-32, T=0.0184669, TIT=0.0369337, IT=0.5
6  before alpha = rho_new / h; 
6  before apply first correction to x 
6  before r = r - alpha*v 
6  before test stop criteria 
6 istlsolver::printOutput 
  0.5      1.54784e-33      1.22181e-16
6 === rate=1.49282e-32, T=0.12215, TIT=0.244301, IT=0.5
6 finished solveLinearSystemImpl_ 1
6 converged value: 1 7 1 let s go get convergedRemote
6 to  comm_.min(converged) 
2  before alpha = rho_new / h; 
2  before apply first correction to x 
2  before r = r - alpha*v 
2  before test stop criteria 
2 istlsolver::printOutput 
  0.5      1.54784e-33      1.22181e-16
2 === rate=1.49282e-32, T=0.122151, TIT=0.244302, IT=0.5
2 finished solveLinearSystemImpl_ 1
2 converged value: 1 7 1 let s go get convergedRemote
2 to  comm_.min(converged) 
1  before v = A * y 
1  before  alpha = rho_new / < rt, v > 
1  before alpha = rho_new / h; 
1  before apply first correction to x 
1  before r = r - alpha*v 
1  before test stop criteria 
1 istlsolver::printOutput 
  0.5      1.54784e-33      1.22181e-16
1 === rate=1.49282e-32, T=0.122228, TIT=0.244456, IT=0.5
1 finished solveLinearSystemImpl_ 1
1 converged value: 1 7 1 let s go get convergedRemote
1 to  comm_.min(converged) 
3  before alpha = rho_new / h; 
3  before apply first correction to x 
3  before r = r - alpha*v 
3  before test stop criteria 
3 istlsolver::printOutput 
  0.5      1.54784e-33      1.22181e-16
3 === rate=1.49282e-32, T=0.1224, TIT=0.244799, IT=0.5
3 finished solveLinearSystemImpl_ 1
3 converged value: 1 7 1 let s go get convergedRemote
3 to  comm_.min(converged) 
1 did  comm_.min(converged) convergedRemote: 1
1 final convergedRemote: 1
1 Update: x^(k+1) = x^k - deltax^k 
1 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
1 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
1 Newton iteration  7 done1 , maximum relative shift = 2.191e-09
1 Assemble/solve/update time: 0.309778(24.9303%)/0.932667(75.0592%)/0.000129703(0.0104382%)
1 gridVariables->advanceTimeStep
1 before, nonLinearSolver->suggestTimeStepSize, current time: 793.767 simTime 57600
1 before assembler->setPreviousSolution, ddt: 25
1 nonLinearSolver->solve
1 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
5 did  comm_.min(converged) convergedRemote: 1
5 final convergedRemote: 1
5 Update: x^(k+1) = x^k - deltax^k 
5 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
5 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
5 Newton iteration  7 done5 , maximum relative shift = 2.191e-09
5 Assemble/solve/update time: 0.309791(24.9311%)/0.932663(75.0582%)/0.000133185(0.0107184%)
5 gridVariables->advanceTimeStep
5 before, nonLinearSolver->suggestTimeStepSize, current time: 793.767 simTime 57600
5 before assembler->setPreviousSolution, ddt: 25
5 nonLinearSolver->solve
5 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
4 did  comm_.min(converged) convergedRemote: 1
4 final convergedRemote: 1
4 Update: x^(k+1) = x^k - deltax^k 
4 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
4 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
4 Newton iteration  7 done4 , maximum relative shift = 2.191e-09
4 Assemble/solve/update time: 0.309856(24.935%)/0.93266(75.0538%)/0.000139323(0.0112117%)
4 gridVariables->advanceTimeStep
4 before, nonLinearSolver->suggestTimeStepSize, current time: 793.767 simTime 57600
4 before assembler->setPreviousSolution, ddt: 25
4 nonLinearSolver->solve
4 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
0 finished solveLinearSystemImpl_ 1
0 converged value: 1 7 1 let s go get convergedRemote
0 to  comm_.min(converged) 
0 did  comm_.min(converged) convergedRemote: 1
0 final convergedRemote: 1
0 Update: x^(k+1) = x^k - deltax^k 
0 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
0 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
0 Newton iteration  7 done0 , maximum relative shift = 2.191e-09
0 Assemble/solve/update time: 0.309334(24.9036%)/0.932645(75.0845%)/0.000148155(0.0119275%)
0 gridVariables->advanceTimeStep
0 before, nonLinearSolver->suggestTimeStepSize, current time: 793.767 simTime 57600
0 before assembler->setPreviousSolution, ddt: 25
0 nonLinearSolver->solve
0 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
6 did  comm_.min(converged) convergedRemote: 1
6 final convergedRemote: 1
6 Update: x^(k+1) = x^k - deltax^k 
6 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
6 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
6 Newton iteration  7 done6 , maximum relative shift = 2.191e-09
6 Assemble/solve/update time: 0.309855(24.9351%)/0.932686(75.0564%)/0.000106022(0.00853195%)
6 gridVariables->advanceTimeStep
6 before, nonLinearSolver->suggestTimeStepSize, current time: 793.767 simTime 57600
6 before assembler->setPreviousSolution, ddt: 25
6 nonLinearSolver->solve
6 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
2 did  comm_.min(converged) convergedRemote: 1
2 final convergedRemote: 1
2 Update: x^(k+1) = x^k - deltax^k 
2 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
2 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
2 Newton iteration  7 done2 , maximum relative shift = 2.191e-09
2 Assemble/solve/update time: 0.309828(24.9328%)/0.932686(75.0559%)/0.000141324(0.0113727%)
2 gridVariables->advanceTimeStep
2 before, nonLinearSolver->suggestTimeStepSize, current time: 793.767 simTime 57600
2 before assembler->setPreviousSolution, ddt: 25
2 nonLinearSolver->solve
2 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
3 did  comm_.min(converged) convergedRemote: 1
3 final convergedRemote: 1
3 Update: x^(k+1) = x^k - deltax^k 
3 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
3 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
3 Newton iteration  7 done3 , maximum relative shift = 2.191e-09
3 Assemble/solve/update time: 0.309754(24.9283%)/0.932687(75.0605%)/0.000138434(0.0111409%)
3 gridVariables->advanceTimeStep
3 before, nonLinearSolver->suggestTimeStepSize, current time: 793.767 simTime 57600
3 before assembler->setPreviousSolution, ddt: 25
3 nonLinearSolver->solve
3 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
3 Solve: M deltax^k = r 
3 Newton::solveLinearSystem : entering the try section 0
3to  comm_.sum(norm2) 
3did  comm_.sum(norm2) 
3 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
3 solveLinearSystemImpl_, before converged = ls.solve 
3 7 amgbackend::solve isParallel 1
2 Solve: M deltax^k = r 
2 Newton::solveLinearSystem : entering the try section 0
2to  comm_.sum(norm2) 
2did  comm_.sum(norm2) 
2 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
2 solveLinearSystemImpl_, before converged = ls.solve 
2 7 amgbackend::solve isParallel 1
1 Solve: M deltax^k = r 
1 Newton::solveLinearSystem : entering the try section 0
1to  comm_.sum(norm2) 
1did  comm_.sum(norm2) 
1 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
1 solveLinearSystemImpl_, before converged = ls.solve 
1 7 amgbackend::solve isParallel 1
5 Solve: M deltax^k = r 
5 Newton::solveLinearSystem : entering the try section 0
5to  comm_.sum(norm2) 
5did  comm_.sum(norm2) 
5 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
5 solveLinearSystemImpl_, before converged = ls.solve 
5 7 amgbackend::solve isParallel 1
4 Solve: M deltax^k = r 
4 Newton::solveLinearSystem : entering the try section 0
4to  comm_.sum(norm2) 
4did  comm_.sum(norm2) 
4 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
4 solveLinearSystemImpl_, before converged = ls.solve 
4 7 amgbackend::solve isParallel 1
0 Solve: M deltax^k = r 
0 Newton::solveLinearSystem : entering the try section 0
0to  comm_.sum(norm2) 
0did  comm_.sum(norm2) 
0 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
0 solveLinearSystemImpl_, before converged = ls.solve 
0 7 amgbackend::solve isParallel 1
6 Solve: M deltax^k = r 
6 Newton::solveLinearSystem : entering the try section 0
6to  comm_.sum(norm2) 
6did  comm_.sum(norm2) 
6 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
6 solveLinearSystemImpl_, before converged = ls.solve 
6 7 amgbackend::solve isParallel 1
3 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
3 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
2 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
2 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
1 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
1 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
5 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
5 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
4 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
4 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
0 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
0 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 7
6 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
6 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 7
6 7 before Dune::BiCGSTABSolver 
6 Dune::BiCGSTABSolver.apply(x, b, result_) 
6 BiCGSTABSolver::apply 
6 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
3 7 before Dune::BiCGSTABSolver 
3 Dune::BiCGSTABSolver.apply(x, b, result_) 
3 BiCGSTABSolver::apply 
3 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
2 7 before Dune::BiCGSTABSolver 
2 Dune::BiCGSTABSolver.apply(x, b, result_) 
2 BiCGSTABSolver::apply 
2 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
1 7 before Dune::BiCGSTABSolver 
1 Dune::BiCGSTABSolver.apply(x, b, result_) 
1 BiCGSTABSolver::apply 
1 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
5 7 before Dune::BiCGSTABSolver 
5 Dune::BiCGSTABSolver.apply(x, b, result_) 
5 BiCGSTABSolver::apply 
5 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
4 7 before Dune::BiCGSTABSolver 
4 Dune::BiCGSTABSolver.apply(x, b, result_) 
4 BiCGSTABSolver::apply 
4 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
Using a direct coarse solver (SuperLU)
Building hierarchy of 1 levels (inclusive coarse solver) took 0.110785 seconds.
0 7 before Dune::BiCGSTABSolver 
0 Dune::BiCGSTABSolver.apply(x, b, result_) 
0 BiCGSTABSolver::apply 
0 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
4 === BiCGSTABSolver
 Iter          Defect            Rate
    0      3.16076e-09
4  (it = 0.5; it < _maxit; it+=.5) 0.5 250
4  before p = r 
4  before  y = W^-1 * p 
4  before _prec->apply(y,p); 0.5 250
4 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
6 === BiCGSTABSolver
 Iter          Defect            Rate
    0      3.16076e-09
6  (it = 0.5; it < _maxit; it+=.5) 0.5 250
6  before p = r 
6  before  y = W^-1 * p 
6  before _prec->apply(y,p); 0.5 250
6 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
3 === BiCGSTABSolver
 Iter          Defect            Rate
    0      3.16076e-09
3  (it = 0.5; it < _maxit; it+=.5) 0.5 250
3  before p = r 
3  before  y = W^-1 * p 
3  before _prec->apply(y,p); 0.5 250
3 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
2 === BiCGSTABSolver
 Iter          Defect            Rate
    0      3.16076e-09
2  (it = 0.5; it < _maxit; it+=.5) 0.5 250
2  before p = r 
2  before  y = W^-1 * p 
2  before _prec->apply(y,p); 0.5 250
2 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
1 === BiCGSTABSolver
 Iter          Defect            Rate
    0      3.16076e-09
1  (it = 0.5; it < _maxit; it+=.5) 0.5 250
1  before p = r 
1  before  y = W^-1 * p 
1  before _prec->apply(y,p); 0.5 250
1 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
5 === BiCGSTABSolver
 Iter          Defect            Rate
    0      3.16076e-09
5  (it = 0.5; it < _maxit; it+=.5) 0.5 250
5  before p = r 
5  before  y = W^-1 * p 
5  before _prec->apply(y,p); 0.5 250
5 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
0 === BiCGSTABSolver
 Iter          Defect            Rate
    0      3.16076e-09
0  (it = 0.5; it < _maxit; it+=.5) 0.5 250
0  before p = r 
0  before  y = W^-1 * p 
0  before _prec->apply(y,p); 0.5 250
0 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 7
0 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
6 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
6 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
6  before v = A * y 
6  before  alpha = rho_new / < rt, v > 
3 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
3 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
3  before v = A * y 
2 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
2 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
1 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
0 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
5 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
5 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
5  before v = A * y 
4 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
4 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
4  before v = A * y 
4  before  alpha = rho_new / < rt, v > 
0 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
5  before  alpha = rho_new / < rt, v > 
1 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
3  before  alpha = rho_new / < rt, v > 
5  before alpha = rho_new / h; 
5  before apply first correction to x 
5  before r = r - alpha*v 
5  before test stop criteria 
5 istlsolver::printOutput 
  0.5      1.08578e-24      3.43519e-16
5 === rate=1.18005e-31, T=0.122919, TIT=0.245838, IT=0.5
5 finished solveLinearSystemImpl_ 1
5 converged value: 1 7 1 let s go get convergedRemote
5 to  comm_.min(converged) 
6  before alpha = rho_new / h; 
6  before apply first correction to x 
6  before r = r - alpha*v 
6  before test stop criteria 
6 istlsolver::printOutput 
  0.5      1.08578e-24      3.43519e-16
6 === rate=1.18005e-31, T=0.122852, TIT=0.245704, IT=0.5
6 finished solveLinearSystemImpl_ 1
6 converged value: 1 7 1 let s go get convergedRemote
6 to  comm_.min(converged) 
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
1  before v = A * y 
1  before  alpha = rho_new / < rt, v > 
1  before alpha = rho_new / h; 
1  before apply first correction to x 
1  before r = r - alpha*v 
1  before test stop criteria 
1 istlsolver::printOutput 
  0.5      1.08578e-24      3.43519e-16
1 === rate=1.18005e-31, T=0.122923, TIT=0.245846, IT=0.5
1 finished solveLinearSystemImpl_ 1
1 converged value: 1 7 1 let s go get convergedRemote
1 to  comm_.min(converged) 
2  before v = A * y 
2  before  alpha = rho_new / < rt, v > 
2  before alpha = rho_new / h; 
2  before apply first correction to x 
2  before r = r - alpha*v 
2  before test stop criteria 
2 istlsolver::printOutput 
  0.5      1.08578e-24      3.43519e-16
2 === rate=1.18005e-31, T=0.122851, TIT=0.245702, IT=0.5
2 finished solveLinearSystemImpl_ 1
2 converged value: 1 7 1 let s go get convergedRemote
2 to  comm_.min(converged) 
3  before alpha = rho_new / h; 
3  before apply first correction to x 
3  before r = r - alpha*v 
3  before test stop criteria 
3 istlsolver::printOutput 
  0.5      1.08578e-24      3.43519e-16
3 === rate=1.18005e-31, T=0.122921, TIT=0.245842, IT=0.5
3 finished solveLinearSystemImpl_ 1
3 converged value: 1 7 1 let s go get convergedRemote
3 to  comm_.min(converged) 
4  before alpha = rho_new / h; 
4  before apply first correction to x 
4  before r = r - alpha*v 
4  before test stop criteria 
4 istlsolver::printOutput 
  0.5      1.08578e-24      3.43519e-16
4 === rate=1.18005e-31, T=0.122852, TIT=0.245705, IT=0.5
4 finished solveLinearSystemImpl_ 1
4 converged value: 1 7 1 let s go get convergedRemote
4 to  comm_.min(converged) 
0  before v = A * y 
0  before  alpha = rho_new / < rt, v > 
0  before alpha = rho_new / h; 
0  before apply first correction to x 
0  before r = r - alpha*v 
0  before test stop criteria 
0 istlsolver::printOutput 
  0.5      1.08578e-24      3.43519e-16
0 === rate=1.18005e-31, T=0.0184033, TIT=0.0368066, IT=0.5
5 did  comm_.min(converged) convergedRemote: 1
5 final convergedRemote: 1
5 Update: x^(k+1) = x^k - deltax^k 
5 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
5 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
5 Newton iteration  1 done5 , maximum relative shift = 5.871e-02
5 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
6 did  comm_.min(converged) convergedRemote: 1
6 final convergedRemote: 1
6 Update: x^(k+1) = x^k - deltax^k 
6 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
6 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
6 Newton iteration  1 done6 , maximum relative shift = 5.871e-02
6 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
1 did  comm_.min(converged) convergedRemote: 1
1 final convergedRemote: 1
1 Update: x^(k+1) = x^k - deltax^k 
1 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
1 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
1 Newton iteration  1 done1 , maximum relative shift = 5.871e-02
1 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
0 finished solveLinearSystemImpl_ 1
0 converged value: 1 7 1 let s go get convergedRemote
0 to  comm_.min(converged) 
0 did  comm_.min(converged) convergedRemote: 1
0 final convergedRemote: 1
0 Update: x^(k+1) = x^k - deltax^k 
0 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
0 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
0 Newton iteration  1 done0 , maximum relative shift = 5.871e-02
0 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
3 did  comm_.min(converged) convergedRemote: 1
3 final convergedRemote: 1
3 Update: x^(k+1) = x^k - deltax^k 
3 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
3 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
3 Newton iteration  1 done3 , maximum relative shift = 5.871e-02
3 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
4 did  comm_.min(converged) convergedRemote: 1
4 final convergedRemote: 1
4 Update: x^(k+1) = x^k - deltax^k 
4 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
4 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
4 Newton iteration  1 done4 , maximum relative shift = 5.871e-02
4 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
2 did  comm_.min(converged) convergedRemote: 1
2 final convergedRemote: 1
2 Update: x^(k+1) = x^k - deltax^k 
2 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
2 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
2 Newton iteration  1 done2 , maximum relative shift = 5.871e-02
2 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
2 Solve: M deltax^k = r 
2 Newton::solveLinearSystem : entering the try section 1
2 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
2 solveLinearSystemImpl_, before converged = ls.solve 
2 7 amgbackend::solve isParallel 1
0 Solve: M deltax^k = r 
0 Newton::solveLinearSystem : entering the try section 1
0 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
0 solveLinearSystemImpl_, before converged = ls.solve 
0 7 amgbackend::solve isParallel 1
3 Solve: M deltax^k = r 
3 Newton::solveLinearSystem : entering the try section 1
3 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
3 solveLinearSystemImpl_, before converged = ls.solve 
3 7 amgbackend::solve isParallel 1
4 Solve: M deltax^k = r 
4 Newton::solveLinearSystem : entering the try section 1
4 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
4 solveLinearSystemImpl_, before converged = ls.solve 
4 7 amgbackend::solve isParallel 1
1 Solve: M deltax^k = r 
1 Newton::solveLinearSystem : entering the try section 1
1 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
1 solveLinearSystemImpl_, before converged = ls.solve 
1 7 amgbackend::solve isParallel 1
5 Solve: M deltax^k = r 
5 Newton::solveLinearSystem : entering the try section 1
5 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
5 solveLinearSystemImpl_, before converged = ls.solve 
5 7 amgbackend::solve isParallel 1
6 Solve: M deltax^k = r 
6 Newton::solveLinearSystem : entering the try section 1
6 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
6 solveLinearSystemImpl_, before converged = ls.solve 
6 7 amgbackend::solve isParallel 1
6 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
6 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
2 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
2 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
0 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
0 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 7
3 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
3 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
4 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
4 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
1 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
1 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
5 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
5 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 7
4 7 before Dune::BiCGSTABSolver 
4 Dune::BiCGSTABSolver.apply(x, b, result_) 
4 BiCGSTABSolver::apply 
4 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
1 7 before Dune::BiCGSTABSolver 
1 Dune::BiCGSTABSolver.apply(x, b, result_) 
1 BiCGSTABSolver::apply 
1 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 7
6 7 before Dune::BiCGSTABSolver 
6 Dune::BiCGSTABSolver.apply(x, b, result_) 
6 BiCGSTABSolver::apply 
6 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
2 7 before Dune::BiCGSTABSolver 
2 Dune::BiCGSTABSolver.apply(x, b, result_) 
2 BiCGSTABSolver::apply 
2 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
5 7 before Dune::BiCGSTABSolver 
5 Dune::BiCGSTABSolver.apply(x, b, result_) 
5 BiCGSTABSolver::apply 
5 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
3 7 before Dune::BiCGSTABSolver 
3 Dune::BiCGSTABSolver.apply(x, b, result_) 
3 BiCGSTABSolver::apply 
3 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
Using a direct coarse solver (SuperLU)
Building hierarchy of 1 levels (inclusive coarse solver) took 0.111538 seconds.
0 7 before Dune::BiCGSTABSolver 
0 Dune::BiCGSTABSolver.apply(x, b, result_) 
0 BiCGSTABSolver::apply 
0 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
0 === BiCGSTABSolver
 Iter          Defect            Rate
    0      4.02449e-11
0  (it = 0.5; it < _maxit; it+=.5) 0.5 250
2 === BiCGSTABSolver
 Iter          Defect            Rate
    0      4.02449e-11
2  (it = 0.5; it < _maxit; it+=.5) 0.5 250
2  before p = r 
2  before  y = W^-1 * p 
2  before _prec->apply(y,p); 0.5 250
2 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
5 === BiCGSTABSolver
 Iter          Defect            Rate
    0      4.02449e-11
5  (it = 0.5; it < _maxit; it+=.5) 0.5 250
5  before p = r 
5  before  y = W^-1 * p 
5  before _prec->apply(y,p); 0.5 250
5 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
1 === BiCGSTABSolver
 Iter          Defect            Rate
    0      4.02449e-11
1  (it = 0.5; it < _maxit; it+=.5) 0.5 250
1  before p = r 
1  before  y = W^-1 * p 
1  before _prec->apply(y,p); 0.5 250
1 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
4 === BiCGSTABSolver
 Iter          Defect            Rate
    0      4.02449e-11
4  (it = 0.5; it < _maxit; it+=.5) 0.5 250
4  before p = r 
4  before  y = W^-1 * p 
4  before _prec->apply(y,p); 0.5 250
4 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
0  before p = r 
0  before  y = W^-1 * p 
0  before _prec->apply(y,p); 0.5 250
0 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 7
0 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
3 === BiCGSTABSolver
 Iter          Defect            Rate
    0      4.02449e-11
3  (it = 0.5; it < _maxit; it+=.5) 0.5 250
3  before p = r 
3  before  y = W^-1 * p 
3  before _prec->apply(y,p); 0.5 250
3 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
6 === BiCGSTABSolver
 Iter          Defect            Rate
    0      4.02449e-11
6  (it = 0.5; it < _maxit; it+=.5) 0.5 250
6  before p = r 
6  before  y = W^-1 * p 
6  before _prec->apply(y,p); 0.5 250
6 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
0 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
3 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
3 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
3  before v = A * y 
4 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
4 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
4  before v = A * y 
2 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
2 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
5 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
5 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
5  before v = A * y 
1 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
6 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
6 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
6  before v = A * y 
6  before  alpha = rho_new / < rt, v > 
1 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
0 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
3  before  alpha = rho_new / < rt, v > 
4  before  alpha = rho_new / < rt, v > 
2  before v = A * y 
2  before  alpha = rho_new / < rt, v > 
5  before  alpha = rho_new / < rt, v > 
6  before alpha = rho_new / h; 
6  before apply first correction to x 
6  before r = r - alpha*v 
6  before test stop criteria 
6 istlsolver::printOutput 
  0.5      3.27992e-27      8.14989e-17
6 === rate=6.64207e-33, T=0.123673, TIT=0.247347, IT=0.5
6 finished solveLinearSystemImpl_ 1
6 converged value: 1 7 1 let s go get convergedRemote
6 to  comm_.min(converged) 
1  before v = A * y 
1  before  alpha = rho_new / < rt, v > 
1  before alpha = rho_new / h; 
1  before apply first correction to x 
1  before r = r - alpha*v 
1  before test stop criteria 
1 istlsolver::printOutput 
  0.5      3.27992e-27      8.14989e-17
1 === rate=6.64207e-33, T=0.12373, TIT=0.247459, IT=0.5
1 finished solveLinearSystemImpl_ 1
1 converged value: 1 7 1 let s go get convergedRemote
1 to  comm_.min(converged) 
0  before v = A * y 
0  before  alpha = rho_new / < rt, v > 
0  before alpha = rho_new / h; 
0  before apply first correction to x 
0  before r = r - alpha*v 
0  before test stop criteria 
0 istlsolver::printOutput 
  0.5      3.27992e-27      8.14989e-17
0 === rate=6.64207e-33, T=0.019833, TIT=0.0396659, IT=0.5
3  before alpha = rho_new / h; 
3  before apply first correction to x 
3  before r = r - alpha*v 
3  before test stop criteria 
3 istlsolver::printOutput 
  0.5      3.27992e-27      8.14989e-17
3 === rate=6.64207e-33, T=0.123729, TIT=0.247457, IT=0.5
3 finished solveLinearSystemImpl_ 1
3 converged value: 1 7 1 let s go get convergedRemote
3 to  comm_.min(converged) 
4  before alpha = rho_new / h; 
4  before apply first correction to x 
4  before r = r - alpha*v 
4  before test stop criteria 
4 istlsolver::printOutput 
  0.5      3.27992e-27      8.14989e-17
4 === rate=6.64207e-33, T=0.123674, TIT=0.247348, IT=0.5
4 finished solveLinearSystemImpl_ 1
4 converged value: 1 7 1 let s go get convergedRemote
4 to  comm_.min(converged) 
2  before alpha = rho_new / h; 
2  before apply first correction to x 
2  before r = r - alpha*v 
2  before test stop criteria 
2 istlsolver::printOutput 
  0.5      3.27992e-27      8.14989e-17
2 === rate=6.64207e-33, T=0.123674, TIT=0.247347, IT=0.5
2 finished solveLinearSystemImpl_ 1
2 converged value: 1 7 1 let s go get convergedRemote
2 to  comm_.min(converged) 
5  before alpha = rho_new / h; 
5  before apply first correction to x 
5  before r = r - alpha*v 
5  before test stop criteria 
5 istlsolver::printOutput 
  0.5      3.27992e-27      8.14989e-17
5 === rate=6.64207e-33, T=0.123727, TIT=0.247454, IT=0.5
5 finished solveLinearSystemImpl_ 1
5 converged value: 1 7 1 let s go get convergedRemote
5 to  comm_.min(converged) 
5 did  comm_.min(converged) convergedRemote: 1
5 final convergedRemote: 1
5 Update: x^(k+1) = x^k - deltax^k 
5 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
5 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
5 Newton iteration  2 done5 , maximum relative shift = 7.550e-03
5 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
2 did  comm_.min(converged) convergedRemote: 1
2 final convergedRemote: 1
2 Update: x^(k+1) = x^k - deltax^k 
2 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
2 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
2 Newton iteration  2 done2 , maximum relative shift = 7.550e-03
2 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
6 did  comm_.min(converged) convergedRemote: 1
6 final convergedRemote: 1
6 Update: x^(k+1) = x^k - deltax^k 
6 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
6 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
6 Newton iteration  2 done6 , maximum relative shift = 7.550e-03
6 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
1 did  comm_.min(converged) convergedRemote: 1
1 final convergedRemote: 1
1 Update: x^(k+1) = x^k - deltax^k 
1 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
1 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
1 Newton iteration  2 done1 , maximum relative shift = 7.550e-03
1 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
0 finished solveLinearSystemImpl_ 1
0 converged value: 1 7 1 let s go get convergedRemote
0 to  comm_.min(converged) 
0 did  comm_.min(converged) convergedRemote: 1
0 final convergedRemote: 1
0 Update: x^(k+1) = x^k - deltax^k 
0 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
0 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
0 Newton iteration  2 done0 , maximum relative shift = 7.550e-03
0 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
3 did  comm_.min(converged) convergedRemote: 1
3 final convergedRemote: 1
3 Update: x^(k+1) = x^k - deltax^k 
3 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
3 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
3 Newton iteration  2 done3 , maximum relative shift = 7.550e-03
3 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
4 did  comm_.min(converged) convergedRemote: 1
4 final convergedRemote: 1
4 Update: x^(k+1) = x^k - deltax^k 
4 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
4 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
4 Newton iteration  2 done4 , maximum relative shift = 7.550e-03
4 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
2 Solve: M deltax^k = r 
2 Newton::solveLinearSystem : entering the try section 2
2 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
2 solveLinearSystemImpl_, before converged = ls.solve 
2 7 amgbackend::solve isParallel 1
6 Solve: M deltax^k = r 
6 Newton::solveLinearSystem : entering the try section 2
6 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
6 solveLinearSystemImpl_, before converged = ls.solve 
6 7 amgbackend::solve isParallel 1
1 Solve: M deltax^k = r 
1 Newton::solveLinearSystem : entering the try section 2
1 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
1 solveLinearSystemImpl_, before converged = ls.solve 
1 7 amgbackend::solve isParallel 1
0 Solve: M deltax^k = r 
0 Newton::solveLinearSystem : entering the try section 2
0 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
0 solveLinearSystemImpl_, before converged = ls.solve 
0 7 amgbackend::solve isParallel 1
3 Solve: M deltax^k = r 
3 Newton::solveLinearSystem : entering the try section 2
3 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
3 solveLinearSystemImpl_, before converged = ls.solve 
3 7 amgbackend::solve isParallel 1
4 Solve: M deltax^k = r 
4 Newton::solveLinearSystem : entering the try section 2
4 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
4 solveLinearSystemImpl_, before converged = ls.solve 
4 7 amgbackend::solve isParallel 1
5 Solve: M deltax^k = r 
5 Newton::solveLinearSystem : entering the try section 2
5 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
5 solveLinearSystemImpl_, before converged = ls.solve 
5 7 amgbackend::solve isParallel 1
5 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
5 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
4 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
4 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
3 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
3 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
2 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
2 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
6 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
6 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
1 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
1 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
0 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
0 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 7
3 7 before Dune::BiCGSTABSolver 
3 Dune::BiCGSTABSolver.apply(x, b, result_) 
3 BiCGSTABSolver::apply 
3 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
2 7 before Dune::BiCGSTABSolver 
2 Dune::BiCGSTABSolver.apply(x, b, result_) 
2 BiCGSTABSolver::apply 
2 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
6 7 before Dune::BiCGSTABSolver 
6 Dune::BiCGSTABSolver.apply(x, b, result_) 
6 BiCGSTABSolver::apply 
6 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
1 7 before Dune::BiCGSTABSolver 
1 Dune::BiCGSTABSolver.apply(x, b, result_) 
1 BiCGSTABSolver::apply 
1 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 7
5 7 before Dune::BiCGSTABSolver 
5 Dune::BiCGSTABSolver.apply(x, b, result_) 
5 BiCGSTABSolver::apply 
5 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
4 7 before Dune::BiCGSTABSolver 
4 Dune::BiCGSTABSolver.apply(x, b, result_) 
4 BiCGSTABSolver::apply 
4 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
Using a direct coarse solver (SuperLU)
Building hierarchy of 1 levels (inclusive coarse solver) took 0.110057 seconds.
0 7 before Dune::BiCGSTABSolver 
0 Dune::BiCGSTABSolver.apply(x, b, result_) 
0 BiCGSTABSolver::apply 
0 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
0 === BiCGSTABSolver
 Iter          Defect            Rate
    0      2.48999e-12
0  (it = 0.5; it < _maxit; it+=.5) 0.5 250
0  before p = r 
0  before  y = W^-1 * p 
0  before _prec->apply(y,p); 0.5 250
0 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
4 === BiCGSTABSolver
 Iter          Defect            Rate
    0      2.48999e-12
4  (it = 0.5; it < _maxit; it+=.5) 0.5 250
4  before p = r 
4  before  y = W^-1 * p 
4  before _prec->apply(y,p); 0.5 250
4 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
5 === BiCGSTABSolver
 Iter          Defect            Rate
    0      2.48999e-12
5  (it = 0.5; it < _maxit; it+=.5) 0.5 250
5  before p = r 
5  before  y = W^-1 * p 
5  before _prec->apply(y,p); 0.5 250
5 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
1 === BiCGSTABSolver
 Iter          Defect            Rate
    0      2.48999e-12
1  (it = 0.5; it < _maxit; it+=.5) 0.5 250
1  before p = r 
1  before  y = W^-1 * p 
1  before _prec->apply(y,p); 0.5 250
1 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
3 === BiCGSTABSolver
 Iter          Defect            Rate
    0      2.48999e-12
3  (it = 0.5; it < _maxit; it+=.5) 0.5 250
3  before p = r 
3  before  y = W^-1 * p 
3  before _prec->apply(y,p); 0.5 250
3 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
2 === BiCGSTABSolver
 Iter          Defect            Rate
    0      2.48999e-12
2  (it = 0.5; it < _maxit; it+=.5) 0.5 250
2  before p = r 
2  before  y = W^-1 * p 
2  before _prec->apply(y,p); 0.5 250
2 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
6 === BiCGSTABSolver
 Iter          Defect            Rate
    0      2.48999e-12
6  (it = 0.5; it < _maxit; it+=.5) 0.5 250
6  before p = r 
6  before  y = W^-1 * p 
6  before _prec->apply(y,p); 0.5 250
6 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 7
0 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
0 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
6 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
6 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
6  before v = A * y 
4 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
4 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
4  before v = A * y 
5 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
5 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
5  before v = A * y 
1 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
3 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
3 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
3  before v = A * y 
2 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
2 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
0 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
6  before  alpha = rho_new / < rt, v > 
4  before  alpha = rho_new / < rt, v > 
5  before  alpha = rho_new / < rt, v > 
1 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
3  before  alpha = rho_new / < rt, v > 
2  before v = A * y 
3  before alpha = rho_new / h; 
3  before apply first correction to x 
3  before r = r - alpha*v 
3  before test stop criteria 
3 istlsolver::printOutput 
  0.5      1.29739e-28      5.21042e-17
3 === rate=2.71485e-33, T=0.125512, TIT=0.251025, IT=0.5
3 finished solveLinearSystemImpl_ 1
3 converged value: 1 7 1 let s go get convergedRemote
3 to  comm_.min(converged) 
3 did  comm_.min(converged) convergedRemote: 1
3 final convergedRemote: 1
3 Update: x^(k+1) = x^k - deltax^k 
3 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
3 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
3 Newton iteration  3 done3 , maximum relative shift = 4.870e-04
3 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
0  before v = A * y 
0  before  alpha = rho_new / < rt, v > 
0  before alpha = rho_new / h; 
0  before apply first correction to x 
0  before r = r - alpha*v 
0  before test stop criteria 
0 istlsolver::printOutput 
  0.5      1.29739e-28      5.21042e-17
0 === rate=2.71485e-33, T=0.023161, TIT=0.046322, IT=0.5
0 finished solveLinearSystemImpl_ 1
0 converged value: 1 7 1 let s go get convergedRemote
0 to  comm_.min(converged) 
0 did  comm_.min(converged) convergedRemote: 1
0 final convergedRemote: 1
0 Update: x^(k+1) = x^k - deltax^k 
0 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
0 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
0 Newton iteration  3 done0 , maximum relative shift = 4.870e-04
0 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
6  before alpha = rho_new / h; 
6  before apply first correction to x 
6  before r = r - alpha*v 
6  before test stop criteria 
6 istlsolver::printOutput 
  0.5      1.29739e-28      5.21042e-17
6 === rate=2.71485e-33, T=0.125469, TIT=0.250937, IT=0.5
6 finished solveLinearSystemImpl_ 1
6 converged value: 1 7 1 let s go get convergedRemote
6 to  comm_.min(converged) 
6 did  comm_.min(converged) convergedRemote: 1
6 final convergedRemote: 1
6 Update: x^(k+1) = x^k - deltax^k 
6 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
6 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
6 Newton iteration  3 done6 , maximum relative shift = 4.870e-04
6 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
4  before alpha = rho_new / h; 
4  before apply first correction to x 
4  before r = r - alpha*v 
4  before test stop criteria 
4 istlsolver::printOutput 
  0.5      1.29739e-28      5.21042e-17
4 === rate=2.71485e-33, T=0.125468, TIT=0.250936, IT=0.5
4 finished solveLinearSystemImpl_ 1
4 converged value: 1 7 1 let s go get convergedRemote
4 to  comm_.min(converged) 
4 did  comm_.min(converged) convergedRemote: 1
4 final convergedRemote: 1
4 Update: x^(k+1) = x^k - deltax^k 
4 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
4 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
4 Newton iteration  3 done4 , maximum relative shift = 4.870e-04
4 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
5  before alpha = rho_new / h; 
5  before apply first correction to x 
5  before r = r - alpha*v 
5  before test stop criteria 
5 istlsolver::printOutput 
  0.5      1.29739e-28      5.21042e-17
5 === rate=2.71485e-33, T=0.12551, TIT=0.251021, IT=0.5
5 finished solveLinearSystemImpl_ 1
5 converged value: 1 7 1 let s go get convergedRemote
5 to  comm_.min(converged) 
5 did  comm_.min(converged) convergedRemote: 1
5 final convergedRemote: 1
5 Update: x^(k+1) = x^k - deltax^k 
5 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
5 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
5 Newton iteration  3 done5 , maximum relative shift = 4.870e-04
5 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
1  before v = A * y 
1  before  alpha = rho_new / < rt, v > 
1  before alpha = rho_new / h; 
1  before apply first correction to x 
1  before r = r - alpha*v 
1  before test stop criteria 
1 istlsolver::printOutput 
  0.5      1.29739e-28      5.21042e-17
1 === rate=2.71485e-33, T=0.125516, TIT=0.251033, IT=0.5
1 finished solveLinearSystemImpl_ 1
1 converged value: 1 7 1 let s go get convergedRemote
1 to  comm_.min(converged) 
1 did  comm_.min(converged) convergedRemote: 1
1 final convergedRemote: 1
1 Update: x^(k+1) = x^k - deltax^k 
1 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
1 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
1 Newton iteration  3 done1 , maximum relative shift = 4.870e-04
1 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
2  before  alpha = rho_new / < rt, v > 
2  before alpha = rho_new / h; 
2  before apply first correction to x 
2  before r = r - alpha*v 
2  before test stop criteria 
2 istlsolver::printOutput 
  0.5      1.29739e-28      5.21042e-17
2 === rate=2.71485e-33, T=0.125468, TIT=0.250937, IT=0.5
2 finished solveLinearSystemImpl_ 1
2 converged value: 1 7 1 let s go get convergedRemote
2 to  comm_.min(converged) 
2 did  comm_.min(converged) convergedRemote: 1
2 final convergedRemote: 1
2 Update: x^(k+1) = x^k - deltax^k 
2 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
2 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
2 Newton iteration  3 done2 , maximum relative shift = 4.870e-04
2 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
0 Solve: M deltax^k = r 
0 Newton::solveLinearSystem : entering the try section 3
0 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
0 solveLinearSystemImpl_, before converged = ls.solve 
0 7 amgbackend::solve isParallel 1
6 Solve: M deltax^k = r 
6 Newton::solveLinearSystem : entering the try section 3
6 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
6 solveLinearSystemImpl_, before converged = ls.solve 
6 7 amgbackend::solve isParallel 1
4 Solve: M deltax^k = r 
4 Newton::solveLinearSystem : entering the try section 3
4 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
4 solveLinearSystemImpl_, before converged = ls.solve 
4 7 amgbackend::solve isParallel 1
5 Solve: M deltax^k = r 
5 Newton::solveLinearSystem : entering the try section 3
5 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
5 solveLinearSystemImpl_, before converged = ls.solve 
5 7 amgbackend::solve isParallel 1
2 Solve: M deltax^k = r 
2 Newton::solveLinearSystem : entering the try section 3
2 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
2 solveLinearSystemImpl_, before converged = ls.solve 
2 7 amgbackend::solve isParallel 1
1 Solve: M deltax^k = r 
1 Newton::solveLinearSystem : entering the try section 3
1 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
1 solveLinearSystemImpl_, before converged = ls.solve 
1 7 amgbackend::solve isParallel 1
3 Solve: M deltax^k = r 
3 Newton::solveLinearSystem : entering the try section 3
3 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
3 solveLinearSystemImpl_, before converged = ls.solve 
3 7 amgbackend::solve isParallel 1
4 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
4 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
3 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
3 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
2 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
2 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
1 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
1 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
5 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
5 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
0 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
0 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 7
6 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
6 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 7
1 7 before Dune::BiCGSTABSolver 
1 Dune::BiCGSTABSolver.apply(x, b, result_) 
1 BiCGSTABSolver::apply 
1 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
5 7 before Dune::BiCGSTABSolver 
5 Dune::BiCGSTABSolver.apply(x, b, result_) 
5 BiCGSTABSolver::apply 
5 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
6 7 before Dune::BiCGSTABSolver 
6 Dune::BiCGSTABSolver.apply(x, b, result_) 
6 BiCGSTABSolver::apply 
6 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
4 7 before Dune::BiCGSTABSolver 
4 Dune::BiCGSTABSolver.apply(x, b, result_) 
4 BiCGSTABSolver::apply 
4 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 7
3 7 before Dune::BiCGSTABSolver 
3 Dune::BiCGSTABSolver.apply(x, b, result_) 
3 BiCGSTABSolver::apply 
3 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
2 7 before Dune::BiCGSTABSolver 
2 Dune::BiCGSTABSolver.apply(x, b, result_) 
2 BiCGSTABSolver::apply 
2 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
Using a direct coarse solver (SuperLU)
Building hierarchy of 1 levels (inclusive coarse solver) took 0.110634 seconds.
0 7 before Dune::BiCGSTABSolver 
0 Dune::BiCGSTABSolver.apply(x, b, result_) 
0 BiCGSTABSolver::apply 
0 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
0 === BiCGSTABSolver
 Iter          Defect            Rate
    0      1.75927e-13
0  (it = 0.5; it < _maxit; it+=.5) 0.5 250
0  before p = r 
0  before  y = W^-1 * p 
0  before _prec->apply(y,p); 0.5 250
0 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 7
0 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
2 === BiCGSTABSolver
 Iter          Defect            Rate
    0      1.75927e-13
2  (it = 0.5; it < _maxit; it+=.5) 0.5 250
2  before p = r 
2  before  y = W^-1 * p 
2  before _prec->apply(y,p); 0.5 250
2 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
3 === BiCGSTABSolver
 Iter          Defect            Rate
    0      1.75927e-13
3  (it = 0.5; it < _maxit; it+=.5) 0.5 250
3  before p = r 
3  before  y = W^-1 * p 
3  before _prec->apply(y,p); 0.5 250
3 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
4 === BiCGSTABSolver
 Iter          Defect            Rate
    0      1.75927e-13
4  (it = 0.5; it < _maxit; it+=.5) 0.5 250
4  before p = r 
4  before  y = W^-1 * p 
4  before _prec->apply(y,p); 0.5 250
4 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
1 === BiCGSTABSolver
 Iter          Defect            Rate
    0      1.75927e-13
1  (it = 0.5; it < _maxit; it+=.5) 0.5 250
1  before p = r 
1  before  y = W^-1 * p 
1  before _prec->apply(y,p); 0.5 250
1 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
5 === BiCGSTABSolver
 Iter          Defect            Rate
    0      1.75927e-13
5  (it = 0.5; it < _maxit; it+=.5) 0.5 250
5  before p = r 
5  before  y = W^-1 * p 
5  before _prec->apply(y,p); 0.5 250
5 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
6 === BiCGSTABSolver
 Iter          Defect            Rate
    0      1.75927e-13
6  (it = 0.5; it < _maxit; it+=.5) 0.5 250
6  before p = r 
6  before  y = W^-1 * p 
6  before _prec->apply(y,p); 0.5 250
6 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
0 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
2 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
2 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
3 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
3 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
3  before v = A * y 
4 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
4 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
4  before v = A * y 
1 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
5 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
5 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
5  before v = A * y 
6 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
6 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
6  before v = A * y 
4  before  alpha = rho_new / < rt, v > 
1 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
6  before  alpha = rho_new / < rt, v > 
5  before  alpha = rho_new / < rt, v > 
0 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
2  before v = A * y 
3  before  alpha = rho_new / < rt, v > 
6  before alpha = rho_new / h; 
6  before apply first correction to x 
6  before r = r - alpha*v 
6  before test stop criteria 
6 istlsolver::printOutput 
  0.5      1.38222e-29      7.85681e-17
6 === rate=6.17294e-33, T=0.122541, TIT=0.245082, IT=0.5
6 finished solveLinearSystemImpl_ 1
6 converged value: 1 7 1 let s go get convergedRemote
6 to  comm_.min(converged) 
5  before alpha = rho_new / h; 
5  before apply first correction to x 
5  before r = r - alpha*v 
5  before test stop criteria 
5 istlsolver::printOutput 
  0.5      1.38222e-29      7.85681e-17
5 === rate=6.17294e-33, T=0.122497, TIT=0.244994, IT=0.5
5 finished solveLinearSystemImpl_ 1
5 converged value: 1 7 1 let s go get convergedRemote
5 to  comm_.min(converged) 
0  before v = A * y 
0  before  alpha = rho_new / < rt, v > 
0  before alpha = rho_new / h; 
0  before apply first correction to x 
0  before r = r - alpha*v 
0  before test stop criteria 
0 istlsolver::printOutput 
  0.5      1.38222e-29      7.85681e-17
0 === rate=6.17294e-33, T=0.0199195, TIT=0.0398391, IT=0.5
2  before  alpha = rho_new / < rt, v > 
2  before alpha = rho_new / h; 
2  before apply first correction to x 
2  before r = r - alpha*v 
2  before test stop criteria 
2 istlsolver::printOutput 
  0.5      1.38222e-29      7.85681e-17
2 === rate=6.17294e-33, T=0.122536, TIT=0.245073, IT=0.5
2 finished solveLinearSystemImpl_ 1
2 converged value: 1 7 1 let s go get convergedRemote
2 to  comm_.min(converged) 
1  before v = A * y 
1  before  alpha = rho_new / < rt, v > 
1  before alpha = rho_new / h; 
1  before apply first correction to x 
1  before r = r - alpha*v 
1  before test stop criteria 
1 istlsolver::printOutput 
  0.5      1.38222e-29      7.85681e-17
1 === rate=6.17294e-33, T=0.122497, TIT=0.244994, IT=0.5
1 finished solveLinearSystemImpl_ 1
1 converged value: 1 7 1 let s go get convergedRemote
1 to  comm_.min(converged) 
4  before alpha = rho_new / h; 
4  before apply first correction to x 
4  before r = r - alpha*v 
4  before test stop criteria 
4 istlsolver::printOutput 
  0.5      1.38222e-29      7.85681e-17
4 === rate=6.17294e-33, T=0.122537, TIT=0.245073, IT=0.5
4 finished solveLinearSystemImpl_ 1
4 converged value: 1 7 1 let s go get convergedRemote
4 to  comm_.min(converged) 
3  before alpha = rho_new / h; 
3  before apply first correction to x 
3  before r = r - alpha*v 
3  before test stop criteria 
3 istlsolver::printOutput 
  0.5      1.38222e-29      7.85681e-17
3 === rate=6.17294e-33, T=0.122498, TIT=0.244997, IT=0.5
3 finished solveLinearSystemImpl_ 1
3 converged value: 1 7 1 let s go get convergedRemote
3 to  comm_.min(converged) 
4 did  comm_.min(converged) convergedRemote: 1
4 final convergedRemote: 1
4 Update: x^(k+1) = x^k - deltax^k 
4 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
4 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
4 Newton iteration  4 done4 , maximum relative shift = 4.016e-05
4 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
6 did  comm_.min(converged) convergedRemote: 1
6 final convergedRemote: 1
6 Update: x^(k+1) = x^k - deltax^k 
6 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
6 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
6 Newton iteration  4 done6 , maximum relative shift = 4.016e-05
6 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
5 did  comm_.min(converged) convergedRemote: 1
5 final convergedRemote: 1
5 Update: x^(k+1) = x^k - deltax^k 
5 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
5 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
5 Newton iteration  4 done5 , maximum relative shift = 4.016e-05
5 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
0 finished solveLinearSystemImpl_ 1
0 converged value: 1 7 1 let s go get convergedRemote
0 to  comm_.min(converged) 
0 did  comm_.min(converged) convergedRemote: 1
0 final convergedRemote: 1
0 Update: x^(k+1) = x^k - deltax^k 
0 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
0 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
0 Newton iteration  4 done0 , maximum relative shift = 4.016e-05
0 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
2 did  comm_.min(converged) convergedRemote: 1
2 final convergedRemote: 1
2 Update: x^(k+1) = x^k - deltax^k 
2 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
2 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
2 Newton iteration  4 done2 , maximum relative shift = 4.016e-05
2 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
1 did  comm_.min(converged) convergedRemote: 1
1 final convergedRemote: 1
1 Update: x^(k+1) = x^k - deltax^k 
1 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
1 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
1 Newton iteration  4 done1 , maximum relative shift = 4.016e-05
1 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
3 did  comm_.min(converged) convergedRemote: 1
3 final convergedRemote: 1
3 Update: x^(k+1) = x^k - deltax^k 
3 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
3 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
3 Newton iteration  4 done3 , maximum relative shift = 4.016e-05
3 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
1 Solve: M deltax^k = r 
1 Newton::solveLinearSystem : entering the try section 4
1 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
1 solveLinearSystemImpl_, before converged = ls.solve 
1 7 amgbackend::solve isParallel 1
4 Solve: M deltax^k = r 
4 Newton::solveLinearSystem : entering the try section 4
4 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
4 solveLinearSystemImpl_, before converged = ls.solve 
4 7 amgbackend::solve isParallel 1
6 Solve: M deltax^k = r 
6 Newton::solveLinearSystem : entering the try section 4
6 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
6 solveLinearSystemImpl_, before converged = ls.solve 
6 7 amgbackend::solve isParallel 1
5 Solve: M deltax^k = r 
5 Newton::solveLinearSystem : entering the try section 4
5 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
5 solveLinearSystemImpl_, before converged = ls.solve 
5 7 amgbackend::solve isParallel 1
0 Solve: M deltax^k = r 
0 Newton::solveLinearSystem : entering the try section 4
0 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
0 solveLinearSystemImpl_, before converged = ls.solve 
0 7 amgbackend::solve isParallel 1
2 Solve: M deltax^k = r 
2 Newton::solveLinearSystem : entering the try section 4
2 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
2 solveLinearSystemImpl_, before converged = ls.solve 
2 7 amgbackend::solve isParallel 1
3 Solve: M deltax^k = r 
3 Newton::solveLinearSystem : entering the try section 4
3 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
3 solveLinearSystemImpl_, before converged = ls.solve 
3 7 amgbackend::solve isParallel 1
5 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
5 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
0 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
0 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 7
3 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
3 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
2 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
2 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
1 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
1 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
4 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
4 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
6 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
6 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 7
3 7 before Dune::BiCGSTABSolver 
3 Dune::BiCGSTABSolver.apply(x, b, result_) 
3 BiCGSTABSolver::apply 
3 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
2 7 before Dune::BiCGSTABSolver 
2 Dune::BiCGSTABSolver.apply(x, b, result_) 
2 BiCGSTABSolver::apply 
2 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
1 7 before Dune::BiCGSTABSolver 
1 Dune::BiCGSTABSolver.apply(x, b, result_) 
1 BiCGSTABSolver::apply 
1 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
4 7 before Dune::BiCGSTABSolver 
4 Dune::BiCGSTABSolver.apply(x, b, result_) 
4 BiCGSTABSolver::apply 
4 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
6 7 before Dune::BiCGSTABSolver 
6 Dune::BiCGSTABSolver.apply(x, b, result_) 
6 BiCGSTABSolver::apply 
6 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
5 7 before Dune::BiCGSTABSolver 
5 Dune::BiCGSTABSolver.apply(x, b, result_) 
5 BiCGSTABSolver::apply 
5 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
Using a direct coarse solver (SuperLU)
Building hierarchy of 1 levels (inclusive coarse solver) took 0.111263 seconds.
0 7 before Dune::BiCGSTABSolver 
0 Dune::BiCGSTABSolver.apply(x, b, result_) 
0 BiCGSTABSolver::apply 
0 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
0 === BiCGSTABSolver
2 === BiCGSTABSolver
 Iter          Defect            Rate
    0      4.37116e-14
2  (it = 0.5; it < _maxit; it+=.5) 0.5 250
2  before p = r 
2  before  y = W^-1 * p 
2  before _prec->apply(y,p); 0.5 250
2 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
1 === BiCGSTABSolver
 Iter          Defect            Rate
    0      4.37116e-14
1  (it = 0.5; it < _maxit; it+=.5) 0.5 250
1  before p = r 
1  before  y = W^-1 * p 
1  before _prec->apply(y,p); 0.5 250
1 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
5 === BiCGSTABSolver
 Iter          Defect            Rate
    0      4.37116e-14
5  (it = 0.5; it < _maxit; it+=.5) 0.5 250
5  before p = r 
5  before  y = W^-1 * p 
5  before _prec->apply(y,p); 0.5 250
5 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
6 === BiCGSTABSolver
 Iter          Defect            Rate
    0      4.37116e-14
6  (it = 0.5; it < _maxit; it+=.5) 0.5 250
6  before p = r 
6  before  y = W^-1 * p 
6  before _prec->apply(y,p); 0.5 250
6 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
 Iter          Defect            Rate
    0      4.37116e-14
0  (it = 0.5; it < _maxit; it+=.5) 0.5 250
0  before p = r 
0  before  y = W^-1 * p 
0  before _prec->apply(y,p); 0.5 250
0 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 7
0 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
4 === BiCGSTABSolver
 Iter          Defect            Rate
    0      4.37116e-14
4  (it = 0.5; it < _maxit; it+=.5) 0.5 250
4  before p = r 
4  before  y = W^-1 * p 
4  before _prec->apply(y,p); 0.5 250
4 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
3 === BiCGSTABSolver
 Iter          Defect            Rate
    0      4.37116e-14
3  (it = 0.5; it < _maxit; it+=.5) 0.5 250
3  before p = r 
3  before  y = W^-1 * p 
3  before _prec->apply(y,p); 0.5 250
3 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
5 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
5 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
5  before v = A * y 
3 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
3 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
3  before v = A * y 
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
0 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
4 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
4 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
4  before v = A * y 
4  before  alpha = rho_new / < rt, v > 
6 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
6 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
6  before v = A * y 
6  before  alpha = rho_new / < rt, v > 
2 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
2 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
1 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
2  before v = A * y 
5  before  alpha = rho_new / < rt, v > 
1 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
3  before  alpha = rho_new / < rt, v > 
0 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
0  before v = A * y 
0  before  alpha = rho_new / < rt, v > 
0  before alpha = rho_new / h; 
0  before apply first correction to x 
0  before r = r - alpha*v 
0  before test stop criteria 
0 istlsolver::printOutput 
  0.5      6.31089e-30      1.44376e-16
0 === rate=2.08444e-32, T=0.0203487, TIT=0.0406973, IT=0.5
1  before v = A * y 
1  before  alpha = rho_new / < rt, v > 
1  before alpha = rho_new / h; 
1  before apply first correction to x 
1  before r = r - alpha*v 
1  before test stop criteria 
1 istlsolver::printOutput 
  0.5      6.31089e-30      1.44376e-16
1 === rate=2.08444e-32, T=0.123669, TIT=0.247338, IT=0.5
1 finished solveLinearSystemImpl_ 1
1 converged value: 1 7 1 let s go get convergedRemote
1 to  comm_.min(converged) 
3  before alpha = rho_new / h; 
3  before apply first correction to x 
3  before r = r - alpha*v 
3  before test stop criteria 
3 istlsolver::printOutput 
  0.5      6.31089e-30      1.44376e-16
3 === rate=2.08444e-32, T=0.123667, TIT=0.247334, IT=0.5
3 finished solveLinearSystemImpl_ 1
3 converged value: 1 7 1 let s go get convergedRemote
3 to  comm_.min(converged) 
4  before alpha = rho_new / h; 
4  before apply first correction to x 
4  before r = r - alpha*v 
4  before test stop criteria 
4 istlsolver::printOutput 
  0.5      6.31089e-30      1.44376e-16
4 === rate=2.08444e-32, T=0.123581, TIT=0.247162, IT=0.5
4 finished solveLinearSystemImpl_ 1
4 converged value: 1 7 1 let s go get convergedRemote
4 to  comm_.min(converged) 
6  before alpha = rho_new / h; 
6  before apply first correction to x 
6  before r = r - alpha*v 
6  before test stop criteria 
6 istlsolver::printOutput 
  0.5      6.31089e-30      1.44376e-16
6 === rate=2.08444e-32, T=0.123597, TIT=0.247194, IT=0.5
6 finished solveLinearSystemImpl_ 1
6 converged value: 1 7 1 let s go get convergedRemote
6 to  comm_.min(converged) 
5  before alpha = rho_new / h; 
5  before apply first correction to x 
5  before r = r - alpha*v 
5  before test stop criteria 
5 istlsolver::printOutput 
  0.5      6.31089e-30      1.44376e-16
5 === rate=2.08444e-32, T=0.123666, TIT=0.247331, IT=0.5
5 finished solveLinearSystemImpl_ 1
5 converged value: 1 7 1 let s go get convergedRemote
5 to  comm_.min(converged) 
2  before  alpha = rho_new / < rt, v > 
2  before alpha = rho_new / h; 
2  before apply first correction to x 
2  before r = r - alpha*v 
2  before test stop criteria 
2 istlsolver::printOutput 
  0.5      6.31089e-30      1.44376e-16
2 === rate=2.08444e-32, T=0.123597, TIT=0.247194, IT=0.5
2 finished solveLinearSystemImpl_ 1
2 converged value: 1 7 1 let s go get convergedRemote
2 to  comm_.min(converged) 
2 did  comm_.min(converged) convergedRemote: 1
2 final convergedRemote: 1
2 Update: x^(k+1) = x^k - deltax^k 
2 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
2 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
2 Newton iteration  5 done2 , maximum relative shift = 7.487e-06
2 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
0 finished solveLinearSystemImpl_ 1
0 converged value: 1 7 1 let s go get convergedRemote
0 to  comm_.min(converged) 
0 did  comm_.min(converged) convergedRemote: 1
0 final convergedRemote: 1
0 Update: x^(k+1) = x^k - deltax^k 
0 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
0 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
0 Newton iteration  5 done0 , maximum relative shift = 7.487e-06
0 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
1 did  comm_.min(converged) convergedRemote: 1
1 final convergedRemote: 1
1 Update: x^(k+1) = x^k - deltax^k 
1 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
1 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
1 Newton iteration  5 done1 , maximum relative shift = 7.487e-06
1 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
3 did  comm_.min(converged) convergedRemote: 1
3 final convergedRemote: 1
3 Update: x^(k+1) = x^k - deltax^k 
3 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
3 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
3 Newton iteration  5 done3 , maximum relative shift = 7.487e-06
3 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
4 did  comm_.min(converged) convergedRemote: 1
4 final convergedRemote: 1
4 Update: x^(k+1) = x^k - deltax^k 
4 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
4 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
4 Newton iteration  5 done4 , maximum relative shift = 7.487e-06
4 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
6 did  comm_.min(converged) convergedRemote: 1
6 final convergedRemote: 1
6 Update: x^(k+1) = x^k - deltax^k 
6 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
6 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
6 Newton iteration  5 done6 , maximum relative shift = 7.487e-06
6 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
5 did  comm_.min(converged) convergedRemote: 1
5 final convergedRemote: 1
5 Update: x^(k+1) = x^k - deltax^k 
5 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
5 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
5 Newton iteration  5 done5 , maximum relative shift = 7.487e-06
5 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
2 Solve: M deltax^k = r 
2 Newton::solveLinearSystem : entering the try section 5
2 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
2 solveLinearSystemImpl_, before converged = ls.solve 
2 7 amgbackend::solve isParallel 1
0 Solve: M deltax^k = r 
0 Newton::solveLinearSystem : entering the try section 5
0 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
0 solveLinearSystemImpl_, before converged = ls.solve 
0 7 amgbackend::solve isParallel 1
1 Solve: M deltax^k = r 
1 Newton::solveLinearSystem : entering the try section 5
1 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
1 solveLinearSystemImpl_, before converged = ls.solve 
1 7 amgbackend::solve isParallel 1
3 Solve: M deltax^k = r 
3 Newton::solveLinearSystem : entering the try section 5
3 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
3 solveLinearSystemImpl_, before converged = ls.solve 
3 7 amgbackend::solve isParallel 1
4 Solve: M deltax^k = r 
4 Newton::solveLinearSystem : entering the try section 5
4 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
4 solveLinearSystemImpl_, before converged = ls.solve 
4 7 amgbackend::solve isParallel 1
6 Solve: M deltax^k = r 
6 Newton::solveLinearSystem : entering the try section 5
6 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
6 solveLinearSystemImpl_, before converged = ls.solve 
6 7 amgbackend::solve isParallel 1
5 Solve: M deltax^k = r 
5 Newton::solveLinearSystem : entering the try section 5
5 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
5 solveLinearSystemImpl_, before converged = ls.solve 
5 7 amgbackend::solve isParallel 1
1 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
1 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
3 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
3 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
4 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
4 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
5 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
5 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
6 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
6 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
2 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
2 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
0 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
0 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 7
6 7 before Dune::BiCGSTABSolver 
6 Dune::BiCGSTABSolver.apply(x, b, result_) 
6 BiCGSTABSolver::apply 
6 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
2 7 before Dune::BiCGSTABSolver 
2 Dune::BiCGSTABSolver.apply(x, b, result_) 
2 BiCGSTABSolver::apply 
2 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
4 7 before Dune::BiCGSTABSolver 
4 Dune::BiCGSTABSolver.apply(x, b, result_) 
4 BiCGSTABSolver::apply 
4 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
1 7 before Dune::BiCGSTABSolver 
1 Dune::BiCGSTABSolver.apply(x, b, result_) 
1 BiCGSTABSolver::apply 
1 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
3 7 before Dune::BiCGSTABSolver 
3 Dune::BiCGSTABSolver.apply(x, b, result_) 
3 BiCGSTABSolver::apply 
3 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 7
5 7 before Dune::BiCGSTABSolver 
5 Dune::BiCGSTABSolver.apply(x, b, result_) 
5 BiCGSTABSolver::apply 
5 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
Using a direct coarse solver (SuperLU)
Building hierarchy of 1 levels (inclusive coarse solver) took 0.110177 seconds.
0 7 before Dune::BiCGSTABSolver 
0 Dune::BiCGSTABSolver.apply(x, b, result_) 
0 BiCGSTABSolver::apply 
0 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
0 === BiCGSTABSolver
 Iter          Defect            Rate
    0      2.76369e-15
0  (it = 0.5; it < _maxit; it+=.5) 0.5 250
3 === BiCGSTABSolver
 Iter          Defect            Rate
    0      2.76369e-15
3  (it = 0.5; it < _maxit; it+=.5) 0.5 250
3  before p = r 
3  before  y = W^-1 * p 
3  before _prec->apply(y,p); 0.5 250
3 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
6 === BiCGSTABSolver
 Iter          Defect            Rate
    0      2.76369e-15
6  (it = 0.5; it < _maxit; it+=.5) 0.5 250
6  before p = r 
6  before  y = W^-1 * p 
6  before _prec->apply(y,p); 0.5 250
6 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
2 === BiCGSTABSolver
 Iter          Defect            Rate
    0      2.76369e-15
2  (it = 0.5; it < _maxit; it+=.5) 0.5 250
2  before p = r 
2  before  y = W^-1 * p 
2  before _prec->apply(y,p); 0.5 250
2 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
4 === BiCGSTABSolver
 Iter          Defect            Rate
    0      2.76369e-15
4  (it = 0.5; it < _maxit; it+=.5) 0.5 250
4  before p = r 
4  before  y = W^-1 * p 
4  before _prec->apply(y,p); 0.5 250
4 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
1 === BiCGSTABSolver
 Iter          Defect            Rate
    0      2.76369e-15
1  (it = 0.5; it < _maxit; it+=.5) 0.5 250
1  before p = r 
1  before  y = W^-1 * p 
1  before _prec->apply(y,p); 0.5 250
1 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
0  before p = r 
0  before  y = W^-1 * p 
0  before _prec->apply(y,p); 0.5 250
0 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 7
0 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
5 === BiCGSTABSolver
 Iter          Defect            Rate
    0      2.76369e-15
5  (it = 0.5; it < _maxit; it+=.5) 0.5 250
5  before p = r 
5  before  y = W^-1 * p 
5  before _prec->apply(y,p); 0.5 250
5 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
5 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
5 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
5  before v = A * y 
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
0 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
1 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
3 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
3 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
3  before v = A * y 
6 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
6 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
6  before v = A * y 
2 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
2 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
4 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
4 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
4  before v = A * y 
4  before  alpha = rho_new / < rt, v > 
5  before  alpha = rho_new / < rt, v > 
0 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
0  before v = A * y 
1 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
1  before v = A * y 
3  before  alpha = rho_new / < rt, v > 
6  before  alpha = rho_new / < rt, v > 
2  before v = A * y 
4  before alpha = rho_new / h; 
4  before apply first correction to x 
4  before r = r - alpha*v 
4  before test stop criteria 
4 istlsolver::printOutput 
  0.5      2.19233e-31      7.93262e-17
4 === rate=6.29265e-33, T=0.120575, TIT=0.24115, IT=0.5
4 finished solveLinearSystemImpl_ 1
4 converged value: 1 7 1 let s go get convergedRemote
4 to  comm_.min(converged) 
5  before alpha = rho_new / h; 
5  before apply first correction to x 
5  before r = r - alpha*v 
5  before test stop criteria 
5 istlsolver::printOutput 
  0.5      2.19233e-31      7.93262e-17
5 === rate=6.29265e-33, T=0.120695, TIT=0.24139, IT=0.5
5 finished solveLinearSystemImpl_ 1
5 converged value: 1 7 1 let s go get convergedRemote
5 to  comm_.min(converged) 
0  before  alpha = rho_new / < rt, v > 
0  before alpha = rho_new / h; 
0  before apply first correction to x 
0  before r = r - alpha*v 
0  before test stop criteria 
0 istlsolver::printOutput 
  0.5      2.19233e-31      7.93262e-17
0 === rate=6.29265e-33, T=0.018302, TIT=0.0366039, IT=0.5
1  before  alpha = rho_new / < rt, v > 
1  before alpha = rho_new / h; 
1  before apply first correction to x 
1  before r = r - alpha*v 
1  before test stop criteria 
1 istlsolver::printOutput 
  0.5      2.19233e-31      7.93262e-17
1 === rate=6.29265e-33, T=0.120695, TIT=0.24139, IT=0.5
1 finished solveLinearSystemImpl_ 1
1 converged value: 1 7 1 let s go get convergedRemote
1 to  comm_.min(converged) 
3  before alpha = rho_new / h; 
3  before apply first correction to x 
3  before r = r - alpha*v 
3  before test stop criteria 
3 istlsolver::printOutput 
  0.5      2.19233e-31      7.93262e-17
3 === rate=6.29265e-33, T=0.120694, TIT=0.241388, IT=0.5
3 finished solveLinearSystemImpl_ 1
3 converged value: 1 7 1 let s go get convergedRemote
3 to  comm_.min(converged) 
6  before alpha = rho_new / h; 
6  before apply first correction to x 
6  before r = r - alpha*v 
6  before test stop criteria 
6 istlsolver::printOutput 
  0.5      2.19233e-31      7.93262e-17
6 === rate=6.29265e-33, T=0.120572, TIT=0.241144, IT=0.5
6 finished solveLinearSystemImpl_ 1
6 converged value: 1 7 1 let s go get convergedRemote
6 to  comm_.min(converged) 
2  before  alpha = rho_new / < rt, v > 
2  before alpha = rho_new / h; 
2  before apply first correction to x 
2  before r = r - alpha*v 
2  before test stop criteria 
2 istlsolver::printOutput 
  0.5      2.19233e-31      7.93262e-17
2 === rate=6.29265e-33, T=0.120571, TIT=0.241143, IT=0.5
2 finished solveLinearSystemImpl_ 1
2 converged value: 1 7 1 let s go get convergedRemote
2 to  comm_.min(converged) 
0 finished solveLinearSystemImpl_ 1
0 converged value: 1 7 1 let s go get convergedRemote
0 to  comm_.min(converged) 
0 did  comm_.min(converged) convergedRemote: 1
0 final convergedRemote: 1
0 Update: x^(k+1) = x^k - deltax^k 
0 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
0 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
0 Newton iteration  6 done0 , maximum relative shift = 5.409e-07
0 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
1 did  comm_.min(converged) convergedRemote: 1
1 final convergedRemote: 1
1 Update: x^(k+1) = x^k - deltax^k 
1 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
1 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
1 Newton iteration  6 done1 , maximum relative shift = 5.409e-07
1 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
3 did  comm_.min(converged) convergedRemote: 1
3 final convergedRemote: 1
3 Update: x^(k+1) = x^k - deltax^k 
3 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
3 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
3 Newton iteration  6 done3 , maximum relative shift = 5.409e-07
3 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
2 did  comm_.min(converged) convergedRemote: 1
2 final convergedRemote: 1
2 Update: x^(k+1) = x^k - deltax^k 
2 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
2 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
2 Newton iteration  6 done2 , maximum relative shift = 5.409e-07
2 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
6 did  comm_.min(converged) convergedRemote: 1
6 final convergedRemote: 1
6 Update: x^(k+1) = x^k - deltax^k 
6 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
6 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
6 Newton iteration  6 done6 , maximum relative shift = 5.409e-07
6 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
4 did  comm_.min(converged) convergedRemote: 1
4 final convergedRemote: 1
4 Update: x^(k+1) = x^k - deltax^k 
4 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
4 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
4 Newton iteration  6 done4 , maximum relative shift = 5.409e-07
4 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
5 did  comm_.min(converged) convergedRemote: 1
5 final convergedRemote: 1
5 Update: x^(k+1) = x^k - deltax^k 
5 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
5 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
5 Newton iteration  6 done5 , maximum relative shift = 5.409e-07
5 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
2 Solve: M deltax^k = r 
2 Newton::solveLinearSystem : entering the try section 6
2 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
2 solveLinearSystemImpl_, before converged = ls.solve 
2 7 amgbackend::solve isParallel 1
6 Solve: M deltax^k = r 
6 Newton::solveLinearSystem : entering the try section 6
6 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
6 solveLinearSystemImpl_, before converged = ls.solve 
6 7 amgbackend::solve isParallel 1
4 Solve: M deltax^k = r 
4 Newton::solveLinearSystem : entering the try section 6
4 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
4 solveLinearSystemImpl_, before converged = ls.solve 
4 7 amgbackend::solve isParallel 1
3 Solve: M deltax^k = r 
3 Newton::solveLinearSystem : entering the try section 6
3 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
3 solveLinearSystemImpl_, before converged = ls.solve 
3 7 amgbackend::solve isParallel 1
0 Solve: M deltax^k = r 
0 Newton::solveLinearSystem : entering the try section 6
0 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
0 solveLinearSystemImpl_, before converged = ls.solve 
0 7 amgbackend::solve isParallel 1
1 Solve: M deltax^k = r 
1 Newton::solveLinearSystem : entering the try section 6
1 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
1 solveLinearSystemImpl_, before converged = ls.solve 
1 7 amgbackend::solve isParallel 1
5 Solve: M deltax^k = r 
5 Newton::solveLinearSystem : entering the try section 6
5 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
5 solveLinearSystemImpl_, before converged = ls.solve 
5 7 amgbackend::solve isParallel 1
5 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
5 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
1 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
1 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
2 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
2 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
6 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
6 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
4 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
4 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
3 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
3 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
0 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
0 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 7
5 7 before Dune::BiCGSTABSolver 
5 Dune::BiCGSTABSolver.apply(x, b, result_) 
5 BiCGSTABSolver::apply 
5 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
1 7 before Dune::BiCGSTABSolver 
1 Dune::BiCGSTABSolver.apply(x, b, result_) 
1 BiCGSTABSolver::apply 
1 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
2 7 before Dune::BiCGSTABSolver 
2 Dune::BiCGSTABSolver.apply(x, b, result_) 
2 BiCGSTABSolver::apply 
2 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
6 7 before Dune::BiCGSTABSolver 
6 Dune::BiCGSTABSolver.apply(x, b, result_) 
6 BiCGSTABSolver::apply 
6 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
4 7 before Dune::BiCGSTABSolver 
4 Dune::BiCGSTABSolver.apply(x, b, result_) 
4 BiCGSTABSolver::apply 
4 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
3 7 before Dune::BiCGSTABSolver 
3 Dune::BiCGSTABSolver.apply(x, b, result_) 
3 BiCGSTABSolver::apply 
3 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
Using a direct coarse solver (SuperLU)
Building hierarchy of 1 levels (inclusive coarse solver) took 0.111387 seconds.
0 7 before Dune::BiCGSTABSolver 
0 Dune::BiCGSTABSolver.apply(x, b, result_) 
0 BiCGSTABSolver::apply 
0 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
0 === BiCGSTABSolver
 Iter          Defect            Rate
    0       1.9451e-16
0  (it = 0.5; it < _maxit; it+=.5) 0.5 250
0  before p = r 
0  before  y = W^-1 * p 
0  before _prec->apply(y,p); 0.5 250
0 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 7
0 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
3 === BiCGSTABSolver
 Iter          Defect            Rate
    0       1.9451e-16
3  (it = 0.5; it < _maxit; it+=.5) 0.5 250
3  before p = r 
3  before  y = W^-1 * p 
3  before _prec->apply(y,p); 0.5 250
3 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
5 === BiCGSTABSolver
 Iter          Defect            Rate
    0       1.9451e-16
5  (it = 0.5; it < _maxit; it+=.5) 0.5 250
5  before p = r 
5  before  y = W^-1 * p 
5  before _prec->apply(y,p); 0.5 250
5 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
1 === BiCGSTABSolver
 Iter          Defect            Rate
    0       1.9451e-16
1  (it = 0.5; it < _maxit; it+=.5) 0.5 250
1  before p = r 
1  before  y = W^-1 * p 
1  before _prec->apply(y,p); 0.5 250
1 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
2 === BiCGSTABSolver
 Iter          Defect            Rate
    0       1.9451e-16
2  (it = 0.5; it < _maxit; it+=.5) 0.5 250
2  before p = r 
2  before  y = W^-1 * p 
2  before _prec->apply(y,p); 0.5 250
2 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
6 === BiCGSTABSolver
 Iter          Defect            Rate
    0       1.9451e-16
6  (it = 0.5; it < _maxit; it+=.5) 0.5 250
6  before p = r 
6  before  y = W^-1 * p 
6  before _prec->apply(y,p); 0.5 250
6 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
4 === BiCGSTABSolver
 Iter          Defect            Rate
    0       1.9451e-16
4  (it = 0.5; it < _maxit; it+=.5) 0.5 250
4  before p = r 
4  before  y = W^-1 * p 
4  before _prec->apply(y,p); 0.5 250
4 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
1 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
2 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
2 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
6 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
6 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
6  before v = A * y 
4 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
4 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
4  before v = A * y 
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
0 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
3 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
3 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
3  before v = A * y 
5 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
5 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
5  before v = A * y 
5  before  alpha = rho_new / < rt, v > 
4  before  alpha = rho_new / < rt, v > 
0 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
0  before v = A * y 
3  before  alpha = rho_new / < rt, v > 
6  before  alpha = rho_new / < rt, v > 
1 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
1  before v = A * y 
2  before v = A * y 
3  before alpha = rho_new / h; 
3  before apply first correction to x 
3  before r = r - alpha*v 
3  before test stop criteria 
3 istlsolver::printOutput 
  0.5      3.06356e-33      1.57502e-17
3 === rate=2.48067e-34, T=0.122263, TIT=0.244525, IT=0.5
3 finished solveLinearSystemImpl_ 1
3 converged value: 1 7 1 let s go get convergedRemote
3 to  comm_.min(converged) 
6  before alpha = rho_new / h; 
6  before apply first correction to x 
6  before r = r - alpha*v 
6  before test stop criteria 
6 istlsolver::printOutput 
  0.5      3.06356e-33      1.57502e-17
6 === rate=2.48067e-34, T=0.122248, TIT=0.244495, IT=0.5
6 finished solveLinearSystemImpl_ 1
6 converged value: 1 7 1 let s go get convergedRemote
6 to  comm_.min(converged) 
1  before  alpha = rho_new / < rt, v > 
1  before alpha = rho_new / h; 
1  before apply first correction to x 
1  before r = r - alpha*v 
1  before test stop criteria 
1 istlsolver::printOutput 
  0.5      3.06356e-33      1.57502e-17
1 === rate=2.48067e-34, T=0.122265, TIT=0.24453, IT=0.5
1 finished solveLinearSystemImpl_ 1
1 converged value: 1 7 1 let s go get convergedRemote
1 to  comm_.min(converged) 
2  before  alpha = rho_new / < rt, v > 
2  before alpha = rho_new / h; 
2  before apply first correction to x 
2  before r = r - alpha*v 
2  before test stop criteria 
2 istlsolver::printOutput 
  0.5      3.06356e-33      1.57502e-17
2 === rate=2.48067e-34, T=0.122249, TIT=0.244497, IT=0.5
2 finished solveLinearSystemImpl_ 1
2 converged value: 1 7 1 let s go get convergedRemote
2 to  comm_.min(converged) 
5  before alpha = rho_new / h; 
5  before apply first correction to x 
5  before r = r - alpha*v 
5  before test stop criteria 
5 istlsolver::printOutput 
  0.5      3.06356e-33      1.57502e-17
5 === rate=2.48067e-34, T=0.122262, TIT=0.244523, IT=0.5
5 finished solveLinearSystemImpl_ 1
5 converged value: 1 7 1 let s go get convergedRemote
5 to  comm_.min(converged) 
4  before alpha = rho_new / h; 
4  before apply first correction to x 
4  before r = r - alpha*v 
4  before test stop criteria 
4 istlsolver::printOutput 
  0.5      3.06356e-33      1.57502e-17
4 === rate=2.48067e-34, T=0.122248, TIT=0.244497, IT=0.5
4 finished solveLinearSystemImpl_ 1
4 converged value: 1 7 1 let s go get convergedRemote
4 to  comm_.min(converged) 
0  before  alpha = rho_new / < rt, v > 
0  before alpha = rho_new / h; 
0  before apply first correction to x 
0  before r = r - alpha*v 
0  before test stop criteria 
0 istlsolver::printOutput 
  0.5      3.06356e-33      1.57502e-17
0 === rate=2.48067e-34, T=0.0187977, TIT=0.0375954, IT=0.5
0 finished solveLinearSystemImpl_ 1
0 converged value: 1 7 1 let s go get convergedRemote
0 to  comm_.min(converged) 
0 did  comm_.min(converged) convergedRemote: 1
0 final convergedRemote: 1
0 Update: x^(k+1) = x^k - deltax^k 
0 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
0 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
0 Newton iteration  7 done0 , maximum relative shift = 3.332e-08
0 Assemble/solve/update time: 0.307329(24.9778%)/0.922924(75.0095%)/0.000156422(0.012713%)
0 gridVariables->advanceTimeStep
0 before, nonLinearSolver->suggestTimeStepSize, current time: 813.767 simTime 57600
0 before assembler->setPreviousSolution, ddt: 25
0 nonLinearSolver->solve
2 did  comm_.min(converged) convergedRemote: 1
2 final convergedRemote: 1
2 Update: x^(k+1) = x^k - deltax^k 
2 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
2 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
2 Newton iteration  7 done2 , maximum relative shift = 3.332e-08
2 Assemble/solve/update time: 0.307706(25.0003%)/0.922974(74.9893%)/0.000127381(0.0103494%)
2 gridVariables->advanceTimeStep
2 before, nonLinearSolver->suggestTimeStepSize, current time: 813.767 simTime 57600
2 before assembler->setPreviousSolution, ddt: 25
2 nonLinearSolver->solve
2 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
5 did  comm_.min(converged) convergedRemote: 1
5 final convergedRemote: 1
5 Update: x^(k+1) = x^k - deltax^k 
5 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
5 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
5 Newton iteration  7 done5 , maximum relative shift = 3.332e-08
5 Assemble/solve/update time: 0.307843(25.0089%)/0.922954(74.98%)/0.000137432(0.0111649%)
5 gridVariables->advanceTimeStep
5 before, nonLinearSolver->suggestTimeStepSize, current time: 813.767 simTime 57600
5 before assembler->setPreviousSolution, ddt: 25
5 nonLinearSolver->solve
5 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
4 did  comm_.min(converged) convergedRemote: 1
4 final convergedRemote: 1
4 Update: x^(k+1) = x^k - deltax^k 
4 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
4 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
4 Newton iteration  7 done4 , maximum relative shift = 3.332e-08
4 Assemble/solve/update time: 0.307814(25.0071%)/0.922952(74.9815%)/0.000140541(0.0114177%)
4 gridVariables->advanceTimeStep
4 before, nonLinearSolver->suggestTimeStepSize, current time: 813.767 simTime 57600
4 before assembler->setPreviousSolution, ddt: 25
4 nonLinearSolver->solve
4 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
1 did  comm_.min(converged) convergedRemote: 1
1 final convergedRemote: 1
1 Update: x^(k+1) = x^k - deltax^k 
1 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
1 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
1 Newton iteration  7 done1 , maximum relative shift = 3.332e-08
1 Assemble/solve/update time: 0.30784(25.0089%)/0.922948(74.98%)/0.000137142(0.0111414%)
1 gridVariables->advanceTimeStep
1 before, nonLinearSolver->suggestTimeStepSize, current time: 813.767 simTime 57600
1 before assembler->setPreviousSolution, ddt: 25
1 nonLinearSolver->solve
1 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
3 did  comm_.min(converged) convergedRemote: 1
3 final convergedRemote: 1
3 Update: x^(k+1) = x^k - deltax^k 
3 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
3 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
3 Newton iteration  7 done3 , maximum relative shift = 3.332e-08
3 Assemble/solve/update time: 0.307818(25.0071%)/0.92297(74.9818%)/0.000136932(0.0111243%)
3 gridVariables->advanceTimeStep
3 before, nonLinearSolver->suggestTimeStepSize, current time: 813.767 simTime 57600
3 before assembler->setPreviousSolution, ddt: 25
3 nonLinearSolver->solve
3 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
6 did  comm_.min(converged) convergedRemote: 1
6 final convergedRemote: 1
6 Update: x^(k+1) = x^k - deltax^k 
6 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
6 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
6 Newton iteration  7 done6 , maximum relative shift = 3.332e-08
6 Assemble/solve/update time: 0.307828(25.0083%)/0.922979(74.9839%)/9.7041e-05(0.00788372%)
6 gridVariables->advanceTimeStep
6 before, nonLinearSolver->suggestTimeStepSize, current time: 813.767 simTime 57600
6 before assembler->setPreviousSolution, ddt: 25
6 nonLinearSolver->solve
6 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
0 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
0 Solve: M deltax^k = r 
0 Newton::solveLinearSystem : entering the try section 0
0to  comm_.sum(norm2) 
0did  comm_.sum(norm2) 
0 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
0 solveLinearSystemImpl_, before converged = ls.solve 
0 7 amgbackend::solve isParallel 1
6 Solve: M deltax^k = r 
6 Newton::solveLinearSystem : entering the try section 0
6to  comm_.sum(norm2) 
6did  comm_.sum(norm2) 
6 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
6 solveLinearSystemImpl_, before converged = ls.solve 
6 7 amgbackend::solve isParallel 1
2 Solve: M deltax^k = r 
2 Newton::solveLinearSystem : entering the try section 0
2to  comm_.sum(norm2) 
2did  comm_.sum(norm2) 
2 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
2 solveLinearSystemImpl_, before converged = ls.solve 
2 7 amgbackend::solve isParallel 1
5 Solve: M deltax^k = r 
5 Newton::solveLinearSystem : entering the try section 0
5to  comm_.sum(norm2) 
5did  comm_.sum(norm2) 
5 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
5 solveLinearSystemImpl_, before converged = ls.solve 
5 7 amgbackend::solve isParallel 1
4 Solve: M deltax^k = r 
4 Newton::solveLinearSystem : entering the try section 0
4to  comm_.sum(norm2) 
4did  comm_.sum(norm2) 
4 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
4 solveLinearSystemImpl_, before converged = ls.solve 
4 7 amgbackend::solve isParallel 1
1 Solve: M deltax^k = r 
1 Newton::solveLinearSystem : entering the try section 0
1to  comm_.sum(norm2) 
1did  comm_.sum(norm2) 
1 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
1 solveLinearSystemImpl_, before converged = ls.solve 
1 7 amgbackend::solve isParallel 1
3 Solve: M deltax^k = r 
3 Newton::solveLinearSystem : entering the try section 0
3to  comm_.sum(norm2) 
3did  comm_.sum(norm2) 
3 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
3 solveLinearSystemImpl_, before converged = ls.solve 
3 7 amgbackend::solve isParallel 1
0 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
0 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 7
6 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
6 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
2 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
2 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
5 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
5 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
4 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
4 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
1 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
1 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
3 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
3 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 7
6 7 before Dune::BiCGSTABSolver 
6 Dune::BiCGSTABSolver.apply(x, b, result_) 
6 BiCGSTABSolver::apply 
6 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
2 7 before Dune::BiCGSTABSolver 
2 Dune::BiCGSTABSolver.apply(x, b, result_) 
2 BiCGSTABSolver::apply 
2 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
5 7 before Dune::BiCGSTABSolver 
5 Dune::BiCGSTABSolver.apply(x, b, result_) 
5 BiCGSTABSolver::apply 
5 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
4 7 before Dune::BiCGSTABSolver 
4 Dune::BiCGSTABSolver.apply(x, b, result_) 
4 BiCGSTABSolver::apply 
4 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 7
1 7 before Dune::BiCGSTABSolver 
1 Dune::BiCGSTABSolver.apply(x, b, result_) 
1 BiCGSTABSolver::apply 
1 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
3 7 before Dune::BiCGSTABSolver 
3 Dune::BiCGSTABSolver.apply(x, b, result_) 
3 BiCGSTABSolver::apply 
3 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
Using a direct coarse solver (SuperLU)
Building hierarchy of 1 levels (inclusive coarse solver) took 0.110448 seconds.
0 7 before Dune::BiCGSTABSolver 
0 Dune::BiCGSTABSolver.apply(x, b, result_) 
0 BiCGSTABSolver::apply 
0 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
6 === BiCGSTABSolver
 Iter          Defect            Rate
    0      3.16058e-09
6  (it = 0.5; it < _maxit; it+=.5) 0.5 250
6  before p = r 
6  before  y = W^-1 * p 
6  before _prec->apply(y,p); 0.5 250
6 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
2 === BiCGSTABSolver
 Iter          Defect            Rate
    0      3.16058e-09
2  (it = 0.5; it < _maxit; it+=.5) 0.5 250
2  before p = r 
2  before  y = W^-1 * p 
2  before _prec->apply(y,p); 0.5 250
2 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
5 === BiCGSTABSolver
 Iter          Defect            Rate
    0      3.16058e-09
5  (it = 0.5; it < _maxit; it+=.5) 0.5 250
5  before p = r 
5  before  y = W^-1 * p 
5  before _prec->apply(y,p); 0.5 250
5 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
0 === BiCGSTABSolver
 Iter          Defect            Rate
    0      3.16058e-09
0  (it = 0.5; it < _maxit; it+=.5) 0.5 250
0  before p = r 
0  before  y = W^-1 * p 
0  before _prec->apply(y,p); 0.5 250
0 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 7
0 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
3 === BiCGSTABSolver
 Iter          Defect            Rate
    0      3.16058e-09
3  (it = 0.5; it < _maxit; it+=.5) 0.5 250
3  before p = r 
3  before  y = W^-1 * p 
3  before _prec->apply(y,p); 0.5 250
3 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
1 === BiCGSTABSolver
 Iter          Defect            Rate
    0      3.16058e-09
1  (it = 0.5; it < _maxit; it+=.5) 0.5 250
1  before p = r 
1  before  y = W^-1 * p 
1  before _prec->apply(y,p); 0.5 250
1 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
4 === BiCGSTABSolver
 Iter          Defect            Rate
    0      3.16058e-09
4  (it = 0.5; it < _maxit; it+=.5) 0.5 250
4  before p = r 
4  before  y = W^-1 * p 
4  before _prec->apply(y,p); 0.5 250
4 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
0 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
3 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
3 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
3  before v = A * y 
1 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
5 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
5 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
5  before v = A * y 
6 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
6 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
6  before v = A * y 
2 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
2 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
4 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
4 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
4  before v = A * y 
0 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
0  before v = A * y 
3  before  alpha = rho_new / < rt, v > 
1 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
5  before  alpha = rho_new / < rt, v > 
6  before  alpha = rho_new / < rt, v > 
4  before  alpha = rho_new / < rt, v > 
3  before alpha = rho_new / h; 
3  before apply first correction to x 
3  before r = r - alpha*v 
3  before test stop criteria 
3 istlsolver::printOutput 
  0.5      4.63102e-25      1.46524e-16
3 === rate=2.14694e-32, T=0.121211, TIT=0.242421, IT=0.5
3 finished solveLinearSystemImpl_ 1
3 converged value: 1 7 1 let s go get convergedRemote
3 to  comm_.min(converged) 
1  before v = A * y 
1  before  alpha = rho_new / < rt, v > 
1  before alpha = rho_new / h; 
1  before apply first correction to x 
1  before r = r - alpha*v 
1  before test stop criteria 
1 istlsolver::printOutput 
  0.5      4.63102e-25      1.46524e-16
1 === rate=2.14694e-32, T=0.121112, TIT=0.242224, IT=0.5
1 finished solveLinearSystemImpl_ 1
1 converged value: 1 7 1 let s go get convergedRemote
1 to  comm_.min(converged) 
5  before alpha = rho_new / h; 
5  before apply first correction to x 
5  before r = r - alpha*v 
5  before test stop criteria 
5 istlsolver::printOutput 
  0.5      4.63102e-25      1.46524e-16
5 === rate=2.14694e-32, T=0.121209, TIT=0.242418, IT=0.5
5 finished solveLinearSystemImpl_ 1
5 converged value: 1 7 1 let s go get convergedRemote
5 to  comm_.min(converged) 
4  before alpha = rho_new / h; 
4  before apply first correction to x 
4  before r = r - alpha*v 
4  before test stop criteria 
4 istlsolver::printOutput 
  0.5      4.63102e-25      1.46524e-16
4 === rate=2.14694e-32, T=0.121104, TIT=0.242209, IT=0.5
4 finished solveLinearSystemImpl_ 1
4 converged value: 1 7 1 let s go get convergedRemote
4 to  comm_.min(converged) 
2  before v = A * y 
2  before  alpha = rho_new / < rt, v > 
2  before alpha = rho_new / h; 
2  before apply first correction to x 
2  before r = r - alpha*v 
2  before test stop criteria 
2 istlsolver::printOutput 
  0.5      4.63102e-25      1.46524e-16
2 === rate=2.14694e-32, T=0.121106, TIT=0.242213, IT=0.5
2 finished solveLinearSystemImpl_ 1
2 converged value: 1 7 1 let s go get convergedRemote
2 to  comm_.min(converged) 
6  before alpha = rho_new / h; 
6  before apply first correction to x 
6  before r = r - alpha*v 
6  before test stop criteria 
6 istlsolver::printOutput 
  0.5      4.63102e-25      1.46524e-16
6 === rate=2.14694e-32, T=0.121105, TIT=0.242211, IT=0.5
6 finished solveLinearSystemImpl_ 1
6 converged value: 1 7 1 let s go get convergedRemote
6 to  comm_.min(converged) 
0  before  alpha = rho_new / < rt, v > 
0  before alpha = rho_new / h; 
0  before apply first correction to x 
0  before r = r - alpha*v 
0  before test stop criteria 
0 istlsolver::printOutput 
  0.5      4.63102e-25      1.46524e-16
0 === rate=2.14694e-32, T=0.0183945, TIT=0.036789, IT=0.5
2 did  comm_.min(converged) convergedRemote: 1
2 final convergedRemote: 1
2 Update: x^(k+1) = x^k - deltax^k 
2 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
2 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
2 Newton iteration  1 done2 , maximum relative shift = 6.593e-02
2 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
6 did  comm_.min(converged) convergedRemote: 1
6 final convergedRemote: 1
6 Update: x^(k+1) = x^k - deltax^k 
6 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
6 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
6 Newton iteration  1 done6 , maximum relative shift = 6.593e-02
6 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
5 did  comm_.min(converged) convergedRemote: 1
5 final convergedRemote: 1
5 Update: x^(k+1) = x^k - deltax^k 
5 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
5 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
5 Newton iteration  1 done5 , maximum relative shift = 6.593e-02
5 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
3 did  comm_.min(converged) convergedRemote: 1
3 final convergedRemote: 1
3 Update: x^(k+1) = x^k - deltax^k 
3 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
3 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
3 Newton iteration  1 done3 , maximum relative shift = 6.593e-02
3 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
1 did  comm_.min(converged) convergedRemote: 1
1 final convergedRemote: 1
1 Update: x^(k+1) = x^k - deltax^k 
1 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
1 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
1 Newton iteration  1 done1 , maximum relative shift = 6.593e-02
1 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
0 finished solveLinearSystemImpl_ 1
0 converged value: 1 7 1 let s go get convergedRemote
0 to  comm_.min(converged) 
0 did  comm_.min(converged) convergedRemote: 1
0 final convergedRemote: 1
0 Update: x^(k+1) = x^k - deltax^k 
0 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
0 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
0 Newton iteration  1 done0 , maximum relative shift = 6.593e-02
0 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
4 did  comm_.min(converged) convergedRemote: 1
4 final convergedRemote: 1
4 Update: x^(k+1) = x^k - deltax^k 
4 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
4 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
4 Newton iteration  1 done4 , maximum relative shift = 6.593e-02
4 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
4 Solve: M deltax^k = r 
4 Newton::solveLinearSystem : entering the try section 1
4 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
4 solveLinearSystemImpl_, before converged = ls.solve 
4 7 amgbackend::solve isParallel 1
0 Solve: M deltax^k = r 
0 Newton::solveLinearSystem : entering the try section 1
0 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
0 solveLinearSystemImpl_, before converged = ls.solve 
0 7 amgbackend::solve isParallel 1
1 Solve: M deltax^k = r 
1 Newton::solveLinearSystem : entering the try section 1
1 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
1 solveLinearSystemImpl_, before converged = ls.solve 
1 7 amgbackend::solve isParallel 1
2 Solve: M deltax^k = r 
2 Newton::solveLinearSystem : entering the try section 1
2 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
2 solveLinearSystemImpl_, before converged = ls.solve 
2 7 amgbackend::solve isParallel 1
6 Solve: M deltax^k = r 
6 Newton::solveLinearSystem : entering the try section 1
6 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
6 solveLinearSystemImpl_, before converged = ls.solve 
6 7 amgbackend::solve isParallel 1
5 Solve: M deltax^k = r 
5 Newton::solveLinearSystem : entering the try section 1
5 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
5 solveLinearSystemImpl_, before converged = ls.solve 
5 7 amgbackend::solve isParallel 1
3 Solve: M deltax^k = r 
3 Newton::solveLinearSystem : entering the try section 1
3 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
3 solveLinearSystemImpl_, before converged = ls.solve 
3 7 amgbackend::solve isParallel 1
5 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
5 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
3 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
3 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
4 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
4 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
0 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
0 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 7
1 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
1 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
2 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
2 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
6 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
6 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 7
2 7 before Dune::BiCGSTABSolver 
2 Dune::BiCGSTABSolver.apply(x, b, result_) 
2 BiCGSTABSolver::apply 
2 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
5 7 before Dune::BiCGSTABSolver 
5 Dune::BiCGSTABSolver.apply(x, b, result_) 
5 BiCGSTABSolver::apply 
5 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 7
3 7 before Dune::BiCGSTABSolver 
3 Dune::BiCGSTABSolver.apply(x, b, result_) 
3 BiCGSTABSolver::apply 
3 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
4 7 before Dune::BiCGSTABSolver 
4 Dune::BiCGSTABSolver.apply(x, b, result_) 
4 BiCGSTABSolver::apply 
4 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
6 7 before Dune::BiCGSTABSolver 
6 Dune::BiCGSTABSolver.apply(x, b, result_) 
6 BiCGSTABSolver::apply 
6 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
1 7 before Dune::BiCGSTABSolver 
1 Dune::BiCGSTABSolver.apply(x, b, result_) 
1 BiCGSTABSolver::apply 
1 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
Using a direct coarse solver (SuperLU)
Building hierarchy of 1 levels (inclusive coarse solver) took 0.110599 seconds.
0 7 before Dune::BiCGSTABSolver 
0 Dune::BiCGSTABSolver.apply(x, b, result_) 
0 BiCGSTABSolver::apply 
0 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
0 === BiCGSTABSolver
 Iter          Defect            Rate
    0      2.56006e-11
0  (it = 0.5; it < _maxit; it+=.5) 0.5 250
2 === BiCGSTABSolver
 Iter          Defect            Rate
    0      2.56006e-11
2  (it = 0.5; it < _maxit; it+=.5) 0.5 250
2  before p = r 
2  before  y = W^-1 * p 
2  before _prec->apply(y,p); 0.5 250
2 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
0  before p = r 
0  before  y = W^-1 * p 
0  before _prec->apply(y,p); 0.5 250
0 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 7
0 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
1 === BiCGSTABSolver
 Iter          Defect            Rate
    0      2.56006e-11
1  (it = 0.5; it < _maxit; it+=.5) 0.5 250
1  before p = r 
1  before  y = W^-1 * p 
1  before _prec->apply(y,p); 0.5 250
1 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
3 === BiCGSTABSolver
 Iter          Defect            Rate
    0      2.56006e-11
3  (it = 0.5; it < _maxit; it+=.5) 0.5 250
3  before p = r 
3  before  y = W^-1 * p 
3  before _prec->apply(y,p); 0.5 250
3 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
4 === BiCGSTABSolver
 Iter          Defect            Rate
    0      2.56006e-11
4  (it = 0.5; it < _maxit; it+=.5) 0.5 250
4  before p = r 
4  before  y = W^-1 * p 
4  before _prec->apply(y,p); 0.5 250
4 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
6 === BiCGSTABSolver
 Iter          Defect            Rate
    0      2.56006e-11
6  (it = 0.5; it < _maxit; it+=.5) 0.5 250
6  before p = r 
6  before  y = W^-1 * p 
6  before _prec->apply(y,p); 0.5 250
6 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
5 === BiCGSTABSolver
 Iter          Defect            Rate
    0      2.56006e-11
5  (it = 0.5; it < _maxit; it+=.5) 0.5 250
5  before p = r 
5  before  y = W^-1 * p 
5  before _prec->apply(y,p); 0.5 250
5 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
0 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
1 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
3 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
3 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
3  before v = A * y 
4 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
4 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
4  before v = A * y 
6 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
6 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
6  before v = A * y 
2 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
2 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
5 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
5 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
5  before v = A * y 
4  before  alpha = rho_new / < rt, v > 
6  before  alpha = rho_new / < rt, v > 
5  before  alpha = rho_new / < rt, v > 
0 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
0  before v = A * y 
1 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
3  before  alpha = rho_new / < rt, v > 
2  before v = A * y 
2  before  alpha = rho_new / < rt, v > 
2  before alpha = rho_new / h; 
2  before apply first correction to x 
2  before r = r - alpha*v 
2  before test stop criteria 
2 istlsolver::printOutput 
  0.5      2.58726e-27      1.01062e-16
2 === rate=1.02136e-32, T=0.121238, TIT=0.242475, IT=0.5
2 finished solveLinearSystemImpl_ 1
2 converged value: 1 7 1 let s go get convergedRemote
2 to  comm_.min(converged) 
5  before alpha = rho_new / h; 
5  before apply first correction to x 
5  before r = r - alpha*v 
5  before test stop criteria 
5 istlsolver::printOutput 
  0.5      2.58726e-27      1.01062e-16
5 === rate=1.02136e-32, T=0.121316, TIT=0.242632, IT=0.5
5 finished solveLinearSystemImpl_ 1
5 converged value: 1 7 1 let s go get convergedRemote
5 to  comm_.min(converged) 
0  before  alpha = rho_new / < rt, v > 
0  before alpha = rho_new / h; 
0  before apply first correction to x 
0  before r = r - alpha*v 
0  before test stop criteria 
0 istlsolver::printOutput 
  0.5      2.58726e-27      1.01062e-16
0 === rate=1.02136e-32, T=0.0185542, TIT=0.0371083, IT=0.5
1  before v = A * y 
1  before  alpha = rho_new / < rt, v > 
1  before alpha = rho_new / h; 
1  before apply first correction to x 
1  before r = r - alpha*v 
1  before test stop criteria 
1 istlsolver::printOutput 
  0.5      2.58726e-27      1.01062e-16
1 === rate=1.02136e-32, T=0.121318, TIT=0.242637, IT=0.5
1 finished solveLinearSystemImpl_ 1
1 converged value: 1 7 1 let s go get convergedRemote
1 to  comm_.min(converged) 
6  before alpha = rho_new / h; 
6  before apply first correction to x 
6  before r = r - alpha*v 
6  before test stop criteria 
6 istlsolver::printOutput 
  0.5      2.58726e-27      1.01062e-16
6 === rate=1.02136e-32, T=0.121236, TIT=0.242472, IT=0.5
6 finished solveLinearSystemImpl_ 1
6 converged value: 1 7 1 let s go get convergedRemote
6 to  comm_.min(converged) 
4  before alpha = rho_new / h; 
4  before apply first correction to x 
4  before r = r - alpha*v 
4  before test stop criteria 
4 istlsolver::printOutput 
  0.5      2.58726e-27      1.01062e-16
4 === rate=1.02136e-32, T=0.121234, TIT=0.242468, IT=0.5
4 finished solveLinearSystemImpl_ 1
4 converged value: 1 7 1 let s go get convergedRemote
4 to  comm_.min(converged) 
3  before alpha = rho_new / h; 
3  before apply first correction to x 
3  before r = r - alpha*v 
3  before test stop criteria 
3 istlsolver::printOutput 
  0.5      2.58726e-27      1.01062e-16
3 === rate=1.02136e-32, T=0.121318, TIT=0.242636, IT=0.5
3 finished solveLinearSystemImpl_ 1
3 converged value: 1 7 1 let s go get convergedRemote
3 to  comm_.min(converged) 
5 did  comm_.min(converged) convergedRemote: 1
5 final convergedRemote: 1
5 Update: x^(k+1) = x^k - deltax^k 
5 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
5 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
5 Newton iteration  2 done5 , maximum relative shift = 4.156e-03
5 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
0 finished solveLinearSystemImpl_ 1
0 converged value: 1 7 1 let s go get convergedRemote
0 to  comm_.min(converged) 
0 did  comm_.min(converged) convergedRemote: 1
0 final convergedRemote: 1
0 Update: x^(k+1) = x^k - deltax^k 
0 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
0 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
0 Newton iteration  2 done0 , maximum relative shift = 4.156e-03
0 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
1 did  comm_.min(converged) convergedRemote: 1
1 final convergedRemote: 1
1 Update: x^(k+1) = x^k - deltax^k 
1 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
1 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
1 Newton iteration  2 done1 , maximum relative shift = 4.156e-03
1 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
6 did  comm_.min(converged) convergedRemote: 1
6 final convergedRemote: 1
6 Update: x^(k+1) = x^k - deltax^k 
6 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
6 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
6 Newton iteration  2 done6 , maximum relative shift = 4.156e-03
6 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
3 did  comm_.min(converged) convergedRemote: 1
3 final convergedRemote: 1
3 Update: x^(k+1) = x^k - deltax^k 
3 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
3 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
3 Newton iteration  2 done3 , maximum relative shift = 4.156e-03
3 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
4 did  comm_.min(converged) convergedRemote: 1
4 final convergedRemote: 1
4 Update: x^(k+1) = x^k - deltax^k 
4 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
4 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
4 Newton iteration  2 done4 , maximum relative shift = 4.156e-03
4 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
2 did  comm_.min(converged) convergedRemote: 1
2 final convergedRemote: 1
2 Update: x^(k+1) = x^k - deltax^k 
2 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
2 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
2 Newton iteration  2 done2 , maximum relative shift = 4.156e-03
2 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
4 Solve: M deltax^k = r 
4 Newton::solveLinearSystem : entering the try section 2
4 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
4 solveLinearSystemImpl_, before converged = ls.solve 
4 7 amgbackend::solve isParallel 1
6 Solve: M deltax^k = r 
6 Newton::solveLinearSystem : entering the try section 2
6 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
6 solveLinearSystemImpl_, before converged = ls.solve 
6 7 amgbackend::solve isParallel 1
5 Solve: M deltax^k = r 
5 Newton::solveLinearSystem : entering the try section 2
5 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
5 solveLinearSystemImpl_, before converged = ls.solve 
5 7 amgbackend::solve isParallel 1
0 Solve: M deltax^k = r 
0 Newton::solveLinearSystem : entering the try section 2
0 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
0 solveLinearSystemImpl_, before converged = ls.solve 
0 7 amgbackend::solve isParallel 1
1 Solve: M deltax^k = r 
1 Newton::solveLinearSystem : entering the try section 2
1 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
1 solveLinearSystemImpl_, before converged = ls.solve 
1 7 amgbackend::solve isParallel 1
2 Solve: M deltax^k = r 
2 Newton::solveLinearSystem : entering the try section 2
2 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
2 solveLinearSystemImpl_, before converged = ls.solve 
2 7 amgbackend::solve isParallel 1
3 Solve: M deltax^k = r 
3 Newton::solveLinearSystem : entering the try section 2
3 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
3 solveLinearSystemImpl_, before converged = ls.solve 
3 7 amgbackend::solve isParallel 1
4 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
4 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
6 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
6 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
5 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
5 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
0 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
0 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 7
3 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
3 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
2 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
2 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
1 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
1 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 7
3 7 before Dune::BiCGSTABSolver 
3 Dune::BiCGSTABSolver.apply(x, b, result_) 
3 BiCGSTABSolver::apply 
3 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
2 7 before Dune::BiCGSTABSolver 
2 Dune::BiCGSTABSolver.apply(x, b, result_) 
2 BiCGSTABSolver::apply 
2 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
1 7 before Dune::BiCGSTABSolver 
1 Dune::BiCGSTABSolver.apply(x, b, result_) 
1 BiCGSTABSolver::apply 
1 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
4 7 before Dune::BiCGSTABSolver 
4 Dune::BiCGSTABSolver.apply(x, b, result_) 
4 BiCGSTABSolver::apply 
4 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
6 7 before Dune::BiCGSTABSolver 
6 Dune::BiCGSTABSolver.apply(x, b, result_) 
6 BiCGSTABSolver::apply 
6 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
5 7 before Dune::BiCGSTABSolver 
5 Dune::BiCGSTABSolver.apply(x, b, result_) 
5 BiCGSTABSolver::apply 
5 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 7
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
Using a direct coarse solver (SuperLU)
Building hierarchy of 1 levels (inclusive coarse solver) took 0.110063 seconds.
0 7 before Dune::BiCGSTABSolver 
0 Dune::BiCGSTABSolver.apply(x, b, result_) 
0 BiCGSTABSolver::apply 
3 === BiCGSTABSolver
 Iter          Defect            Rate
    0      3.39967e-13
3  (it = 0.5; it < _maxit; it+=.5) 0.5 250
3  before p = r 
3  before  y = W^-1 * p 
3  before _prec->apply(y,p); 0.5 250
3 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
2 === BiCGSTABSolver
 Iter          Defect            Rate
    0      3.39967e-13
2  (it = 0.5; it < _maxit; it+=.5) 0.5 250
2  before p = r 
2  before  y = W^-1 * p 
2  before _prec->apply(y,p); 0.5 250
2 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
4 === BiCGSTABSolver
 Iter          Defect            Rate
    0      3.39967e-13
4  (it = 0.5; it < _maxit; it+=.5) 0.5 250
4  before p = r 
4  before  y = W^-1 * p 
4  before _prec->apply(y,p); 0.5 250
4 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
6 === BiCGSTABSolver
 Iter          Defect            Rate
    0      3.39967e-13
6  (it = 0.5; it < _maxit; it+=.5) 0.5 250
6  before p = r 
6  before  y = W^-1 * p 
6  before _prec->apply(y,p); 0.5 250
6 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
0 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
0 === BiCGSTABSolver
 Iter          Defect            Rate
    0      3.39967e-13
0  (it = 0.5; it < _maxit; it+=.5) 0.5 250
0  before p = r 
0  before  y = W^-1 * p 
0  before _prec->apply(y,p); 0.5 250
0 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 7
0 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
1 === BiCGSTABSolver
 Iter          Defect            Rate
    0      3.39967e-13
1  (it = 0.5; it < _maxit; it+=.5) 0.5 250
1  before p = r 
1  before  y = W^-1 * p 
1  before _prec->apply(y,p); 0.5 250
1 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
5 === BiCGSTABSolver
 Iter          Defect            Rate
    0      3.39967e-13
5  (it = 0.5; it < _maxit; it+=.5) 0.5 250
5  before p = r 
5  before  y = W^-1 * p 
5  before _prec->apply(y,p); 0.5 250
5 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
0 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
1 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
6 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
6 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
6  before v = A * y 
3 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
3 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
3  before v = A * y 
2 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
2 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
4 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
4 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
4  before v = A * y 
5 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
5 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
5  before v = A * y 
4  before  alpha = rho_new / < rt, v > 
0 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
0  before v = A * y 
1 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
1  before v = A * y 
6  before  alpha = rho_new / < rt, v > 
3  before  alpha = rho_new / < rt, v > 
2  before v = A * y 
2  before  alpha = rho_new / < rt, v > 
5  before  alpha = rho_new / < rt, v > 
0  before  alpha = rho_new / < rt, v > 
0  before alpha = rho_new / h; 
0  before apply first correction to x 
0  before r = r - alpha*v 
0  before test stop criteria 
0 istlsolver::printOutput 
  0.5      3.57145e-29      1.05053e-16
0 === rate=1.10361e-32, T=0.0178901, TIT=0.0357802, IT=0.5
1  before  alpha = rho_new / < rt, v > 
1  before alpha = rho_new / h; 
1  before apply first correction to x 
1  before r = r - alpha*v 
1  before test stop criteria 
1 istlsolver::printOutput 
  0.5      3.57145e-29      1.05053e-16
1 === rate=1.10361e-32, T=0.120421, TIT=0.240842, IT=0.5
1 finished solveLinearSystemImpl_ 1
1 converged value: 1 7 1 let s go get convergedRemote
1 to  comm_.min(converged) 
6  before alpha = rho_new / h; 
6  before apply first correction to x 
6  before r = r - alpha*v 
6  before test stop criteria 
6 istlsolver::printOutput 
  0.5      3.57145e-29      1.05053e-16
6 === rate=1.10361e-32, T=0.120291, TIT=0.240583, IT=0.5
6 finished solveLinearSystemImpl_ 1
6 converged value: 1 7 1 let s go get convergedRemote
6 to  comm_.min(converged) 
3  before alpha = rho_new / h; 
3  before apply first correction to x 
3  before r = r - alpha*v 
3  before test stop criteria 
3 istlsolver::printOutput 
  0.5      3.57145e-29      1.05053e-16
3 === rate=1.10361e-32, T=0.120422, TIT=0.240844, IT=0.5
3 finished solveLinearSystemImpl_ 1
3 converged value: 1 7 1 let s go get convergedRemote
3 to  comm_.min(converged) 
5  before alpha = rho_new / h; 
5  before apply first correction to x 
5  before r = r - alpha*v 
5  before test stop criteria 
5 istlsolver::printOutput 
  0.5      3.57145e-29      1.05053e-16
5 === rate=1.10361e-32, T=0.120421, TIT=0.240842, IT=0.5
5 finished solveLinearSystemImpl_ 1
5 converged value: 1 7 1 let s go get convergedRemote
5 to  comm_.min(converged) 
2  before alpha = rho_new / h; 
2  before apply first correction to x 
2  before r = r - alpha*v 
2  before test stop criteria 
2 istlsolver::printOutput 
  0.5      3.57145e-29      1.05053e-16
2 === rate=1.10361e-32, T=0.120294, TIT=0.240587, IT=0.5
2 finished solveLinearSystemImpl_ 1
2 converged value: 1 7 1 let s go get convergedRemote
2 to  comm_.min(converged) 
4  before alpha = rho_new / h; 
4  before apply first correction to x 
4  before r = r - alpha*v 
4  before test stop criteria 
4 istlsolver::printOutput 
  0.5      3.57145e-29      1.05053e-16
4 === rate=1.10361e-32, T=0.120295, TIT=0.24059, IT=0.5
4 finished solveLinearSystemImpl_ 1
4 converged value: 1 7 1 let s go get convergedRemote
4 to  comm_.min(converged) 
5 did  comm_.min(converged) convergedRemote: 1
5 final convergedRemote: 1
5 Update: x^(k+1) = x^k - deltax^k 
5 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
5 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
5 Newton iteration  3 done5 , maximum relative shift = 5.031e-05
5 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
2 did  comm_.min(converged) convergedRemote: 1
2 final convergedRemote: 1
2 Update: x^(k+1) = x^k - deltax^k 
2 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
2 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
2 Newton iteration  3 done2 , maximum relative shift = 5.031e-05
2 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
3 did  comm_.min(converged) convergedRemote: 1
3 final convergedRemote: 1
3 Update: x^(k+1) = x^k - deltax^k 
3 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
3 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
3 Newton iteration  3 done3 , maximum relative shift = 5.031e-05
3 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
0 finished solveLinearSystemImpl_ 1
0 converged value: 1 7 1 let s go get convergedRemote
0 to  comm_.min(converged) 
0 did  comm_.min(converged) convergedRemote: 1
0 final convergedRemote: 1
0 Update: x^(k+1) = x^k - deltax^k 
0 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
0 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
0 Newton iteration  3 done0 , maximum relative shift = 5.031e-05
0 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
1 did  comm_.min(converged) convergedRemote: 1
1 final convergedRemote: 1
1 Update: x^(k+1) = x^k - deltax^k 
1 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
1 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
1 Newton iteration  3 done1 , maximum relative shift = 5.031e-05
1 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
6 did  comm_.min(converged) convergedRemote: 1
6 final convergedRemote: 1
6 Update: x^(k+1) = x^k - deltax^k 
6 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
6 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
6 Newton iteration  3 done6 , maximum relative shift = 5.031e-05
6 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
4 did  comm_.min(converged) convergedRemote: 1
4 final convergedRemote: 1
4 Update: x^(k+1) = x^k - deltax^k 
4 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
4 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
4 Newton iteration  3 done4 , maximum relative shift = 5.031e-05
4 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
6 Solve: M deltax^k = r 
6 Newton::solveLinearSystem : entering the try section 3
6 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
6 solveLinearSystemImpl_, before converged = ls.solve 
6 7 amgbackend::solve isParallel 1
5 Solve: M deltax^k = r 
5 Newton::solveLinearSystem : entering the try section 3
5 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
5 solveLinearSystemImpl_, before converged = ls.solve 
5 7 amgbackend::solve isParallel 1
2 Solve: M deltax^k = r 
2 Newton::solveLinearSystem : entering the try section 3
2 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
2 solveLinearSystemImpl_, before converged = ls.solve 
2 7 amgbackend::solve isParallel 1
3 Solve: M deltax^k = r 
3 Newton::solveLinearSystem : entering the try section 3
3 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
3 solveLinearSystemImpl_, before converged = ls.solve 
3 7 amgbackend::solve isParallel 1
0 Solve: M deltax^k = r 
0 Newton::solveLinearSystem : entering the try section 3
0 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
0 solveLinearSystemImpl_, before converged = ls.solve 
0 7 amgbackend::solve isParallel 1
1 Solve: M deltax^k = r 
1 Newton::solveLinearSystem : entering the try section 3
1 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
1 solveLinearSystemImpl_, before converged = ls.solve 
1 7 amgbackend::solve isParallel 1
4 Solve: M deltax^k = r 
4 Newton::solveLinearSystem : entering the try section 3
4 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
4 solveLinearSystemImpl_, before converged = ls.solve 
4 7 amgbackend::solve isParallel 1
1 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
1 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
6 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
6 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
5 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
5 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
2 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
2 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
3 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
3 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
0 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
0 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 7
4 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
4 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 7
4 7 before Dune::BiCGSTABSolver 
4 Dune::BiCGSTABSolver.apply(x, b, result_) 
4 BiCGSTABSolver::apply 
4 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
1 7 before Dune::BiCGSTABSolver 
1 Dune::BiCGSTABSolver.apply(x, b, result_) 
1 BiCGSTABSolver::apply 
1 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
6 7 before Dune::BiCGSTABSolver 
6 Dune::BiCGSTABSolver.apply(x, b, result_) 
6 BiCGSTABSolver::apply 
6 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
5 7 before Dune::BiCGSTABSolver 
5 Dune::BiCGSTABSolver.apply(x, b, result_) 
5 BiCGSTABSolver::apply 
5 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
2 7 before Dune::BiCGSTABSolver 
2 Dune::BiCGSTABSolver.apply(x, b, result_) 
2 BiCGSTABSolver::apply 
2 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
3 7 before Dune::BiCGSTABSolver 
3 Dune::BiCGSTABSolver.apply(x, b, result_) 
3 BiCGSTABSolver::apply 
3 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 7
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
Using a direct coarse solver (SuperLU)
Building hierarchy of 1 levels (inclusive coarse solver) took 0.110534 seconds.
0 7 before Dune::BiCGSTABSolver 
0 Dune::BiCGSTABSolver.apply(x, b, result_) 
0 BiCGSTABSolver::apply 
0 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
0 === BiCGSTABSolver
 Iter          Defect            Rate
    0      8.18543e-15
0  (it = 0.5; it < _maxit; it+=.5) 0.5 250
2 === BiCGSTABSolver
 Iter          Defect            Rate
    0      8.18543e-15
2  (it = 0.5; it < _maxit; it+=.5) 0.5 250
2  before p = r 
2  before  y = W^-1 * p 
2  before _prec->apply(y,p); 0.5 250
2 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
0  before p = r 
0  before  y = W^-1 * p 
0  before _prec->apply(y,p); 0.5 250
0 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 7
0 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
5 === BiCGSTABSolver
 Iter          Defect            Rate
    0      8.18543e-15
5  (it = 0.5; it < _maxit; it+=.5) 0.5 250
5  before p = r 
5  before  y = W^-1 * p 
5  before _prec->apply(y,p); 0.5 250
5 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
4 === BiCGSTABSolver
 Iter          Defect            Rate
    0      8.18543e-15
4  (it = 0.5; it < _maxit; it+=.5) 0.5 250
4  before p = r 
4  before  y = W^-1 * p 
4  before _prec->apply(y,p); 0.5 250
4 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
3 === BiCGSTABSolver
 Iter          Defect            Rate
    0      8.18543e-15
3  (it = 0.5; it < _maxit; it+=.5) 0.5 250
3  before p = r 
3  before  y = W^-1 * p 
3  before _prec->apply(y,p); 0.5 250
3 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
6 === BiCGSTABSolver
 Iter          Defect            Rate
    0      8.18543e-15
6  (it = 0.5; it < _maxit; it+=.5) 0.5 250
6  before p = r 
6  before  y = W^-1 * p 
6  before _prec->apply(y,p); 0.5 250
6 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
1 === BiCGSTABSolver
 Iter          Defect            Rate
    0      8.18543e-15
1  (it = 0.5; it < _maxit; it+=.5) 0.5 250
1  before p = r 
1  before  y = W^-1 * p 
1  before _prec->apply(y,p); 0.5 250
1 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
1 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
0 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
5 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
5 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
5  before v = A * y 
4 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
4 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
4  before v = A * y 
4  before  alpha = rho_new / < rt, v > 
3 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
3 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
3  before v = A * y 
6 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
6 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
6  before v = A * y 
6  before  alpha = rho_new / < rt, v > 
2 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
2 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
0 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
0  before v = A * y 
5  before  alpha = rho_new / < rt, v > 
3  before  alpha = rho_new / < rt, v > 
2  before v = A * y 
2  before  alpha = rho_new / < rt, v > 
1 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
1  before v = A * y 
0  before  alpha = rho_new / < rt, v > 
0  before alpha = rho_new / h; 
0  before apply first correction to x 
0  before r = r - alpha*v 
0  before test stop criteria 
0 istlsolver::printOutput 
  0.5      7.77463e-31      9.49814e-17
0 === rate=9.02146e-33, T=0.0185315, TIT=0.037063, IT=0.5
0 finished solveLinearSystemImpl_ 1
0 converged value: 1 7 1 let s go get convergedRemote
0 to  comm_.min(converged) 
0 did  comm_.min(converged) convergedRemote: 1
0 final convergedRemote: 1
0 Update: x^(k+1) = x^k - deltax^k 
0 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
0 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
0 Newton iteration  4 done0 , maximum relative shift = 8.878e-07
0 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
4  before alpha = rho_new / h; 
4  before apply first correction to x 
4  before r = r - alpha*v 
4  before test stop criteria 
4 istlsolver::printOutput 
  0.5      7.77463e-31      9.49814e-17
4 === rate=9.02146e-33, T=0.121107, TIT=0.242214, IT=0.5
4 finished solveLinearSystemImpl_ 1
4 converged value: 1 7 1 let s go get convergedRemote
4 to  comm_.min(converged) 
4 did  comm_.min(converged) convergedRemote: 1
4 final convergedRemote: 1
4 Update: x^(k+1) = x^k - deltax^k 
4 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
4 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
4 Newton iteration  4 done4 , maximum relative shift = 8.878e-07
4 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
5  before alpha = rho_new / h; 
5  before apply first correction to x 
5  before r = r - alpha*v 
5  before test stop criteria 
5 istlsolver::printOutput 
  0.5      7.77463e-31      9.49814e-17
5 === rate=9.02146e-33, T=0.121146, TIT=0.242292, IT=0.5
5 finished solveLinearSystemImpl_ 1
5 converged value: 1 7 1 let s go get convergedRemote
5 to  comm_.min(converged) 
5 did  comm_.min(converged) convergedRemote: 1
5 final convergedRemote: 1
5 Update: x^(k+1) = x^k - deltax^k 
5 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
5 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
5 Newton iteration  4 done5 , maximum relative shift = 8.878e-07
5 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
6  before alpha = rho_new / h; 
6  before apply first correction to x 
6  before r = r - alpha*v 
6  before test stop criteria 
6 istlsolver::printOutput 
  0.5      7.77463e-31      9.49814e-17
6 === rate=9.02146e-33, T=0.12111, TIT=0.242219, IT=0.5
6 finished solveLinearSystemImpl_ 1
6 converged value: 1 7 1 let s go get convergedRemote
6 to  comm_.min(converged) 
6 did  comm_.min(converged) convergedRemote: 1
6 final convergedRemote: 1
6 Update: x^(k+1) = x^k - deltax^k 
6 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
6 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
6 Newton iteration  4 done6 , maximum relative shift = 8.878e-07
6 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
1  before  alpha = rho_new / < rt, v > 
1  before alpha = rho_new / h; 
1  before apply first correction to x 
1  before r = r - alpha*v 
1  before test stop criteria 
1 istlsolver::printOutput 
  0.5      7.77463e-31      9.49814e-17
1 === rate=9.02146e-33, T=0.121144, TIT=0.242289, IT=0.5
1 finished solveLinearSystemImpl_ 1
1 converged value: 1 7 1 let s go get convergedRemote
1 to  comm_.min(converged) 
1 did  comm_.min(converged) convergedRemote: 1
1 final convergedRemote: 1
1 Update: x^(k+1) = x^k - deltax^k 
1 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
1 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
1 Newton iteration  4 done1 , maximum relative shift = 8.878e-07
1 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
2  before alpha = rho_new / h; 
2  before apply first correction to x 
2  before r = r - alpha*v 
2  before test stop criteria 
2 istlsolver::printOutput 
  0.5      7.77463e-31      9.49814e-17
2 === rate=9.02146e-33, T=0.121109, TIT=0.242218, IT=0.5
2 finished solveLinearSystemImpl_ 1
2 converged value: 1 7 1 let s go get convergedRemote
2 to  comm_.min(converged) 
2 did  comm_.min(converged) convergedRemote: 1
2 final convergedRemote: 1
2 Update: x^(k+1) = x^k - deltax^k 
2 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
2 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
2 Newton iteration  4 done2 , maximum relative shift = 8.878e-07
2 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
3  before alpha = rho_new / h; 
3  before apply first correction to x 
3  before r = r - alpha*v 
3  before test stop criteria 
3 istlsolver::printOutput 
  0.5      7.77463e-31      9.49814e-17
3 === rate=9.02146e-33, T=0.121147, TIT=0.242295, IT=0.5
3 finished solveLinearSystemImpl_ 1
3 converged value: 1 7 1 let s go get convergedRemote
3 to  comm_.min(converged) 
3 did  comm_.min(converged) convergedRemote: 1
3 final convergedRemote: 1
3 Update: x^(k+1) = x^k - deltax^k 
3 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
3 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
3 Newton iteration  4 done3 , maximum relative shift = 8.878e-07
3 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
0 Solve: M deltax^k = r 
0 Newton::solveLinearSystem : entering the try section 4
0 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
0 solveLinearSystemImpl_, before converged = ls.solve 
0 7 amgbackend::solve isParallel 1
4 Solve: M deltax^k = r 
4 Newton::solveLinearSystem : entering the try section 4
4 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
4 solveLinearSystemImpl_, before converged = ls.solve 
4 7 amgbackend::solve isParallel 1
5 Solve: M deltax^k = r 
5 Newton::solveLinearSystem : entering the try section 4
5 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
5 solveLinearSystemImpl_, before converged = ls.solve 
5 7 amgbackend::solve isParallel 1
3 Solve: M deltax^k = r 
3 Newton::solveLinearSystem : entering the try section 4
3 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
3 solveLinearSystemImpl_, before converged = ls.solve 
3 7 amgbackend::solve isParallel 1
1 Solve: M deltax^k = r 
1 Newton::solveLinearSystem : entering the try section 4
1 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
1 solveLinearSystemImpl_, before converged = ls.solve 
1 7 amgbackend::solve isParallel 1
2 Solve: M deltax^k = r 
2 Newton::solveLinearSystem : entering the try section 4
2 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
2 solveLinearSystemImpl_, before converged = ls.solve 
2 7 amgbackend::solve isParallel 1
6 Solve: M deltax^k = r 
6 Newton::solveLinearSystem : entering the try section 4
6 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
6 solveLinearSystemImpl_, before converged = ls.solve 
6 7 amgbackend::solve isParallel 1
1 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
1 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
2 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
2 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
5 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
5 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
0 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
0 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 7
4 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
4 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
6 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
6 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
3 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
3 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 7
6 7 before Dune::BiCGSTABSolver 
6 Dune::BiCGSTABSolver.apply(x, b, result_) 
6 BiCGSTABSolver::apply 
6 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
4 7 before Dune::BiCGSTABSolver 
4 Dune::BiCGSTABSolver.apply(x, b, result_) 
4 BiCGSTABSolver::apply 
4 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
1 7 before Dune::BiCGSTABSolver 
1 Dune::BiCGSTABSolver.apply(x, b, result_) 
1 BiCGSTABSolver::apply 
1 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
2 7 before Dune::BiCGSTABSolver 
2 Dune::BiCGSTABSolver.apply(x, b, result_) 
2 BiCGSTABSolver::apply 
2 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
5 7 before Dune::BiCGSTABSolver 
5 Dune::BiCGSTABSolver.apply(x, b, result_) 
5 BiCGSTABSolver::apply 
5 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
3 7 before Dune::BiCGSTABSolver 
3 Dune::BiCGSTABSolver.apply(x, b, result_) 
3 BiCGSTABSolver::apply 
3 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 7
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
Using a direct coarse solver (SuperLU)
Building hierarchy of 1 levels (inclusive coarse solver) took 0.110598 seconds.
0 7 before Dune::BiCGSTABSolver 
0 Dune::BiCGSTABSolver.apply(x, b, result_) 
0 BiCGSTABSolver::apply 
0 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
0 === BiCGSTABSolver
 Iter          Defect            Rate
    0       5.8043e-16
0  (it = 0.5; it < _maxit; it+=.5) 0.5 250
3 === BiCGSTABSolver
 Iter          Defect            Rate
    0       5.8043e-16
3  (it = 0.5; it < _maxit; it+=.5) 0.5 250
3  before p = r 
3  before  y = W^-1 * p 
3  before _prec->apply(y,p); 0.5 250
3 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
6 === BiCGSTABSolver
 Iter          Defect            Rate
    0       5.8043e-16
6  (it = 0.5; it < _maxit; it+=.5) 0.5 250
6  before p = r 
6  before  y = W^-1 * p 
6  before _prec->apply(y,p); 0.5 250
6 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
2 === BiCGSTABSolver
 Iter          Defect            Rate
    0       5.8043e-16
2  (it = 0.5; it < _maxit; it+=.5) 0.5 250
2  before p = r 
2  before  y = W^-1 * p 
2  before _prec->apply(y,p); 0.5 250
2 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
1 === BiCGSTABSolver
 Iter          Defect            Rate
    0       5.8043e-16
1  (it = 0.5; it < _maxit; it+=.5) 0.5 250
1  before p = r 
1  before  y = W^-1 * p 
1  before _prec->apply(y,p); 0.5 250
1 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
4 === BiCGSTABSolver
 Iter          Defect            Rate
    0       5.8043e-16
4  (it = 0.5; it < _maxit; it+=.5) 0.5 250
4  before p = r 
4  before  y = W^-1 * p 
4  before _prec->apply(y,p); 0.5 250
4 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
5 === BiCGSTABSolver
 Iter          Defect            Rate
    0       5.8043e-16
5  (it = 0.5; it < _maxit; it+=.5) 0.5 250
5  before p = r 
5  before  y = W^-1 * p 
5  before _prec->apply(y,p); 0.5 250
5 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
0  before p = r 
0  before  y = W^-1 * p 
0  before _prec->apply(y,p); 0.5 250
0 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 7
0 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
5 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
5 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
5  before v = A * y 
3 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
3 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
3  before v = A * y 
6 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
6 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
6  before v = A * y 
2 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
2 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
1 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
4 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
4 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
4  before v = A * y 
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
0 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
4  before  alpha = rho_new / < rt, v > 
5  before  alpha = rho_new / < rt, v > 
3  before  alpha = rho_new / < rt, v > 
6  before  alpha = rho_new / < rt, v > 
2  before v = A * y 
1 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
0 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
0  before v = A * y 
0  before  alpha = rho_new / < rt, v > 
0  before alpha = rho_new / h; 
0  before apply first correction to x 
0  before r = r - alpha*v 
0  before test stop criteria 
0 istlsolver::printOutput 
  0.5      3.08727e-33      5.31894e-18
0 === rate=2.82911e-35, T=0.0183078, TIT=0.0366157, IT=0.5
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
1  before v = A * y 
1  before  alpha = rho_new / < rt, v > 
1  before alpha = rho_new / h; 
1  before apply first correction to x 
1  before r = r - alpha*v 
1  before test stop criteria 
1 istlsolver::printOutput 
  0.5      3.08727e-33      5.31894e-18
1 === rate=2.82911e-35, T=0.121013, TIT=0.242025, IT=0.5
1 finished solveLinearSystemImpl_ 1
1 converged value: 1 7 1 let s go get convergedRemote
1 to  comm_.min(converged) 
4  before alpha = rho_new / h; 
4  before apply first correction to x 
4  before r = r - alpha*v 
4  before test stop criteria 
4 istlsolver::printOutput 
  0.5      3.08727e-33      5.31894e-18
4 === rate=2.82911e-35, T=0.121028, TIT=0.242055, IT=0.5
4 finished solveLinearSystemImpl_ 1
4 converged value: 1 7 1 let s go get convergedRemote
4 to  comm_.min(converged) 
5  before alpha = rho_new / h; 
5  before apply first correction to x 
5  before r = r - alpha*v 
5  before test stop criteria 
5 istlsolver::printOutput 
  0.5      3.08727e-33      5.31894e-18
5 === rate=2.82911e-35, T=0.121012, TIT=0.242025, IT=0.5
5 finished solveLinearSystemImpl_ 1
5 converged value: 1 7 1 let s go get convergedRemote
5 to  comm_.min(converged) 
3  before alpha = rho_new / h; 
3  before apply first correction to x 
3  before r = r - alpha*v 
3  before test stop criteria 
3 istlsolver::printOutput 
  0.5      3.08727e-33      5.31894e-18
3 === rate=2.82911e-35, T=0.121013, TIT=0.242025, IT=0.5
3 finished solveLinearSystemImpl_ 1
3 converged value: 1 7 1 let s go get convergedRemote
3 to  comm_.min(converged) 
6  before alpha = rho_new / h; 
6  before apply first correction to x 
6  before r = r - alpha*v 
6  before test stop criteria 
6 istlsolver::printOutput 
  0.5      3.08727e-33      5.31894e-18
6 === rate=2.82911e-35, T=0.121031, TIT=0.242062, IT=0.5
6 finished solveLinearSystemImpl_ 1
6 converged value: 1 7 1 let s go get convergedRemote
6 to  comm_.min(converged) 
2  before  alpha = rho_new / < rt, v > 
2  before alpha = rho_new / h; 
2  before apply first correction to x 
2  before r = r - alpha*v 
2  before test stop criteria 
2 istlsolver::printOutput 
  0.5      3.08727e-33      5.31894e-18
2 === rate=2.82911e-35, T=0.121028, TIT=0.242057, IT=0.5
2 finished solveLinearSystemImpl_ 1
2 converged value: 1 7 1 let s go get convergedRemote
2 to  comm_.min(converged) 
0 finished solveLinearSystemImpl_ 1
0 converged value: 1 7 1 let s go get convergedRemote
0 to  comm_.min(converged) 
0 did  comm_.min(converged) convergedRemote: 1
0 final convergedRemote: 1
0 Update: x^(k+1) = x^k - deltax^k 
0 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
0 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
0 Newton iteration  5 done0 , maximum relative shift = 9.709e-08
0 Assemble/solve/update time: 0.218902(25.2131%)/0.649198(74.7746%)/0.000107533(0.0123856%)
0 gridVariables->advanceTimeStep
1 did  comm_.min(converged) convergedRemote: 1
1 final convergedRemote: 1
1 Update: x^(k+1) = x^k - deltax^k 
1 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
1 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
1 Newton iteration  5 done1 , maximum relative shift = 9.709e-08
1 Assemble/solve/update time: 0.219708(25.2822%)/0.649222(74.7073%)/9.1633e-05(0.0105444%)
1 gridVariables->advanceTimeStep
1 before, nonLinearSolver->suggestTimeStepSize, current time: 833.767 simTime 57600
1 before assembler->setPreviousSolution, ddt: 28.3333
1 nonLinearSolver->solve
1 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
4 did  comm_.min(converged) convergedRemote: 1
4 final convergedRemote: 1
4 Update: x^(k+1) = x^k - deltax^k 
4 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
4 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
4 Newton iteration  5 done4 , maximum relative shift = 9.709e-08
4 Assemble/solve/update time: 0.219558(25.2694%)/0.649218(74.7198%)/9.3883e-05(0.0108052%)
4 gridVariables->advanceTimeStep
4 before, nonLinearSolver->suggestTimeStepSize, current time: 833.767 simTime 57600
4 before assembler->setPreviousSolution, ddt: 28.3333
4 nonLinearSolver->solve
4 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
5 did  comm_.min(converged) convergedRemote: 1
5 final convergedRemote: 1
5 Update: x^(k+1) = x^k - deltax^k 
5 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
5 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
5 Newton iteration  5 done5 , maximum relative shift = 9.709e-08
5 Assemble/solve/update time: 0.219715(25.2828%)/0.649219(74.7064%)/9.3853e-05(0.0107998%)
5 gridVariables->advanceTimeStep
5 before, nonLinearSolver->suggestTimeStepSize, current time: 833.767 simTime 57600
5 before assembler->setPreviousSolution, ddt: 28.3333
5 nonLinearSolver->solve
5 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
3 did  comm_.min(converged) convergedRemote: 1
3 final convergedRemote: 1
3 Update: x^(k+1) = x^k - deltax^k 
3 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
3 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
3 Newton iteration  5 done3 , maximum relative shift = 9.709e-08
3 Assemble/solve/update time: 0.219691(25.2801%)/0.649238(74.7085%)/9.9433e-05(0.0114419%)
3 gridVariables->advanceTimeStep
3 before, nonLinearSolver->suggestTimeStepSize, current time: 833.767 simTime 57600
3 before assembler->setPreviousSolution, ddt: 28.3333
3 nonLinearSolver->solve
3 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
6 did  comm_.min(converged) convergedRemote: 1
6 final convergedRemote: 1
6 Update: x^(k+1) = x^k - deltax^k 
6 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
6 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
6 Newton iteration  5 done6 , maximum relative shift = 9.709e-08
6 Assemble/solve/update time: 0.219571(25.2706%)/0.649235(74.7212%)/7.1182e-05(0.00819242%)
6 gridVariables->advanceTimeStep
6 before, nonLinearSolver->suggestTimeStepSize, current time: 833.767 simTime 57600
6 before assembler->setPreviousSolution, ddt: 28.3333
6 nonLinearSolver->solve
6 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
2 did  comm_.min(converged) convergedRemote: 1
2 final convergedRemote: 1
2 Update: x^(k+1) = x^k - deltax^k 
2 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
2 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
2 Newton iteration  5 done2 , maximum relative shift = 9.709e-08
2 Assemble/solve/update time: 0.219543(25.2675%)/0.649233(74.7212%)/9.8703e-05(0.0113599%)
2 gridVariables->advanceTimeStep
2 before, nonLinearSolver->suggestTimeStepSize, current time: 833.767 simTime 57600
2 before assembler->setPreviousSolution, ddt: 28.3333
2 nonLinearSolver->solve
2 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
0 before, nonLinearSolver->suggestTimeStepSize, current time: 833.767 simTime 57600
0 before assembler->setPreviousSolution, ddt: 28.3333
0 nonLinearSolver->solve
0 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
1 Solve: M deltax^k = r 
1 Newton::solveLinearSystem : entering the try section 0
1to  comm_.sum(norm2) 
1did  comm_.sum(norm2) 
1 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
1 solveLinearSystemImpl_, before converged = ls.solve 
1 7 amgbackend::solve isParallel 1
4 Solve: M deltax^k = r 
4 Newton::solveLinearSystem : entering the try section 0
4to  comm_.sum(norm2) 
4did  comm_.sum(norm2) 
4 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
4 solveLinearSystemImpl_, before converged = ls.solve 
4 7 amgbackend::solve isParallel 1
5 Solve: M deltax^k = r 
5 Newton::solveLinearSystem : entering the try section 0
5to  comm_.sum(norm2) 
5did  comm_.sum(norm2) 
5 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
5 solveLinearSystemImpl_, before converged = ls.solve 
5 7 amgbackend::solve isParallel 1
3 Solve: M deltax^k = r 
3 Newton::solveLinearSystem : entering the try section 0
3to  comm_.sum(norm2) 
3did  comm_.sum(norm2) 
3 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
3 solveLinearSystemImpl_, before converged = ls.solve 
3 7 amgbackend::solve isParallel 1
0 Solve: M deltax^k = r 
0 Newton::solveLinearSystem : entering the try section 0
0to  comm_.sum(norm2) 
0did  comm_.sum(norm2) 
0 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
0 solveLinearSystemImpl_, before converged = ls.solve 
0 7 amgbackend::solve isParallel 1
6 Solve: M deltax^k = r 
6 Newton::solveLinearSystem : entering the try section 0
6to  comm_.sum(norm2) 
6did  comm_.sum(norm2) 
6 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
6 solveLinearSystemImpl_, before converged = ls.solve 
6 7 amgbackend::solve isParallel 1
2 Solve: M deltax^k = r 
2 Newton::solveLinearSystem : entering the try section 0
2to  comm_.sum(norm2) 
2did  comm_.sum(norm2) 
2 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
2 solveLinearSystemImpl_, before converged = ls.solve 
2 7 amgbackend::solve isParallel 1
2 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
2 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
0 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
0 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 7
6 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
6 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
3 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
3 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
1 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
1 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
4 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
4 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
5 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
5 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 7
1 7 before Dune::BiCGSTABSolver 
1 Dune::BiCGSTABSolver.apply(x, b, result_) 
1 BiCGSTABSolver::apply 
1 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
4 7 before Dune::BiCGSTABSolver 
4 Dune::BiCGSTABSolver.apply(x, b, result_) 
4 BiCGSTABSolver::apply 
4 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 7
2 7 before Dune::BiCGSTABSolver 
2 Dune::BiCGSTABSolver.apply(x, b, result_) 
2 BiCGSTABSolver::apply 
2 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
5 7 before Dune::BiCGSTABSolver 
5 Dune::BiCGSTABSolver.apply(x, b, result_) 
5 BiCGSTABSolver::apply 
5 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
6 7 before Dune::BiCGSTABSolver 
6 Dune::BiCGSTABSolver.apply(x, b, result_) 
6 BiCGSTABSolver::apply 
6 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
3 7 before Dune::BiCGSTABSolver 
3 Dune::BiCGSTABSolver.apply(x, b, result_) 
3 BiCGSTABSolver::apply 
3 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
Using a direct coarse solver (SuperLU)
Building hierarchy of 1 levels (inclusive coarse solver) took 0.112378 seconds.
0 7 before Dune::BiCGSTABSolver 
0 Dune::BiCGSTABSolver.apply(x, b, result_) 
0 BiCGSTABSolver::apply 
0 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
4 === BiCGSTABSolver
 Iter          Defect            Rate
    0      3.16041e-09
4  (it = 0.5; it < _maxit; it+=.5) 0.5 250
4  before p = r 
4  before  y = W^-1 * p 
4  before _prec->apply(y,p); 0.5 250
4 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
1 === BiCGSTABSolver
 Iter          Defect            Rate
    0      3.16041e-09
1  (it = 0.5; it < _maxit; it+=.5) 0.5 250
1  before p = r 
1  before  y = W^-1 * p 
1  before _prec->apply(y,p); 0.5 250
1 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
0 === BiCGSTABSolver
 Iter          Defect            Rate
    0      3.16041e-09
0  (it = 0.5; it < _maxit; it+=.5) 0.5 250
0  before p = r 
0  before  y = W^-1 * p 
0  before _prec->apply(y,p); 0.5 250
0 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 7
0 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
3 === BiCGSTABSolver
 Iter          Defect            Rate
    0      3.16041e-09
3  (it = 0.5; it < _maxit; it+=.5) 0.5 250
3  before p = r 
3  before  y = W^-1 * p 
3  before _prec->apply(y,p); 0.5 250
3 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
2 === BiCGSTABSolver
 Iter          Defect            Rate
    0      3.16041e-09
2  (it = 0.5; it < _maxit; it+=.5) 0.5 250
2  before p = r 
2  before  y = W^-1 * p 
2  before _prec->apply(y,p); 0.5 250
2 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
5 === BiCGSTABSolver
 Iter          Defect            Rate
    0      3.16041e-09
5  (it = 0.5; it < _maxit; it+=.5) 0.5 250
5  before p = r 
5  before  y = W^-1 * p 
5  before _prec->apply(y,p); 0.5 250
5 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
6 === BiCGSTABSolver
 Iter          Defect            Rate
    0      3.16041e-09
6  (it = 0.5; it < _maxit; it+=.5) 0.5 250
6  before p = r 
6  before  y = W^-1 * p 
6  before _prec->apply(y,p); 0.5 250
6 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
6 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
6 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
6  before v = A * y 
6  before  alpha = rho_new / < rt, v > 
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
0 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
0 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
3 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
3 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
3  before v = A * y 
3  before  alpha = rho_new / < rt, v > 
2 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
2 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
5 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
5 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
5  before v = A * y 
5  before  alpha = rho_new / < rt, v > 
1 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
1 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
4 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
4 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
4  before v = A * y 
4  before  alpha = rho_new / < rt, v > 
2  before v = A * y 
2  before  alpha = rho_new / < rt, v > 
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
1  before v = A * y 
1  before  alpha = rho_new / < rt, v > 
1  before alpha = rho_new / h; 
1  before apply first correction to x 
0  before v = A * y 
0  before  alpha = rho_new / < rt, v > 
1  before r = r - alpha*v 
1  before test stop criteria 
1 istlsolver::printOutput 
  0.5       4.3566e-25      1.37849e-16
1 === rate=1.90024e-32, T=0.121108, TIT=0.242216, IT=0.5
1 finished solveLinearSystemImpl_ 1
1 converged value: 1 7 1 let s go get convergedRemote
1 to  comm_.min(converged) 
3  before alpha = rho_new / h; 
3  before apply first correction to x 
3  before r = r - alpha*v 
3  before test stop criteria 
3 istlsolver::printOutput 
  0.5       4.3566e-25      1.37849e-16
3 === rate=1.90024e-32, T=0.121109, TIT=0.242218, IT=0.5
3 finished solveLinearSystemImpl_ 1
3 converged value: 1 7 1 let s go get convergedRemote
3 to  comm_.min(converged) 
4  before alpha = rho_new / h; 
4  before apply first correction to x 
4  before r = r - alpha*v 
4  before test stop criteria 
4 istlsolver::printOutput 
  0.5       4.3566e-25      1.37849e-16
4 === rate=1.90024e-32, T=0.12111, TIT=0.24222, IT=0.5
4 finished solveLinearSystemImpl_ 1
4 converged value: 1 7 1 let s go get convergedRemote
4 to  comm_.min(converged) 
5  before alpha = rho_new / h; 
5  before apply first correction to x 
5  before r = r - alpha*v 
5  before test stop criteria 
5 istlsolver::printOutput 
  0.5       4.3566e-25      1.37849e-16
5 === rate=1.90024e-32, T=0.121108, TIT=0.242215, IT=0.5
5 finished solveLinearSystemImpl_ 1
5 converged value: 1 7 1 let s go get convergedRemote
5 to  comm_.min(converged) 
2  before alpha = rho_new / h; 
2  before apply first correction to x 
2  before r = r - alpha*v 
2  before test stop criteria 
2 istlsolver::printOutput 
  0.5       4.3566e-25      1.37849e-16
2 === rate=1.90024e-32, T=0.12111, TIT=0.24222, IT=0.5
2 finished solveLinearSystemImpl_ 1
2 converged value: 1 7 1 let s go get convergedRemote
2 to  comm_.min(converged) 
0  before alpha = rho_new / h; 
0  before apply first correction to x 
0  before r = r - alpha*v 
0  before test stop criteria 
0 istlsolver::printOutput 
  0.5       4.3566e-25      1.37849e-16
0 === rate=1.90024e-32, T=0.018721, TIT=0.037442, IT=0.5
6  before alpha = rho_new / h; 
6  before apply first correction to x 
6  before r = r - alpha*v 
6  before test stop criteria 
6 istlsolver::printOutput 
  0.5       4.3566e-25      1.37849e-16
6 === rate=1.90024e-32, T=0.12111, TIT=0.24222, IT=0.5
6 finished solveLinearSystemImpl_ 1
6 converged value: 1 7 1 let s go get convergedRemote
6 to  comm_.min(converged) 
3 did  comm_.min(converged) convergedRemote: 1
3 final convergedRemote: 1
3 Update: x^(k+1) = x^k - deltax^k 
3 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
3 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
3 Newton iteration  1 done3 , maximum relative shift = 5.107e-02
3 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
4 did  comm_.min(converged) convergedRemote: 1
4 final convergedRemote: 1
4 Update: x^(k+1) = x^k - deltax^k 
4 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
4 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
4 Newton iteration  1 done4 , maximum relative shift = 5.107e-02
4 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
5 did  comm_.min(converged) convergedRemote: 1
5 final convergedRemote: 1
5 Update: x^(k+1) = x^k - deltax^k 
5 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
5 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
5 Newton iteration  1 done5 , maximum relative shift = 5.107e-02
5 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
6 did  comm_.min(converged) convergedRemote: 1
6 final convergedRemote: 1
6 Update: x^(k+1) = x^k - deltax^k 
6 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
6 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
6 Newton iteration  1 done6 , maximum relative shift = 5.107e-02
6 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
0 finished solveLinearSystemImpl_ 1
0 converged value: 1 7 1 let s go get convergedRemote
0 to  comm_.min(converged) 
0 did  comm_.min(converged) convergedRemote: 1
0 final convergedRemote: 1
0 Update: x^(k+1) = x^k - deltax^k 
0 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
0 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
0 Newton iteration  1 done0 , maximum relative shift = 5.107e-02
0 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
2 did  comm_.min(converged) convergedRemote: 1
2 final convergedRemote: 1
2 Update: x^(k+1) = x^k - deltax^k 
2 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
2 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
2 Newton iteration  1 done2 , maximum relative shift = 5.107e-02
2 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
1 did  comm_.min(converged) convergedRemote: 1
1 final convergedRemote: 1
1 Update: x^(k+1) = x^k - deltax^k 
1 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
1 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
1 Newton iteration  1 done1 , maximum relative shift = 5.107e-02
1 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
6 Solve: M deltax^k = r 
6 Newton::solveLinearSystem : entering the try section 1
6 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
6 solveLinearSystemImpl_, before converged = ls.solve 
6 7 amgbackend::solve isParallel 1
0 Solve: M deltax^k = r 
0 Newton::solveLinearSystem : entering the try section 1
0 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
0 solveLinearSystemImpl_, before converged = ls.solve 
0 7 amgbackend::solve isParallel 1
2 Solve: M deltax^k = r 
2 Newton::solveLinearSystem : entering the try section 1
2 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
2 solveLinearSystemImpl_, before converged = ls.solve 
2 7 amgbackend::solve isParallel 1
5 Solve: M deltax^k = r 
5 Newton::solveLinearSystem : entering the try section 1
5 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
5 solveLinearSystemImpl_, before converged = ls.solve 
5 7 amgbackend::solve isParallel 1
3 Solve: M deltax^k = r 
3 Newton::solveLinearSystem : entering the try section 1
3 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
3 solveLinearSystemImpl_, before converged = ls.solve 
3 7 amgbackend::solve isParallel 1
4 Solve: M deltax^k = r 
4 Newton::solveLinearSystem : entering the try section 1
4 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
4 solveLinearSystemImpl_, before converged = ls.solve 
4 7 amgbackend::solve isParallel 1
1 Solve: M deltax^k = r 
1 Newton::solveLinearSystem : entering the try section 1
1 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
1 solveLinearSystemImpl_, before converged = ls.solve 
1 7 amgbackend::solve isParallel 1
6 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
6 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
0 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
0 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 7
2 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
2 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
5 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
5 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
3 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
3 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
1 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
1 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
4 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
4 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 7
1 7 before Dune::BiCGSTABSolver 
1 Dune::BiCGSTABSolver.apply(x, b, result_) 
1 BiCGSTABSolver::apply 
1 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
3 7 before Dune::BiCGSTABSolver 
3 Dune::BiCGSTABSolver.apply(x, b, result_) 
3 BiCGSTABSolver::apply 
3 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
6 7 before Dune::BiCGSTABSolver 
6 Dune::BiCGSTABSolver.apply(x, b, result_) 
6 BiCGSTABSolver::apply 
6 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
4 7 before Dune::BiCGSTABSolver 
4 Dune::BiCGSTABSolver.apply(x, b, result_) 
4 BiCGSTABSolver::apply 
4 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
2 7 before Dune::BiCGSTABSolver 
2 Dune::BiCGSTABSolver.apply(x, b, result_) 
2 BiCGSTABSolver::apply 
2 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
5 7 before Dune::BiCGSTABSolver 
5 Dune::BiCGSTABSolver.apply(x, b, result_) 
5 BiCGSTABSolver::apply 
5 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 7
Using a direct coarse solver (SuperLU)
Building hierarchy of 1 levels (inclusive coarse solver) took 0.112862 seconds.
0 7 before Dune::BiCGSTABSolver 
0 Dune::BiCGSTABSolver.apply(x, b, result_) 
0 BiCGSTABSolver::apply 
5 === BiCGSTABSolver
 Iter          Defect            Rate
    0      4.37221e-11
5  (it = 0.5; it < _maxit; it+=.5) 0.5 250
1 === BiCGSTABSolver
 Iter          Defect            Rate
    0      4.37221e-11
1  (it = 0.5; it < _maxit; it+=.5) 0.5 250
1  before p = r 
1  before  y = W^-1 * p 
3 === BiCGSTABSolver
 Iter          Defect            Rate
    0      4.37221e-11
3  (it = 0.5; it < _maxit; it+=.5) 0.5 250
3  before p = r 
3  before  y = W^-1 * p 
3  before _prec->apply(y,p); 0.5 250
6 === BiCGSTABSolver
 Iter          Defect            Rate
    0      4.37221e-11
6  (it = 0.5; it < _maxit; it+=.5) 0.5 250
6  before p = r 
6  before  y = W^-1 * p 
6  before _prec->apply(y,p); 0.5 250
6 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
4 === BiCGSTABSolver
 Iter          Defect            Rate
    0      4.37221e-11
4  (it = 0.5; it < _maxit; it+=.5) 0.5 250
4  before p = r 
4  before  y = W^-1 * p 
4  before _prec->apply(y,p); 0.5 250
4 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
2 === BiCGSTABSolver
 Iter          Defect            Rate
    0      4.37221e-11
2  (it = 0.5; it < _maxit; it+=.5) 0.5 250
2  before p = r 
2  before  y = W^-1 * p 
2  before _prec->apply(y,p); 0.5 250
2 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
0 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
0 === BiCGSTABSolver
 Iter          Defect            Rate
    0      4.37221e-11
0  (it = 0.5; it < _maxit; it+=.5) 0.5 250
0  before p = r 
0  before  y = W^-1 * p 
0  before _prec->apply(y,p); 0.5 250
0 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 7
5  before p = r 
5  before  y = W^-1 * p 
5  before _prec->apply(y,p); 0.5 250
5 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
1  before _prec->apply(y,p); 0.5 250
1 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
3 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 7
0 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
2 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
2 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
5 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
5 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
5  before v = A * y 
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
0 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
0 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
3 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
3 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
3  before v = A * y 
3  before  alpha = rho_new / < rt, v > 
6 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
6 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
6  before v = A * y 
6  before  alpha = rho_new / < rt, v > 
1 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
1 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
1  before v = A * y 
4 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
4 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
4  before v = A * y 
4  before  alpha = rho_new / < rt, v > 
2  before v = A * y 
2  before  alpha = rho_new / < rt, v > 
5  before  alpha = rho_new / < rt, v > 
0  before v = A * y 
0  before  alpha = rho_new / < rt, v > 
6  before alpha = rho_new / h; 
6  before apply first correction to x 
6  before r = r - alpha*v 
6  before test stop criteria 
6 istlsolver::printOutput 
  0.5      3.31952e-27      7.59233e-17
6 === rate=5.76434e-33, T=0.123506, TIT=0.247012, IT=0.5
6 finished solveLinearSystemImpl_ 1
6 converged value: 1 7 1 let s go get convergedRemote
6 to  comm_.min(converged) 
4  before alpha = rho_new / h; 
4  before apply first correction to x 
4  before r = r - alpha*v 
4  before test stop criteria 
4 istlsolver::printOutput 
  0.5      3.31952e-27      7.59233e-17
4 === rate=5.76434e-33, T=0.123505, TIT=0.247009, IT=0.5
4 finished solveLinearSystemImpl_ 1
4 converged value: 1 7 1 let s go get convergedRemote
4 to  comm_.min(converged) 
2  before alpha = rho_new / h; 
2  before apply first correction to x 
2  before r = r - alpha*v 
2  before test stop criteria 
2 istlsolver::printOutput 
  0.5      3.31952e-27      7.59233e-17
2 === rate=5.76434e-33, T=0.123505, TIT=0.24701, IT=0.5
2 finished solveLinearSystemImpl_ 1
2 converged value: 1 7 1 let s go get convergedRemote
2 to  comm_.min(converged) 
5  before alpha = rho_new / h; 
5  before apply first correction to x 
5  before r = r - alpha*v 
5  before test stop criteria 
5 istlsolver::printOutput 
  0.5      3.31952e-27      7.59233e-17
5 === rate=5.76434e-33, T=0.123526, TIT=0.247051, IT=0.5
5 finished solveLinearSystemImpl_ 1
5 converged value: 1 7 1 let s go get convergedRemote
5 to  comm_.min(converged) 
3  before alpha = rho_new / h; 
3  before apply first correction to x 
3  before r = r - alpha*v 
3  before test stop criteria 
3 istlsolver::printOutput 
  0.5      3.31952e-27      7.59233e-17
3 === rate=5.76434e-33, T=0.123527, TIT=0.247053, IT=0.5
3 finished solveLinearSystemImpl_ 1
3 converged value: 1 7 1 let s go get convergedRemote
3 to  comm_.min(converged) 
0  before alpha = rho_new / h; 
0  before apply first correction to x 
0  before r = r - alpha*v 
0  before test stop criteria 
0 istlsolver::printOutput 
  0.5      3.31952e-27      7.59233e-17
0 === rate=5.76434e-33, T=0.0183574, TIT=0.0367148, IT=0.5
1  before  alpha = rho_new / < rt, v > 
1  before alpha = rho_new / h; 
1  before apply first correction to x 
1  before r = r - alpha*v 
1  before test stop criteria 
1 istlsolver::printOutput 
  0.5      3.31952e-27      7.59233e-17
1 === rate=5.76434e-33, T=0.123527, TIT=0.247054, IT=0.5
1 finished solveLinearSystemImpl_ 1
1 converged value: 1 7 1 let s go get convergedRemote
1 to  comm_.min(converged) 
2 did  comm_.min(converged) convergedRemote: 1
2 final convergedRemote: 1
2 Update: x^(k+1) = x^k - deltax^k 
2 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
2 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
2 Newton iteration  2 done2 , maximum relative shift = 6.455e-03
2 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
5 did  comm_.min(converged) convergedRemote: 1
5 final convergedRemote: 1
5 Update: x^(k+1) = x^k - deltax^k 
5 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
5 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
5 Newton iteration  2 done5 , maximum relative shift = 6.455e-03
5 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
1 did  comm_.min(converged) convergedRemote: 1
1 final convergedRemote: 1
1 Update: x^(k+1) = x^k - deltax^k 
1 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
1 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
1 Newton iteration  2 done1 , maximum relative shift = 6.455e-03
1 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
0 finished solveLinearSystemImpl_ 1
0 converged value: 1 7 1 let s go get convergedRemote
0 to  comm_.min(converged) 
0 did  comm_.min(converged) convergedRemote: 1
0 final convergedRemote: 1
0 Update: x^(k+1) = x^k - deltax^k 
0 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
0 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
0 Newton iteration  2 done0 , maximum relative shift = 6.455e-03
0 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
3 did  comm_.min(converged) convergedRemote: 1
3 final convergedRemote: 1
3 Update: x^(k+1) = x^k - deltax^k 
3 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
3 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
3 Newton iteration  2 done3 , maximum relative shift = 6.455e-03
3 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
6 did  comm_.min(converged) convergedRemote: 1
6 final convergedRemote: 1
6 Update: x^(k+1) = x^k - deltax^k 
6 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
6 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
6 Newton iteration  2 done6 , maximum relative shift = 6.455e-03
6 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
4 did  comm_.min(converged) convergedRemote: 1
4 final convergedRemote: 1
4 Update: x^(k+1) = x^k - deltax^k 
4 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
4 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
4 Newton iteration  2 done4 , maximum relative shift = 6.455e-03
4 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
2 Solve: M deltax^k = r 
2 Newton::solveLinearSystem : entering the try section 2
2 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
2 solveLinearSystemImpl_, before converged = ls.solve 
2 7 amgbackend::solve isParallel 1
4 Solve: M deltax^k = r 
4 Newton::solveLinearSystem : entering the try section 2
4 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
4 solveLinearSystemImpl_, before converged = ls.solve 
4 7 amgbackend::solve isParallel 1
1 Solve: M deltax^k = r 
1 Newton::solveLinearSystem : entering the try section 2
1 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
1 solveLinearSystemImpl_, before converged = ls.solve 
1 7 amgbackend::solve isParallel 1
0 Solve: M deltax^k = r 
0 Newton::solveLinearSystem : entering the try section 2
0 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
0 solveLinearSystemImpl_, before converged = ls.solve 
0 7 amgbackend::solve isParallel 1
3 Solve: M deltax^k = r 
3 Newton::solveLinearSystem : entering the try section 2
3 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
3 solveLinearSystemImpl_, before converged = ls.solve 
3 7 amgbackend::solve isParallel 1
6 Solve: M deltax^k = r 
6 Newton::solveLinearSystem : entering the try section 2
6 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
6 solveLinearSystemImpl_, before converged = ls.solve 
6 7 amgbackend::solve isParallel 1
5 Solve: M deltax^k = r 
5 Newton::solveLinearSystem : entering the try section 2
5 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
5 solveLinearSystemImpl_, before converged = ls.solve 
5 7 amgbackend::solve isParallel 1
5 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
5 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
4 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
4 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
1 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
1 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
0 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
0 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 7
3 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
3 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
6 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
6 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
2 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
2 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 7
6 7 before Dune::BiCGSTABSolver 
6 Dune::BiCGSTABSolver.apply(x, b, result_) 
6 BiCGSTABSolver::apply 
6 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 7
5 7 before Dune::BiCGSTABSolver 
5 Dune::BiCGSTABSolver.apply(x, b, result_) 
5 BiCGSTABSolver::apply 
5 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
4 7 before Dune::BiCGSTABSolver 
4 Dune::BiCGSTABSolver.apply(x, b, result_) 
4 BiCGSTABSolver::apply 
4 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
1 7 before Dune::BiCGSTABSolver 
1 Dune::BiCGSTABSolver.apply(x, b, result_) 
1 BiCGSTABSolver::apply 
1 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
2 7 before Dune::BiCGSTABSolver 
2 Dune::BiCGSTABSolver.apply(x, b, result_) 
2 BiCGSTABSolver::apply 
2 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
3 7 before Dune::BiCGSTABSolver 
3 Dune::BiCGSTABSolver.apply(x, b, result_) 
3 BiCGSTABSolver::apply 
3 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
Using a direct coarse solver (SuperLU)
Building hierarchy of 1 levels (inclusive coarse solver) took 0.11433 seconds.
0 7 before Dune::BiCGSTABSolver 
0 Dune::BiCGSTABSolver.apply(x, b, result_) 
0 BiCGSTABSolver::apply 
0 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
2 === BiCGSTABSolver
 Iter          Defect            Rate
    0      3.80799e-12
2  (it = 0.5; it < _maxit; it+=.5) 0.5 250
2  before p = r 
2  before  y = W^-1 * p 
2  before _prec->apply(y,p); 0.5 250
2 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
6 === BiCGSTABSolver
 Iter          Defect            Rate
    0      3.80799e-12
6  (it = 0.5; it < _maxit; it+=.5) 0.5 250
6  before p = r 
6  before  y = W^-1 * p 
6  before _prec->apply(y,p); 0.5 250
6 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
0 === BiCGSTABSolver
 Iter          Defect            Rate
    0      3.80799e-12
0  (it = 0.5; it < _maxit; it+=.5) 0.5 250
0  before p = r 
0  before  y = W^-1 * p 
0  before _prec->apply(y,p); 0.5 250
0 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 7
0 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
3 === BiCGSTABSolver
 Iter          Defect            Rate
    0      3.80799e-12
3  (it = 0.5; it < _maxit; it+=.5) 0.5 250
3  before p = r 
3  before  y = W^-1 * p 
3  before _prec->apply(y,p); 0.5 250
3 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
5 === BiCGSTABSolver
 Iter          Defect            Rate
    0      3.80799e-12
5  (it = 0.5; it < _maxit; it+=.5) 0.5 250
5  before p = r 
5  before  y = W^-1 * p 
5  before _prec->apply(y,p); 0.5 250
5 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
4 === BiCGSTABSolver
 Iter          Defect            Rate
    0      3.80799e-12
4  (it = 0.5; it < _maxit; it+=.5) 0.5 250
4  before p = r 
4  before  y = W^-1 * p 
4  before _prec->apply(y,p); 0.5 250
4 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
1 === BiCGSTABSolver
 Iter          Defect            Rate
    0      3.80799e-12
1  (it = 0.5; it < _maxit; it+=.5) 0.5 250
1  before p = r 
1  before  y = W^-1 * p 
1  before _prec->apply(y,p); 0.5 250
1 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
1 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
3 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
3 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
3  before v = A * y 
3  before  alpha = rho_new / < rt, v > 
5 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
5 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
5  before v = A * y 
5  before  alpha = rho_new / < rt, v > 
4 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
4 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
4  before v = A * y 
6 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
6 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
6  before v = A * y 
2 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
2 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
6  before  alpha = rho_new / < rt, v > 
0 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
4  before  alpha = rho_new / < rt, v > 
6  before alpha = rho_new / h; 
6  before apply first correction to x 
6  before r = r - alpha*v 
6  before test stop criteria 
6 istlsolver::printOutput 
  0.5      4.49297e-29      1.17988e-17
6 === rate=1.39212e-34, T=0.125393, TIT=0.250787, IT=0.5
6 finished solveLinearSystemImpl_ 1
6 converged value: 1 7 1 let s go get convergedRemote
6 to  comm_.min(converged) 
3  before alpha = rho_new / h; 
3  before apply first correction to x 
3  before r = r - alpha*v 
3  before test stop criteria 
3 istlsolver::printOutput 
  0.5      4.49297e-29      1.17988e-17
3 === rate=1.39212e-34, T=0.125287, TIT=0.250574, IT=0.5
3 finished solveLinearSystemImpl_ 1
3 converged value: 1 7 1 let s go get convergedRemote
3 to  comm_.min(converged) 
5  before alpha = rho_new / h; 
5  before apply first correction to x 
5  before r = r - alpha*v 
5  before test stop criteria 
5 istlsolver::printOutput 
  0.5      4.49297e-29      1.17988e-17
5 === rate=1.39212e-34, T=0.125286, TIT=0.250572, IT=0.5
5 finished solveLinearSystemImpl_ 1
5 converged value: 1 7 1 let s go get convergedRemote
5 to  comm_.min(converged) 
0 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
0  before v = A * y 
0  before  alpha = rho_new / < rt, v > 
0  before alpha = rho_new / h; 
0  before apply first correction to x 
0  before r = r - alpha*v 
0  before test stop criteria 
0 istlsolver::printOutput 
  0.5      4.49297e-29      1.17988e-17
0 === rate=1.39212e-34, T=0.0187517, TIT=0.0375034, IT=0.5
2  before v = A * y 
2  before  alpha = rho_new / < rt, v > 
2  before alpha = rho_new / h; 
2  before apply first correction to x 
2  before r = r - alpha*v 
2  before test stop criteria 
2 istlsolver::printOutput 
  0.5      4.49297e-29      1.17988e-17
2 === rate=1.39212e-34, T=0.125394, TIT=0.250788, IT=0.5
2 finished solveLinearSystemImpl_ 1
2 converged value: 1 7 1 let s go get convergedRemote
2 to  comm_.min(converged) 
4  before alpha = rho_new / h; 
4  before apply first correction to x 
4  before r = r - alpha*v 
4  before test stop criteria 
4 istlsolver::printOutput 
  0.5      4.49297e-29      1.17988e-17
4 === rate=1.39212e-34, T=0.125394, TIT=0.250787, IT=0.5
4 finished solveLinearSystemImpl_ 1
4 converged value: 1 7 1 let s go get convergedRemote
4 to  comm_.min(converged) 
1 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
1  before v = A * y 
1  before  alpha = rho_new / < rt, v > 
1  before alpha = rho_new / h; 
1  before apply first correction to x 
1  before r = r - alpha*v 
1  before test stop criteria 
1 istlsolver::printOutput 
  0.5      4.49297e-29      1.17988e-17
1 === rate=1.39212e-34, T=0.125286, TIT=0.250572, IT=0.5
1 finished solveLinearSystemImpl_ 1
1 converged value: 1 7 1 let s go get convergedRemote
1 to  comm_.min(converged) 
4 did  comm_.min(converged) convergedRemote: 1
4 final convergedRemote: 1
4 Update: x^(k+1) = x^k - deltax^k 
4 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
4 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
4 Newton iteration  3 done4 , maximum relative shift = 7.280e-04
4 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
2 did  comm_.min(converged) convergedRemote: 1
2 final convergedRemote: 1
2 Update: x^(k+1) = x^k - deltax^k 
2 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
2 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
2 Newton iteration  3 done2 , maximum relative shift = 7.280e-04
2 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
6 did  comm_.min(converged) convergedRemote: 1
6 final convergedRemote: 1
6 Update: x^(k+1) = x^k - deltax^k 
6 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
6 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
6 Newton iteration  3 done6 , maximum relative shift = 7.280e-04
6 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
3 did  comm_.min(converged) convergedRemote: 1
3 final convergedRemote: 1
3 Update: x^(k+1) = x^k - deltax^k 
3 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
3 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
3 Newton iteration  3 done3 , maximum relative shift = 7.280e-04
3 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
5 did  comm_.min(converged) convergedRemote: 1
5 final convergedRemote: 1
5 Update: x^(k+1) = x^k - deltax^k 
5 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
5 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
5 Newton iteration  3 done5 , maximum relative shift = 7.280e-04
5 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
0 finished solveLinearSystemImpl_ 1
0 converged value: 1 7 1 let s go get convergedRemote
0 to  comm_.min(converged) 
0 did  comm_.min(converged) convergedRemote: 1
0 final convergedRemote: 1
0 Update: x^(k+1) = x^k - deltax^k 
0 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
0 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
0 Newton iteration  3 done0 , maximum relative shift = 7.280e-04
0 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
1 did  comm_.min(converged) convergedRemote: 1
1 final convergedRemote: 1
1 Update: x^(k+1) = x^k - deltax^k 
1 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
1 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
1 Newton iteration  3 done1 , maximum relative shift = 7.280e-04
1 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
1 Solve: M deltax^k = r 
1 Newton::solveLinearSystem : entering the try section 3
1 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
1 solveLinearSystemImpl_, before converged = ls.solve 
1 7 amgbackend::solve isParallel 1
0 Solve: M deltax^k = r 
0 Newton::solveLinearSystem : entering the try section 3
0 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
0 solveLinearSystemImpl_, before converged = ls.solve 
0 7 amgbackend::solve isParallel 1
4 Solve: M deltax^k = r 
4 Newton::solveLinearSystem : entering the try section 3
4 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
4 solveLinearSystemImpl_, before converged = ls.solve 
4 7 amgbackend::solve isParallel 1
2 Solve: M deltax^k = r 
2 Newton::solveLinearSystem : entering the try section 3
2 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
2 solveLinearSystemImpl_, before converged = ls.solve 
2 7 amgbackend::solve isParallel 1
6 Solve: M deltax^k = r 
6 Newton::solveLinearSystem : entering the try section 3
6 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
6 solveLinearSystemImpl_, before converged = ls.solve 
6 7 amgbackend::solve isParallel 1
3 Solve: M deltax^k = r 
3 Newton::solveLinearSystem : entering the try section 3
3 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
3 solveLinearSystemImpl_, before converged = ls.solve 
3 7 amgbackend::solve isParallel 1
5 Solve: M deltax^k = r 
5 Newton::solveLinearSystem : entering the try section 3
5 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
5 solveLinearSystemImpl_, before converged = ls.solve 
5 7 amgbackend::solve isParallel 1
3 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
3 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
5 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
5 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
1 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
1 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
0 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
0 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 7
4 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
4 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
2 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
2 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
6 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
6 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 7
3 7 before Dune::BiCGSTABSolver 
3 Dune::BiCGSTABSolver.apply(x, b, result_) 
3 BiCGSTABSolver::apply 
3 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 7
5 7 before Dune::BiCGSTABSolver 
5 Dune::BiCGSTABSolver.apply(x, b, result_) 
5 BiCGSTABSolver::apply 
5 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
1 7 before Dune::BiCGSTABSolver 
1 Dune::BiCGSTABSolver.apply(x, b, result_) 
1 BiCGSTABSolver::apply 
1 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
6 7 before Dune::BiCGSTABSolver 
6 Dune::BiCGSTABSolver.apply(x, b, result_) 
6 BiCGSTABSolver::apply 
6 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
4 7 before Dune::BiCGSTABSolver 
4 Dune::BiCGSTABSolver.apply(x, b, result_) 
4 BiCGSTABSolver::apply 
4 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
2 7 before Dune::BiCGSTABSolver 
2 Dune::BiCGSTABSolver.apply(x, b, result_) 
2 BiCGSTABSolver::apply 
2 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
Using a direct coarse solver (SuperLU)
Building hierarchy of 1 levels (inclusive coarse solver) took 0.114483 seconds.
0 7 before Dune::BiCGSTABSolver 
0 Dune::BiCGSTABSolver.apply(x, b, result_) 
0 BiCGSTABSolver::apply 
0 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
1 === BiCGSTABSolver
 Iter          Defect            Rate
    0      6.80907e-13
1  (it = 0.5; it < _maxit; it+=.5) 0.5 250
1  before p = r 
1  before  y = W^-1 * p 
1  before _prec->apply(y,p); 0.5 250
1 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
6 === BiCGSTABSolver
 Iter          Defect            Rate
    0      6.80907e-13
6  (it = 0.5; it < _maxit; it+=.5) 0.5 250
6  before p = r 
6  before  y = W^-1 * p 
6  before _prec->apply(y,p); 0.5 250
6 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
4 === BiCGSTABSolver
 Iter          Defect            Rate
    0      6.80907e-13
4  (it = 0.5; it < _maxit; it+=.5) 0.5 250
4  before p = r 
4  before  y = W^-1 * p 
4  before _prec->apply(y,p); 0.5 250
4 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
3 === BiCGSTABSolver
 Iter          Defect            Rate
    0      6.80907e-13
3  (it = 0.5; it < _maxit; it+=.5) 0.5 250
3  before p = r 
3  before  y = W^-1 * p 
3  before _prec->apply(y,p); 0.5 250
3 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
0 === BiCGSTABSolver
 Iter          Defect            Rate
    0      6.80907e-13
0  (it = 0.5; it < _maxit; it+=.5) 0.5 250
0  before p = r 
0  before  y = W^-1 * p 
0  before _prec->apply(y,p); 0.5 250
0 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 7
2 === BiCGSTABSolver
 Iter          Defect            Rate
    0      6.80907e-13
2  (it = 0.5; it < _maxit; it+=.5) 0.5 250
2  before p = r 
2  before  y = W^-1 * p 
2  before _prec->apply(y,p); 0.5 250
2 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
5 === BiCGSTABSolver
 Iter          Defect            Rate
    0      6.80907e-13
5  (it = 0.5; it < _maxit; it+=.5) 0.5 250
5  before p = r 
5  before  y = W^-1 * p 
5  before _prec->apply(y,p); 0.5 250
5 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
0 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
5 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
5 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
5  before v = A * y 
2 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
2 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
3 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
3 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
3  before v = A * y 
1 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
6 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
6 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
6  before v = A * y 
4 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
4 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
4  before v = A * y 
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
3  before  alpha = rho_new / < rt, v > 
1 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
1  before v = A * y 
1  before  alpha = rho_new / < rt, v > 
6  before  alpha = rho_new / < rt, v > 
0 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
0 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
4  before  alpha = rho_new / < rt, v > 
5  before  alpha = rho_new / < rt, v > 
2  before v = A * y 
5  before alpha = rho_new / h; 
5  before apply first correction to x 
5  before r = r - alpha*v 
5  before test stop criteria 
5 istlsolver::printOutput 
  0.5      3.55826e-32      5.22577e-20
5 === rate=2.73086e-39, T=0.125513, TIT=0.251027, IT=0.5
5 finished solveLinearSystemImpl_ 1
5 converged value: 1 7 1 let s go get convergedRemote
5 to  comm_.min(converged) 
6  before alpha = rho_new / h; 
6  before apply first correction to x 
6  before r = r - alpha*v 
6  before test stop criteria 
6 istlsolver::printOutput 
  0.5      3.55826e-32      5.22577e-20
6 === rate=2.73086e-39, T=0.125657, TIT=0.251314, IT=0.5
6 finished solveLinearSystemImpl_ 1
6 converged value: 1 7 1 let s go get convergedRemote
6 to  comm_.min(converged) 
3  before alpha = rho_new / h; 
3  before apply first correction to x 
3  before r = r - alpha*v 
3  before test stop criteria 
3 istlsolver::printOutput 
  0.5      3.55826e-32      5.22577e-20
3 === rate=2.73086e-39, T=0.125514, TIT=0.251028, IT=0.5
3 finished solveLinearSystemImpl_ 1
3 converged value: 1 7 1 let s go get convergedRemote
3 to  comm_.min(converged) 
1  before alpha = rho_new / h; 
1  before apply first correction to x 
1  before r = r - alpha*v 
1  before test stop criteria 
1 istlsolver::printOutput 
  0.5      3.55826e-32      5.22577e-20
1 === rate=2.73086e-39, T=0.125513, TIT=0.251026, IT=0.5
1 finished solveLinearSystemImpl_ 1
1 converged value: 1 7 1 let s go get convergedRemote
1 to  comm_.min(converged) 
2  before  alpha = rho_new / < rt, v > 
2  before alpha = rho_new / h; 
2  before apply first correction to x 
2  before r = r - alpha*v 
2  before test stop criteria 
2 istlsolver::printOutput 
  0.5      3.55826e-32      5.22577e-20
2 === rate=2.73086e-39, T=0.125655, TIT=0.251311, IT=0.5
2 finished solveLinearSystemImpl_ 1
2 converged value: 1 7 1 let s go get convergedRemote
2 to  comm_.min(converged) 
0  before v = A * y 
0  before  alpha = rho_new / < rt, v > 
0  before alpha = rho_new / h; 
0  before apply first correction to x 
0  before r = r - alpha*v 
0  before test stop criteria 
0 istlsolver::printOutput 
  0.5      3.55826e-32      5.22577e-20
0 === rate=2.73086e-39, T=0.0188413, TIT=0.0376826, IT=0.5
4  before alpha = rho_new / h; 
4  before apply first correction to x 
4  before r = r - alpha*v 
4  before test stop criteria 
4 istlsolver::printOutput 
  0.5      3.55826e-32      5.22577e-20
4 === rate=2.73086e-39, T=0.125657, TIT=0.251314, IT=0.5
4 finished solveLinearSystemImpl_ 1
4 converged value: 1 7 1 let s go get convergedRemote
4 to  comm_.min(converged) 
0 finished solveLinearSystemImpl_ 1
0 converged value: 1 7 1 let s go get convergedRemote
0 to  comm_.min(converged) 
0 did  comm_.min(converged) convergedRemote: 1
0 final convergedRemote: 1
0 Update: x^(k+1) = x^k - deltax^k 
0 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
1 did  comm_.min(converged) convergedRemote: 1
1 final convergedRemote: 1
1 Update: x^(k+1) = x^k - deltax^k 
1 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
1 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
1 Newton iteration  4 done1 , maximum relative shift = 9.123e-05
1 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
5 did  comm_.min(converged) convergedRemote: 1
5 final convergedRemote: 1
5 Update: x^(k+1) = x^k - deltax^k 
5 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
5 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
5 Newton iteration  4 done5 , maximum relative shift = 9.123e-05
5 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
6 did  comm_.min(converged) convergedRemote: 1
6 final convergedRemote: 1
6 Update: x^(k+1) = x^k - deltax^k 
6 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
6 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
3 did  comm_.min(converged) convergedRemote: 1
3 final convergedRemote: 1
3 Update: x^(k+1) = x^k - deltax^k 
3 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
3 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
3 Newton iteration  4 done3 , maximum relative shift = 9.123e-05
3 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
4 did  comm_.min(converged) convergedRemote: 1
4 final convergedRemote: 1
4 Update: x^(k+1) = x^k - deltax^k 
4 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
2 did  comm_.min(converged) convergedRemote: 1
2 final convergedRemote: 1
2 Update: x^(k+1) = x^k - deltax^k 
2 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
4 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
4 Newton iteration  4 done4 , maximum relative shift = 9.123e-05
4 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
0 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
0 Newton iteration  4 done0 , maximum relative shift = 9.123e-05
0 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
6 Newton iteration  4 done6 , maximum relative shift = 9.123e-05
6 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
2 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
2 Newton iteration  4 done2 , maximum relative shift = 9.123e-05
2 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
5 Solve: M deltax^k = r 
5 Newton::solveLinearSystem : entering the try section 4
5 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
5 solveLinearSystemImpl_, before converged = ls.solve 
5 7 amgbackend::solve isParallel 1
0 Solve: M deltax^k = r 
0 Newton::solveLinearSystem : entering the try section 4
0 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
0 solveLinearSystemImpl_, before converged = ls.solve 
0 7 amgbackend::solve isParallel 1
2 Solve: M deltax^k = r 
2 Newton::solveLinearSystem : entering the try section 4
2 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
2 solveLinearSystemImpl_, before converged = ls.solve 
2 7 amgbackend::solve isParallel 1
6 Solve: M deltax^k = r 
6 Newton::solveLinearSystem : entering the try section 4
6 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
6 solveLinearSystemImpl_, before converged = ls.solve 
6 7 amgbackend::solve isParallel 1
3 Solve: M deltax^k = r 
3 Newton::solveLinearSystem : entering the try section 4
3 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
3 solveLinearSystemImpl_, before converged = ls.solve 
3 7 amgbackend::solve isParallel 1
4 Solve: M deltax^k = r 
4 Newton::solveLinearSystem : entering the try section 4
4 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
4 solveLinearSystemImpl_, before converged = ls.solve 
4 7 amgbackend::solve isParallel 1
1 Solve: M deltax^k = r 
1 Newton::solveLinearSystem : entering the try section 4
1 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
1 solveLinearSystemImpl_, before converged = ls.solve 
1 7 amgbackend::solve isParallel 1
2 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
2 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
6 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
6 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
3 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
3 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
4 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
4 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
0 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
0 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 7
5 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
5 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
1 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
1 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 7
5 7 before Dune::BiCGSTABSolver 
5 Dune::BiCGSTABSolver.apply(x, b, result_) 
5 BiCGSTABSolver::apply 
5 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
2 7 before Dune::BiCGSTABSolver 
2 Dune::BiCGSTABSolver.apply(x, b, result_) 
2 BiCGSTABSolver::apply 
2 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
6 7 before Dune::BiCGSTABSolver 
6 Dune::BiCGSTABSolver.apply(x, b, result_) 
6 BiCGSTABSolver::apply 
6 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
3 7 before Dune::BiCGSTABSolver 
3 Dune::BiCGSTABSolver.apply(x, b, result_) 
3 BiCGSTABSolver::apply 
3 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
4 7 before Dune::BiCGSTABSolver 
4 Dune::BiCGSTABSolver.apply(x, b, result_) 
4 BiCGSTABSolver::apply 
4 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
1 7 before Dune::BiCGSTABSolver 
1 Dune::BiCGSTABSolver.apply(x, b, result_) 
1 BiCGSTABSolver::apply 
1 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
Using a direct coarse solver (SuperLU)
Building hierarchy of 1 levels (inclusive coarse solver) took 0.113545 seconds.
0 7 before Dune::BiCGSTABSolver 
0 Dune::BiCGSTABSolver.apply(x, b, result_) 
0 BiCGSTABSolver::apply 
0 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
6 === BiCGSTABSolver
 Iter          Defect            Rate
    0      1.18402e-13
6  (it = 0.5; it < _maxit; it+=.5) 0.5 250
6  before p = r 
6  before  y = W^-1 * p 
6  before _prec->apply(y,p); 0.5 250
6 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
4 === BiCGSTABSolver
 Iter          Defect            Rate
    0      1.18402e-13
4  (it = 0.5; it < _maxit; it+=.5) 0.5 250
4  before p = r 
4  before  y = W^-1 * p 
4  before _prec->apply(y,p); 0.5 250
4 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
3 === BiCGSTABSolver
 Iter          Defect            Rate
    0      1.18402e-13
3  (it = 0.5; it < _maxit; it+=.5) 0.5 250
3  before p = r 
3  before  y = W^-1 * p 
3  before _prec->apply(y,p); 0.5 250
3 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
0 === BiCGSTABSolver
 Iter          Defect            Rate
    0      1.18402e-13
0  (it = 0.5; it < _maxit; it+=.5) 0.5 250
0  before p = r 
0  before  y = W^-1 * p 
0  before _prec->apply(y,p); 0.5 250
0 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
1 === BiCGSTABSolver
 Iter          Defect            Rate
    0      1.18402e-13
1  (it = 0.5; it < _maxit; it+=.5) 0.5 250
1  before p = r 
1  before  y = W^-1 * p 
1  before _prec->apply(y,p); 0.5 250
1 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
5 === BiCGSTABSolver
 Iter          Defect            Rate
    0      1.18402e-13
5  (it = 0.5; it < _maxit; it+=.5) 0.5 250
5  before p = r 
5  before  y = W^-1 * p 
5  before _prec->apply(y,p); 0.5 250
5 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
2 === BiCGSTABSolver
 Iter          Defect            Rate
    0      1.18402e-13
2  (it = 0.5; it < _maxit; it+=.5) 0.5 250
2  before p = r 
2  before  y = W^-1 * p 
2  before _prec->apply(y,p); 0.5 250
2 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 7
0 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
2 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
2 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
0 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
4 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
4 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
4  before v = A * y 
4  before  alpha = rho_new / < rt, v > 
5 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
5 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
5  before v = A * y 
3 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
3 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
3  before v = A * y 
6 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
6 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
6  before v = A * y 
6  before  alpha = rho_new / < rt, v > 
1 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
5  before  alpha = rho_new / < rt, v > 
3  before  alpha = rho_new / < rt, v > 
0 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
0  before v = A * y 
0  before  alpha = rho_new / < rt, v > 
0  before alpha = rho_new / h; 
0  before apply first correction to x 
0  before r = r - alpha*v 
0  before test stop criteria 
0 istlsolver::printOutput 
  0.5      1.62175e-32       1.3697e-19
0 === rate=1.87608e-38, T=0.0187866, TIT=0.0375732, IT=0.5
5  before alpha = rho_new / h; 
5  before apply first correction to x 
5  before r = r - alpha*v 
5  before test stop criteria 
5 istlsolver::printOutput 
  0.5      1.62175e-32       1.3697e-19
5 === rate=1.87608e-38, T=0.124579, TIT=0.249158, IT=0.5
5 finished solveLinearSystemImpl_ 1
5 converged value: 1 7 1 let s go get convergedRemote
5 to  comm_.min(converged) 
3  before alpha = rho_new / h; 
3  before apply first correction to x 
3  before r = r - alpha*v 
3  before test stop criteria 
3 istlsolver::printOutput 
  0.5      1.62175e-32       1.3697e-19
3 === rate=1.87608e-38, T=0.12458, TIT=0.24916, IT=0.5
3 finished solveLinearSystemImpl_ 1
3 converged value: 1 7 1 let s go get convergedRemote
3 to  comm_.min(converged) 
6  before alpha = rho_new / h; 
6  before apply first correction to x 
6  before r = r - alpha*v 
6  before test stop criteria 
6 istlsolver::printOutput 
  0.5      1.62175e-32       1.3697e-19
6 === rate=1.87608e-38, T=0.124587, TIT=0.249173, IT=0.5
6 finished solveLinearSystemImpl_ 1
6 converged value: 1 7 1 let s go get convergedRemote
6 to  comm_.min(converged) 
2  before v = A * y 
2  before  alpha = rho_new / < rt, v > 
2  before alpha = rho_new / h; 
2  before apply first correction to x 
2  before r = r - alpha*v 
2  before test stop criteria 
2 istlsolver::printOutput 
  0.5      1.62175e-32       1.3697e-19
2 === rate=1.87608e-38, T=0.124583, TIT=0.249167, IT=0.5
2 finished solveLinearSystemImpl_ 1
2 converged value: 1 7 1 let s go get convergedRemote
2 to  comm_.min(converged) 
1 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
1  before v = A * y 
1  before  alpha = rho_new / < rt, v > 
1  before alpha = rho_new / h; 
1  before apply first correction to x 
1  before r = r - alpha*v 
1  before test stop criteria 
1 istlsolver::printOutput 
  0.5      1.62175e-32       1.3697e-19
1 === rate=1.87608e-38, T=0.124568, TIT=0.249136, IT=0.5
1 finished solveLinearSystemImpl_ 1
1 converged value: 1 7 1 let s go get convergedRemote
1 to  comm_.min(converged) 
4  before alpha = rho_new / h; 
4  before apply first correction to x 
4  before r = r - alpha*v 
4  before test stop criteria 
4 istlsolver::printOutput 
  0.5      1.62175e-32       1.3697e-19
4 === rate=1.87608e-38, T=0.124574, TIT=0.249148, IT=0.5
4 finished solveLinearSystemImpl_ 1
4 converged value: 1 7 1 let s go get convergedRemote
4 to  comm_.min(converged) 
3 did  comm_.min(converged) convergedRemote: 1
3 final convergedRemote: 1
3 Update: x^(k+1) = x^k - deltax^k 
3 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
3 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
3 Newton iteration  5 done3 , maximum relative shift = 2.265e-05
3 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
6 did  comm_.min(converged) convergedRemote: 1
6 final convergedRemote: 1
6 Update: x^(k+1) = x^k - deltax^k 
6 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
6 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
6 Newton iteration  5 done6 , maximum relative shift = 2.265e-05
6 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
2 did  comm_.min(converged) convergedRemote: 1
2 final convergedRemote: 1
2 Update: x^(k+1) = x^k - deltax^k 
2 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
2 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
2 Newton iteration  5 done2 , maximum relative shift = 2.265e-05
2 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
1 did  comm_.min(converged) convergedRemote: 1
1 final convergedRemote: 1
1 Update: x^(k+1) = x^k - deltax^k 
1 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
1 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
1 Newton iteration  5 done1 , maximum relative shift = 2.265e-05
1 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
5 did  comm_.min(converged) convergedRemote: 1
5 final convergedRemote: 1
5 Update: x^(k+1) = x^k - deltax^k 
5 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
5 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
5 Newton iteration  5 done5 , maximum relative shift = 2.265e-05
5 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
0 finished solveLinearSystemImpl_ 1
0 converged value: 1 7 1 let s go get convergedRemote
0 to  comm_.min(converged) 
0 did  comm_.min(converged) convergedRemote: 1
0 final convergedRemote: 1
0 Update: x^(k+1) = x^k - deltax^k 
0 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
0 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
0 Newton iteration  5 done0 , maximum relative shift = 2.265e-05
4 did  comm_.min(converged) convergedRemote: 1
4 final convergedRemote: 1
4 Update: x^(k+1) = x^k - deltax^k 
4 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
4 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
4 Newton iteration  5 done4 , maximum relative shift = 2.265e-05
4 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
0 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
0 Solve: M deltax^k = r 
0 Newton::solveLinearSystem : entering the try section 5
0 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
0 solveLinearSystemImpl_, before converged = ls.solve 
0 7 amgbackend::solve isParallel 1
4 Solve: M deltax^k = r 
4 Newton::solveLinearSystem : entering the try section 5
4 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
4 solveLinearSystemImpl_, before converged = ls.solve 
4 7 amgbackend::solve isParallel 1
3 Solve: M deltax^k = r 
3 Newton::solveLinearSystem : entering the try section 5
3 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
3 solveLinearSystemImpl_, before converged = ls.solve 
3 7 amgbackend::solve isParallel 1
6 Solve: M deltax^k = r 
6 Newton::solveLinearSystem : entering the try section 5
6 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
6 solveLinearSystemImpl_, before converged = ls.solve 
6 7 amgbackend::solve isParallel 1
2 Solve: M deltax^k = r 
2 Newton::solveLinearSystem : entering the try section 5
2 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
2 solveLinearSystemImpl_, before converged = ls.solve 
2 7 amgbackend::solve isParallel 1
1 Solve: M deltax^k = r 
1 Newton::solveLinearSystem : entering the try section 5
1 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
1 solveLinearSystemImpl_, before converged = ls.solve 
1 7 amgbackend::solve isParallel 1
5 Solve: M deltax^k = r 
5 Newton::solveLinearSystem : entering the try section 5
5 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
5 solveLinearSystemImpl_, before converged = ls.solve 
5 7 amgbackend::solve isParallel 1
0 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
0 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 7
4 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
4 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
3 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
3 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
6 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
6 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
2 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
2 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
1 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
1 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
5 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
5 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 7
6 7 before Dune::BiCGSTABSolver 
6 Dune::BiCGSTABSolver.apply(x, b, result_) 
6 BiCGSTABSolver::apply 
6 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
2 7 before Dune::BiCGSTABSolver 
2 Dune::BiCGSTABSolver.apply(x, b, result_) 
2 BiCGSTABSolver::apply 
2 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 7
1 7 before Dune::BiCGSTABSolver 
1 Dune::BiCGSTABSolver.apply(x, b, result_) 
1 BiCGSTABSolver::apply 
1 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
5 7 before Dune::BiCGSTABSolver 
5 Dune::BiCGSTABSolver.apply(x, b, result_) 
5 BiCGSTABSolver::apply 
5 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
4 7 before Dune::BiCGSTABSolver 
4 Dune::BiCGSTABSolver.apply(x, b, result_) 
4 BiCGSTABSolver::apply 
4 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
3 7 before Dune::BiCGSTABSolver 
3 Dune::BiCGSTABSolver.apply(x, b, result_) 
3 BiCGSTABSolver::apply 
3 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
Using a direct coarse solver (SuperLU)
Building hierarchy of 1 levels (inclusive coarse solver) took 0.131229 seconds.
0 7 before Dune::BiCGSTABSolver 
0 Dune::BiCGSTABSolver.apply(x, b, result_) 
0 BiCGSTABSolver::apply 
3 === BiCGSTABSolver
 Iter          Defect            Rate
    0       2.1254e-14
3  (it = 0.5; it < _maxit; it+=.5) 0.5 250
3  before p = r 
3  before  y = W^-1 * p 
3  before _prec->apply(y,p); 0.5 250
3 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
1 === BiCGSTABSolver
 Iter          Defect            Rate
    0       2.1254e-14
1  (it = 0.5; it < _maxit; it+=.5) 0.5 250
1  before p = r 
1  before  y = W^-1 * p 
1  before _prec->apply(y,p); 0.5 250
1 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
5 === BiCGSTABSolver
 Iter          Defect            Rate
    0       2.1254e-14
5  (it = 0.5; it < _maxit; it+=.5) 0.5 250
5  before p = r 
5  before  y = W^-1 * p 
5  before _prec->apply(y,p); 0.5 250
5 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
4 === BiCGSTABSolver
 Iter          Defect            Rate
    0       2.1254e-14
4  (it = 0.5; it < _maxit; it+=.5) 0.5 250
4  before p = r 
4  before  y = W^-1 * p 
4  before _prec->apply(y,p); 0.5 250
4 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
2 === BiCGSTABSolver
 Iter          Defect            Rate
    0       2.1254e-14
2  (it = 0.5; it < _maxit; it+=.5) 0.5 250
2  before p = r 
2  before  y = W^-1 * p 
2  before _prec->apply(y,p); 0.5 250
2 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
6 === BiCGSTABSolver
 Iter          Defect            Rate
    0       2.1254e-14
6  (it = 0.5; it < _maxit; it+=.5) 0.5 250
6  before p = r 
6  before  y = W^-1 * p 
6  before _prec->apply(y,p); 0.5 250
6 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
0 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
0 === BiCGSTABSolver
 Iter          Defect            Rate
    0       2.1254e-14
0  (it = 0.5; it < _maxit; it+=.5) 0.5 250
0  before p = r 
0  before  y = W^-1 * p 
0  before _prec->apply(y,p); 0.5 250
0 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 7
0 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
6 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
6 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
6  before v = A * y 
6  before  alpha = rho_new / < rt, v > 
3 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
3 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
3  before v = A * y 
1 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
5 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
5 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
5  before v = A * y 
4 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
4 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
4  before v = A * y 
4  before  alpha = rho_new / < rt, v > 
2 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
2 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
0 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
3  before  alpha = rho_new / < rt, v > 
1 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
5  before  alpha = rho_new / < rt, v > 
0 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
0  before v = A * y 
0  before  alpha = rho_new / < rt, v > 
6  before alpha = rho_new / h; 
6  before apply first correction to x 
6  before r = r - alpha*v 
6  before test stop criteria 
6 istlsolver::printOutput 
  0.5       6.5695e-33      3.09095e-19
6 === rate=9.55398e-38, T=0.141797, TIT=0.283594, IT=0.5
6 finished solveLinearSystemImpl_ 1
6 converged value: 1 7 1 let s go get convergedRemote
6 to  comm_.min(converged) 
5  before alpha = rho_new / h; 
5  before apply first correction to x 
5  before r = r - alpha*v 
5  before test stop criteria 
5 istlsolver::printOutput 
  0.5       6.5695e-33      3.09095e-19
5 === rate=9.55398e-38, T=0.141847, TIT=0.283693, IT=0.5
5 finished solveLinearSystemImpl_ 1
5 converged value: 1 7 1 let s go get convergedRemote
5 to  comm_.min(converged) 
3  before alpha = rho_new / h; 
3  before apply first correction to x 
3  before r = r - alpha*v 
3  before test stop criteria 
3 istlsolver::printOutput 
  0.5       6.5695e-33      3.09095e-19
3 === rate=9.55398e-38, T=0.141845, TIT=0.28369, IT=0.5
3 finished solveLinearSystemImpl_ 1
3 converged value: 1 7 1 let s go get convergedRemote
3 to  comm_.min(converged) 
1  before v = A * y 
1  before  alpha = rho_new / < rt, v > 
1  before alpha = rho_new / h; 
1  before apply first correction to x 
1  before r = r - alpha*v 
1  before test stop criteria 
1 istlsolver::printOutput 
  0.5       6.5695e-33      3.09095e-19
1 === rate=9.55398e-38, T=0.141847, TIT=0.283693, IT=0.5
1 finished solveLinearSystemImpl_ 1
1 converged value: 1 7 1 let s go get convergedRemote
1 to  comm_.min(converged) 
4  before alpha = rho_new / h; 
4  before apply first correction to x 
4  before r = r - alpha*v 
4  before test stop criteria 
4 istlsolver::printOutput 
  0.5       6.5695e-33      3.09095e-19
4 === rate=9.55398e-38, T=0.141797, TIT=0.283593, IT=0.5
4 finished solveLinearSystemImpl_ 1
4 converged value: 1 7 1 let s go get convergedRemote
4 to  comm_.min(converged) 
0  before alpha = rho_new / h; 
0  before apply first correction to x 
0  before r = r - alpha*v 
0  before test stop criteria 
0 istlsolver::printOutput 
  0.5       6.5695e-33      3.09095e-19
0 === rate=9.55398e-38, T=0.0180017, TIT=0.0360035, IT=0.5
2  before v = A * y 
2  before  alpha = rho_new / < rt, v > 
2  before alpha = rho_new / h; 
2  before apply first correction to x 
2  before r = r - alpha*v 
2  before test stop criteria 
2 istlsolver::printOutput 
  0.5       6.5695e-33      3.09095e-19
2 === rate=9.55398e-38, T=0.141796, TIT=0.283592, IT=0.5
2 finished solveLinearSystemImpl_ 1
2 converged value: 1 7 1 let s go get convergedRemote
2 to  comm_.min(converged) 
6 did  comm_.min(converged) convergedRemote: 1
6 final convergedRemote: 1
6 Update: x^(k+1) = x^k - deltax^k 
6 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
6 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
6 Newton iteration  6 done6 , maximum relative shift = 4.066e-06
6 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
5 did  comm_.min(converged) convergedRemote: 1
5 final convergedRemote: 1
5 Update: x^(k+1) = x^k - deltax^k 
5 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
5 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
5 Newton iteration  6 done5 , maximum relative shift = 4.066e-06
5 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
3 did  comm_.min(converged) convergedRemote: 1
3 final convergedRemote: 1
3 Update: x^(k+1) = x^k - deltax^k 
3 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
3 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
3 Newton iteration  6 done3 , maximum relative shift = 4.066e-06
3 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
1 did  comm_.min(converged) convergedRemote: 1
1 final convergedRemote: 1
1 Update: x^(k+1) = x^k - deltax^k 
1 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
1 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
1 Newton iteration  6 done1 , maximum relative shift = 4.066e-06
1 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
2 did  comm_.min(converged) convergedRemote: 1
2 final convergedRemote: 1
2 Update: x^(k+1) = x^k - deltax^k 
2 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
2 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
2 Newton iteration  6 done2 , maximum relative shift = 4.066e-06
2 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
0 finished solveLinearSystemImpl_ 1
0 converged value: 1 7 1 let s go get convergedRemote
0 to  comm_.min(converged) 
0 did  comm_.min(converged) convergedRemote: 1
0 final convergedRemote: 1
0 Update: x^(k+1) = x^k - deltax^k 
0 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
0 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
0 Newton iteration  6 done0 , maximum relative shift = 4.066e-06
0 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
4 did  comm_.min(converged) convergedRemote: 1
4 final convergedRemote: 1
4 Update: x^(k+1) = x^k - deltax^k 
4 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
4 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
4 Newton iteration  6 done4 , maximum relative shift = 4.066e-06
4 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
1 Solve: M deltax^k = r 
1 Newton::solveLinearSystem : entering the try section 6
1 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
1 solveLinearSystemImpl_, before converged = ls.solve 
1 7 amgbackend::solve isParallel 1
6 Solve: M deltax^k = r 
6 Newton::solveLinearSystem : entering the try section 6
6 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
6 solveLinearSystemImpl_, before converged = ls.solve 
6 7 amgbackend::solve isParallel 1
5 Solve: M deltax^k = r 
5 Newton::solveLinearSystem : entering the try section 6
5 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
5 solveLinearSystemImpl_, before converged = ls.solve 
5 7 amgbackend::solve isParallel 1
3 Solve: M deltax^k = r 
3 Newton::solveLinearSystem : entering the try section 6
3 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
3 solveLinearSystemImpl_, before converged = ls.solve 
3 7 amgbackend::solve isParallel 1
4 Solve: M deltax^k = r 
4 Newton::solveLinearSystem : entering the try section 6
4 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
4 solveLinearSystemImpl_, before converged = ls.solve 
4 7 amgbackend::solve isParallel 1
2 Solve: M deltax^k = r 
2 Newton::solveLinearSystem : entering the try section 6
2 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
2 solveLinearSystemImpl_, before converged = ls.solve 
2 7 amgbackend::solve isParallel 1
0 Solve: M deltax^k = r 
0 Newton::solveLinearSystem : entering the try section 6
0 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
0 solveLinearSystemImpl_, before converged = ls.solve 
0 7 amgbackend::solve isParallel 1
0 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
0 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 7
4 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
4 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
2 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
2 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
3 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
3 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
1 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
1 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
6 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
6 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
5 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
5 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 7
5 7 before Dune::BiCGSTABSolver 
5 Dune::BiCGSTABSolver.apply(x, b, result_) 
5 BiCGSTABSolver::apply 
5 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
4 7 before Dune::BiCGSTABSolver 
4 Dune::BiCGSTABSolver.apply(x, b, result_) 
4 BiCGSTABSolver::apply 
4 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
2 7 before Dune::BiCGSTABSolver 
2 Dune::BiCGSTABSolver.apply(x, b, result_) 
2 BiCGSTABSolver::apply 
2 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
3 7 before Dune::BiCGSTABSolver 
3 Dune::BiCGSTABSolver.apply(x, b, result_) 
3 BiCGSTABSolver::apply 
3 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
1 7 before Dune::BiCGSTABSolver 
1 Dune::BiCGSTABSolver.apply(x, b, result_) 
1 BiCGSTABSolver::apply 
1 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
6 7 before Dune::BiCGSTABSolver 
6 Dune::BiCGSTABSolver.apply(x, b, result_) 
6 BiCGSTABSolver::apply 
6 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 7
Using a direct coarse solver (SuperLU)
Building hierarchy of 1 levels (inclusive coarse solver) took 0.11083 seconds.
0 7 before Dune::BiCGSTABSolver 
0 Dune::BiCGSTABSolver.apply(x, b, result_) 
0 BiCGSTABSolver::apply 
0 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
0 === BiCGSTABSolver
 Iter          Defect            Rate
    0      3.81485e-15
0  (it = 0.5; it < _maxit; it+=.5) 0.5 250
0  before p = r 
0  before  y = W^-1 * p 
0  before _prec->apply(y,p); 0.5 250
0 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 7
0 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
6 === BiCGSTABSolver
 Iter          Defect            Rate
    0      3.81485e-15
6  (it = 0.5; it < _maxit; it+=.5) 0.5 250
6  before p = r 
6  before  y = W^-1 * p 
6  before _prec->apply(y,p); 0.5 250
6 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
5 === BiCGSTABSolver
 Iter          Defect            Rate
    0      3.81485e-15
5  (it = 0.5; it < _maxit; it+=.5) 0.5 250
5  before p = r 
5  before  y = W^-1 * p 
5  before _prec->apply(y,p); 0.5 250
5 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
4 === BiCGSTABSolver
 Iter          Defect            Rate
    0      3.81485e-15
4  (it = 0.5; it < _maxit; it+=.5) 0.5 250
4  before p = r 
4  before  y = W^-1 * p 
4  before _prec->apply(y,p); 0.5 250
4 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
2 === BiCGSTABSolver
 Iter          Defect            Rate
    0      3.81485e-15
2  (it = 0.5; it < _maxit; it+=.5) 0.5 250
2  before p = r 
2  before  y = W^-1 * p 
2  before _prec->apply(y,p); 0.5 250
2 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
3 === BiCGSTABSolver
 Iter          Defect            Rate
    0      3.81485e-15
3  (it = 0.5; it < _maxit; it+=.5) 0.5 250
3  before p = r 
3  before  y = W^-1 * p 
3  before _prec->apply(y,p); 0.5 250
3 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
1 === BiCGSTABSolver
 Iter          Defect            Rate
    0      3.81485e-15
1  (it = 0.5; it < _maxit; it+=.5) 0.5 250
1  before p = r 
1  before  y = W^-1 * p 
1  before _prec->apply(y,p); 0.5 250
1 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
1 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
0 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
6 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
6 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
6  before v = A * y 
6  before  alpha = rho_new / < rt, v > 
5 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
5 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
5  before v = A * y 
5  before  alpha = rho_new / < rt, v > 
4 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
4 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
4  before v = A * y 
4  before  alpha = rho_new / < rt, v > 
2 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
2 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
3 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
3 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
3  before v = A * y 
3  before  alpha = rho_new / < rt, v > 
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
0 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
2  before v = A * y 
1 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
1  before v = A * y 
1  before  alpha = rho_new / < rt, v > 
1  before alpha = rho_new / h; 
1  before apply first correction to x 
1  before r = r - alpha*v 
1  before test stop criteria 
1 istlsolver::printOutput 
  0.5      5.82076e-32      1.52581e-17
1 === rate=2.32811e-34, T=0.121844, TIT=0.243689, IT=0.5
1 finished solveLinearSystemImpl_ 1
1 converged value: 1 7 1 let s go get convergedRemote
1 to  comm_.min(converged) 
3  before alpha = rho_new / h; 
3  before apply first correction to x 
3  before r = r - alpha*v 
3  before test stop criteria 
3 istlsolver::printOutput 
  0.5      5.82076e-32      1.52581e-17
3 === rate=2.32811e-34, T=0.121848, TIT=0.243696, IT=0.5
3 finished solveLinearSystemImpl_ 1
3 converged value: 1 7 1 let s go get convergedRemote
3 to  comm_.min(converged) 
6  before alpha = rho_new / h; 
6  before apply first correction to x 
6  before r = r - alpha*v 
6  before test stop criteria 
6 istlsolver::printOutput 
  0.5      5.82076e-32      1.52581e-17
6 === rate=2.32811e-34, T=0.121838, TIT=0.243677, IT=0.5
6 finished solveLinearSystemImpl_ 1
6 converged value: 1 7 1 let s go get convergedRemote
6 to  comm_.min(converged) 
5  before alpha = rho_new / h; 
5  before apply first correction to x 
5  before r = r - alpha*v 
5  before test stop criteria 
5 istlsolver::printOutput 
  0.5      5.82076e-32      1.52581e-17
5 === rate=2.32811e-34, T=0.121845, TIT=0.243689, IT=0.5
5 finished solveLinearSystemImpl_ 1
5 converged value: 1 7 1 let s go get convergedRemote
5 to  comm_.min(converged) 
4  before alpha = rho_new / h; 
4  before apply first correction to x 
4  before r = r - alpha*v 
4  before test stop criteria 
4 istlsolver::printOutput 
  0.5      5.82076e-32      1.52581e-17
4 === rate=2.32811e-34, T=0.121835, TIT=0.24367, IT=0.5
4 finished solveLinearSystemImpl_ 1
4 converged value: 1 7 1 let s go get convergedRemote
4 to  comm_.min(converged) 
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
0  before v = A * y 
0  before  alpha = rho_new / < rt, v > 
0  before alpha = rho_new / h; 
0  before apply first correction to x 
0  before r = r - alpha*v 
0  before test stop criteria 
0 istlsolver::printOutput 
  0.5      5.82076e-32      1.52581e-17
0 === rate=2.32811e-34, T=0.0188743, TIT=0.0377485, IT=0.5
2  before  alpha = rho_new / < rt, v > 
2  before alpha = rho_new / h; 
2  before apply first correction to x 
2  before r = r - alpha*v 
2  before test stop criteria 
2 istlsolver::printOutput 
  0.5      5.82076e-32      1.52581e-17
2 === rate=2.32811e-34, T=0.121821, TIT=0.243643, IT=0.5
2 finished solveLinearSystemImpl_ 1
2 converged value: 1 7 1 let s go get convergedRemote
2 to  comm_.min(converged) 
2 did  comm_.min(converged) convergedRemote: 1
2 final convergedRemote: 1
2 Update: x^(k+1) = x^k - deltax^k 
2 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
2 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
2 Newton iteration  7 done2 , maximum relative shift = 5.678e-07
2 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
0 finished solveLinearSystemImpl_ 1
0 converged value: 1 7 1 let s go get convergedRemote
0 to  comm_.min(converged) 
0 did  comm_.min(converged) convergedRemote: 1
0 final convergedRemote: 1
0 Update: x^(k+1) = x^k - deltax^k 
0 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
0 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
0 Newton iteration  7 done0 , maximum relative shift = 5.678e-07
0 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
1 did  comm_.min(converged) convergedRemote: 1
1 final convergedRemote: 1
1 Update: x^(k+1) = x^k - deltax^k 
1 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
1 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
1 Newton iteration  7 done1 , maximum relative shift = 5.678e-07
1 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
3 did  comm_.min(converged) convergedRemote: 1
3 final convergedRemote: 1
3 Update: x^(k+1) = x^k - deltax^k 
3 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
3 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
3 Newton iteration  7 done3 , maximum relative shift = 5.678e-07
3 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
6 did  comm_.min(converged) convergedRemote: 1
6 final convergedRemote: 1
6 Update: x^(k+1) = x^k - deltax^k 
6 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
6 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
6 Newton iteration  7 done6 , maximum relative shift = 5.678e-07
6 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
5 did  comm_.min(converged) convergedRemote: 1
5 final convergedRemote: 1
5 Update: x^(k+1) = x^k - deltax^k 
5 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
5 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
5 Newton iteration  7 done5 , maximum relative shift = 5.678e-07
5 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
4 did  comm_.min(converged) convergedRemote: 1
4 final convergedRemote: 1
4 Update: x^(k+1) = x^k - deltax^k 
4 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
4 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
4 Newton iteration  7 done4 , maximum relative shift = 5.678e-07
4 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
6 Solve: M deltax^k = r 
6 Newton::solveLinearSystem : entering the try section 7
6 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
6 solveLinearSystemImpl_, before converged = ls.solve 
6 7 amgbackend::solve isParallel 1
5 Solve: M deltax^k = r 
5 Newton::solveLinearSystem : entering the try section 7
5 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
5 solveLinearSystemImpl_, before converged = ls.solve 
5 7 amgbackend::solve isParallel 1
4 Solve: M deltax^k = r 
4 Newton::solveLinearSystem : entering the try section 7
4 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
4 solveLinearSystemImpl_, before converged = ls.solve 
4 7 amgbackend::solve isParallel 1
2 Solve: M deltax^k = r 
2 Newton::solveLinearSystem : entering the try section 7
2 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
2 solveLinearSystemImpl_, before converged = ls.solve 
2 7 amgbackend::solve isParallel 1
0 Solve: M deltax^k = r 
0 Newton::solveLinearSystem : entering the try section 7
0 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
0 solveLinearSystemImpl_, before converged = ls.solve 
0 7 amgbackend::solve isParallel 1
1 Solve: M deltax^k = r 
1 Newton::solveLinearSystem : entering the try section 7
1 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
1 solveLinearSystemImpl_, before converged = ls.solve 
1 7 amgbackend::solve isParallel 1
3 Solve: M deltax^k = r 
3 Newton::solveLinearSystem : entering the try section 7
3 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
3 solveLinearSystemImpl_, before converged = ls.solve 
3 7 amgbackend::solve isParallel 1
3 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
3 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
4 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
4 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
2 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
2 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
0 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
0 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 7
1 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
1 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
5 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
5 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
6 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
6 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 7
3 7 before Dune::BiCGSTABSolver 
3 Dune::BiCGSTABSolver.apply(x, b, result_) 
3 BiCGSTABSolver::apply 
3 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
4 7 before Dune::BiCGSTABSolver 
4 Dune::BiCGSTABSolver.apply(x, b, result_) 
4 BiCGSTABSolver::apply 
4 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
2 7 before Dune::BiCGSTABSolver 
2 Dune::BiCGSTABSolver.apply(x, b, result_) 
2 BiCGSTABSolver::apply 
2 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
6 7 before Dune::BiCGSTABSolver 
6 Dune::BiCGSTABSolver.apply(x, b, result_) 
6 BiCGSTABSolver::apply 
6 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
1 7 before Dune::BiCGSTABSolver 
1 Dune::BiCGSTABSolver.apply(x, b, result_) 
1 BiCGSTABSolver::apply 
1 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
5 7 before Dune::BiCGSTABSolver 
5 Dune::BiCGSTABSolver.apply(x, b, result_) 
5 BiCGSTABSolver::apply 
5 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
Using a direct coarse solver (SuperLU)
Building hierarchy of 1 levels (inclusive coarse solver) took 0.111517 seconds.
0 7 before Dune::BiCGSTABSolver 
0 Dune::BiCGSTABSolver.apply(x, b, result_) 
0 BiCGSTABSolver::apply 
0 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
0 === BiCGSTABSolver
 Iter          Defect            Rate
    0      3.13956e-16
0  (it = 0.5; it < _maxit; it+=.5) 0.5 250
0  before p = r 
0  before  y = W^-1 * p 
0  before _prec->apply(y,p); 0.5 250
0 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 7
0 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
5 === BiCGSTABSolver
 Iter          Defect            Rate
    0      3.13956e-16
5  (it = 0.5; it < _maxit; it+=.5) 0.5 250
5  before p = r 
5  before  y = W^-1 * p 
5  before _prec->apply(y,p); 0.5 250
5 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
3 === BiCGSTABSolver
 Iter          Defect            Rate
    0      3.13956e-16
3  (it = 0.5; it < _maxit; it+=.5) 0.5 250
3  before p = r 
3  before  y = W^-1 * p 
3  before _prec->apply(y,p); 0.5 250
3 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
4 === BiCGSTABSolver
 Iter          Defect            Rate
    0      3.13956e-16
4  (it = 0.5; it < _maxit; it+=.5) 0.5 250
4  before p = r 
4  before  y = W^-1 * p 
4  before _prec->apply(y,p); 0.5 250
4 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
2 === BiCGSTABSolver
 Iter          Defect            Rate
    0      3.13956e-16
2  (it = 0.5; it < _maxit; it+=.5) 0.5 250
2  before p = r 
2  before  y = W^-1 * p 
2  before _prec->apply(y,p); 0.5 250
2 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
6 === BiCGSTABSolver
 Iter          Defect            Rate
    0      3.13956e-16
6  (it = 0.5; it < _maxit; it+=.5) 0.5 250
6  before p = r 
6  before  y = W^-1 * p 
6  before _prec->apply(y,p); 0.5 250
6 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
1 === BiCGSTABSolver
 Iter          Defect            Rate
    0      3.13956e-16
1  (it = 0.5; it < _maxit; it+=.5) 0.5 250
1  before p = r 
1  before  y = W^-1 * p 
1  before _prec->apply(y,p); 0.5 250
1 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
0 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
0 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
5 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
5 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
5  before v = A * y 
3 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
3 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
3  before v = A * y 
4 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
4 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
4  before v = A * y 
4  before  alpha = rho_new / < rt, v > 
2 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
2 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
6 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
6 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
6  before v = A * y 
6  before  alpha = rho_new / < rt, v > 
1 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
2  before v = A * y 
2  before  alpha = rho_new / < rt, v > 
2  before alpha = rho_new / h; 
2  before apply first correction to x 
2  before r = r - alpha*v 
2  before test stop criteria 
1 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
1  before v = A * y 
1  before  alpha = rho_new / < rt, v > 
1  before alpha = rho_new / h; 
1  before apply first correction to x 
1  before r = r - alpha*v 
1  before test stop criteria 
5  before  alpha = rho_new / < rt, v > 
5  before alpha = rho_new / h; 
5  before apply first correction to x 
5  before r = r - alpha*v 
5  before test stop criteria 
3  before  alpha = rho_new / < rt, v > 
3  before alpha = rho_new / h; 
3  before apply first correction to x 
3  before r = r - alpha*v 
3  before test stop criteria 
6  before alpha = rho_new / h; 
6  before apply first correction to x 
6  before r = r - alpha*v 
6  before test stop criteria 
6 istlsolver::printOutput 
  0.5      2.88976e-32      9.20433e-17
6 === rate=8.47197e-33, T=0.121939, TIT=0.243878, IT=0.5
6 finished solveLinearSystemImpl_ 1
6 converged value: 1 7 1 let s go get convergedRemote
6 to  comm_.min(converged) 
0  before v = A * y 
0  before  alpha = rho_new / < rt, v > 
0  before alpha = rho_new / h; 
0  before apply first correction to x 
0  before r = r - alpha*v 
0  before test stop criteria 
0 istlsolver::printOutput 
  0.5      2.88976e-32      9.20433e-17
0 === rate=8.47197e-33, T=0.018356, TIT=0.036712, IT=0.5
1 istlsolver::printOutput 
  0.5      2.88976e-32      9.20433e-17
1 === rate=8.47197e-33, T=0.122012, TIT=0.244023, IT=0.5
1 finished solveLinearSystemImpl_ 1
1 converged value: 1 7 1 let s go get convergedRemote
1 to  comm_.min(converged) 
5 istlsolver::printOutput 
  0.5      2.88976e-32      9.20433e-17
5 === rate=8.47197e-33, T=0.122012, TIT=0.244024, IT=0.5
5 finished solveLinearSystemImpl_ 1
5 converged value: 1 7 1 let s go get convergedRemote
5 to  comm_.min(converged) 
4  before alpha = rho_new / h; 
4  before apply first correction to x 
4  before r = r - alpha*v 
4  before test stop criteria 
4 istlsolver::printOutput 
  0.5      2.88976e-32      9.20433e-17
4 === rate=8.47197e-33, T=0.121939, TIT=0.243878, IT=0.5
4 finished solveLinearSystemImpl_ 1
4 converged value: 1 7 1 let s go get convergedRemote
4 to  comm_.min(converged) 
2 istlsolver::printOutput 
  0.5      2.88976e-32      9.20433e-17
2 === rate=8.47197e-33, T=0.121941, TIT=0.243881, IT=0.5
2 finished solveLinearSystemImpl_ 1
2 converged value: 1 7 1 let s go get convergedRemote
2 to  comm_.min(converged) 
3 istlsolver::printOutput 
  0.5      2.88976e-32      9.20433e-17
3 === rate=8.47197e-33, T=0.122013, TIT=0.244027, IT=0.5
3 finished solveLinearSystemImpl_ 1
3 converged value: 1 7 1 let s go get convergedRemote
3 to  comm_.min(converged) 
6 did  comm_.min(converged) convergedRemote: 1
6 final convergedRemote: 1
6 Update: x^(k+1) = x^k - deltax^k 
6 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
6 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
6 Newton iteration  8 done6 , maximum relative shift = 5.256e-08
6 Assemble/solve/update time: 0.352504(24.6277%)/1.07858(75.3553%)/0.000244293(0.0170675%)
6 gridVariables->advanceTimeStep
6 before, nonLinearSolver->suggestTimeStepSize, current time: 853.767 simTime 57600
6 before assembler->setPreviousSolution, ddt: 23.3333
6 nonLinearSolver->solve
6 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
0 finished solveLinearSystemImpl_ 1
0 converged value: 1 7 1 let s go get convergedRemote
0 to  comm_.min(converged) 
0 did  comm_.min(converged) convergedRemote: 1
0 final convergedRemote: 1
0 Update: x^(k+1) = x^k - deltax^k 
0 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
0 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
0 Newton iteration  8 done0 , maximum relative shift = 5.256e-08
0 Assemble/solve/update time: 0.351685(24.5823%)/1.07853(75.388%)/0.000425246(0.0297241%)
0 gridVariables->advanceTimeStep
0 before, nonLinearSolver->suggestTimeStepSize, current time: 853.767 simTime 57600
0 before assembler->setPreviousSolution, ddt: 23.3333
0 nonLinearSolver->solve
0 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
1 did  comm_.min(converged) convergedRemote: 1
1 final convergedRemote: 1
1 Update: x^(k+1) = x^k - deltax^k 
1 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
1 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
1 Newton iteration  8 done1 , maximum relative shift = 5.256e-08
1 Assemble/solve/update time: 0.352771(24.6438%)/1.07856(75.3458%)/0.000148901(0.0104019%)
1 gridVariables->advanceTimeStep
1 before, nonLinearSolver->suggestTimeStepSize, current time: 853.767 simTime 57600
1 before assembler->setPreviousSolution, ddt: 23.3333
1 nonLinearSolver->solve
1 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
5 did  comm_.min(converged) convergedRemote: 1
5 final convergedRemote: 1
5 Update: x^(k+1) = x^k - deltax^k 
5 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
5 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
5 Newton iteration  8 done5 , maximum relative shift = 5.256e-08
5 Assemble/solve/update time: 0.352782(24.6444%)/1.07856(75.3451%)/0.000149941(0.0104745%)
5 gridVariables->advanceTimeStep
5 before, nonLinearSolver->suggestTimeStepSize, current time: 853.767 simTime 57600
5 before assembler->setPreviousSolution, ddt: 23.3333
5 nonLinearSolver->solve
5 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
4 did  comm_.min(converged) convergedRemote: 1
4 final convergedRemote: 1
4 Update: x^(k+1) = x^k - deltax^k 
4 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
4 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
4 Newton iteration  8 done4 , maximum relative shift = 5.256e-08
4 Assemble/solve/update time: 0.352509(24.6257%)/1.07856(75.3463%)/0.000400116(0.0279515%)
4 gridVariables->advanceTimeStep
4 before, nonLinearSolver->suggestTimeStepSize, current time: 853.767 simTime 57600
4 before assembler->setPreviousSolution, ddt: 23.3333
4 nonLinearSolver->solve
4 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
3 did  comm_.min(converged) convergedRemote: 1
3 final convergedRemote: 1
3 Update: x^(k+1) = x^k - deltax^k 
3 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
3 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
3 Newton iteration  8 done3 , maximum relative shift = 5.256e-08
3 Assemble/solve/update time: 0.352736(24.6414%)/1.07858(75.3475%)/0.000158431(0.0110676%)
3 gridVariables->advanceTimeStep
3 before, nonLinearSolver->suggestTimeStepSize, current time: 853.767 simTime 57600
3 before assembler->setPreviousSolution, ddt: 23.3333
3 nonLinearSolver->solve
3 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
2 did  comm_.min(converged) convergedRemote: 1
2 final convergedRemote: 1
2 Update: x^(k+1) = x^k - deltax^k 
2 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
2 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
2 Newton iteration  8 done2 , maximum relative shift = 5.256e-08
2 Assemble/solve/update time: 0.352464(24.6227%)/1.07859(75.3488%)/0.000408056(0.0285063%)
2 gridVariables->advanceTimeStep
2 before, nonLinearSolver->suggestTimeStepSize, current time: 853.767 simTime 57600
2 before assembler->setPreviousSolution, ddt: 23.3333
2 nonLinearSolver->solve
2 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
6 Solve: M deltax^k = r 
6 Newton::solveLinearSystem : entering the try section 0
6to  comm_.sum(norm2) 
6did  comm_.sum(norm2) 
6 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
6 solveLinearSystemImpl_, before converged = ls.solve 
6 7 amgbackend::solve isParallel 1
0 Solve: M deltax^k = r 
0 Newton::solveLinearSystem : entering the try section 0
0to  comm_.sum(norm2) 
0did  comm_.sum(norm2) 
0 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
0 solveLinearSystemImpl_, before converged = ls.solve 
0 7 amgbackend::solve isParallel 1
1 Solve: M deltax^k = r 
1 Newton::solveLinearSystem : entering the try section 0
1to  comm_.sum(norm2) 
1did  comm_.sum(norm2) 
1 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
1 solveLinearSystemImpl_, before converged = ls.solve 
1 7 amgbackend::solve isParallel 1
5 Solve: M deltax^k = r 
5 Newton::solveLinearSystem : entering the try section 0
5to  comm_.sum(norm2) 
5did  comm_.sum(norm2) 
5 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
5 solveLinearSystemImpl_, before converged = ls.solve 
5 7 amgbackend::solve isParallel 1
2 Solve: M deltax^k = r 
2 Newton::solveLinearSystem : entering the try section 0
2to  comm_.sum(norm2) 
2did  comm_.sum(norm2) 
2 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
2 solveLinearSystemImpl_, before converged = ls.solve 
2 7 amgbackend::solve isParallel 1
3 Solve: M deltax^k = r 
3 Newton::solveLinearSystem : entering the try section 0
3to  comm_.sum(norm2) 
3did  comm_.sum(norm2) 
3 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
3 solveLinearSystemImpl_, before converged = ls.solve 
3 7 amgbackend::solve isParallel 1
4 Solve: M deltax^k = r 
4 Newton::solveLinearSystem : entering the try section 0
4to  comm_.sum(norm2) 
4did  comm_.sum(norm2) 
4 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
4 solveLinearSystemImpl_, before converged = ls.solve 
4 7 amgbackend::solve isParallel 1
4 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
4 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
2 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
2 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
3 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
3 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
5 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
5 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
6 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
6 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
0 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
0 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 7
1 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
1 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 7
4 7 before Dune::BiCGSTABSolver 
4 Dune::BiCGSTABSolver.apply(x, b, result_) 
4 BiCGSTABSolver::apply 
4 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
2 7 before Dune::BiCGSTABSolver 
2 Dune::BiCGSTABSolver.apply(x, b, result_) 
2 BiCGSTABSolver::apply 
2 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
3 7 before Dune::BiCGSTABSolver 
3 Dune::BiCGSTABSolver.apply(x, b, result_) 
3 BiCGSTABSolver::apply 
3 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
5 7 before Dune::BiCGSTABSolver 
5 Dune::BiCGSTABSolver.apply(x, b, result_) 
5 BiCGSTABSolver::apply 
5 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
6 7 before Dune::BiCGSTABSolver 
6 Dune::BiCGSTABSolver.apply(x, b, result_) 
6 BiCGSTABSolver::apply 
6 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
1 7 before Dune::BiCGSTABSolver 
1 Dune::BiCGSTABSolver.apply(x, b, result_) 
1 BiCGSTABSolver::apply 
1 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
Using a direct coarse solver (SuperLU)
Building hierarchy of 1 levels (inclusive coarse solver) took 0.11048 seconds.
0 7 before Dune::BiCGSTABSolver 
0 Dune::BiCGSTABSolver.apply(x, b, result_) 
0 BiCGSTABSolver::apply 
0 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
6 === BiCGSTABSolver
 Iter          Defect            Rate
    0      3.16024e-09
6  (it = 0.5; it < _maxit; it+=.5) 0.5 250
6  before p = r 
6  before  y = W^-1 * p 
6  before _prec->apply(y,p); 0.5 250
6 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
0 === BiCGSTABSolver
 Iter          Defect            Rate
    0      3.16024e-09
0  (it = 0.5; it < _maxit; it+=.5) 0.5 250
0  before p = r 
0  before  y = W^-1 * p 
0  before _prec->apply(y,p); 0.5 250
0 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
1 === BiCGSTABSolver
 Iter          Defect            Rate
    0      3.16024e-09
1  (it = 0.5; it < _maxit; it+=.5) 0.5 250
1  before p = r 
1  before  y = W^-1 * p 
1  before _prec->apply(y,p); 0.5 250
1 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
4 === BiCGSTABSolver
 Iter          Defect            Rate
    0      3.16024e-09
4  (it = 0.5; it < _maxit; it+=.5) 0.5 250
4  before p = r 
4  before  y = W^-1 * p 
4  before _prec->apply(y,p); 0.5 250
4 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
2 === BiCGSTABSolver
 Iter          Defect            Rate
    0      3.16024e-09
2  (it = 0.5; it < _maxit; it+=.5) 0.5 250
2  before p = r 
2  before  y = W^-1 * p 
2  before _prec->apply(y,p); 0.5 250
2 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
3 === BiCGSTABSolver
 Iter          Defect            Rate
    0      3.16024e-09
3  (it = 0.5; it < _maxit; it+=.5) 0.5 250
3  before p = r 
3  before  y = W^-1 * p 
3  before _prec->apply(y,p); 0.5 250
3 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
5 === BiCGSTABSolver
 Iter          Defect            Rate
    0      3.16024e-09
5  (it = 0.5; it < _maxit; it+=.5) 0.5 250
5  before p = r 
5  before  y = W^-1 * p 
5  before _prec->apply(y,p); 0.5 250
5 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 7
0 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
0 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
0 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
5 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
5 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
5  before v = A * y 
5  before  alpha = rho_new / < rt, v > 
6 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
6 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
6  before v = A * y 
6  before  alpha = rho_new / < rt, v > 
1 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
1 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
4 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
4 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
4  before v = A * y 
4  before  alpha = rho_new / < rt, v > 
2 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
2 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
3 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
3 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
3  before v = A * y 
3  before  alpha = rho_new / < rt, v > 
4  before alpha = rho_new / h; 
4  before apply first correction to x 
4  before r = r - alpha*v 
4  before test stop criteria 
4 istlsolver::printOutput 
  0.5       8.6185e-25      2.72717e-16
4 === rate=7.43746e-32, T=0.121428, TIT=0.242855, IT=0.5
4 finished solveLinearSystemImpl_ 1
4 converged value: 1 7 1 let s go get convergedRemote
4 to  comm_.min(converged) 
2  before v = A * y 
2  before  alpha = rho_new / < rt, v > 
2  before alpha = rho_new / h; 
2  before apply first correction to x 
2  before r = r - alpha*v 
2  before test stop criteria 
2 istlsolver::printOutput 
  0.5       8.6185e-25      2.72717e-16
2 === rate=7.43746e-32, T=0.12143, TIT=0.24286, IT=0.5
2 finished solveLinearSystemImpl_ 1
2 converged value: 1 7 1 let s go get convergedRemote
2 to  comm_.min(converged) 
3  before alpha = rho_new / h; 
3  before apply first correction to x 
3  before r = r - alpha*v 
3  before test stop criteria 
3 istlsolver::printOutput 
  0.5       8.6185e-25      2.72717e-16
3 === rate=7.43746e-32, T=0.121444, TIT=0.242888, IT=0.5
3 finished solveLinearSystemImpl_ 1
3 converged value: 1 7 1 let s go get convergedRemote
3 to  comm_.min(converged) 
0  before v = A * y 
0  before  alpha = rho_new / < rt, v > 
0  before alpha = rho_new / h; 
0  before apply first correction to x 
0  before r = r - alpha*v 
0  before test stop criteria 
0 istlsolver::printOutput 
  0.5       8.6185e-25      2.72717e-16
0 === rate=7.43746e-32, T=0.0187212, TIT=0.0374423, IT=0.5
5  before alpha = rho_new / h; 
5  before apply first correction to x 
5  before r = r - alpha*v 
5  before test stop criteria 
5 istlsolver::printOutput 
  0.5       8.6185e-25      2.72717e-16
5 === rate=7.43746e-32, T=0.121443, TIT=0.242886, IT=0.5
5 finished solveLinearSystemImpl_ 1
5 converged value: 1 7 1 let s go get convergedRemote
5 to  comm_.min(converged) 
6  before alpha = rho_new / h; 
6  before apply first correction to x 
6  before r = r - alpha*v 
6  before test stop criteria 
6 istlsolver::printOutput 
  0.5       8.6185e-25      2.72717e-16
6 === rate=7.43746e-32, T=0.121429, TIT=0.242858, IT=0.5
6 finished solveLinearSystemImpl_ 1
6 converged value: 1 7 1 let s go get convergedRemote
6 to  comm_.min(converged) 
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
1  before v = A * y 
1  before  alpha = rho_new / < rt, v > 
1  before alpha = rho_new / h; 
1  before apply first correction to x 
1  before r = r - alpha*v 
1  before test stop criteria 
1 istlsolver::printOutput 
  0.5       8.6185e-25      2.72717e-16
1 === rate=7.43746e-32, T=0.121446, TIT=0.242892, IT=0.5
1 finished solveLinearSystemImpl_ 1
1 converged value: 1 7 1 let s go get convergedRemote
1 to  comm_.min(converged) 
1 did  comm_.min(converged) convergedRemote: 1
1 final convergedRemote: 1
1 Update: x^(k+1) = x^k - deltax^k 
1 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
1 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
1 Newton iteration  1 done1 , maximum relative shift = 5.666e-02
1 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
3 did  comm_.min(converged) convergedRemote: 1
3 final convergedRemote: 1
3 Update: x^(k+1) = x^k - deltax^k 
3 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
3 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
3 Newton iteration  1 done3 , maximum relative shift = 5.666e-02
3 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
0 finished solveLinearSystemImpl_ 1
0 converged value: 1 7 1 let s go get convergedRemote
0 to  comm_.min(converged) 
0 did  comm_.min(converged) convergedRemote: 1
0 final convergedRemote: 1
0 Update: x^(k+1) = x^k - deltax^k 
0 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
0 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
0 Newton iteration  1 done0 , maximum relative shift = 5.666e-02
5 did  comm_.min(converged) convergedRemote: 1
5 final convergedRemote: 1
5 Update: x^(k+1) = x^k - deltax^k 
5 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
5 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
5 Newton iteration  1 done5 , maximum relative shift = 5.666e-02
5 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
6 did  comm_.min(converged) convergedRemote: 1
6 final convergedRemote: 1
6 Update: x^(k+1) = x^k - deltax^k 
6 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
6 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
6 Newton iteration  1 done6 , maximum relative shift = 5.666e-02
6 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
2 did  comm_.min(converged) convergedRemote: 1
2 final convergedRemote: 1
2 Update: x^(k+1) = x^k - deltax^k 
2 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
2 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
2 Newton iteration  1 done2 , maximum relative shift = 5.666e-02
2 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
4 did  comm_.min(converged) convergedRemote: 1
4 final convergedRemote: 1
4 Update: x^(k+1) = x^k - deltax^k 
4 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
4 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
4 Newton iteration  1 done4 , maximum relative shift = 5.666e-02
4 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
0 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
4 Solve: M deltax^k = r 
4 Newton::solveLinearSystem : entering the try section 1
4 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
4 solveLinearSystemImpl_, before converged = ls.solve 
4 7 amgbackend::solve isParallel 1
5 Solve: M deltax^k = r 
5 Newton::solveLinearSystem : entering the try section 1
5 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
5 solveLinearSystemImpl_, before converged = ls.solve 
5 7 amgbackend::solve isParallel 1
6 Solve: M deltax^k = r 
6 Newton::solveLinearSystem : entering the try section 1
6 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
6 solveLinearSystemImpl_, before converged = ls.solve 
6 7 amgbackend::solve isParallel 1
2 Solve: M deltax^k = r 
2 Newton::solveLinearSystem : entering the try section 1
2 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
2 solveLinearSystemImpl_, before converged = ls.solve 
2 7 amgbackend::solve isParallel 1
1 Solve: M deltax^k = r 
1 Newton::solveLinearSystem : entering the try section 1
1 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
1 solveLinearSystemImpl_, before converged = ls.solve 
1 7 amgbackend::solve isParallel 1
3 Solve: M deltax^k = r 
3 Newton::solveLinearSystem : entering the try section 1
3 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
3 solveLinearSystemImpl_, before converged = ls.solve 
3 7 amgbackend::solve isParallel 1
0 Solve: M deltax^k = r 
0 Newton::solveLinearSystem : entering the try section 1
0 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
0 solveLinearSystemImpl_, before converged = ls.solve 
0 7 amgbackend::solve isParallel 1
4 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
4 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
5 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
5 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
6 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
6 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
0 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
0 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
2 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
2 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
1 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
1 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
3 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
3 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 7
3 7 before Dune::BiCGSTABSolver 
3 Dune::BiCGSTABSolver.apply(x, b, result_) 
3 BiCGSTABSolver::apply 
3 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
2 7 before Dune::BiCGSTABSolver 
2 Dune::BiCGSTABSolver.apply(x, b, result_) 
2 BiCGSTABSolver::apply 
2 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
1 7 before Dune::BiCGSTABSolver 
1 Dune::BiCGSTABSolver.apply(x, b, result_) 
1 BiCGSTABSolver::apply 
1 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
6 7 before Dune::BiCGSTABSolver 
6 Dune::BiCGSTABSolver.apply(x, b, result_) 
6 BiCGSTABSolver::apply 
6 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
4 7 before Dune::BiCGSTABSolver 
4 Dune::BiCGSTABSolver.apply(x, b, result_) 
4 BiCGSTABSolver::apply 
4 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
5 7 before Dune::BiCGSTABSolver 
5 Dune::BiCGSTABSolver.apply(x, b, result_) 
5 BiCGSTABSolver::apply 
5 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
Using a direct coarse solver (SuperLU)
Building hierarchy of 1 levels (inclusive coarse solver) took 0.11209 seconds.
0 7 before Dune::BiCGSTABSolver 
0 Dune::BiCGSTABSolver.apply(x, b, result_) 
0 BiCGSTABSolver::apply 
0 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
3 === BiCGSTABSolver
 Iter          Defect            Rate
    0      1.58471e-11
3  (it = 0.5; it < _maxit; it+=.5) 0.5 250
3  before p = r 
3  before  y = W^-1 * p 
3  before _prec->apply(y,p); 0.5 250
3 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
2 === BiCGSTABSolver
 Iter          Defect            Rate
    0      1.58471e-11
2  (it = 0.5; it < _maxit; it+=.5) 0.5 250
2  before p = r 
2  before  y = W^-1 * p 
2  before _prec->apply(y,p); 0.5 250
2 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
1 === BiCGSTABSolver
 Iter          Defect            Rate
    0      1.58471e-11
1  (it = 0.5; it < _maxit; it+=.5) 0.5 250
1  before p = r 
1  before  y = W^-1 * p 
1  before _prec->apply(y,p); 0.5 250
1 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
6 === BiCGSTABSolver
 Iter          Defect            Rate
    0      1.58471e-11
6  (it = 0.5; it < _maxit; it+=.5) 0.5 250
6  before p = r 
6  before  y = W^-1 * p 
6  before _prec->apply(y,p); 0.5 250
6 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
4 === BiCGSTABSolver
 Iter          Defect            Rate
    0      1.58471e-11
4  (it = 0.5; it < _maxit; it+=.5) 0.5 250
4  before p = r 
4  before  y = W^-1 * p 
4  before _prec->apply(y,p); 0.5 250
4 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
0 === BiCGSTABSolver
 Iter          Defect            Rate
    0      1.58471e-11
0  (it = 0.5; it < _maxit; it+=.5) 0.5 250
0  before p = r 
0  before  y = W^-1 * p 
0  before _prec->apply(y,p); 0.5 250
0 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 7
0 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
5 === BiCGSTABSolver
 Iter          Defect            Rate
    0      1.58471e-11
5  (it = 0.5; it < _maxit; it+=.5) 0.5 250
5  before p = r 
5  before  y = W^-1 * p 
5  before _prec->apply(y,p); 0.5 250
5 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
3 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
3 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
3  before v = A * y 
3  before  alpha = rho_new / < rt, v > 
2 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
2 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
1 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
1 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
6 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
6 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
6  before v = A * y 
6  before  alpha = rho_new / < rt, v > 
5 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
5 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
5  before v = A * y 
5  before  alpha = rho_new / < rt, v > 
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
0 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
0 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
4 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
4 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
4  before v = A * y 
4  before  alpha = rho_new / < rt, v > 
2  before v = A * y 
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
0  before v = A * y 
0  before  alpha = rho_new / < rt, v > 
0  before alpha = rho_new / h; 
0  before apply first correction to x 
0  before r = r - alpha*v 
0  before test stop criteria 
0 istlsolver::printOutput 
  0.5      3.62037e-27      2.28457e-16
0 === rate=5.21924e-32, T=0.0214887, TIT=0.0429774, IT=0.5
5  before alpha = rho_new / h; 
5  before apply first correction to x 
5  before r = r - alpha*v 
5  before test stop criteria 
5 istlsolver::printOutput 
  0.5      3.62037e-27      2.28457e-16
5 === rate=5.21924e-32, T=0.125854, TIT=0.251708, IT=0.5
5 finished solveLinearSystemImpl_ 1
5 converged value: 1 7 1 let s go get convergedRemote
5 to  comm_.min(converged) 
1  before v = A * y 
1  before  alpha = rho_new / < rt, v > 
1  before alpha = rho_new / h; 
1  before apply first correction to x 
1  before r = r - alpha*v 
1  before test stop criteria 
1 istlsolver::printOutput 
  0.5      3.62037e-27      2.28457e-16
1 === rate=5.21924e-32, T=0.125856, TIT=0.251712, IT=0.5
1 finished solveLinearSystemImpl_ 1
1 converged value: 1 7 1 let s go get convergedRemote
1 to  comm_.min(converged) 
6  before alpha = rho_new / h; 
6  before apply first correction to x 
6  before r = r - alpha*v 
6  before test stop criteria 
6 istlsolver::printOutput 
  0.5      3.62037e-27      2.28457e-16
6 === rate=5.21924e-32, T=0.125788, TIT=0.251575, IT=0.5
6 finished solveLinearSystemImpl_ 1
6 converged value: 1 7 1 let s go get convergedRemote
6 to  comm_.min(converged) 
3  before alpha = rho_new / h; 
3  before apply first correction to x 
3  before r = r - alpha*v 
3  before test stop criteria 
3 istlsolver::printOutput 
  0.5      3.62037e-27      2.28457e-16
3 === rate=5.21924e-32, T=0.125853, TIT=0.251706, IT=0.5
3 finished solveLinearSystemImpl_ 1
3 converged value: 1 7 1 let s go get convergedRemote
3 to  comm_.min(converged) 
4  before alpha = rho_new / h; 
4  before apply first correction to x 
4  before r = r - alpha*v 
4  before test stop criteria 
4 istlsolver::printOutput 
  0.5      3.62037e-27      2.28457e-16
4 === rate=5.21924e-32, T=0.125789, TIT=0.251577, IT=0.5
4 finished solveLinearSystemImpl_ 1
4 converged value: 1 7 1 let s go get convergedRemote
4 to  comm_.min(converged) 
2  before  alpha = rho_new / < rt, v > 
2  before alpha = rho_new / h; 
2  before apply first correction to x 
2  before r = r - alpha*v 
2  before test stop criteria 
2 istlsolver::printOutput 
  0.5      3.62037e-27      2.28457e-16
2 === rate=5.21924e-32, T=0.125789, TIT=0.251578, IT=0.5
2 finished solveLinearSystemImpl_ 1
2 converged value: 1 7 1 let s go get convergedRemote
2 to  comm_.min(converged) 
3 did  comm_.min(converged) convergedRemote: 1
3 final convergedRemote: 1
3 Update: x^(k+1) = x^k - deltax^k 
3 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
3 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
3 Newton iteration  2 done3 , maximum relative shift = 1.657e-03
3 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
4 did  comm_.min(converged) convergedRemote: 1
4 final convergedRemote: 1
4 Update: x^(k+1) = x^k - deltax^k 
4 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
4 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
4 Newton iteration  2 done4 , maximum relative shift = 1.657e-03
4 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
2 did  comm_.min(converged) convergedRemote: 1
2 final convergedRemote: 1
2 Update: x^(k+1) = x^k - deltax^k 
2 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
2 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
2 Newton iteration  2 done2 , maximum relative shift = 1.657e-03
2 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
0 finished solveLinearSystemImpl_ 1
0 converged value: 1 7 1 let s go get convergedRemote
0 to  comm_.min(converged) 
0 did  comm_.min(converged) convergedRemote: 1
0 final convergedRemote: 1
0 Update: x^(k+1) = x^k - deltax^k 
0 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
0 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
0 Newton iteration  2 done0 , maximum relative shift = 1.657e-03
5 did  comm_.min(converged) convergedRemote: 1
5 final convergedRemote: 1
5 Update: x^(k+1) = x^k - deltax^k 
5 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
5 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
5 Newton iteration  2 done5 , maximum relative shift = 1.657e-03
5 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
1 did  comm_.min(converged) convergedRemote: 1
1 final convergedRemote: 1
1 Update: x^(k+1) = x^k - deltax^k 
1 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
1 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
1 Newton iteration  2 done1 , maximum relative shift = 1.657e-03
1 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
6 did  comm_.min(converged) convergedRemote: 1
6 final convergedRemote: 1
6 Update: x^(k+1) = x^k - deltax^k 
6 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
6 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
6 Newton iteration  2 done6 , maximum relative shift = 1.657e-03
6 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
0 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
4 Solve: M deltax^k = r 
4 Newton::solveLinearSystem : entering the try section 2
4 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
4 solveLinearSystemImpl_, before converged = ls.solve 
4 7 amgbackend::solve isParallel 1
3 Solve: M deltax^k = r 
3 Newton::solveLinearSystem : entering the try section 2
3 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
3 solveLinearSystemImpl_, before converged = ls.solve 
3 7 amgbackend::solve isParallel 1
0 Solve: M deltax^k = r 
0 Newton::solveLinearSystem : entering the try section 2
0 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
0 solveLinearSystemImpl_, before converged = ls.solve 
0 7 amgbackend::solve isParallel 1
2 Solve: M deltax^k = r 
2 Newton::solveLinearSystem : entering the try section 2
2 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
2 solveLinearSystemImpl_, before converged = ls.solve 
2 7 amgbackend::solve isParallel 1
6 Solve: M deltax^k = r 
6 Newton::solveLinearSystem : entering the try section 2
6 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
6 solveLinearSystemImpl_, before converged = ls.solve 
6 7 amgbackend::solve isParallel 1
5 Solve: M deltax^k = r 
5 Newton::solveLinearSystem : entering the try section 2
5 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
5 solveLinearSystemImpl_, before converged = ls.solve 
5 7 amgbackend::solve isParallel 1
1 Solve: M deltax^k = r 
1 Newton::solveLinearSystem : entering the try section 2
1 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
1 solveLinearSystemImpl_, before converged = ls.solve 
1 7 amgbackend::solve isParallel 1
1 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
1 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
0 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
0 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 7
2 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
2 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
6 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
6 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
5 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
5 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
3 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
3 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
4 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
4 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 7
4 7 before Dune::BiCGSTABSolver 
4 Dune::BiCGSTABSolver.apply(x, b, result_) 
4 BiCGSTABSolver::apply 
4 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
2 7 before Dune::BiCGSTABSolver 
2 Dune::BiCGSTABSolver.apply(x, b, result_) 
2 BiCGSTABSolver::apply 
2 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
6 7 before Dune::BiCGSTABSolver 
6 Dune::BiCGSTABSolver.apply(x, b, result_) 
6 BiCGSTABSolver::apply 
6 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
5 7 before Dune::BiCGSTABSolver 
5 Dune::BiCGSTABSolver.apply(x, b, result_) 
5 BiCGSTABSolver::apply 
5 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
3 7 before Dune::BiCGSTABSolver 
3 Dune::BiCGSTABSolver.apply(x, b, result_) 
3 BiCGSTABSolver::apply 
3 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 7
1 7 before Dune::BiCGSTABSolver 
1 Dune::BiCGSTABSolver.apply(x, b, result_) 
1 BiCGSTABSolver::apply 
1 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
Using a direct coarse solver (SuperLU)
Building hierarchy of 1 levels (inclusive coarse solver) took 0.110172 seconds.
0 7 before Dune::BiCGSTABSolver 
0 Dune::BiCGSTABSolver.apply(x, b, result_) 
0 BiCGSTABSolver::apply 
0 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
4 === BiCGSTABSolver
 Iter          Defect            Rate
    0      1.20002e-12
4  (it = 0.5; it < _maxit; it+=.5) 0.5 250
4  before p = r 
4  before  y = W^-1 * p 
4  before _prec->apply(y,p); 0.5 250
4 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
2 === BiCGSTABSolver
 Iter          Defect            Rate
    0      1.20002e-12
2  (it = 0.5; it < _maxit; it+=.5) 0.5 250
2  before p = r 
2  before  y = W^-1 * p 
2  before _prec->apply(y,p); 0.5 250
2 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
6 === BiCGSTABSolver
 Iter          Defect            Rate
    0      1.20002e-12
6  (it = 0.5; it < _maxit; it+=.5) 0.5 250
6  before p = r 
6  before  y = W^-1 * p 
6  before _prec->apply(y,p); 0.5 250
6 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
5 === BiCGSTABSolver
 Iter          Defect            Rate
    0      1.20002e-12
5  (it = 0.5; it < _maxit; it+=.5) 0.5 250
5  before p = r 
5  before  y = W^-1 * p 
5  before _prec->apply(y,p); 0.5 250
5 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
0 === BiCGSTABSolver
 Iter          Defect            Rate
    0      1.20002e-12
0  (it = 0.5; it < _maxit; it+=.5) 0.5 250
0  before p = r 
0  before  y = W^-1 * p 
0  before _prec->apply(y,p); 0.5 250
0 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 7
1 === BiCGSTABSolver
 Iter          Defect            Rate
    0      1.20002e-12
1  (it = 0.5; it < _maxit; it+=.5) 0.5 250
1  before p = r 
1  before  y = W^-1 * p 
1  before _prec->apply(y,p); 0.5 250
1 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
3 === BiCGSTABSolver
 Iter          Defect            Rate
    0      1.20002e-12
3  (it = 0.5; it < _maxit; it+=.5) 0.5 250
3  before p = r 
3  before  y = W^-1 * p 
3  before _prec->apply(y,p); 0.5 250
3 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
0 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
0 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
3 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
3 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
3  before v = A * y 
1 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
5 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
5 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
5  before v = A * y 
4 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
4 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
4  before v = A * y 
4  before  alpha = rho_new / < rt, v > 
2 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
2 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
6 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
6 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
6  before v = A * y 
6  before  alpha = rho_new / < rt, v > 
0 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
0  before v = A * y 
0  before  alpha = rho_new / < rt, v > 
3  before  alpha = rho_new / < rt, v > 
1 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
5  before  alpha = rho_new / < rt, v > 
2  before v = A * y 
2  before  alpha = rho_new / < rt, v > 
6  before alpha = rho_new / h; 
6  before apply first correction to x 
6  before r = r - alpha*v 
6  before test stop criteria 
6 istlsolver::printOutput 
  0.5      5.05185e-29      4.20982e-17
6 === rate=1.77226e-33, T=0.121205, TIT=0.24241, IT=0.5
6 finished solveLinearSystemImpl_ 1
6 converged value: 1 7 1 let s go get convergedRemote
6 to  comm_.min(converged) 
0  before alpha = rho_new / h; 
0  before apply first correction to x 
0  before r = r - alpha*v 
0  before test stop criteria 
0 istlsolver::printOutput 
  0.5      5.05185e-29      4.20982e-17
0 === rate=1.77226e-33, T=0.018514, TIT=0.0370281, IT=0.5
3  before alpha = rho_new / h; 
3  before apply first correction to x 
3  before r = r - alpha*v 
3  before test stop criteria 
3 istlsolver::printOutput 
  0.5      5.05185e-29      4.20982e-17
3 === rate=1.77226e-33, T=0.121241, TIT=0.242482, IT=0.5
3 finished solveLinearSystemImpl_ 1
3 converged value: 1 7 1 let s go get convergedRemote
3 to  comm_.min(converged) 
1  before v = A * y 
1  before  alpha = rho_new / < rt, v > 
1  before alpha = rho_new / h; 
1  before apply first correction to x 
1  before r = r - alpha*v 
1  before test stop criteria 
1 istlsolver::printOutput 
  0.5      5.05185e-29      4.20982e-17
1 === rate=1.77226e-33, T=0.121244, TIT=0.242489, IT=0.5
1 finished solveLinearSystemImpl_ 1
1 converged value: 1 7 1 let s go get convergedRemote
1 to  comm_.min(converged) 
4  before alpha = rho_new / h; 
4  before apply first correction to x 
4  before r = r - alpha*v 
4  before test stop criteria 
4 istlsolver::printOutput 
  0.5      5.05185e-29      4.20982e-17
4 === rate=1.77226e-33, T=0.121205, TIT=0.24241, IT=0.5
4 finished solveLinearSystemImpl_ 1
4 converged value: 1 7 1 let s go get convergedRemote
4 to  comm_.min(converged) 
5  before alpha = rho_new / h; 
5  before apply first correction to x 
5  before r = r - alpha*v 
5  before test stop criteria 
5 istlsolver::printOutput 
  0.5      5.05185e-29      4.20982e-17
5 === rate=1.77226e-33, T=0.12124, TIT=0.242479, IT=0.5
5 finished solveLinearSystemImpl_ 1
5 converged value: 1 7 1 let s go get convergedRemote
5 to  comm_.min(converged) 
2  before alpha = rho_new / h; 
2  before apply first correction to x 
2  before r = r - alpha*v 
2  before test stop criteria 
2 istlsolver::printOutput 
  0.5      5.05185e-29      4.20982e-17
2 === rate=1.77226e-33, T=0.121205, TIT=0.242411, IT=0.5
2 finished solveLinearSystemImpl_ 1
2 converged value: 1 7 1 let s go get convergedRemote
2 to  comm_.min(converged) 
2 did  comm_.min(converged) convergedRemote: 1
2 final convergedRemote: 1
2 Update: x^(k+1) = x^k - deltax^k 
2 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
2 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
2 Newton iteration  3 done2 , maximum relative shift = 9.807e-05
2 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
5 did  comm_.min(converged) convergedRemote: 1
5 final convergedRemote: 1
5 Update: x^(k+1) = x^k - deltax^k 
5 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
5 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
5 Newton iteration  3 done5 , maximum relative shift = 9.807e-05
5 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
6 did  comm_.min(converged) convergedRemote: 1
6 final convergedRemote: 1
6 Update: x^(k+1) = x^k - deltax^k 
6 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
6 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
6 Newton iteration  3 done6 , maximum relative shift = 9.807e-05
6 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
0 finished solveLinearSystemImpl_ 1
0 converged value: 1 7 1 let s go get convergedRemote
0 to  comm_.min(converged) 
0 did  comm_.min(converged) convergedRemote: 1
0 final convergedRemote: 1
0 Update: x^(k+1) = x^k - deltax^k 
0 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
0 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
0 Newton iteration  3 done0 , maximum relative shift = 9.807e-05
0 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
3 did  comm_.min(converged) convergedRemote: 1
3 final convergedRemote: 1
3 Update: x^(k+1) = x^k - deltax^k 
3 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
3 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
3 Newton iteration  3 done3 , maximum relative shift = 9.807e-05
3 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
1 did  comm_.min(converged) convergedRemote: 1
1 final convergedRemote: 1
1 Update: x^(k+1) = x^k - deltax^k 
1 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
1 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
1 Newton iteration  3 done1 , maximum relative shift = 9.807e-05
1 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
4 did  comm_.min(converged) convergedRemote: 1
4 final convergedRemote: 1
4 Update: x^(k+1) = x^k - deltax^k 
4 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
4 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
4 Newton iteration  3 done4 , maximum relative shift = 9.807e-05
4 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
4 Solve: M deltax^k = r 
4 Newton::solveLinearSystem : entering the try section 3
4 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
4 solveLinearSystemImpl_, before converged = ls.solve 
4 7 amgbackend::solve isParallel 1
2 Solve: M deltax^k = r 
2 Newton::solveLinearSystem : entering the try section 3
2 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
2 solveLinearSystemImpl_, before converged = ls.solve 
2 7 amgbackend::solve isParallel 1
5 Solve: M deltax^k = r 
5 Newton::solveLinearSystem : entering the try section 3
5 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
5 solveLinearSystemImpl_, before converged = ls.solve 
5 7 amgbackend::solve isParallel 1
6 Solve: M deltax^k = r 
6 Newton::solveLinearSystem : entering the try section 3
6 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
6 solveLinearSystemImpl_, before converged = ls.solve 
6 7 amgbackend::solve isParallel 1
0 Solve: M deltax^k = r 
0 Newton::solveLinearSystem : entering the try section 3
0 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
0 solveLinearSystemImpl_, before converged = ls.solve 
0 7 amgbackend::solve isParallel 1
3 Solve: M deltax^k = r 
3 Newton::solveLinearSystem : entering the try section 3
3 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
3 solveLinearSystemImpl_, before converged = ls.solve 
3 7 amgbackend::solve isParallel 1
1 Solve: M deltax^k = r 
1 Newton::solveLinearSystem : entering the try section 3
1 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
1 solveLinearSystemImpl_, before converged = ls.solve 
1 7 amgbackend::solve isParallel 1
4 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
4 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
2 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
2 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
5 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
5 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
6 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
6 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
0 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
0 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 7
3 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
3 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
1 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
1 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 7
3 7 before Dune::BiCGSTABSolver 
3 Dune::BiCGSTABSolver.apply(x, b, result_) 
3 BiCGSTABSolver::apply 
3 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
4 7 before Dune::BiCGSTABSolver 
4 Dune::BiCGSTABSolver.apply(x, b, result_) 
4 BiCGSTABSolver::apply 
4 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
2 7 before Dune::BiCGSTABSolver 
2 Dune::BiCGSTABSolver.apply(x, b, result_) 
2 BiCGSTABSolver::apply 
2 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
5 7 before Dune::BiCGSTABSolver 
5 Dune::BiCGSTABSolver.apply(x, b, result_) 
5 BiCGSTABSolver::apply 
5 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
6 7 before Dune::BiCGSTABSolver 
6 Dune::BiCGSTABSolver.apply(x, b, result_) 
6 BiCGSTABSolver::apply 
6 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
1 7 before Dune::BiCGSTABSolver 
1 Dune::BiCGSTABSolver.apply(x, b, result_) 
1 BiCGSTABSolver::apply 
1 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
Using a direct coarse solver (SuperLU)
Building hierarchy of 1 levels (inclusive coarse solver) took 0.115958 seconds.
0 7 before Dune::BiCGSTABSolver 
0 Dune::BiCGSTABSolver.apply(x, b, result_) 
0 BiCGSTABSolver::apply 
0 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
1 === BiCGSTABSolver
 Iter          Defect            Rate
    0      6.08104e-14
1  (it = 0.5; it < _maxit; it+=.5) 0.5 250
1  before p = r 
1  before  y = W^-1 * p 
1  before _prec->apply(y,p); 0.5 250
1 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
3 === BiCGSTABSolver
 Iter          Defect            Rate
    0      6.08104e-14
3  (it = 0.5; it < _maxit; it+=.5) 0.5 250
3  before p = r 
3  before  y = W^-1 * p 
3  before _prec->apply(y,p); 0.5 250
3 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
4 === BiCGSTABSolver
 Iter          Defect            Rate
    0      6.08104e-14
4  (it = 0.5; it < _maxit; it+=.5) 0.5 250
4  before p = r 
4  before  y = W^-1 * p 
4  before _prec->apply(y,p); 0.5 250
4 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
2 === BiCGSTABSolver
 Iter          Defect            Rate
    0      6.08104e-14
2  (it = 0.5; it < _maxit; it+=.5) 0.5 250
2  before p = r 
2  before  y = W^-1 * p 
2  before _prec->apply(y,p); 0.5 250
2 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
5 === BiCGSTABSolver
 Iter          Defect            Rate
    0      6.08104e-14
5  (it = 0.5; it < _maxit; it+=.5) 0.5 250
5  before p = r 
5  before  y = W^-1 * p 
5  before _prec->apply(y,p); 0.5 250
5 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
6 === BiCGSTABSolver
 Iter          Defect            Rate
    0      6.08104e-14
6  (it = 0.5; it < _maxit; it+=.5) 0.5 250
6  before p = r 
6  before  y = W^-1 * p 
6  before _prec->apply(y,p); 0.5 250
6 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
0 === BiCGSTABSolver
 Iter          Defect            Rate
    0      6.08104e-14
0  (it = 0.5; it < _maxit; it+=.5) 0.5 250
0  before p = r 
0  before  y = W^-1 * p 
0  before _prec->apply(y,p); 0.5 250
0 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 7
0 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
0 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
6 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
6 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
6  before v = A * y 
6  before  alpha = rho_new / < rt, v > 
1 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
3 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
3 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
3  before v = A * y 
4 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
4 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
4  before v = A * y 
4  before  alpha = rho_new / < rt, v > 
2 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
2 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
5 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
5 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
5  before v = A * y 
1 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
1  before v = A * y 
3  before  alpha = rho_new / < rt, v > 
2  before v = A * y 
2  before  alpha = rho_new / < rt, v > 
5  before  alpha = rho_new / < rt, v > 
0 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
0  before v = A * y 
0  before  alpha = rho_new / < rt, v > 
1  before  alpha = rho_new / < rt, v > 
1  before alpha = rho_new / h; 
1  before apply first correction to x 
1  before r = r - alpha*v 
1  before test stop criteria 
1 istlsolver::printOutput 
  0.5      3.56786e-30      5.86719e-17
1 === rate=3.44239e-33, T=0.126682, TIT=0.253364, IT=0.5
1 finished solveLinearSystemImpl_ 1
1 converged value: 1 7 1 let s go get convergedRemote
1 to  comm_.min(converged) 
1 did  comm_.min(converged) convergedRemote: 1
1 final convergedRemote: 1
1 Update: x^(k+1) = x^k - deltax^k 
1 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
1 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
1 Newton iteration  4 done1 , maximum relative shift = 8.649e-06
1 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
4  before alpha = rho_new / h; 
4  before apply first correction to x 
4  before r = r - alpha*v 
4  before test stop criteria 
4 istlsolver::printOutput 
  0.5      3.56786e-30      5.86719e-17
4 === rate=3.44239e-33, T=0.126621, TIT=0.253241, IT=0.5
4 finished solveLinearSystemImpl_ 1
4 converged value: 1 7 1 let s go get convergedRemote
4 to  comm_.min(converged) 
4 did  comm_.min(converged) convergedRemote: 1
4 final convergedRemote: 1
4 Update: x^(k+1) = x^k - deltax^k 
4 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
4 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
4 Newton iteration  4 done4 , maximum relative shift = 8.649e-06
3  before alpha = rho_new / h; 
3  before apply first correction to x 
3  before r = r - alpha*v 
3  before test stop criteria 
3 istlsolver::printOutput 
  0.5      3.56786e-30      5.86719e-17
3 === rate=3.44239e-33, T=0.126679, TIT=0.253358, IT=0.5
3 finished solveLinearSystemImpl_ 1
3 converged value: 1 7 1 let s go get convergedRemote
3 to  comm_.min(converged) 
3 did  comm_.min(converged) convergedRemote: 1
3 final convergedRemote: 1
3 Update: x^(k+1) = x^k - deltax^k 
3 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
3 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
3 Newton iteration  4 done3 , maximum relative shift = 8.649e-06
3 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
0  before alpha = rho_new / h; 
0  before apply first correction to x 
0  before r = r - alpha*v 
0  before test stop criteria 
0 istlsolver::printOutput 
  0.5      3.56786e-30      5.86719e-17
0 === rate=3.44239e-33, T=0.0181669, TIT=0.0363339, IT=0.5
0 finished solveLinearSystemImpl_ 1
0 converged value: 1 7 1 let s go get convergedRemote
0 to  comm_.min(converged) 
0 did  comm_.min(converged) convergedRemote: 1
0 final convergedRemote: 1
0 Update: x^(k+1) = x^k - deltax^k 
0 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
0 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
5  before alpha = rho_new / h; 
5  before apply first correction to x 
5  before r = r - alpha*v 
5  before test stop criteria 
5 istlsolver::printOutput 
  0.5      3.56786e-30      5.86719e-17
5 === rate=3.44239e-33, T=0.126681, TIT=0.253362, IT=0.5
5 finished solveLinearSystemImpl_ 1
5 converged value: 1 7 1 let s go get convergedRemote
5 to  comm_.min(converged) 
5 did  comm_.min(converged) convergedRemote: 1
5 final convergedRemote: 1
5 Update: x^(k+1) = x^k - deltax^k 
5 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
5 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
5 Newton iteration  4 done5 , maximum relative shift = 8.649e-06
5 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
6  before alpha = rho_new / h; 
6  before apply first correction to x 
6  before r = r - alpha*v 
6  before test stop criteria 
6 istlsolver::printOutput 
  0.5      3.56786e-30      5.86719e-17
6 === rate=3.44239e-33, T=0.126621, TIT=0.253242, IT=0.5
6 finished solveLinearSystemImpl_ 1
6 converged value: 1 7 1 let s go get convergedRemote
6 to  comm_.min(converged) 
6 did  comm_.min(converged) convergedRemote: 1
6 final convergedRemote: 1
6 Update: x^(k+1) = x^k - deltax^k 
6 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
6 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
6 Newton iteration  4 done6 , maximum relative shift = 8.649e-06
6 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
2  before alpha = rho_new / h; 
2  before apply first correction to x 
2  before r = r - alpha*v 
2  before test stop criteria 
2 istlsolver::printOutput 
  0.5      3.56786e-30      5.86719e-17
2 === rate=3.44239e-33, T=0.12662, TIT=0.25324, IT=0.5
2 finished solveLinearSystemImpl_ 1
2 converged value: 1 7 1 let s go get convergedRemote
2 to  comm_.min(converged) 
2 did  comm_.min(converged) convergedRemote: 1
2 final convergedRemote: 1
2 Update: x^(k+1) = x^k - deltax^k 
2 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
2 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
2 Newton iteration  4 done2 , maximum relative shift = 8.649e-06
2 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
4 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
0 Newton iteration  4 done0 , maximum relative shift = 8.649e-06
0 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
2 Solve: M deltax^k = r 
2 Newton::solveLinearSystem : entering the try section 4
2 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
2 solveLinearSystemImpl_, before converged = ls.solve 
2 7 amgbackend::solve isParallel 1
0 Solve: M deltax^k = r 
0 Newton::solveLinearSystem : entering the try section 4
0 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
0 solveLinearSystemImpl_, before converged = ls.solve 
0 7 amgbackend::solve isParallel 1
4 Solve: M deltax^k = r 
4 Newton::solveLinearSystem : entering the try section 4
4 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
4 solveLinearSystemImpl_, before converged = ls.solve 
4 7 amgbackend::solve isParallel 1
5 Solve: M deltax^k = r 
5 Newton::solveLinearSystem : entering the try section 4
5 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
5 solveLinearSystemImpl_, before converged = ls.solve 
5 7 amgbackend::solve isParallel 1
6 Solve: M deltax^k = r 
6 Newton::solveLinearSystem : entering the try section 4
6 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
6 solveLinearSystemImpl_, before converged = ls.solve 
6 7 amgbackend::solve isParallel 1
3 Solve: M deltax^k = r 
3 Newton::solveLinearSystem : entering the try section 4
3 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
3 solveLinearSystemImpl_, before converged = ls.solve 
3 7 amgbackend::solve isParallel 1
1 Solve: M deltax^k = r 
1 Newton::solveLinearSystem : entering the try section 4
1 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
1 solveLinearSystemImpl_, before converged = ls.solve 
1 7 amgbackend::solve isParallel 1
6 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
6 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
3 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
3 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
2 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
2 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
1 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
1 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
0 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
0 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 7
4 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
4 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
5 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
5 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 7
2 7 before Dune::BiCGSTABSolver 
2 Dune::BiCGSTABSolver.apply(x, b, result_) 
2 BiCGSTABSolver::apply 
2 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
6 7 before Dune::BiCGSTABSolver 
6 Dune::BiCGSTABSolver.apply(x, b, result_) 
6 BiCGSTABSolver::apply 
6 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
3 7 before Dune::BiCGSTABSolver 
3 Dune::BiCGSTABSolver.apply(x, b, result_) 
3 BiCGSTABSolver::apply 
3 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 7
1 7 before Dune::BiCGSTABSolver 
1 Dune::BiCGSTABSolver.apply(x, b, result_) 
1 BiCGSTABSolver::apply 
1 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
5 7 before Dune::BiCGSTABSolver 
5 Dune::BiCGSTABSolver.apply(x, b, result_) 
5 BiCGSTABSolver::apply 
5 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
4 7 before Dune::BiCGSTABSolver 
4 Dune::BiCGSTABSolver.apply(x, b, result_) 
4 BiCGSTABSolver::apply 
4 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
Using a direct coarse solver (SuperLU)
Building hierarchy of 1 levels (inclusive coarse solver) took 0.110432 seconds.
0 7 before Dune::BiCGSTABSolver 
0 Dune::BiCGSTABSolver.apply(x, b, result_) 
0 BiCGSTABSolver::apply 
0 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
0 === BiCGSTABSolver
 Iter          Defect            Rate
    0      2.34935e-15
0  (it = 0.5; it < _maxit; it+=.5) 0.5 250
5 === BiCGSTABSolver
 Iter          Defect            Rate
    0      2.34935e-15
5  (it = 0.5; it < _maxit; it+=.5) 0.5 250
5  before p = r 
5  before  y = W^-1 * p 
5  before _prec->apply(y,p); 0.5 250
5 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
3 === BiCGSTABSolver
 Iter          Defect            Rate
    0      2.34935e-15
3  (it = 0.5; it < _maxit; it+=.5) 0.5 250
3  before p = r 
3  before  y = W^-1 * p 
3  before _prec->apply(y,p); 0.5 250
3 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
1 === BiCGSTABSolver
 Iter          Defect            Rate
    0      2.34935e-15
1  (it = 0.5; it < _maxit; it+=.5) 0.5 250
1  before p = r 
1  before  y = W^-1 * p 
1  before _prec->apply(y,p); 0.5 250
1 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
4 === BiCGSTABSolver
 Iter          Defect            Rate
    0      2.34935e-15
4  (it = 0.5; it < _maxit; it+=.5) 0.5 250
4  before p = r 
4  before  y = W^-1 * p 
4  before _prec->apply(y,p); 0.5 250
4 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
2 === BiCGSTABSolver
 Iter          Defect            Rate
    0      2.34935e-15
2  (it = 0.5; it < _maxit; it+=.5) 0.5 250
2  before p = r 
2  before  y = W^-1 * p 
2  before _prec->apply(y,p); 0.5 250
2 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
6 === BiCGSTABSolver
 Iter          Defect            Rate
    0      2.34935e-15
6  (it = 0.5; it < _maxit; it+=.5) 0.5 250
6  before p = r 
6  before  y = W^-1 * p 
6  before _prec->apply(y,p); 0.5 250
6 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
0  before p = r 
0  before  y = W^-1 * p 
0  before _prec->apply(y,p); 0.5 250
0 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 7
0 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
0 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
6 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
6 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
6  before v = A * y 
5 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
5 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
5  before v = A * y 
3 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
3 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
3  before v = A * y 
1 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
4 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
4 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
4  before v = A * y 
2 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
2 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
0 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
0  before v = A * y 
0  before  alpha = rho_new / < rt, v > 
6  before  alpha = rho_new / < rt, v > 
5  before  alpha = rho_new / < rt, v > 
3  before  alpha = rho_new / < rt, v > 
1 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
1  before v = A * y 
1  before  alpha = rho_new / < rt, v > 
4  before  alpha = rho_new / < rt, v > 
2  before v = A * y 
2  before  alpha = rho_new / < rt, v > 
4  before alpha = rho_new / h; 
4  before apply first correction to x 
4  before r = r - alpha*v 
4  before test stop criteria 
4 istlsolver::printOutput 
  0.5      1.34713e-31      5.73407e-17
4 === rate=3.28796e-33, T=0.120971, TIT=0.241943, IT=0.5
4 finished solveLinearSystemImpl_ 1
4 converged value: 1 7 1 let s go get convergedRemote
4 to  comm_.min(converged) 
0  before alpha = rho_new / h; 
0  before apply first correction to x 
0  before r = r - alpha*v 
0  before test stop criteria 
0 istlsolver::printOutput 
  0.5      1.34713e-31      5.73407e-17
0 === rate=3.28796e-33, T=0.0182865, TIT=0.036573, IT=0.5
0 finished solveLinearSystemImpl_ 1
0 converged value: 1 7 1 let s go get convergedRemote
0 to  comm_.min(converged) 
0 did  comm_.min(converged) convergedRemote: 1
0 final convergedRemote: 1
0 Update: x^(k+1) = x^k - deltax^k 
0 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
6  before alpha = rho_new / h; 
6  before apply first correction to x 
6  before r = r - alpha*v 
6  before test stop criteria 
6 istlsolver::printOutput 
  0.5      1.34713e-31      5.73407e-17
6 === rate=3.28796e-33, T=0.120971, TIT=0.241941, IT=0.5
6 finished solveLinearSystemImpl_ 1
6 converged value: 1 7 1 let s go get convergedRemote
6 to  comm_.min(converged) 
6 did  comm_.min(converged) convergedRemote: 1
6 final convergedRemote: 1
6 Update: x^(k+1) = x^k - deltax^k 
6 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
6 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
5  before alpha = rho_new / h; 
5  before apply first correction to x 
5  before r = r - alpha*v 
5  before test stop criteria 
5 istlsolver::printOutput 
  0.5      1.34713e-31      5.73407e-17
5 === rate=3.28796e-33, T=0.120968, TIT=0.241936, IT=0.5
5 finished solveLinearSystemImpl_ 1
5 converged value: 1 7 1 let s go get convergedRemote
5 to  comm_.min(converged) 
3  before alpha = rho_new / h; 
3  before apply first correction to x 
3  before r = r - alpha*v 
3  before test stop criteria 
3 istlsolver::printOutput 
  0.5      1.34713e-31      5.73407e-17
3 === rate=3.28796e-33, T=0.12097, TIT=0.24194, IT=0.5
3 finished solveLinearSystemImpl_ 1
3 converged value: 1 7 1 let s go get convergedRemote
3 to  comm_.min(converged) 
1  before alpha = rho_new / h; 
1  before apply first correction to x 
1  before r = r - alpha*v 
1  before test stop criteria 
1 istlsolver::printOutput 
  0.5      1.34713e-31      5.73407e-17
1 === rate=3.28796e-33, T=0.120967, TIT=0.241935, IT=0.5
1 finished solveLinearSystemImpl_ 1
1 converged value: 1 7 1 let s go get convergedRemote
1 to  comm_.min(converged) 
2  before alpha = rho_new / h; 
2  before apply first correction to x 
2  before r = r - alpha*v 
2  before test stop criteria 
2 istlsolver::printOutput 
  0.5      1.34713e-31      5.73407e-17
2 === rate=3.28796e-33, T=0.120972, TIT=0.241944, IT=0.5
2 finished solveLinearSystemImpl_ 1
2 converged value: 1 7 1 let s go get convergedRemote
2 to  comm_.min(converged) 
2 did  comm_.min(converged) convergedRemote: 1
2 final convergedRemote: 1
2 Update: x^(k+1) = x^k - deltax^k 
2 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
2 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
2 Newton iteration  5 done2 , maximum relative shift = 3.877e-07
2 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
1 did  comm_.min(converged) convergedRemote: 1
1 final convergedRemote: 1
1 Update: x^(k+1) = x^k - deltax^k 
1 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
1 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
1 Newton iteration  5 done1 , maximum relative shift = 3.877e-07
1 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
4 did  comm_.min(converged) convergedRemote: 1
4 final convergedRemote: 1
4 Update: x^(k+1) = x^k - deltax^k 
4 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
4 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
4 Newton iteration  5 done4 , maximum relative shift = 3.877e-07
4 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
0 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
0 Newton iteration  5 done0 , maximum relative shift = 3.877e-07
0 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
6 Newton iteration  5 done6 , maximum relative shift = 3.877e-07
6 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
5 did  comm_.min(converged) convergedRemote: 1
5 final convergedRemote: 1
5 Update: x^(k+1) = x^k - deltax^k 
5 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
5 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
5 Newton iteration  5 done5 , maximum relative shift = 3.877e-07
5 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
3 did  comm_.min(converged) convergedRemote: 1
3 final convergedRemote: 1
3 Update: x^(k+1) = x^k - deltax^k 
3 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
3 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
3 Newton iteration  5 done3 , maximum relative shift = 3.877e-07
3 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
3 Solve: M deltax^k = r 
3 Newton::solveLinearSystem : entering the try section 5
3 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
3 solveLinearSystemImpl_, before converged = ls.solve 
3 7 amgbackend::solve isParallel 1
2 Solve: M deltax^k = r 
2 Newton::solveLinearSystem : entering the try section 5
2 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
2 solveLinearSystemImpl_, before converged = ls.solve 
2 7 amgbackend::solve isParallel 1
1 Solve: M deltax^k = r 
1 Newton::solveLinearSystem : entering the try section 5
1 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
1 solveLinearSystemImpl_, before converged = ls.solve 
1 7 amgbackend::solve isParallel 1
4 Solve: M deltax^k = r 
4 Newton::solveLinearSystem : entering the try section 5
4 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
4 solveLinearSystemImpl_, before converged = ls.solve 
4 7 amgbackend::solve isParallel 1
0 Solve: M deltax^k = r 
0 Newton::solveLinearSystem : entering the try section 5
0 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
0 solveLinearSystemImpl_, before converged = ls.solve 
0 7 amgbackend::solve isParallel 1
6 Solve: M deltax^k = r 
6 Newton::solveLinearSystem : entering the try section 5
6 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
6 solveLinearSystemImpl_, before converged = ls.solve 
6 7 amgbackend::solve isParallel 1
5 Solve: M deltax^k = r 
5 Newton::solveLinearSystem : entering the try section 5
5 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
5 solveLinearSystemImpl_, before converged = ls.solve 
5 7 amgbackend::solve isParallel 1
4 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
4 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
0 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
0 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 7
6 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
6 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
5 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
5 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
3 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
3 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
2 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
2 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
1 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
1 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 7
5 7 before Dune::BiCGSTABSolver 
5 Dune::BiCGSTABSolver.apply(x, b, result_) 
5 BiCGSTABSolver::apply 
5 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
3 7 before Dune::BiCGSTABSolver 
3 Dune::BiCGSTABSolver.apply(x, b, result_) 
3 BiCGSTABSolver::apply 
3 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
2 7 before Dune::BiCGSTABSolver 
2 Dune::BiCGSTABSolver.apply(x, b, result_) 
2 BiCGSTABSolver::apply 
2 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
6 7 before Dune::BiCGSTABSolver 
6 Dune::BiCGSTABSolver.apply(x, b, result_) 
6 BiCGSTABSolver::apply 
6 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
4 7 before Dune::BiCGSTABSolver 
4 Dune::BiCGSTABSolver.apply(x, b, result_) 
4 BiCGSTABSolver::apply 
4 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
1 7 before Dune::BiCGSTABSolver 
1 Dune::BiCGSTABSolver.apply(x, b, result_) 
1 BiCGSTABSolver::apply 
1 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
Using a direct coarse solver (SuperLU)
Building hierarchy of 1 levels (inclusive coarse solver) took 0.114949 seconds.
0 7 before Dune::BiCGSTABSolver 
0 Dune::BiCGSTABSolver.apply(x, b, result_) 
0 BiCGSTABSolver::apply 
0 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
4 === BiCGSTABSolver
 Iter          Defect            Rate
    0      1.87025e-16
4  (it = 0.5; it < _maxit; it+=.5) 0.5 250
4  before p = r 
4  before  y = W^-1 * p 
4  before _prec->apply(y,p); 0.5 250
4 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
0 === BiCGSTABSolver
 Iter          Defect            Rate
    0      1.87025e-16
0  (it = 0.5; it < _maxit; it+=.5) 0.5 250
0  before p = r 
0  before  y = W^-1 * p 
0  before _prec->apply(y,p); 0.5 250
0 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 7
0 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
1 === BiCGSTABSolver
 Iter          Defect            Rate
    0      1.87025e-16
1  (it = 0.5; it < _maxit; it+=.5) 0.5 250
1  before p = r 
1  before  y = W^-1 * p 
1  before _prec->apply(y,p); 0.5 250
1 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
5 === BiCGSTABSolver
 Iter          Defect            Rate
    0      1.87025e-16
5  (it = 0.5; it < _maxit; it+=.5) 0.5 250
5  before p = r 
5  before  y = W^-1 * p 
5  before _prec->apply(y,p); 0.5 250
5 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
3 === BiCGSTABSolver
 Iter          Defect            Rate
    0      1.87025e-16
3  (it = 0.5; it < _maxit; it+=.5) 0.5 250
3  before p = r 
3  before  y = W^-1 * p 
3  before _prec->apply(y,p); 0.5 250
3 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
2 === BiCGSTABSolver
 Iter          Defect            Rate
    0      1.87025e-16
2  (it = 0.5; it < _maxit; it+=.5) 0.5 250
2  before p = r 
2  before  y = W^-1 * p 
2  before _prec->apply(y,p); 0.5 250
2 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
6 === BiCGSTABSolver
 Iter          Defect            Rate
    0      1.87025e-16
6  (it = 0.5; it < _maxit; it+=.5) 0.5 250
6  before p = r 
6  before  y = W^-1 * p 
6  before _prec->apply(y,p); 0.5 250
6 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
3 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
3 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
3  before v = A * y 
2 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
2 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
4 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
4 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
4  before v = A * y 
6 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
6 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
6  before v = A * y 
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
0 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
1 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
5 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
5 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
5  before v = A * y 
6  before  alpha = rho_new / < rt, v > 
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
0 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
0  before v = A * y 
1 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
1  before v = A * y 
4  before  alpha = rho_new / < rt, v > 
3  before  alpha = rho_new / < rt, v > 
2  before v = A * y 
5  before  alpha = rho_new / < rt, v > 
2  before  alpha = rho_new / < rt, v > 
2  before alpha = rho_new / h; 
2  before apply first correction to x 
2  before r = r - alpha*v 
2  before test stop criteria 
2 istlsolver::printOutput 
  0.5      7.92824e-33      4.23914e-17
2 === rate=1.79703e-33, T=0.125704, TIT=0.251407, IT=0.5
2 finished solveLinearSystemImpl_ 1
2 converged value: 1 7 1 let s go get convergedRemote
2 to  comm_.min(converged) 
6  before alpha = rho_new / h; 
6  before apply first correction to x 
6  before r = r - alpha*v 
6  before test stop criteria 
6 istlsolver::printOutput 
  0.5      7.92824e-33      4.23914e-17
6 === rate=1.79703e-33, T=0.125717, TIT=0.251435, IT=0.5
6 finished solveLinearSystemImpl_ 1
6 converged value: 1 7 1 let s go get convergedRemote
6 to  comm_.min(converged) 
0  before  alpha = rho_new / < rt, v > 
0  before alpha = rho_new / h; 
0  before apply first correction to x 
0  before r = r - alpha*v 
0  before test stop criteria 
0 istlsolver::printOutput 
  0.5      7.92824e-33      4.23914e-17
0 === rate=1.79703e-33, T=0.0186535, TIT=0.037307, IT=0.5
1  before  alpha = rho_new / < rt, v > 
1  before alpha = rho_new / h; 
1  before apply first correction to x 
1  before r = r - alpha*v 
1  before test stop criteria 
1 istlsolver::printOutput 
  0.5      7.92824e-33      4.23914e-17
1 === rate=1.79703e-33, T=0.125683, TIT=0.251367, IT=0.5
1 finished solveLinearSystemImpl_ 1
1 converged value: 1 7 1 let s go get convergedRemote
1 to  comm_.min(converged) 
4  before alpha = rho_new / h; 
4  before apply first correction to x 
4  before r = r - alpha*v 
4  before test stop criteria 
4 istlsolver::printOutput 
  0.5      7.92824e-33      4.23914e-17
4 === rate=1.79703e-33, T=0.125694, TIT=0.251388, IT=0.5
4 finished solveLinearSystemImpl_ 1
4 converged value: 1 7 1 let s go get convergedRemote
4 to  comm_.min(converged) 
3  before alpha = rho_new / h; 
3  before apply first correction to x 
3  before r = r - alpha*v 
3  before test stop criteria 
3 istlsolver::printOutput 
  0.5      7.92824e-33      4.23914e-17
3 === rate=1.79703e-33, T=0.125671, TIT=0.251341, IT=0.5
3 finished solveLinearSystemImpl_ 1
3 converged value: 1 7 1 let s go get convergedRemote
3 to  comm_.min(converged) 
5  before alpha = rho_new / h; 
5  before apply first correction to x 
5  before r = r - alpha*v 
5  before test stop criteria 
5 istlsolver::printOutput 
  0.5      7.92824e-33      4.23914e-17
5 === rate=1.79703e-33, T=0.125679, TIT=0.251357, IT=0.5
5 finished solveLinearSystemImpl_ 1
5 converged value: 1 7 1 let s go get convergedRemote
5 to  comm_.min(converged) 
1 did  comm_.min(converged) convergedRemote: 1
1 final convergedRemote: 1
1 Update: x^(k+1) = x^k - deltax^k 
1 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
1 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
1 Newton iteration  6 done1 , maximum relative shift = 2.756e-08
1 Assemble/solve/update time: 0.265779(24.8844%)/0.80216(75.1047%)/0.000116081(0.0108684%)
1 gridVariables->advanceTimeStep
1 before, nonLinearSolver->suggestTimeStepSize, current time: 873.767 simTime 57600
1 before assembler->setPreviousSolution, ddt: 26.6667
1 nonLinearSolver->solve
1 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
4 did  comm_.min(converged) convergedRemote: 1
4 final convergedRemote: 1
4 Update: x^(k+1) = x^k - deltax^k 
4 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
4 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
5 did  comm_.min(converged) convergedRemote: 1
5 final convergedRemote: 1
5 Update: x^(k+1) = x^k - deltax^k 
5 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
5 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
5 Newton iteration  6 done5 , maximum relative shift = 2.756e-08
5 Assemble/solve/update time: 0.265789(24.8851%)/0.80216(75.104%)/0.000116511(0.0109086%)
5 gridVariables->advanceTimeStep
5 before, nonLinearSolver->suggestTimeStepSize, current time: 873.767 simTime 57600
5 before assembler->setPreviousSolution, ddt: 26.6667
5 nonLinearSolver->solve
5 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
3 did  comm_.min(converged) convergedRemote: 1
3 final convergedRemote: 1
3 Update: x^(k+1) = x^k - deltax^k 
3 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
3 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
3 Newton iteration  6 done3 , maximum relative shift = 2.756e-08
3 Assemble/solve/update time: 0.265762(24.8827%)/0.802179(75.1063%)/0.000117551(0.011006%)
3 gridVariables->advanceTimeStep
3 before, nonLinearSolver->suggestTimeStepSize, current time: 873.767 simTime 57600
3 before assembler->setPreviousSolution, ddt: 26.6667
3 nonLinearSolver->solve
3 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
2 did  comm_.min(converged) convergedRemote: 1
2 final convergedRemote: 1
2 Update: x^(k+1) = x^k - deltax^k 
2 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
2 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
6 did  comm_.min(converged) convergedRemote: 1
6 final convergedRemote: 1
6 Update: x^(k+1) = x^k - deltax^k 
6 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
6 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
6 Newton iteration  6 done6 , maximum relative shift = 2.756e-08
0 finished solveLinearSystemImpl_ 1
0 converged value: 1 7 1 let s go get convergedRemote
0 to  comm_.min(converged) 
0 did  comm_.min(converged) convergedRemote: 1
0 final convergedRemote: 1
0 Update: x^(k+1) = x^k - deltax^k 
0 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
0 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
2 Newton iteration  6 done2 , maximum relative shift = 2.756e-08
2 Assemble/solve/update time: 0.26563(24.8736%)/0.802178(75.1159%)/0.000112761(0.0105589%)
2 gridVariables->advanceTimeStep
2 before, nonLinearSolver->suggestTimeStepSize, current time: 873.767 simTime 57600
2 before assembler->setPreviousSolution, ddt: 26.6667
2 nonLinearSolver->solve
2 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
6 Assemble/solve/update time: 0.265779(24.8847%)/0.80218(75.1073%)/8.6111e-05(0.00806248%)
6 gridVariables->advanceTimeStep
6 before, nonLinearSolver->suggestTimeStepSize, current time: 873.767 simTime 57600
6 before assembler->setPreviousSolution, ddt: 26.6667
6 nonLinearSolver->solve
6 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
4 Newton iteration  6 done4 , maximum relative shift = 2.756e-08
4 Assemble/solve/update time: 0.265313(24.8515%)/0.802153(75.1367%)/0.000125591(0.011764%)
4 gridVariables->advanceTimeStep
4 before, nonLinearSolver->suggestTimeStepSize, current time: 873.767 simTime 57600
4 before assembler->setPreviousSolution, ddt: 26.6667
4 nonLinearSolver->solve
4 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
0 Newton iteration  6 done0 , maximum relative shift = 2.756e-08
0 Assemble/solve/update time: 0.272343(25.52%)/0.794694(74.467%)/0.000138681(0.0129951%)
0 gridVariables->advanceTimeStep
0 before, nonLinearSolver->suggestTimeStepSize, current time: 873.767 simTime 57600
0 before assembler->setPreviousSolution, ddt: 26.6667
0 nonLinearSolver->solve
0 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
5 Solve: M deltax^k = r 
5 Newton::solveLinearSystem : entering the try section 0
5to  comm_.sum(norm2) 
5did  comm_.sum(norm2) 
5 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
5 solveLinearSystemImpl_, before converged = ls.solve 
5 7 amgbackend::solve isParallel 1
3 Solve: M deltax^k = r 
3 Newton::solveLinearSystem : entering the try section 0
3to  comm_.sum(norm2) 
3did  comm_.sum(norm2) 
3 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
3 solveLinearSystemImpl_, before converged = ls.solve 
3 7 amgbackend::solve isParallel 1
4 Solve: M deltax^k = r 
4 Newton::solveLinearSystem : entering the try section 0
4to  comm_.sum(norm2) 
4did  comm_.sum(norm2) 
4 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
4 solveLinearSystemImpl_, before converged = ls.solve 
4 7 amgbackend::solve isParallel 1
2 Solve: M deltax^k = r 
2 Newton::solveLinearSystem : entering the try section 0
2to  comm_.sum(norm2) 
2did  comm_.sum(norm2) 
2 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
2 solveLinearSystemImpl_, before converged = ls.solve 
2 7 amgbackend::solve isParallel 1
6 Solve: M deltax^k = r 
6 Newton::solveLinearSystem : entering the try section 0
6to  comm_.sum(norm2) 
6did  comm_.sum(norm2) 
6 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
6 solveLinearSystemImpl_, before converged = ls.solve 
6 7 amgbackend::solve isParallel 1
1 Solve: M deltax^k = r 
1 Newton::solveLinearSystem : entering the try section 0
1to  comm_.sum(norm2) 
1did  comm_.sum(norm2) 
1 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
1 solveLinearSystemImpl_, before converged = ls.solve 
1 7 amgbackend::solve isParallel 1
0 Solve: M deltax^k = r 
0 Newton::solveLinearSystem : entering the try section 0
0to  comm_.sum(norm2) 
0did  comm_.sum(norm2) 
0 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
0 solveLinearSystemImpl_, before converged = ls.solve 
0 7 amgbackend::solve isParallel 1
1 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
1 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
5 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
5 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
3 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
3 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
4 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
4 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
2 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
2 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
6 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
6 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
0 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
0 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 7
1 7 before Dune::BiCGSTABSolver 
1 Dune::BiCGSTABSolver.apply(x, b, result_) 
1 BiCGSTABSolver::apply 
1 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
5 7 before Dune::BiCGSTABSolver 
5 Dune::BiCGSTABSolver.apply(x, b, result_) 
5 BiCGSTABSolver::apply 
5 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
3 7 before Dune::BiCGSTABSolver 
3 Dune::BiCGSTABSolver.apply(x, b, result_) 
3 BiCGSTABSolver::apply 
3 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
4 7 before Dune::BiCGSTABSolver 
4 Dune::BiCGSTABSolver.apply(x, b, result_) 
4 BiCGSTABSolver::apply 
4 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
2 7 before Dune::BiCGSTABSolver 
2 Dune::BiCGSTABSolver.apply(x, b, result_) 
2 BiCGSTABSolver::apply 
2 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 7
6 7 before Dune::BiCGSTABSolver 
6 Dune::BiCGSTABSolver.apply(x, b, result_) 
6 BiCGSTABSolver::apply 
6 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
Using a direct coarse solver (SuperLU)
Building hierarchy of 1 levels (inclusive coarse solver) took 0.113196 seconds.
0 7 before Dune::BiCGSTABSolver 
0 Dune::BiCGSTABSolver.apply(x, b, result_) 
0 BiCGSTABSolver::apply 
0 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
1 === BiCGSTABSolver
 Iter          Defect            Rate
    0      3.16006e-09
1  (it = 0.5; it < _maxit; it+=.5) 0.5 250
1  before p = r 
1  before  y = W^-1 * p 
1  before _prec->apply(y,p); 0.5 250
1 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
5 === BiCGSTABSolver
 Iter          Defect            Rate
    0      3.16006e-09
5  (it = 0.5; it < _maxit; it+=.5) 0.5 250
5  before p = r 
5  before  y = W^-1 * p 
5  before _prec->apply(y,p); 0.5 250
5 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
3 === BiCGSTABSolver
 Iter          Defect            Rate
    0      3.16006e-09
3  (it = 0.5; it < _maxit; it+=.5) 0.5 250
3  before p = r 
3  before  y = W^-1 * p 
3  before _prec->apply(y,p); 0.5 250
3 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
4 === BiCGSTABSolver
 Iter          Defect            Rate
    0      3.16006e-09
4  (it = 0.5; it < _maxit; it+=.5) 0.5 250
4  before p = r 
4  before  y = W^-1 * p 
4  before _prec->apply(y,p); 0.5 250
4 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
0 === BiCGSTABSolver
 Iter          Defect            Rate
    0      3.16006e-09
0  (it = 0.5; it < _maxit; it+=.5) 0.5 250
0  before p = r 
0  before  y = W^-1 * p 
0  before _prec->apply(y,p); 0.5 250
0 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 7
0 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
6 === BiCGSTABSolver
 Iter          Defect            Rate
    0      3.16006e-09
6  (it = 0.5; it < _maxit; it+=.5) 0.5 250
6  before p = r 
6  before  y = W^-1 * p 
6  before _prec->apply(y,p); 0.5 250
6 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
2 === BiCGSTABSolver
 Iter          Defect            Rate
    0      3.16006e-09
2  (it = 0.5; it < _maxit; it+=.5) 0.5 250
2  before p = r 
2  before  y = W^-1 * p 
2  before _prec->apply(y,p); 0.5 250
2 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
3 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
3 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
3  before v = A * y 
2 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
2 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
0 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
6 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
6 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
6  before v = A * y 
4 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
4 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
4  before v = A * y 
1 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
5 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
5 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
5  before v = A * y 
0 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
6  before  alpha = rho_new / < rt, v > 
4  before  alpha = rho_new / < rt, v > 
1 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
3  before  alpha = rho_new / < rt, v > 
5  before  alpha = rho_new / < rt, v > 
6  before alpha = rho_new / h; 
6  before apply first correction to x 
6  before r = r - alpha*v 
6  before test stop criteria 
6 istlsolver::printOutput 
  0.5      5.01812e-25      1.58798e-16
6 === rate=2.52168e-32, T=0.123627, TIT=0.247255, IT=0.5
6 finished solveLinearSystemImpl_ 1
6 converged value: 1 7 1 let s go get convergedRemote
6 to  comm_.min(converged) 
4  before alpha = rho_new / h; 
4  before apply first correction to x 
4  before r = r - alpha*v 
4  before test stop criteria 
4 istlsolver::printOutput 
  0.5      5.01812e-25      1.58798e-16
4 === rate=2.52168e-32, T=0.12363, TIT=0.247261, IT=0.5
4 finished solveLinearSystemImpl_ 1
4 converged value: 1 7 1 let s go get convergedRemote
4 to  comm_.min(converged) 
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
1  before v = A * y 
1  before  alpha = rho_new / < rt, v > 
1  before alpha = rho_new / h; 
1  before apply first correction to x 
1  before r = r - alpha*v 
1  before test stop criteria 
1 istlsolver::printOutput 
  0.5      5.01812e-25      1.58798e-16
1 === rate=2.52168e-32, T=0.12377, TIT=0.247539, IT=0.5
1 finished solveLinearSystemImpl_ 1
1 converged value: 1 7 1 let s go get convergedRemote
1 to  comm_.min(converged) 
5  before alpha = rho_new / h; 
5  before apply first correction to x 
5  before r = r - alpha*v 
5  before test stop criteria 
5 istlsolver::printOutput 
  0.5      5.01812e-25      1.58798e-16
5 === rate=2.52168e-32, T=0.123769, TIT=0.247539, IT=0.5
5 finished solveLinearSystemImpl_ 1
5 converged value: 1 7 1 let s go get convergedRemote
5 to  comm_.min(converged) 
2  before v = A * y 
2  before  alpha = rho_new / < rt, v > 
2  before alpha = rho_new / h; 
2  before apply first correction to x 
2  before r = r - alpha*v 
2  before test stop criteria 
2 istlsolver::printOutput 
  0.5      5.01812e-25      1.58798e-16
2 === rate=2.52168e-32, T=0.123629, TIT=0.247259, IT=0.5
2 finished solveLinearSystemImpl_ 1
2 converged value: 1 7 1 let s go get convergedRemote
2 to  comm_.min(converged) 
3  before alpha = rho_new / h; 
3  before apply first correction to x 
3  before r = r - alpha*v 
3  before test stop criteria 
3 istlsolver::printOutput 
  0.5      5.01812e-25      1.58798e-16
3 === rate=2.52168e-32, T=0.123771, TIT=0.247541, IT=0.5
3 finished solveLinearSystemImpl_ 1
3 converged value: 1 7 1 let s go get convergedRemote
3 to  comm_.min(converged) 
0  before v = A * y 
0  before  alpha = rho_new / < rt, v > 
0  before alpha = rho_new / h; 
0  before apply first correction to x 
0  before r = r - alpha*v 
0  before test stop criteria 
0 istlsolver::printOutput 
  0.5      5.01812e-25      1.58798e-16
0 === rate=2.52168e-32, T=0.0184062, TIT=0.0368123, IT=0.5
3 did  comm_.min(converged) convergedRemote: 1
3 final convergedRemote: 1
3 Update: x^(k+1) = x^k - deltax^k 
3 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
3 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
3 Newton iteration  1 done3 , maximum relative shift = 5.613e-02
3 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
1 did  comm_.min(converged) convergedRemote: 1
1 final convergedRemote: 1
1 Update: x^(k+1) = x^k - deltax^k 
1 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
1 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
1 Newton iteration  1 done1 , maximum relative shift = 5.613e-02
1 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
6 did  comm_.min(converged) convergedRemote: 1
6 final convergedRemote: 1
6 Update: x^(k+1) = x^k - deltax^k 
6 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
6 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
6 Newton iteration  1 done6 , maximum relative shift = 5.613e-02
6 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
4 did  comm_.min(converged) convergedRemote: 1
4 final convergedRemote: 1
4 Update: x^(k+1) = x^k - deltax^k 
4 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
4 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
4 Newton iteration  1 done4 , maximum relative shift = 5.613e-02
4 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
0 finished solveLinearSystemImpl_ 1
0 converged value: 1 7 1 let s go get convergedRemote
0 to  comm_.min(converged) 
0 did  comm_.min(converged) convergedRemote: 1
0 final convergedRemote: 1
0 Update: x^(k+1) = x^k - deltax^k 
0 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
0 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
0 Newton iteration  1 done0 , maximum relative shift = 5.613e-02
0 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
5 did  comm_.min(converged) convergedRemote: 1
5 final convergedRemote: 1
5 Update: x^(k+1) = x^k - deltax^k 
5 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
5 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
5 Newton iteration  1 done5 , maximum relative shift = 5.613e-02
5 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
2 did  comm_.min(converged) convergedRemote: 1
2 final convergedRemote: 1
2 Update: x^(k+1) = x^k - deltax^k 
2 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
2 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
2 Newton iteration  1 done2 , maximum relative shift = 5.613e-02
2 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
2 Solve: M deltax^k = r 
2 Newton::solveLinearSystem : entering the try section 1
2 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
2 solveLinearSystemImpl_, before converged = ls.solve 
2 7 amgbackend::solve isParallel 1
0 Solve: M deltax^k = r 
0 Newton::solveLinearSystem : entering the try section 1
0 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
0 solveLinearSystemImpl_, before converged = ls.solve 
0 7 amgbackend::solve isParallel 1
5 Solve: M deltax^k = r 
5 Newton::solveLinearSystem : entering the try section 1
5 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
5 solveLinearSystemImpl_, before converged = ls.solve 
5 7 amgbackend::solve isParallel 1
4 Solve: M deltax^k = r 
4 Newton::solveLinearSystem : entering the try section 1
4 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
4 solveLinearSystemImpl_, before converged = ls.solve 
4 7 amgbackend::solve isParallel 1
3 Solve: M deltax^k = r 
3 Newton::solveLinearSystem : entering the try section 1
3 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
3 solveLinearSystemImpl_, before converged = ls.solve 
3 7 amgbackend::solve isParallel 1
1 Solve: M deltax^k = r 
1 Newton::solveLinearSystem : entering the try section 1
1 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
1 solveLinearSystemImpl_, before converged = ls.solve 
1 7 amgbackend::solve isParallel 1
6 Solve: M deltax^k = r 
6 Newton::solveLinearSystem : entering the try section 1
6 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
6 solveLinearSystemImpl_, before converged = ls.solve 
6 7 amgbackend::solve isParallel 1
0 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
0 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 7
5 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
5 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
4 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
4 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
3 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
3 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
1 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
1 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
6 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
6 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
2 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
2 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 7
2 7 before Dune::BiCGSTABSolver 
2 Dune::BiCGSTABSolver.apply(x, b, result_) 
2 BiCGSTABSolver::apply 
2 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
5 7 before Dune::BiCGSTABSolver 
5 Dune::BiCGSTABSolver.apply(x, b, result_) 
5 BiCGSTABSolver::apply 
5 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
4 7 before Dune::BiCGSTABSolver 
4 Dune::BiCGSTABSolver.apply(x, b, result_) 
4 BiCGSTABSolver::apply 
3 7 before Dune::BiCGSTABSolver 
3 Dune::BiCGSTABSolver.apply(x, b, result_) 
3 BiCGSTABSolver::apply 
3 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 7
6 7 before Dune::BiCGSTABSolver 
6 Dune::BiCGSTABSolver.apply(x, b, result_) 
6 BiCGSTABSolver::apply 
6 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
1 7 before Dune::BiCGSTABSolver 
1 Dune::BiCGSTABSolver.apply(x, b, result_) 
1 BiCGSTABSolver::apply 
1 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
4 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
Using a direct coarse solver (SuperLU)
Building hierarchy of 1 levels (inclusive coarse solver) took 0.112632 seconds.
0 7 before Dune::BiCGSTABSolver 
0 Dune::BiCGSTABSolver.apply(x, b, result_) 
0 BiCGSTABSolver::apply 
0 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
6 === BiCGSTABSolver
 Iter          Defect            Rate
    0      2.82719e-11
6  (it = 0.5; it < _maxit; it+=.5) 0.5 250
6  before p = r 
6  before  y = W^-1 * p 
6  before _prec->apply(y,p); 0.5 250
6 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
1 === BiCGSTABSolver
 Iter          Defect            Rate
    0      2.82719e-11
1  (it = 0.5; it < _maxit; it+=.5) 0.5 250
1  before p = r 
1  before  y = W^-1 * p 
1  before _prec->apply(y,p); 0.5 250
1 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
2 === BiCGSTABSolver
 Iter          Defect            Rate
    0      2.82719e-11
2  (it = 0.5; it < _maxit; it+=.5) 0.5 250
2  before p = r 
2  before  y = W^-1 * p 
2  before _prec->apply(y,p); 0.5 250
2 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
3 === BiCGSTABSolver
 Iter          Defect            Rate
    0      2.82719e-11
3  (it = 0.5; it < _maxit; it+=.5) 0.5 250
3  before p = r 
3  before  y = W^-1 * p 
3  before _prec->apply(y,p); 0.5 250
3 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
5 === BiCGSTABSolver
 Iter          Defect            Rate
    0      2.82719e-11
5  (it = 0.5; it < _maxit; it+=.5) 0.5 250
5  before p = r 
5  before  y = W^-1 * p 
5  before _prec->apply(y,p); 0.5 250
5 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
0 === BiCGSTABSolver
 Iter          Defect            Rate
    0      2.82719e-11
0  (it = 0.5; it < _maxit; it+=.5) 0.5 250
0  before p = r 
0  before  y = W^-1 * p 
0  before _prec->apply(y,p); 0.5 250
0 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 7
0 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
4 === BiCGSTABSolver
 Iter          Defect            Rate
    0      2.82719e-11
4  (it = 0.5; it < _maxit; it+=.5) 0.5 250
4  before p = r 
4  before  y = W^-1 * p 
4  before _prec->apply(y,p); 0.5 250
4 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
6 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
6 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
6  before v = A * y 
6  before  alpha = rho_new / < rt, v > 
1 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
2 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
2 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
3 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
3 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
3  before v = A * y 
4 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
4 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
4  before v = A * y 
4  before  alpha = rho_new / < rt, v > 
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
0 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
5 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
5 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
5  before v = A * y 
0 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
3  before  alpha = rho_new / < rt, v > 
1 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
1  before v = A * y 
2  before v = A * y 
5  before  alpha = rho_new / < rt, v > 
3  before alpha = rho_new / h; 
3  before apply first correction to x 
3  before r = r - alpha*v 
3  before test stop criteria 
3 istlsolver::printOutput 
  0.5      7.39378e-28      2.61524e-17
3 === rate=6.83949e-34, T=0.123396, TIT=0.246792, IT=0.5
3 finished solveLinearSystemImpl_ 1
3 converged value: 1 7 1 let s go get convergedRemote
3 to  comm_.min(converged) 
1  before  alpha = rho_new / < rt, v > 
1  before alpha = rho_new / h; 
1  before apply first correction to x 
1  before r = r - alpha*v 
1  before test stop criteria 
1 istlsolver::printOutput 
  0.5      7.39378e-28      2.61524e-17
1 === rate=6.83949e-34, T=0.123398, TIT=0.246796, IT=0.5
1 finished solveLinearSystemImpl_ 1
1 converged value: 1 7 1 let s go get convergedRemote
1 to  comm_.min(converged) 
5  before alpha = rho_new / h; 
5  before apply first correction to x 
5  before r = r - alpha*v 
5  before test stop criteria 
5 istlsolver::printOutput 
  0.5      7.39378e-28      2.61524e-17
5 === rate=6.83949e-34, T=0.123396, TIT=0.246791, IT=0.5
5 finished solveLinearSystemImpl_ 1
5 converged value: 1 7 1 let s go get convergedRemote
5 to  comm_.min(converged) 
4  before alpha = rho_new / h; 
4  before apply first correction to x 
4  before r = r - alpha*v 
4  before test stop criteria 
4 istlsolver::printOutput 
  0.5      7.39378e-28      2.61524e-17
4 === rate=6.83949e-34, T=0.12336, TIT=0.24672, IT=0.5
4 finished solveLinearSystemImpl_ 1
4 converged value: 1 7 1 let s go get convergedRemote
4 to  comm_.min(converged) 
2  before  alpha = rho_new / < rt, v > 
2  before alpha = rho_new / h; 
2  before apply first correction to x 
2  before r = r - alpha*v 
2  before test stop criteria 
2 istlsolver::printOutput 
  0.5      7.39378e-28      2.61524e-17
2 === rate=6.83949e-34, T=0.123371, TIT=0.246742, IT=0.5
2 finished solveLinearSystemImpl_ 1
2 converged value: 1 7 1 let s go get convergedRemote
2 to  comm_.min(converged) 
0  before v = A * y 
0  before  alpha = rho_new / < rt, v > 
0  before alpha = rho_new / h; 
0  before apply first correction to x 
0  before r = r - alpha*v 
0  before test stop criteria 
0 istlsolver::printOutput 
  0.5      7.39378e-28      2.61524e-17
0 === rate=6.83949e-34, T=0.0185828, TIT=0.0371656, IT=0.5
6  before alpha = rho_new / h; 
6  before apply first correction to x 
6  before r = r - alpha*v 
6  before test stop criteria 
6 istlsolver::printOutput 
  0.5      7.39378e-28      2.61524e-17
6 === rate=6.83949e-34, T=0.123373, TIT=0.246745, IT=0.5
6 finished solveLinearSystemImpl_ 1
6 converged value: 1 7 1 let s go get convergedRemote
6 to  comm_.min(converged) 
5 did  comm_.min(converged) convergedRemote: 1
5 final convergedRemote: 1
5 Update: x^(k+1) = x^k - deltax^k 
5 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
5 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
5 Newton iteration  2 done5 , maximum relative shift = 4.076e-03
5 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
4 did  comm_.min(converged) convergedRemote: 1
4 final convergedRemote: 1
4 Update: x^(k+1) = x^k - deltax^k 
4 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
4 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
4 Newton iteration  2 done4 , maximum relative shift = 4.076e-03
4 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
2 did  comm_.min(converged) convergedRemote: 1
2 final convergedRemote: 1
2 Update: x^(k+1) = x^k - deltax^k 
2 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
2 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
2 Newton iteration  2 done2 , maximum relative shift = 4.076e-03
2 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
0 finished solveLinearSystemImpl_ 1
0 converged value: 1 7 1 let s go get convergedRemote
0 to  comm_.min(converged) 
0 did  comm_.min(converged) convergedRemote: 1
0 final convergedRemote: 1
0 Update: x^(k+1) = x^k - deltax^k 
0 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
0 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
0 Newton iteration  2 done0 , maximum relative shift = 4.076e-03
0 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
1 did  comm_.min(converged) convergedRemote: 1
1 final convergedRemote: 1
1 Update: x^(k+1) = x^k - deltax^k 
1 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
1 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
1 Newton iteration  2 done1 , maximum relative shift = 4.076e-03
1 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
3 did  comm_.min(converged) convergedRemote: 1
3 final convergedRemote: 1
3 Update: x^(k+1) = x^k - deltax^k 
3 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
3 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
3 Newton iteration  2 done3 , maximum relative shift = 4.076e-03
3 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
6 did  comm_.min(converged) convergedRemote: 1
6 final convergedRemote: 1
6 Update: x^(k+1) = x^k - deltax^k 
6 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
6 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
6 Newton iteration  2 done6 , maximum relative shift = 4.076e-03
6 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
6 Solve: M deltax^k = r 
6 Newton::solveLinearSystem : entering the try section 2
6 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
6 solveLinearSystemImpl_, before converged = ls.solve 
6 7 amgbackend::solve isParallel 1
3 Solve: M deltax^k = r 
3 Newton::solveLinearSystem : entering the try section 2
3 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
3 solveLinearSystemImpl_, before converged = ls.solve 
3 7 amgbackend::solve isParallel 1
5 Solve: M deltax^k = r 
5 Newton::solveLinearSystem : entering the try section 2
5 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
5 solveLinearSystemImpl_, before converged = ls.solve 
5 7 amgbackend::solve isParallel 1
4 Solve: M deltax^k = r 
4 Newton::solveLinearSystem : entering the try section 2
4 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
4 solveLinearSystemImpl_, before converged = ls.solve 
4 7 amgbackend::solve isParallel 1
2 Solve: M deltax^k = r 
2 Newton::solveLinearSystem : entering the try section 2
2 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
2 solveLinearSystemImpl_, before converged = ls.solve 
2 7 amgbackend::solve isParallel 1
0 Solve: M deltax^k = r 
0 Newton::solveLinearSystem : entering the try section 2
0 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
0 solveLinearSystemImpl_, before converged = ls.solve 
0 7 amgbackend::solve isParallel 1
1 Solve: M deltax^k = r 
1 Newton::solveLinearSystem : entering the try section 2
1 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
1 solveLinearSystemImpl_, before converged = ls.solve 
1 7 amgbackend::solve isParallel 1
4 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
4 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
2 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
2 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
0 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
0 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 7
1 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
1 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
6 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
6 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
3 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
3 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
5 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
5 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 7
4 7 before Dune::BiCGSTABSolver 
4 Dune::BiCGSTABSolver.apply(x, b, result_) 
4 BiCGSTABSolver::apply 
4 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
2 7 before Dune::BiCGSTABSolver 
2 Dune::BiCGSTABSolver.apply(x, b, result_) 
2 BiCGSTABSolver::apply 
2 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 7
1 7 before Dune::BiCGSTABSolver 
1 Dune::BiCGSTABSolver.apply(x, b, result_) 
1 BiCGSTABSolver::apply 
1 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
6 7 before Dune::BiCGSTABSolver 
6 Dune::BiCGSTABSolver.apply(x, b, result_) 
6 BiCGSTABSolver::apply 
6 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
3 7 before Dune::BiCGSTABSolver 
3 Dune::BiCGSTABSolver.apply(x, b, result_) 
3 BiCGSTABSolver::apply 
3 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
5 7 before Dune::BiCGSTABSolver 
5 Dune::BiCGSTABSolver.apply(x, b, result_) 
5 BiCGSTABSolver::apply 
5 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
Using a direct coarse solver (SuperLU)
Building hierarchy of 1 levels (inclusive coarse solver) took 0.119584 seconds.
0 7 before Dune::BiCGSTABSolver 
0 Dune::BiCGSTABSolver.apply(x, b, result_) 
0 BiCGSTABSolver::apply 
0 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
0 === BiCGSTABSolver
 Iter          Defect            Rate
    0       1.6992e-13
0  (it = 0.5; it < _maxit; it+=.5) 0.5 250
1 === BiCGSTABSolver
 Iter          Defect            Rate
    0       1.6992e-13
1  (it = 0.5; it < _maxit; it+=.5) 0.5 250
1  before p = r 
1  before  y = W^-1 * p 
1  before _prec->apply(y,p); 0.5 250
1 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
6 === BiCGSTABSolver
 Iter          Defect            Rate
    0       1.6992e-13
6  (it = 0.5; it < _maxit; it+=.5) 0.5 250
6  before p = r 
6  before  y = W^-1 * p 
6  before _prec->apply(y,p); 0.5 250
6 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
3 === BiCGSTABSolver
 Iter          Defect            Rate
    0       1.6992e-13
3  (it = 0.5; it < _maxit; it+=.5) 0.5 250
3  before p = r 
3  before  y = W^-1 * p 
3  before _prec->apply(y,p); 0.5 250
3 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
2 === BiCGSTABSolver
 Iter          Defect            Rate
    0       1.6992e-13
2  (it = 0.5; it < _maxit; it+=.5) 0.5 250
2  before p = r 
2  before  y = W^-1 * p 
2  before _prec->apply(y,p); 0.5 250
2 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
4 === BiCGSTABSolver
 Iter          Defect            Rate
    0       1.6992e-13
4  (it = 0.5; it < _maxit; it+=.5) 0.5 250
4  before p = r 
4  before  y = W^-1 * p 
4  before _prec->apply(y,p); 0.5 250
4 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
0  before p = r 
0  before  y = W^-1 * p 
0  before _prec->apply(y,p); 0.5 250
0 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 7
0 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
5 === BiCGSTABSolver
 Iter          Defect            Rate
    0       1.6992e-13
5  (it = 0.5; it < _maxit; it+=.5) 0.5 250
5  before p = r 
5  before  y = W^-1 * p 
5  before _prec->apply(y,p); 0.5 250
5 void AMG<M,X,S,PI,A>::apply(Domain& v, const Range& d), before mgc(levelContext) 
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
0 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
4 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
4 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
4  before v = A * y 
1 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
6 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
6 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
6  before v = A * y 
3 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
3 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
3  before v = A * y 
2 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
2 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
5 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
5 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
5  before v = A * y 
0 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
4  before  alpha = rho_new / < rt, v > 
1 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
6  before  alpha = rho_new / < rt, v > 
3  before  alpha = rho_new / < rt, v > 
5  before  alpha = rho_new / < rt, v > 
4  before alpha = rho_new / h; 
4  before apply first correction to x 
4  before r = r - alpha*v 
4  before test stop criteria 
4 istlsolver::printOutput 
  0.5      1.52954e-29      9.00148e-17
4 === rate=8.10267e-33, T=0.133077, TIT=0.266153, IT=0.5
4 finished solveLinearSystemImpl_ 1
4 converged value: 1 7 1 let s go get convergedRemote
4 to  comm_.min(converged) 
1  before v = A * y 
1  before  alpha = rho_new / < rt, v > 
1  before alpha = rho_new / h; 
1  before apply first correction to x 
1  before r = r - alpha*v 
1  before test stop criteria 
1 istlsolver::printOutput 
  0.5      1.52954e-29      9.00148e-17
1 === rate=8.10267e-33, T=0.133181, TIT=0.266362, IT=0.5
1 finished solveLinearSystemImpl_ 1
1 converged value: 1 7 1 let s go get convergedRemote
1 to  comm_.min(converged) 
6  before alpha = rho_new / h; 
6  before apply first correction to x 
6  before r = r - alpha*v 
6  before test stop criteria 
6 istlsolver::printOutput 
  0.5      1.52954e-29      9.00148e-17
6 === rate=8.10267e-33, T=0.133078, TIT=0.266156, IT=0.5
6 finished solveLinearSystemImpl_ 1
6 converged value: 1 7 1 let s go get convergedRemote
6 to  comm_.min(converged) 
5  before alpha = rho_new / h; 
5  before apply first correction to x 
5  before r = r - alpha*v 
5  before test stop criteria 
5 istlsolver::printOutput 
  0.5      1.52954e-29      9.00148e-17
5 === rate=8.10267e-33, T=0.133178, TIT=0.266356, IT=0.5
5 finished solveLinearSystemImpl_ 1
5 converged value: 1 7 1 let s go get convergedRemote
5 to  comm_.min(converged) 
2  before v = A * y 
2  before  alpha = rho_new / < rt, v > 
2  before alpha = rho_new / h; 
2  before apply first correction to x 
2  before r = r - alpha*v 
2  before test stop criteria 
2 istlsolver::printOutput 
  0.5      1.52954e-29      9.00148e-17
2 === rate=8.10267e-33, T=0.133079, TIT=0.266159, IT=0.5
2 finished solveLinearSystemImpl_ 1
2 converged value: 1 7 1 let s go get convergedRemote
2 to  comm_.min(converged) 
3  before alpha = rho_new / h; 
3  before apply first correction to x 
3  before r = r - alpha*v 
3  before test stop criteria 
3 istlsolver::printOutput 
  0.5      1.52954e-29      9.00148e-17
3 === rate=8.10267e-33, T=0.13317, TIT=0.266341, IT=0.5
3 finished solveLinearSystemImpl_ 1
3 converged value: 1 7 1 let s go get convergedRemote
3 to  comm_.min(converged) 
0  before v = A * y 
0  before  alpha = rho_new / < rt, v > 
0  before alpha = rho_new / h; 
0  before apply first correction to x 
0  before r = r - alpha*v 
0  before test stop criteria 
0 istlsolver::printOutput 
  0.5      1.52954e-29      9.00148e-17
0 === rate=8.10267e-33, T=0.0214191, TIT=0.0428382, IT=0.5
5 did  comm_.min(converged) convergedRemote: 1
5 final convergedRemote: 1
5 Update: x^(k+1) = x^k - deltax^k 
5 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
5 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
5 Newton iteration  3 done5 , maximum relative shift = 2.189e-05
5 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
2 did  comm_.min(converged) convergedRemote: 1
2 final convergedRemote: 1
2 Update: x^(k+1) = x^k - deltax^k 
2 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
2 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
2 Newton iteration  3 done2 , maximum relative shift = 2.189e-05
2 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
3 did  comm_.min(converged) convergedRemote: 1
3 final convergedRemote: 1
3 Update: x^(k+1) = x^k - deltax^k 
3 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
3 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
3 Newton iteration  3 done3 , maximum relative shift = 2.189e-05
3 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
6 did  comm_.min(converged) convergedRemote: 1
6 final convergedRemote: 1
6 Update: x^(k+1) = x^k - deltax^k 
6 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
6 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
6 Newton iteration  3 done6 , maximum relative shift = 2.189e-05
6 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
4 did  comm_.min(converged) convergedRemote: 1
4 final convergedRemote: 1
4 Update: x^(k+1) = x^k - deltax^k 
4 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
4 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
4 Newton iteration  3 done4 , maximum relative shift = 2.189e-05
4 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
1 did  comm_.min(converged) convergedRemote: 1
1 final convergedRemote: 1
1 Update: x^(k+1) = x^k - deltax^k 
1 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
1 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
1 Newton iteration  3 done1 , maximum relative shift = 2.189e-05
1 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
0 finished solveLinearSystemImpl_ 1
0 converged value: 1 7 1 let s go get convergedRemote
0 to  comm_.min(converged) 
0 did  comm_.min(converged) convergedRemote: 1
0 final convergedRemote: 1
0 Update: x^(k+1) = x^k - deltax^k 
0 newtonUpdateShift_: to shift_ = comm_.max(shift_) 
0 newtonUpdateShift_: did shift_ = comm_.max(shift_) 
0 Newton iteration  3 done0 , maximum relative shift = 2.189e-05
0 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
3 Solve: M deltax^k = r 
3 Newton::solveLinearSystem : entering the try section 3
3 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
3 solveLinearSystemImpl_, before converged = ls.solve 
3 7 amgbackend::solve isParallel 1
6 Solve: M deltax^k = r 
6 Newton::solveLinearSystem : entering the try section 3
6 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
6 solveLinearSystemImpl_, before converged = ls.solve 
6 7 amgbackend::solve isParallel 1
4 Solve: M deltax^k = r 
4 Newton::solveLinearSystem : entering the try section 3
4 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
4 solveLinearSystemImpl_, before converged = ls.solve 
4 7 amgbackend::solve isParallel 1
0 Solve: M deltax^k = r 
0 Newton::solveLinearSystem : entering the try section 3
0 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
0 solveLinearSystemImpl_, before converged = ls.solve 
0 7 amgbackend::solve isParallel 1
1 Solve: M deltax^k = r 
1 Newton::solveLinearSystem : entering the try section 3
1 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
1 solveLinearSystemImpl_, before converged = ls.solve 
1 7 amgbackend::solve isParallel 1
5 Solve: M deltax^k = r 
5 Newton::solveLinearSystem : entering the try section 3
5 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
5 solveLinearSystemImpl_, before converged = ls.solve 
5 7 amgbackend::solve isParallel 1
2 Solve: M deltax^k = r 
2 Newton::solveLinearSystem : entering the try section 3
2 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
2 solveLinearSystemImpl_, before converged = ls.solve 
2 7 amgbackend::solve isParallel 1
3 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
3 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
6 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
6 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
2 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
2 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
0 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
0 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 7
1 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
1 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
5 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
5 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
4 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
4 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 7
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 7
2 7 before Dune::BiCGSTABSolver 
2 Dune::BiCGSTABSolver.apply(x, b, result_) 
2 BiCGSTABSolver::apply 
2 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
4 7 before Dune::BiCGSTABSolver 
4 Dune::BiCGSTABSolver.apply(x, b, result_) 
4 BiCGSTABSolver::apply 
4 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
1 7 before Dune::BiCGSTABSolver 
1 Dune::BiCGSTABSolver.apply(x, b, result_) 
1 BiCGSTABSolver::apply 
1 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
5 7 before Dune::BiCGSTABSolver 
5 Dune::BiCGSTABSolver.apply(x, b, result_) 
5 BiCGSTABSolver::apply 
5 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
6 7 before Dune::BiCGSTABSolver 
6 Dune::BiCGSTABSolver.apply(x, b, result_) 
6 BiCGSTABSolver::apply 
6 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 1
3 7 before Dune::BiCGSTABSolver 
3 Dune::BiCGSTABSolver.apply(x, b, result_) 
3 BiCGSTABSolver::apply 
3 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 2
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 2
