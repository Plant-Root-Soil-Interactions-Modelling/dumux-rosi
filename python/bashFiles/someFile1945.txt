92: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 0 1 -32766 0 0
92: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 16
92: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 0 1 -32766 0 0
92: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 16
92: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 0 1 -32766 0 0
92: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 16
92: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 0 1 -32766 0 0
92: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 16
92: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 0 1 -32766 0 0
92: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 16
92: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 0 1 -32766 0 0
92: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 16
92: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 0 1 -32766 0 0
92: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 16
92: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 0 1 -32766 0 0
92: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 16
92: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 0 1 -32766 0 0
92: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 10 16
92: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 0 1 -32766 0 0
92: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 11 16
92: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 0 1 -32766 0 0
92: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 12 16
92: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 0 1 -32766 0 0
92: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 13 16
92: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 0 1 -32766 0 0
92: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 14 16
92: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 0 1 -32766 0 0
92: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 15 16
92: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 0 1 -32766 0 0
Newton::solveLinearSystem : Caught exception from the linear solver when getting converged: "BCRSMatrixError [operator++:/home/m.giraud/DUMUXexud/dune-istl/dune/istl/bcrsmatrix.hh:940]: matrix already built up"
92 converged value: 0 121 1 let s go get convergedRemote
92 to  comm_.min(converged) 
92 did  comm_.min(converged) convergedRemote: 0
92 final convergedRemote: 0
92 about to throw Linear solver did not converge
92 Newton::solveLinearSystem : Caught exception: "NumericalProblem [solveLinearSystem:/home/m113 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
113: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 18
113: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 4.87e-06
113: communicator.hh::sendRecv,MPI_Testany 2 1 -32766 0 0 1.051e-05
113: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 2 1 -32766 0 0
113: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 18
113: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 2 1 -32766 0 0
113: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 18
113: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 2 1 -32766 0 0
113: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 18
113: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 2 1 -32766 0 0
113: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 18
113: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 2 1 -32766 0 0
113: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 18
113: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 2 1 -32766 0 0
113: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 18
113: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 2 1 -32766 0 0
113: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 18
113: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 2 1 -32766 0 0
113: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 18
113: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 2 1 -32766 0 0
113: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 18
113: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 2 1 -32766 0 0
113: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 10 18
113: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 2 1 -32766 0 0
113: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 11 18
113: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 2 1 -32766 0 0
113: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 12 18
113: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 2 1 -32766 0 0
113: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 13 18
113: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 2 1 -32766 0 0
113: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 14 18
113: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 2 1 -32766 0 0
113: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 15 18
113: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 2 1 -32766 0 0
113: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 16 18
113: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 2 1 -32766 0 0
113: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 17 18
113: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 2 1 -32766 0 0
Newton::solveLinearSystem : Caught exception from the linear 7 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
7: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 13
7: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 4.56e-06
7: communicator.hh::sendRecv,MPI_Testany 1 1 -32766 0 0 1.029e-05
7: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 1 1 -32766 0 0
7: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 13
7: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 1 1 -32766 0 0
7: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 13
7: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 1 1 -32766 0 0
7: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 13
7: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 1 1 -32766 0 0
7: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 13
7: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 1 1 -32766 0 0
7: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 13
7: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 1 1 -32766 0 0
7: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 13
7: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 1 1 -32766 0 0
7: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 13
7: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 1 1 -32766 0 0
7: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 13
7: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 1 1 -32766 0 0
7: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 13
7: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 1 1 -32766 0 0
7: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 10 13
7: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 1 1 -32766 0 0
7: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 11 13
7: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 1 1 -32766 0 0
7: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 12 13
7: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 1 1 -32766 0 0
Newton::solveLinearSystem : Caught exception from the linear solver when getting converged: "BCRSMatrixError [operator++:/home/m.giraud/DUMUXexud/dune-istl/dune/istl/bcrsmatrix.hh:940]: matrix already built up"
7 converged value: 0 121 1 let s go get convergedRemote
7 to  comm_.min(converged) 
7 did  comm_.min(converged) convergedRemote: 0
7 final convergedRemote: 0
7 about to throw Linear solver did not converge
7 Newton::solveLinearSystem : Caught exception: "NumericalProblem [solveLinearSystem:/home/m.giraud/DUMUXexud/dumux/dumux/nonlinear/newtonsolver.hh:381]: Linear solver did not converge when getting convergedRemote "
7 Newton: Caught exception: "NumericalProblem [solveLinearSystem:/home/m.giraud/DUMUXexud/dumux/dumux/nonlinear/newtonsolver.hh:381]: Linear solver did not converge"
7 Newton solver did not converge with dt = 0.099 seconds. Retrying with time step of 0.0495 seconds
7 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
23 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
23: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 11
23: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 9.8e-07
23: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 6.97e-06
23: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 9.9e-06
23: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 1.264e-05
23: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 1.574e-05
23: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 1.852e-05
23: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 2.121e-05
23: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 2.385e-05
23: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 2.648e-05
23: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 2.91e-05
23: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 3.185e-05
23: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 3.475e-05
23: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 3.88e-05
23: communicator.hh::sendRecv,MPI_Testany 6 1 -32766 0 0 7.2131e-05
23: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 6 1 -32766 0 0
23: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 11
23: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 6 1 -32766 0 0
23: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 11
23: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 6 1 -32766 0 0
23: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 11
23: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 6 1 -32766 0 0
23: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 11
23: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 6 1 -32766 0 0
23: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 11
23: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 6 1 -32766 0 0
23: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 11
23: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 6 1 -32766 0 0
23: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 11
23: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 6 1 -32766 0 0
23: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 11
23: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 6 1 -32766 0 0
23: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 11
23: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 6 1 -32766 0 0
23: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 10 11
23: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 6 1 -32766 0 0
Newton::solveLinearSystem : Caught exception from the linear solver when getting converged: "BCRSMatrixError [operator++:/home/m.giraud/DUMUXexud/dune-istl/dune/istl/bcrsmatrix.hh:940]: matrix already built up"
23 converged value: 0 121 1 let s go get convergedRemote
23 to  comm_.min(converged) 
23 did  comm_.min(converged) convergedRemote: 0
23 final convergedRemote: 0
23 about to throw Linear solver did not converge
23 Newton::solveLinearSystem : Caught exception: "NumericalProblem [solveLinearSystem:/home/m.giraud/DUMUXexud/dumux/dumux/nonlinear/newtonsolver.hh:381]: Linear solver did not converge when getting convergedRemote "
23 Newton: Caught exception: "NumericalProblem [sol11 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
11: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 17
11: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 1.95e-06
11: communicator.hh::sendRecv,MPI_Testany 0 1 -32766 0 0 7.6e-06
11: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 0 1 -32766 0 0
11: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 17
11: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 0 1 -32766 0 0
11: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 17
11: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 0 1 -32766 0 0
11: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 17
11: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 0 1 -32766 0 0
11: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 17
11: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 0 1 -32766 0 0
11: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 17
11: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 0 1 -32766 0 0
11: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 17
11: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 0 1 -32766 0 0
11: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 17
11: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 0 1 -32766 0 0
11: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 17
11: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 0 1 -32766 0 0
11: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 17
11: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 0 1 -32766 0 0
11: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 10 17
11: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 0 1 -32766 0 0
11: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 11 17
11: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 0 1 -32766 0 0
11: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 12 17
11: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 0 1 -32766 0 0
11: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 13 17
11: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 0 1 -32766 0 0
11: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 14 17
11: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 0 1 -32766 0 0
11: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 15 17
11: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 0 1 -32766 0 0
11: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 16 17
11: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 0 1 -32766 0 0
Newton::solveLinearSystem : Caught exception from the linear solver when getting converged: "BCRSMatrixError [operator++:/home/m.giraud/DUMUXexud/dune-istl/dune/istl/bcrsmatrix.hh:940]: matrix already built up"
11 converged value: 0 121 1 let s go get convergedRemote
11 to  comm_.min(converged) 
11 did  comm20 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
20: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 10
20: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 5.16e-06
20: communicator.hh::sendRecv,MPI_Testany 1 1 -32766 0 0 1.35e-05
20: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 1 1 -32766 0 0
20: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 10
20: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 1 1 -32766 0 0
20: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 10
20: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 1 1 -32766 0 0
20: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 10
20: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 1 1 -32766 0 0
20: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 10
20: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 1 1 -32766 0 0
20: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 10
20: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 1 1 -32766 0 0
20: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 10
20: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 1 1 -32766 0 0
20: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 10
20: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 1 1 -32766 0 0
20: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 10
20: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 1 1 -32766 0 0
20: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 10
20: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 1 1 -32766 0 0
Newton::solveLinearSystem : Caught exception from the linear solver when getting converged: "BCRSMatrixError [operator++:/home/m.giraud/DUMUXexud/dune-istl/dune/istl/bcrsmatrix.hh:940]: matrix already built up"
20 converged value: 0 121 1 let s go get convergedRemote
20 to  comm_.min(converged) 
20 did  comm_.min(converged) convergedRemote: 0
20 final convergedRemote: 0
20 about to throw Linear solver did not converge
20 Newton::solveLinearSystem : Caught exception: "NumericalProblem [solveLinearSystem:/home/m.giraud/DUMUXexud/dumux/dumux/nonlinear/newtonsolver.hh:381]: Linear solver did not converge when getting convergedRemote "
20 Newton: Caught exception: "NumericalProblem [solveLinearSystem:/home/m.giraud/DUMUXexud/dumux/dumux/nonlinear/newtonsolver.hh:381]: Linear solver did not converge"
20 Newton solver did not converge with dt = 0.099 seconds. Retrying with time step of 0.0495 seconds
20 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
90 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
90: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 6
90: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 1.74e-06
90: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000168362
90: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000187572
90: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000319154
90: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000444495
90: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000466266
90: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000471026
90: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000602607
90: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000628228
90: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000632188
90: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.00076897
90: communicator.hh::sendRecv,MPI_Testany 0 1 -32766 0 0 0.00081038
90: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 0 1 -32766 0 0
90: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 6
90: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 0 1 -32766 0 0
90: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 6
90: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 0 1 -32766 0 0
90: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 6
90: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 0 1 -32766 0 0
90: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 6
90: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 0 1 -32766 0 0
90: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 6
90: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 0 1 -32766 0 0
Newton::solveLinearSystem : Caught exception from the linear solver when getting converged: "BCRSMatrixError [operator++:/home/m.giraud/DUMUXexud/dune-istl/dune/istl/bcrsmatrix.hh:940]: matrix already built up"
90 converged value: 0 121 1 let s go get convergedRemote
90 to  comm_.min(converged) 
90 did  comm_.min(converged) convergedRemote: 0
90 final convergedRemote: 0
90 about to throw Linear solver did not converge
90 Newton::solveLinearSystem : Caught exception: "NumericalProblem [solveLinearSystem:/home/m.giraud/DUMUXexud/dumux/dumux/nonlinear/newtonsolver.hh:381]: Linear solver did not converge when getting convergedRemote "
90 Newton: Caught exception: "NumericalProblem [solveLinearSystem:/home/m.giraud/DUMUXexud/dumux/dumux/nonlinear/newtonsolver.hh:381]: Linear solver did not converge"
90 Newton solver did not converge with dt = 0.099 seconds. Retrying with time step of 0.0495 seconds
90 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
110 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
110: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 11
110: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 5.31e-06
110: communicator.hh::sendRecv,MPI_Testany 0 1 -32766 0 0 1.19e-05
110: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 0 1 -32766 0 0
110: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 11
110: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 0 1 -32766 0 0
110: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 11
110: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 0 1 -32766 0 0
110: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 11
110: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 0 1 -32766 0 0
110: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 11
110: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 0 1 -32766 0 0
110: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 11
110: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 0 1 -32766 0 0
110: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 11
110: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 0 1 -32766 0 0
110: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 11
110: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 0 1 -32766 0 0
110: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 11
110: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 0 1 -32766 0 0
110: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 11
110: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 0 1 -32766 0 0
110: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 10 11
110: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 0 1 -32766 0 0
Newton::solveLinearSystem : Caught exception from the linear solver when getting converged: "BCRSMatrixError [operator++:/home/m.giraud/DUMUXexud/dune-istl/dune/istl/bcrsmatrix.hh:940]: matrix already built up"
110 converged value: 0 121 1 let s go get convergedRemote
110 to  comm_.min(converged) 
110 did  comm_.min(converged) convergedRemote: 0
110 final convergedRemote: 0
110 about to throw Linear solver did not converge
110 Newton::solveLinearSystem : Caught exception: "NumericalProblem [solveLinearSystem:/home/m.giraud/DUMUXexud/dumux/dumux/nonlinear/newtonsolver.hh:381]: Linear solver did not converge when getting convergedRemote "
110 Newton: Caught exception: "NumericalProblem [solveLinearSystem:/home/m.giraud/DUMUXexud/dumux/dumux/nonlinear/newtonsolver.hh:381]: Linear solver did not converge"
110 Newton solver did not converge with dt = 0.099 seconds. Retrying with time step of 0.0495 seconds
110 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
73 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
73: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 11
73: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 2.1e-06
73: communicator.hh::sendRecv,MPI_Testany 3 1 -32766 0 0 7.7e-06
73: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 3 1 -32766 0 0
73: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 11
73: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 3 1 -32766 0 0
73: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 11
73: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 3 1 -32766 0 0
73: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 11
73: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 3 1 -32766 0 0
73: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 11
73: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 3 1 -32766 0 0
73: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 11
73: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 3 1 -32766 0 0
73: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 11
73: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 3 1 -32766 0 0
73: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 11
73: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 3 1 -32766 0 0
73: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 11
73: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 3 1 -32766 0 0
73: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 11
73: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 3 1 -32766 0 0
73: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 10 11
73: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 3 1 -32766 0 0
Newton::solveLinearSystem : Caught exception from the linear solver when getting converged: "BCRSMatrixError [operator++:/home/m.giraud/DUMUXexud/dune-istl/dune/istl/bcrsmatrix.hh:940]: matrix already built up"
73 converged value: 0 121 1 let s go get convergedRemote
73 to  comm_.min(converged) 
73 did  comm_.min(converged) convergedRemote: 0
73 final convergedRemote: 0
73 about to throw Linear solver did not converge
73 Newton::solveLinearSystem : Caught exception: "NumericalProblem [solveLinearSystem:/home/m.giraud/DUMUXexud/dumux/dumux/nonlinear/newtonsolver.hh:381]: Linear solver did not converge when getting convergedRemote "
73 Newton: Caught exception: "NumericalProblem [solveLinearSystem:/home/m.giraud/DUMUXexud/dumux/dumux/nonlinear/newtonsolver.hh:381]: Linear solver did not converge"
73 Newton solver did not converge with dt = 0.099 seconds. Retrying with time step of 0.0495 seconds
73 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
18 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
18: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 14
18: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 6.95e-06
18: communicator.hh::sendRecv,MPI_Testany 0 1 -32766 0 0 1.535e-05
18: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 0 1 -32766 0 0
18: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 14
18: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 0 1 -32766 0 0
18: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 14
18: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 0 1 -32766 0 0
18: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 14
18: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 0 1 -32766 0 0
18: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 14
18: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 0 1 -32766 0 0
18: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 14
18: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 0 1 -32766 0 0
18: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 14
18: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 0 1 -32766 0 0
18: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 14
18: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 0 1 -32766 0 0
18: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 14
18: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 0 1 -32766 0 0
18: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 14
18: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 0 1 -32766 0 0
18: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 10 14
18: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 0 1 -32766 0 0
18: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 11 14
18: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 0 1 -32766 0 0
18: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 12 14
18: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 0 1 -32766 0 0
18: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 13 14
18: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 0 1 -32766 0 0
Newton::solveLinearSystem : Caught exception from the linear solver when getting converged: "BCRSMatrixError [operator++:/home/m.giraud/DUMUXexud/dune-istl/dune/istl/bcrsmatrix.hh:940]: matrix already built up"
18 converged value: 0 121 1 let s go get convergedRemote
18 to  comm_.min(converged) 
18 did  comm_.min(converged) convergedRemote: 0
18 final convergedRemote: 0
18 about to throw Linear solver did not converge
18 Newton::solveLinearSystem : Caught exception: "NumericalProblem [solveLinearSystem:/home/m.giraud/DUMUXexud/dumux/dumux/nonlinear/newtonsolver.hh:381]: Linear solver did not converge when getting convergedRemote "
18 Newton: Caught exception: "NumericalProblem [solveLinearSystem:/home/m.giraud/DUMUXexud/dumux/dumux/nonlinear/newtonsolver.hh:381]: Linear solver did not converge"
18 Newton solver did not converge with dt = 0.099 seconds. Retrying with time step of 0.0495 seconds
18 Assemble: r(x^k) =53 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
53: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 17
53: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 2.3e-06
53: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 8.04e-06
53: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 1.102e-05
53: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 1.393e-05
53: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 1.686e-05
53: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 1.966e-05
53: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 2.2311e-05
53: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 2.6351e-05
53: communicator.hh::sendRecv,MPI_Testany 12 1 -32766 0 0 2.9101e-05
53: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 12 1 -32766 0 0
53: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 17
53: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 12 1 -32766 0 0
53: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 17
53: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 12 1 -32766 0 0
53: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 17
53: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 12 1 -32766 0 0
53: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 17
53: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 12 1 -32766 0 0
53: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 17
53: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 12 1 -32766 0 0
53: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 17
53: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 12 1 -32766 0 0
53: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 17
53: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 12 1 -32766 0 0
53: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 17
53: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 12 1 -32766 0 0
53: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 17
53: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 12 1 -32766 0 0
53: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 10 17
53: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 12 1 -32766 0 0
53: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 11 17
53: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 12 1 -32766 0 0
53: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 12 17
53: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 12 1 -32766 0 0
53: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 13 17
53: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 12 1 -32766 0 0
53: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 14 17
53: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 12 1 -32766 0 0
53: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 15 17
53: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 12 1 -3231 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
31: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 10
31: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 1.72e-06
31: communicator.hh::sendRecv,MPI_Testany 6 1 -32766 0 0 7.471e-06
31: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 6 1 -32766 0 0
31: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 10
31: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 6 1 -32766 0 0
31: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 10
31: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 6 1 -32766 0 0
31: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 10
31: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 6 1 -32766 0 0
31: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 10
31: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 6 1 -32766 0 0
31: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 10
31: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 6 1 -32766 0 0
31: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 10
31: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 6 1 -32766 0 0
31: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 10
31: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 6 1 -32766 0 0
31: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 10
31: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 6 1 -32766 0 0
31: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 10
31: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 6 1 -32766 0 0
Newton::solveLinearSystem : Caught exception from the linear solver when getting converged: "BCRSMatrixError [operator++:/home/m.giraud/DUMUXexud/dune-istl/dune/istl/bcrsmatrix.hh:940]: matrix already built up"
31 converged value: 0 121 1 let s go get convergedRemote
31 to  comm_.min(converged) 
31 did  comm_.min(converged) convergedRemote: 0
31 final convergedRemote: 0
31 about to throw Linear solver did not converge
31 Newton::solveLinearSystem : Caught exception: "NumericalProblem [solveLinearSystem:/home/m.giraud/DUMUXexud/dumux/dumux/nonlinear/newtonsolver.hh:381]: Linear solver did not converge when getting convergedRemote "
31 Newton: Caught exception: "NumericalProblem [solveLinearSystem:/home/m.giraud/DUMUXexud/dumux/dumux/nonlinear/newtonsolver.hh:381]: Linear solver did not converge"
31 Newton solver did not converge with dt = 0.099 seconds. Retrying with time step of 0.0495 seconds
31 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
96 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
96: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 12
96: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 2.85e-06
96: communicator.hh::sendRecv,MPI_Testany 6 1 -32766 0 0 9.26e-06
96: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 6 1 -32766 0 0
96: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 12
96: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 6 1 -32766 0 0
96: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 12
96: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 6 1 -32766 0 0
96: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 12
96: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 6 1 -32766 0 0
96: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 12
96: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 6 1 -32766 0 0
96: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 12
96: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 6 1 -32766 0 0
96: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 12
96: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 6 1 -32766 0 0
96: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 12
96: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 6 1 -32766 0 0
96: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 12
96: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 6 1 -32766 0 0
96: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 12
96: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 6 1 -32766 0 0
96: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 10 12
96: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 6 1 -32766 0 0
96: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 11 12
96: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 6 1 -32766 0 0
Newton::solveLinearSystem : Caught exception from the linear solver when getting converged: "BCRSMatrixError [operator++:/home/m.giraud/DUMUXexud/dune-istl/dune/istl/bcrsmatrix.hh:940]: matrix already built up"
96 converged value: 0 121 1 let s go get convergedRemote
96 to  comm_.min(converged) 
96 did  comm_.min(converged) convergedRemote: 0
96 final convergedRemote: 0
96 about to throw Linear solver did not converge
96 Newton::solveLinearSystem : Caught exception: "NumericalProblem [solveLinearSystem:/home/m.giraud/DUMUXexud/dumux/dumux/nonlinear/newtonsolver.hh:381]: Linear solver did not converge when getting convergedRemote "
96 Newton: Caught exception: "NumericalProblem [solveLinearSystem:/home/m.giraud/DUMUXexud/dumux/dumux/nonlinear/newtonsolver.hh:381]: Linear solver did not converge"
96 Newton solver did not converge with dt = 0.099 seconds. Retrying with time step of 0.0495 seconds
96 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
47 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
47: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 6
47: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 8.7e-07
47: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000106641
47: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000109881
47: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000114961
47: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000222492
47: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000225852
47: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000235073
47: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000238503
47: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000241563
47: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000386504
47: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000389554
47: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000402975
47: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000406335
47: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000420705
47: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000424205
47: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000646798
47: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000663818
47: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000666868
47: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000681748
47: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000685128
47: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000704798
47: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000708208
47: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000728859
47: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000731839
47: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000748559
47: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000751739
47: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.00086196
47: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.00086497
47: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.00122
47: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.00125716
47: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.00125997
47: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.00126274
47: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.00126549
47: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.00126814
47: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.00127092
47: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.00127386
47: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.00127681
47: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.00127961
47: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.00128238
47: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.00128547
47: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.00128812
47: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.00129117
47: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.00129399
47: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.00129675
47: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.00129941
47: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.00132928
47: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.00133232
47: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.00133496
47: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.00133783
47: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.00134068
47: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.00134346
47: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.0013461
47: communicator.hh::se22 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
22: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 20
22: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 7.57e-06
22: communicator.hh::sendRecv,MPI_Testany 0 1 -32766 0 0 1.5151e-05
22: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 0 1 -32766 0 0
22: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 20
22: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 0 1 -32766 0 0
22: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 20
22: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 0 1 -32766 0 0
22: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 20
22: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 0 1 -32766 0 0
22: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 20
22: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 0 1 -32766 0 0
22: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 20
22: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 0 1 -32766 0 0
22: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 20
22: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 0 1 -32766 0 0
22: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 20
22: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 0 1 -32766 0 0
22: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 20
22: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 0 1 -32766 0 0
22: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 20
22: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 0 1 -32766 0 0
22: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 10 20
22: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 0 1 -32766 0 0
22: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 11 20
22: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 0 1 -32766 0 0
22: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 12 20
22: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 0 1 -32766 0 0
22: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 13 20
22: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 0 1 -32766 0 0
22: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 14 20
22: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 0 1 -32766 0 0
22: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 15 20
22: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 0 1 -32766 0 0
22: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 16 20
22: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 0 1 -32766 0 0
22: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 17 20
22: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 0 1 -32766 0 0
22: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finish84 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
84: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 18
84: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 2.961e-06
84: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 1.1591e-05
84: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 1.5831e-05
84: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 1.9781e-05
84: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 2.3811e-05
84: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 2.7621e-05
84: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 3.1561e-05
84: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 6.4771e-05
84: communicator.hh::sendRecv,MPI_Testany 12 1 -32766 0 0 6.8811e-05
84: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 12 1 -32766 0 0
84: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 18
84: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 12 1 -32766 0 0
84: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 18
84: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 12 1 -32766 0 0
84: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 18
84: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 12 1 -32766 0 0
84: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 18
84: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 12 1 -32766 0 0
84: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 18
84: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 12 1 -32766 0 0
84: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 18
84: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 12 1 -32766 0 0
84: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 18
84: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 12 1 -32766 0 0
84: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 18
84: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 12 1 -32766 0 0
84: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 18
84: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 12 1 -32766 0 0
84: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 10 18
84: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 12 1 -32766 0 0
84: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 11 18
84: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 12 1 -32766 0 0
84: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 12 18
84: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 12 1 -32766 0 0
84: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 13 18
84: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 12 1 -32766 0 0
84: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 14 18
84: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 12 1 -32766 0 0
84: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 15 18
84: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 108 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
108: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 9
108: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 3.7e-06
108: communicator.hh::sendRecv,MPI_Testany 2 1 -32766 0 0 0.000127492
108: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 2 1 -32766 0 0
108: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 9
108: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 2 1 -32766 0 0
108: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 9
108: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 2 1 -32766 0 0
108: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 9
108: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 2 1 -32766 0 0
108: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 9
108: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 2 1 -32766 0 0
108: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 9
108: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 2 1 -32766 0 0
108: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 9
108: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 2 1 -32766 0 0
108: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 9
108: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 2 1 -32766 0 0
108: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 9
108: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 2 1 -32766 0 0
Newton::solveLinearSystem : Caught exception from the linear solver when getting converged: "BCRSMatrixError [operator++:/home/m.giraud/DUMUXexud/dune-istl/dune/istl/bcrsmatrix.hh:940]: matrix already built up"
108 converged value: 0 121 1 let s go get convergedRemote
108 to  comm_.min(converged) 
108 did  comm_.min(converged) convergedRemote: 0
108 final convergedRemote: 0
108 about to throw Linear solver did not converge
108 Newton::solveLinearSystem : Caught exception: "NumericalProblem [solveLinearSystem:/home/m.giraud/DUMUXexud/dumux/dumux/nonlinear/newtonsolver.hh:381]: Linear solver did not converge when getting convergedRemote "
108 Newton: Caught exception: "NumericalProblem [solveLinearSystem:/home/m.giraud/DUMUXexud/dumux/dumux/nonlinear/newtonsolver.hh:381]: Linear solver did not converge"
108 Newton solver did not converge with dt = 0.099 seconds. Retrying with time step of 0.0495 seconds
108 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
44 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
44: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 11
44: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 4.821e-06
44: communicator.hh::sendRecv,MPI_Testany 2 1 -32766 0 0 1.1771e-05
44: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 2 1 -32766 0 0
44: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 11
44: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 2 1 -32766 0 0
44: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 11
44: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 2 1 -32766 0 0
44: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 11
44: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 2 1 -32766 0 0
44: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 11
44: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 2 1 -32766 0 0
44: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 11
44: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 2 1 -32766 0 0
44: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 11
44: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 2 1 -32766 0 0
44: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 11
44: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 2 1 -32766 0 0
44: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 11
44: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 2 1 -32766 0 0
44: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 11
44: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 2 1 -32766 0 0
44: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 10 11
44: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 2 1 -32766 0 0
Newton::solveLinearSystem : Caught exception from the linear solver when getting converged: "BCRSMatrixError [operator++:/home/m.giraud/DUMUXexud/dune-istl/dune/istl/bcrsmatrix.hh:940]: matrix already built up"
44 converged value: 0 121 1 let s go get convergedRemote
44 to  comm_.min(converged) 
44 did  comm_.min(converged) convergedRemote: 0
44 final convergedRemote: 0
44 about to throw Linear solver did not converge
44 Newton::solveLinearSystem : Caught exception: "NumericalProblem [solveLinearSystem:/home/m.giraud/DUMUXexud/dumux/dumux/nonlinear/newtonsolver.hh:381]: Linear solver did not converge when getting convergedRemote "
44 Newton: Caught exception: "NumericalProblem [solveLinearSystem:/home/m.giraud/DUMUXexud/dumux/dumux/nonlinear/newtonsolver.hh:381]: Linear solver did not converge"
44 Newton solver did not converge with dt = 0.099 seconds. Retrying with time step of 0.0495 seconds
44 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
4 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 14
4: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 3.25e-06
4: communicator.hh::sendRecv,MPI_Testany 0 1 -32766 0 0 1.002e-05
4: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 0 1 -32766 0 0
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 14
4: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 0 1 -32766 0 0
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 14
4: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 0 1 -32766 0 0
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 14
4: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 0 1 -32766 0 0
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 14
4: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 0 1 -32766 0 0
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 14
4: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 0 1 -32766 0 0
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 14
4: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 0 1 -32766 0 0
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 14
4: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 0 1 -32766 0 0
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 14
4: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 0 1 -32766 0 0
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 14
4: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 0 1 -32766 0 0
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 10 14
4: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 0 1 -32766 0 0
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 11 14
4: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 0 1 -32766 0 0
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 12 14
4: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 0 1 -32766 0 0
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 13 14
4: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 0 1 -32766 0 0
Newton::solveLinearSystem : Caught exception from the linear solver when getting converged: "BCRSMatrixError [operator++:/home/m.giraud/DUMUXexud/dune-istl/dune/istl/bcrsmatrix.hh:940]: matrix already built up"
4 converged value: 0 121 1 let s go get convergedRemote
4 to  comm_.min(converged) 
4 did  comm_.min(converged) convergedRemote: 0
4 final convergedRemote: 0
4 about to throw Linear solver did not converge
4 Newton::solveLinearSystem : Caught exception: "NumericalProblem [solveLinearSystem:/home/m.giraud/DUMUXexud/dumux/dumux/nonlinear/newtonsolver.hh:381]: Linear solver did not converge when getting convergedRemote "
4 Newton: Caught exception: "NumericalProblem [solveLinearSystem:/home/m.giraud/DUMUXexud/dumux/dumux/nonlinear/newtonsolver.hh:381]: Linear solver did not converge"
4 Newton solver did not converge with dt = 0.099 seconds. Retrying with time step of 0.0495 seconds
4 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
45 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
45: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 17
45: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 1.34e-06
45: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 7.5e-06
45: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 1.055e-05
45: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 1.341e-05
45: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 1.632e-05
45: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 1.908e-05
45: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 2.175e-05
45: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 2.479e-05
45: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 2.749e-05
45: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 3.019e-05
45: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 3.283e-05
45: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 3.551e-05
45: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 3.816e-05
45: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 4.08e-05
45: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 4.347e-05
45: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 4.614e-05
45: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 4.906e-05
45: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 7.1321e-05
45: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 7.4731e-05
45: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 7.7561e-05
45: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 8.0241e-05
45: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 8.2871e-05
45: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 8.5651e-05
45: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 8.8431e-05
45: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 9.1081e-05
45: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 9.3891e-05
45: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 9.6681e-05
45: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 9.9321e-05
45: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000259573
45: communicator.hh::sendRecv,MPI_Testany 3 1 -32766 0 0 0.0008384
45: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 3 1 -32766 0 0
45: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 17
45: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 3 1 -32766 0 0
45: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 17
45: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 3 1 -32766 0 0
45: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 17
45: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 3 1 -32766 0 0
45: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 17
45: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 3 1 -32766 0 0
45: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 17
45: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 3 1 -32766 0 0
45: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 17
45: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 3 1 -32766 0 0
45: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 17
45: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 3 1 -32766 0 0
45: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 17
45: communicator.hh::senNewton::solveLinearSystem : Caught exception from the linear solver when getting converged: "BCRSMatrixError [operator++:/home/m.giraud/DUMUXexud/dune-istl/dune/istl/bcrsmatrix.hh:940]: matrix already built up"
50 converged value: 0 121 1 let s go get convergedRemote
50 to  comm_.min(converged) 
50 did  comm_.min(converged) convergedRemote: 0
50 final convergedRemote: 0
50 about to throw Linear solver did not converge
50 Newton::solveLinearSystem : Caught exception: "NumericalProblem [solveLinearSystem:/home/m.giraud/DUMUXexud/dumux/dumux/nonlinear/newtonsolver.hh:381]: Linear solver did not converge when getting convergedRemote "
50 Newton: Caught exception: "NumericalProblem [solveLinearSystem:/home/m.giraud/DUMUXexud/dumux/dumux/nonlinear/newtonsolver.hh:381]: Linear solver did not converge"
50 Newton solver did not converge with dt = 0.099 seconds. Retrying with time step of 0.0495 seconds
50 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
109 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
109: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 20
109: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 7.7e-06
109: communicator.hh::sendRecv,MPI_Testany 1 1 -32766 0 0 1.425e-05
109: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 1 1 -32766 0 0
109: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 20
109: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 1 1 -32766 0 0
109: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 20
109: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 1 1 -32766 0 0
109: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 20
109: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 1 1 -32766 0 0
109: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 20
109: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 1 1 -32766 0 0
109: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 20
109: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 1 1 -32766 0 0
109: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 20
109: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 1 1 -32766 0 0
109: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 20
109: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 1 1 -32766 0 0
109: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 20
109: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 1 1 -32766 0 0
109: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 20
109: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 1 1 -32766 0 0
109: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 10 20
109: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 1 1 -32766 0 0
109: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 11 20
109: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 1 1 -32766 0 0
109: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 12 20
109: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 1 1 -32766 0 0
109: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 13 20
109: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 1 1 -32766 0 0
109: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 14 20
109: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 1 1 -32766 0 0
109: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 15 20
109: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 1 1 -32766 0 0
109: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 16 20
109: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 1 1 -32766 0 0
109: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 17 20
109: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 1 1 -32766 0 0
109: communicator.hh::sendRecv, before MPI_Waitany(messageInfo71 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
71: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 20
71: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 5.02e-06
71: communicator.hh::sendRecv,MPI_Testany 3 1 -32766 0 0 1.128e-05
71: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 3 1 -32766 0 0
71: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 20
71: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 3 1 -32766 0 0
71: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 20
71: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 3 1 -32766 0 0
71: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 20
71: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 3 1 -32766 0 0
71: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 20
71: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 3 1 -32766 0 0
71: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 20
71: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 3 1 -32766 0 0
71: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 20
71: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 3 1 -32766 0 0
71: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 20
71: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 3 1 -32766 0 0
71: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 20
71: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 3 1 -32766 0 0
71: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 20
71: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 3 1 -32766 0 0
71: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 10 20
71: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 3 1 -32766 0 0
71: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 11 20
71: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 3 1 -32766 0 0
71: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 12 20
71: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 3 1 -32766 0 0
71: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 13 20
71: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 3 1 -32766 0 0
71: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 14 20
71: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 3 1 -32766 0 0
71: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 15 20
71: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 3 1 -32766 0 0
71: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 16 20
71: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 3 1 -32766 0 0
71: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 17 20
71: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 3 1 -32766 0 0
71: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finishe62 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
62: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 15
62: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 1.64e-06
62: communicator.hh::sendRecv,MPI_Testany 0 1 -32766 0 0 7.64e-06
62: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 0 1 -32766 0 0
62: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 15
62: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 0 1 -32766 0 0
62: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 15
62: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 0 1 -32766 0 0
62: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 15
62: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 0 1 -32766 0 0
62: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 15
62: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 0 1 -32766 0 0
62: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 15
62: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 0 1 -32766 0 0
62: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 15
62: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 0 1 -32766 0 0
62: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 15
62: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 0 1 -32766 0 0
62: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 15
62: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 0 1 -32766 0 0
62: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 15
62: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 0 1 -32766 0 0
62: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 10 15
62: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 0 1 -32766 0 0
62: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 11 15
62: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 0 1 -32766 0 0
62: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 12 15
62: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 0 1 -32766 0 0
62: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 13 15
62: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 0 1 -32766 0 0
62: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 14 15
62: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 0 1 -32766 0 0
Newton::solveLinearSystem : Caught exception from the linear solver when getting converged: "BCRSMatrixError [operator++:/home/m.giraud/DUMUXexud/dune-istl/dune/istl/bcrsmatrix.hh:940]: matrix already built up"
62 converged value: 0 121 1 let s go get convergedRemote
62 to  comm_.min(converged) 
62 did  comm_.min(converged) convergedRemote: 0
62 final convergedRemote: 0
62 about to throw Linear solver did not converge
62 Newton::solveLinearSystem : Caught exception: "NumericalProblem [solveLinearSystem:/home/m.giraud/DUMUXexud/dumux/dumux/nonlinear/newtonsolver.hh:381]: Linear solver did not converge when getting convergedRemote "
62 Newton: Caught exception: "NumericalProblem [solveLinearSystem:/home/m.giraud/DU17 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
17: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 3
17: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 4.5e-07
17: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000132761
17: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000139431
17: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000229503
17: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000232743
17: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000235513
17: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000238123
17: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000240753
17: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000243353
17: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000245973
17: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000248773
17: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000251503
17: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000254113
17: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000257003
17: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000259623
17: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000262243
17: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000265033
17: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000267743
17: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000270363
17: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000272993
17: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000275603
17: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000278213
17: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000281013
17: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000283753
17: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000286373
17: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000288983
17: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000291593
17: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000294223
17: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000297243
17: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000300143
17: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000302763
17: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000305384
17: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000307994
17: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000310614
17: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000313384
17: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000316134
17: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000318764
17: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000321374
17: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000323994
17: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000326604
17: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000329394
17: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000337044
17: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000340174
17: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000343094
17: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000449975
17: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000462005
17: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000465076
17: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000474026
17: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000622398
17: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000625198
17: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000627938
17: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.00061 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
61: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 9
61: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 1.27e-06
61: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 4.0041e-05
61: communicator.hh::sendRecv,MPI_Testany 0 1 -32766 0 0 4.3131e-05
61: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 0 1 -32766 0 0
61: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 9
61: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 0 1 -32766 0 0
61: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 9
61: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 0 1 -32766 0 0
61: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 9
61: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 0 1 -32766 0 0
61: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 9
61: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 0 1 -32766 0 0
61: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 9
61: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 0 1 -32766 0 0
61: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 9
61: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 0 1 -32766 0 0
61: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 9
61: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 0 1 -32766 0 0
61: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 9
61: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 0 1 -32766 0 0
Newton::solveLinearSystem : Caught exception from the linear solver when getting converged: "BCRSMatrixError [operator++:/home/m.giraud/DUMUXexud/dune-istl/dune/istl/bcrsmatrix.hh:940]: matrix already built up"
61 converged value: 0 121 1 let s go get convergedRemote
61 to  comm_.min(converged) 
61 did  comm_.min(converged) convergedRemote: 0
61 final convergedRemote: 0
61 about to throw Linear solver did not converge
61 Newton::solveLinearSystem : Caught exception: "NumericalProblem [solveLinearSystem:/home/m.giraud/DUMUXexud/dumux/dumux/nonlinear/newtonsolver.hh:381]: Linear solver did not converge when getting convergedRemote "
61 Newton: Caught exception: "NumericalProblem [solveLinearSystem:/home/m.giraud/DUMUXexud/dumux/dumux/nonlinear/newtonsolver.hh:381]: Linear solver did not converge"
61 Newton solver did not converge with dt = 0.099 seconds. Retrying with time step of 0.0495 seconds
61 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
89 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
89: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 11
89: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 2.1e-06
89: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 8.291e-06
89: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 1.1391e-05
89: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 1.4631e-05
89: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 5.0851e-05
89: communicator.hh::sendRecv,MPI_Testany 10 1 -32766 0 0 5.4121e-05
89: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 10 1 -32766 0 0
89: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 11
89: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 10 1 -32766 0 0
89: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 11
89: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 10 1 -32766 0 0
89: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 11
89: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 10 1 -32766 0 0
89: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 11
89: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 10 1 -32766 0 0
89: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 11
89: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 10 1 -32766 0 0
89: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 11
89: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 10 1 -32766 0 0
89: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 11
89: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 10 1 -32766 0 0
89: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 11
89: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 10 1 -32766 0 0
89: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 11
89: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 10 1 -32766 0 0
89: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 10 11
89: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 10 1 -32766 0 0
Newton::solveLinearSystem : Caught exception from the linear solver when getting converged: "BCRSMatrixError [operator++:/home/m.giraud/DUMUXexud/dune-istl/dune/istl/bcrsmatrix.hh:940]: matrix already built up"
89 converged value: 0 121 1 let s go get convergedRemote
89 to  comm_.min(converged) 
89 did  comm_.min(converged) convergedRemote: 0
89 final convergedRemote: 0
89 about to throw Linear solver did not converge
89 Newton::solveLinearSystem : Caught exception: "NumericalProblem [solveLinearSystem:/home/m.giraud/DUMUXexud/dumux/dumux/nonlinear/newtonsolver.hh:381]: Linear solver did not converge when getting convergedRemote "
89 Newton: Caught exception: "NumericalProblem [solveLinearSystem:/home/m.giraud/DUMUXexud/dumux/dumux/nonlinear/newtonsolver.hh:381]: Linear solver did not converge"
89 Newton solver did not converge with dt = 0.099 seconds. Retrying with time step of 0.0495 seconds
89 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
102 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
102: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 15
102: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 2.36e-06
102: communicator.hh::sendRecv,MPI_Testany 10 1 -32766 0 0 8.88e-06
102: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 10 1 -32766 0 0
102: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 15
102: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 10 1 -32766 0 0
102: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 15
102: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 10 1 -32766 0 0
102: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 15
102: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 10 1 -32766 0 0
102: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 15
102: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 10 1 -32766 0 0
102: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 15
102: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 10 1 -32766 0 0
102: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 15
102: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 10 1 -32766 0 0
102: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 15
102: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 10 1 -32766 0 0
102: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 15
102: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 10 1 -32766 0 0
102: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 15
102: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 10 1 -32766 0 0
102: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 10 15
102: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 10 1 -32766 0 0
102: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 11 15
102: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 10 1 -32766 0 0
102: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 12 15
102: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 10 1 -32766 0 0
102: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 13 15
102: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 10 1 -32766 0 0
102: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 14 15
102: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 10 1 -32766 0 0
Newton::solveLinearSystem : Caught exception from the linear solver when getting converged: "BCRSMatrixError [operator++:/home/m.giraud/DUMUXexud/dune-istl/dune/istl/bcrsmatrix.hh:940]: matrix already built up"
102 converged value: 0 121 1 let s go get convergedRemote
102 to  comm_.min(converged) 
102 did  comm_.min(converged) convergedRemote: 0
102 final convergedRemote: 0
102 about to throw Linear solver did not converge
102 Newton::solveLinearSystem : Caught exception: "NumericalProblem [solveLinearSystem:/home/m.giraud/DUMUXexud/dumux/dumux/nonlinear/newtonsolver.hh:381]: Linear solver did not converge when getting convergedRemote "
102 Newton: Caught exception78 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
78: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 15
78: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 1.84e-06
78: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 7.62e-06
78: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 1.077e-05
78: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 5.5871e-05
78: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 5.9501e-05
78: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 6.2341e-05
78: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 6.5111e-05
78: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 6.7881e-05
78: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 7.0511e-05
78: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 7.3381e-05
78: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 7.6281e-05
78: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 8.0311e-05
78: communicator.hh::sendRecv,MPI_Testany 10 1 -32766 0 0 8.3081e-05
78: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 10 1 -32766 0 0
78: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 15
78: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 10 1 -32766 0 0
78: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 15
78: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 10 1 -32766 0 0
78: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 15
78: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 10 1 -32766 0 0
78: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 15
78: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 10 1 -32766 0 0
78: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 15
78: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 10 1 -32766 0 0
78: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 15
78: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 10 1 -32766 0 0
78: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 15
78: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 10 1 -32766 0 0
78: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 15
78: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 10 1 -32766 0 0
78: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 15
78: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 10 1 -32766 0 0
78: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 10 15
78: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 10 1 -32766 0 0
78: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 11 15
78: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 10 1 -32766 0 0
78: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 12 15
78: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 10 1 -32766 0 0
78: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 13 15
78: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 10 1 -32766 0 0
78: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status);9 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
9: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 7
9: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 1.02e-06
9: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 2.529e-05
9: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 2.912e-05
9: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 5.1671e-05
9: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 5.5121e-05
9: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 7.3151e-05
9: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 7.6481e-05
9: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000187442
9: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000542037
9: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000545467
9: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000582477
9: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000587188
9: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000590648
9: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000594058
9: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000597178
9: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000600258
9: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000603338
9: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000606408
9: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000609918
9: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000613138
9: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000616208
9: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000619308
9: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000623068
9: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000626298
9: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000629328
9: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000665889
9: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000670019
9: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000673559
9: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000676849
9: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000680069
9: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000683259
9: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000686459
9: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000689809
9: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000693119
9: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000696249
9: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000699579
9: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000702699
9: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000705819
9: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000708859
9: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000712219
9: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000715519
9: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000718749
9: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000722039
9: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000725169
9: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000728399
9: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.00075085
9: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.00075455
9: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.0007576
9: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.00076067
9: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.00076371
9: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.0007668
9: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.00076992
9: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.72 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
72: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 20
72: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 3.71e-06
72: communicator.hh::sendRecv,MPI_Testany 0 1 -32766 0 0 9.31e-06
72: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 0 1 -32766 0 0
72: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 20
72: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 0 1 -32766 0 0
72: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 20
72: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 0 1 -32766 0 0
72: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 20
72: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 0 1 -32766 0 0
72: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 20
72: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 0 1 -32766 0 0
72: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 20
72: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 0 1 -32766 0 0
72: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 20
72: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 0 1 -32766 0 0
72: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 20
72: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 0 1 -32766 0 0
72: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 20
72: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 0 1 -32766 0 0
72: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 20
72: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 0 1 -32766 0 0
72: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 10 20
72: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 0 1 -32766 0 0
72: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 11 20
72: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 0 1 -32766 0 0
72: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 12 20
72: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 0 1 -32766 0 0
72: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 13 20
72: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 0 1 -32766 0 0
72: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 14 20
72: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 0 1 -32766 0 0
72: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 15 20
72: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 0 1 -32766 0 0
72: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 16 20
72: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 0 1 -32766 0 0
72: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 17 20
72: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 0 1 -32766 0 0
72: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished41 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
41: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 20
41: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 5.07e-06
41: communicator.hh::sendRecv,MPI_Testany 3 1 -32766 0 0 1.189e-05
41: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 3 1 -32766 0 0
41: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 20
41: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 3 1 -32766 0 0
41: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 20
41: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 3 1 -32766 0 0
41: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 20
41: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 3 1 -32766 0 0
41: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 20
41: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 3 1 -32766 0 0
41: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 20
41: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 3 1 -32766 0 0
41: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 20
41: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 3 1 -32766 0 0
41: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 20
41: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 3 1 -32766 0 0
41: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 20
41: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 3 1 -32766 0 0
41: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 20
41: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 3 1 -32766 0 0
41: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 10 20
41: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 3 1 -32766 0 0
41: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 11 20
41: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 3 1 -32766 0 0
41: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 12 20
41: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 3 1 -32766 0 0
41: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 13 20
41: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 3 1 -32766 0 0
41: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 14 20
41: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 3 1 -32766 0 0
41: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 15 20
41: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 3 1 -32766 0 0
41: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 16 20
41: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 3 1 -32766 0 0
41: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 17 20
41: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 3 1 -32766 0 0
41: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finishe63 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
63: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 9
63: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 2.63e-06
63: communicator.hh::sendRecv,MPI_Testany 4 1 -32766 0 0 4.0411e-05
63: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 4 1 -32766 0 0
63: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 9
63: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 4 1 -32766 0 0
63: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 9
63: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 4 1 -32766 0 0
63: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 9
63: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 4 1 -32766 0 0
63: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 9
63: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 4 1 -32766 0 0
63: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 9
63: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 4 1 -32766 0 0
63: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 9
63: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 4 1 -32766 0 0
63: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 9
63: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 4 1 -32766 0 0
63: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 9
63: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 4 1 -32766 0 0
Newton::solveLinearSystem : Caught exception from the linear solver when getting converged: "BCRSMatrixError [operator++:/home/m.giraud/DUMUXexud/dune-istl/dune/istl/bcrsmatrix.hh:940]: matrix already built up"
63 converged value: 0 121 1 let s go get convergedRemote
63 to  comm_.min(converged) 
63 did  comm_.min(converged) convergedRemote: 0
63 final convergedRemote: 0
63 about to throw Linear solver did not converge
63 Newton::solveLinearSystem : Caught exception: "NumericalProblem [solveLinearSystem:/home/m.giraud/DUMUXexud/dumux/dumux/nonlinear/newtonsolver.hh:381]: Linear solver did not converge when getting convergedRemote "
63 Newton: Caught exception: "NumericalProblem [solveLinearSystem:/home/m.giraud/DUMUXexud/dumux/dumux/nonlinear/newtonsolver.hh:381]: Linear solver did not converge"
63 Newton solver did not converge with dt = 0.099 seconds. Retrying with time step of 0.0495 seconds
63 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
 dS/dt + div F - q;   M = grad r
766 0 0
53: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 16 17
53: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 12 1 -32766 0 0
Newton::solveLinearSystem : Caught exception from the linear solver when getting converged: "BCRSMatrixError [operator++:/home/m.giraud/DUMUXexud/dune-istl/dune/istl/bcrsmatrix.hh:940]: matrix already built up"
53 converged value: 0 121 1 let s go get convergedRemote
53 to  comm_.min(converged) 
53 did  comm_.min(converged) convergedRemote: 0
53 final convergedRemote: 0
53 about to throw Linear solver did not converge
53 Newton::solveLinearSystem : Caught exception: "NumericalProblem [solveLinearSystem:/home/m.giraud/DUMUXexud/dumux/dumux/nonlinear/newtonsolver.hh:381]: Linear solver did not converge when getting convergedRemote "
53 Newton: Caught exception: "NumericalProblem [solveLinearSystem:/home/m.giraud/DUMUXexud/dumux/dumux/nonlinear/newtonsolver.hh:381]: Linear solver did not converge"
53 Newton solver did not converge with dt = 0.099 seconds. Retrying with time step of 0.0495 seconds
53 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
ndRecv,MPI_Testany -32766 0 -32766 0 0 0.00134876
47: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.00135143
47: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.00135406
47: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.0013567
47: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.00135949
47: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.00136223
47: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.00136486
47: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.00136749
47: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.00137013
47: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.00137279
47: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.00137542
47: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.00137832
47: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.00138103
47: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.00138365
47: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.00138629
47: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.00138892
47: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.00139155
47: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.0013942
47: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.00142039
47: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.00142342
47: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.00142606
47: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.00142869
47: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.00143132
47: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.00143395
47: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.0014367
47: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.00143951
47: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.00144223
47: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.00144487
47: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.0014475
47: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.00145013
47: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.00145275
47: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.00145538
47: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.00145832
47: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.00148589
47: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.00148904
47: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.00207744
47: communicator.hh::sendRecv,MPI_Testany 0 1 -32766 0 0 0.00223069
47: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 0 1 -32766 0 0
47: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 6
47: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 0 1 -32766 0 0
47: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 6
47: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 0 1 -32766 0 0
47: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 6
47: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 0 1 -32766 0 0
47: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 6
47: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 0 1 -32766 0 0
47: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 6
47: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 0 1 -32766 0 0
Newton::solveLinearSystem : Caught exception from the linear solver when getting converged: "BCRSMatrixError [operator++:/home/m.giraud/DUMUXexud/dune-istl/dune/istl/bcrsmatrix.hh:940]: matrix already built up"
47 converged value: 0 121 1 let s go get convergedRemote
47 to  comm_.min(converged) 
47 did  comm_.mied, &status); 18 20
22: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 0 1 -32766 0 0
22: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 19 20
22: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 0 1 -32766 0 0
Newton::solveLinearSystem : Caught exception from the linear solver when getting converged: "BCRSMatrixError [operator++:/home/m.giraud/DUMUXexud/dune-istl/dune/istl/bcrsmatrix.hh:940]: matrix already built up"
22 converged value: 0 121 1 let s go get convergedRemote
22 to  comm_.min(converged) 
22 did  comm_.min(converged) convergedRemote: 0
22 final convergedRemote: 0
22 about to throw Linear solver did not converge
22 Newton::solveLinearSystem : Caught exception: "NumericalProblem [solveLinearSystem:/home/m.giraud/DUMUXexud/dumux/dumux/nonlinear/newtonsolver.hh:381]: Linear solver did not converge when getting convergedRemote "
22 Newton: Caught exception: "NumericalProblem [solveLinearSystem:/home/m.giraud/DUMUXexud/dumux/dumux/nonlinear/newtonsolver.hh:381]: Linear solver did not converge"
22 Newton solver did not converge with dt = 0.099 seconds. Retrying with time step of 0.0495 seconds
22 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
12 1 -32766 0 0
84: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 16 18
84: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 12 1 -32766 0 0
84: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 17 18
84: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 12 1 -32766 0 0
Newton::solveLinearSystem : Caught exception from the linear solver when getting converged: "BCRSMatrixError [operator++:/home/m.giraud/DUMUXexud/dune-istl/dune/istl/bcrsmatrix.hh:940]: matrix already built up"
84 converged value: 0 121 1 let s go get convergedRemote
84 to  comm_.min(converged) 
84 did  comm_.min(converged) convergedRemote: 0
84 final convergedRemote: 0
84 about to throw Linear solver did not converge
84 Newton::solveLinearSystem : Caught exception: "NumericalProblem [solveLinearSystem:/home/m.giraud/DUMUXexud/dumux/dumux/nonlinear/newtonsolver.hh:381]: Linear solver did not converge when getting convergedRemote "
84 Newton: Caught exception: "NumericalProblem [solveLinearSystem:/home/m.giraud/DUMUXexud/dumux/dumux/nonlinear/newtonsolver.hh:381]: Linear solver did not converge"
84 Newton solver did not converge with dt = 0.099 seconds. Retrying with time step of 0.0495 seconds
84 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
dRecv, before assert(finished != MPI_UNDEFINED); 3 1 -32766 0 0
45: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 17
45: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 3 1 -32766 0 0
45: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 10 17
45: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 3 1 -32766 0 0
45: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 11 17
45: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 3 1 -32766 0 0
45: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 12 17
45: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 3 1 -32766 0 0
45: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 13 17
45: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 3 1 -32766 0 0
45: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 14 17
45: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 3 1 -32766 0 0
45: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 15 17
45: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 3 1 -32766 0 0
45: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 16 17
45: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 3 1 -32766 0 0
Newton::solveLinearSystem : Caught exception from the linear solver when getting converged: "BCRSMatrixError [operator++:/home/m.giraud/DUMUXexud/dune-istl/dune/istl/bcrsmatrix.hh:940]: matrix already built up"
45 converged value: 0 121 1 let s go get convergedRemote
45 to  comm_.min(converged) 
45 did  comm_.min(converged) convergedRemote: 0
45 final convergedRemote: 0
45 about to throw Linear solver did not converge
45 Newton::solveLinearSystem : Caught exception: "NumericalProblem [solveLinearSystem:/home/m.giraud/DUMUXexud/dumux/dumux/nonlinear/newtonsolver.hh:381]: Linear solver did not converge when getting convergedRemote "
45 Newton: Caught exception: "NumericalProblem [solveLinearSystem:/home/m.giraud/DUMUXexud/dumux/dumux/nonlinear/newtonsolver.hh:381]: Linear solver did not converge"
45 Newton solver did not converge with dt = 0.099 seconds. Retrying with time step of 0.0495 seconds
45 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
rmation_.size(), recvRequests, &finished, &status); 18 20
109: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 1 1 -32766 0 0
109: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 19 20
109: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 1 1 -32766 0 0
Newton::solveLinearSystem : Caught exception from the linear solver when getting converged: "BCRSMatrixError [operator++:/home/m.giraud/DUMUXexud/dune-istl/dune/istl/bcrsmatrix.hh:940]: matrix already built up"
109 converged value: 0 121 1 let s go get convergedRemote
109 to  comm_.min(converged) 
109 did  comm_.min(converged) convergedRemote: 0
109 final convergedRemote: 0
109 about to throw Linear solver did not converge
109 Newton::solveLinearSystem : Caught exception: "NumericalProblem [solveLinearSystem:/home/m.giraud/DUMUXexud/dumux/dumux/nonlinear/newtonsolver.hh:381]: Linear solver did not converge when getting convergedRemote "
109 Newton: Caught exception: "NumericalProblem [solveLinearSystem:/home/m.giraud/DUMUXexud/dumux/dumux/nonlinear/newtonsolver.hh:381]: Linear solver did not converge"
109 Newton solver did not converge with dt = 0.099 seconds. Retrying with time step of 0.0495 seconds
109 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
d, &status); 18 20
71: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 3 1 -32766 0 0
71: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 19 20
71: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 3 1 -32766 0 0
Newton::solveLinearSystem : Caught exception from the linear solver when getting converged: "BCRSMatrixError [operator++:/home/m.giraud/DUMUXexud/dune-istl/dune/istl/bcrsmatrix.hh:940]: matrix already built up"
71 converged value: 0 121 1 let s go get convergedRemote
71 to  comm_.min(converged) 
71 did  comm_.min(converged) convergedRemote: 0
71 final convergedRemote: 0
71 about to throw Linear solver did not converge
71 Newton::solveLinearSystem : Caught exception: "NumericalProblem [solveLinearSystem:/home/m.giraud/DUMUXexud/dumux/dumux/nonlinear/newtonsolver.hh:381]: Linear solver did not converge when getting convergedRemote "
71 Newton: Caught exception: "NumericalProblem [solveLinearSystem:/home/m.giraud/DUMUXexud/dumux/dumux/nonlinear/newtonsolver.hh:381]: Linear solver did not converge"
71 Newton solver did not converge with dt = 0.099 seconds. Retrying with time step of 0.0495 seconds
71 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
MUXexud/dumux/dumux/nonlinear/newtonsolver.hh:381]: Linear solver did not converge"
62 Newton solver did not converge with dt = 0.099 seconds. Retrying with time step of 0.0495 seconds
62 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
630598
17: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000644608
17: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000647368
17: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000650438
17: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000749519
17: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000884841
17: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000887851
17: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000906231
17: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000909091
17: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000929181
17: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000950582
17: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000953582
17: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000973782
17: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000977032
17: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.00108775
17: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.00131217
17: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.00131507
17: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.00147169
17: communicator.hh::sendRecv,MPI_Testany 0 1 -32766 0 0 0.00147467
17: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 0 1 -32766 0 0
17: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 3
17: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 0 1 -32766 0 0
17: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 3
17: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 0 1 -32766 0 0
Newton::solveLinearSystem : Caught exception from the linear solver when getting converged: "BCRSMatrixError [operator++:/home/m.giraud/DUMUXexud/dune-istl/dune/istl/bcrsmatrix.hh:940]: matrix already built up"
17 converged value: 0 121 1 let s go get convergedRemote
17 to  comm_.min(converged) 
17 did  comm_.min(converged) convergedRemote: 0
17 final convergedRemote: 0
17 about to throw Linear solver did not converge
17 Newton::solveLinearSystem : Caught exception: "NumericalProblem [solveLinearSystem:/home/m.giraud/DUMUXexud/dumux/dumux/nonlinear/newtonsolver.hh:381]: Linear solver did not converge when getting convergedRemote "
17 Newton: Caught exception: "NumericalProblem [solveLinearSystem:/home/m.giraud/DUMUXexud/dumux/dumux/nonlinear/newtonsolver.hh:381]: Linear solver did not converge"
17 Newton solver did not converge with dt = 0.099 seconds. Retrying with time step of 0.0495 seconds
17 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
: "NumericalProblem [solveLinearSystem:/home/m.giraud/DUMUXexud/dumux/dumux/nonlinear/newtonsolver.hh:381]: Linear solver did not converge"
102 Newton solver did not converge with dt = 0.099 seconds. Retrying with time step of 0.0495 seconds
102 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
 14 15
78: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 10 1 -32766 0 0
Newton::solveLinearSystem : Caught exception from the linear solver when getting converged: "BCRSMatrixError [operator++:/home/m.giraud/DUMUXexud/dune-istl/dune/istl/bcrsmatrix.hh:940]: matrix already built up"
78 converged value: 0 121 1 let s go get convergedRemote
78 to  comm_.min(converged) 
78 did  comm_.min(converged) convergedRemote: 0
78 final convergedRemote: 0
78 about to throw Linear solver did not converge
78 Newton::solveLinearSystem : Caught exception: "NumericalProblem [solveLinearSystem:/home/m.giraud/DUMUXexud/dumux/dumux/nonlinear/newtonsolver.hh:381]: Linear solver did not converge when getting convergedRemote "
78 Newton: Caught exception: "NumericalProblem [solveLinearSystem:/home/m.giraud/DUMUXexud/dumux/dumux/nonlinear/newtonsolver.hh:381]: Linear solver did not converge"
78 Newton solver did not converge with dt = 0.099 seconds. Retrying with time step of 0.0495 seconds
78 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
00077328
9: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.00077649
9: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.00077956
9: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.00078257
9: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000936462
9: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.00140349
9: communicator.hh::sendRecv,MPI_Testany 0 1 -32766 0 0 0.00155186
9: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 0 1 -32766 0 0
9: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 7
9: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 0 1 -32766 0 0
9: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 7
9: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 0 1 -32766 0 0
9: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 7
9: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 0 1 -32766 0 0
9: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 7
9: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 0 1 -32766 0 0
9: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 7
9: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 0 1 -32766 0 0
9: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 7
9: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 0 1 -32766 0 0
Newton::solveLinearSystem : Caught exception from the linear solver when getting converged: "BCRSMatrixError [operator++:/home/m.giraud/DUMUXexud/dune-istl/dune/istl/bcrsmatrix.hh:940]: matrix already built up"
9 converged value: 0 121 1 let s go get convergedRemote
9 to  comm_.min(converged) 
9 did  comm_.min(converged) convergedRemote: 0
9 final convergedRemote: 0
9 about to throw Linear solver did not converge
9 Newton::solveLinearSystem : Caught exception: "NumericalProblem [solveLinearSystem:/home/m.giraud/DUMUXexud/dumux/dumux/nonlinear/newtonsolver.hh:381]: Linear solver did not converge when getting convergedRemote "
9 Newton: Caught exception: "NumericalProblem [solveLinearSystem:/home/m.giraud/DUMUXexud/dumux/dumux/nonlinear/newtonsolver.hh:381]: Linear solver did not converge"
9 Newton solver did not converge with dt = 0.099 seconds. Retrying with time step of 0.0495 seconds
9 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
, &status); 18 20
72: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 0 1 -32766 0 0
72: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 19 20
72: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 0 1 -32766 0 0
Newton::solveLinearSystem : Caught exception from the linear solver when getting converged: "BCRSMatrixError [operator++:/home/m.giraud/DUMUXexud/dune-istl/dune/istl/bcrsmatrix.hh:940]: matrix already built up"
72 converged value: 0 121 1 let s go get convergedRemote
72 to  comm_.min(converged) 
72 did  comm_.min(converged) convergedRemote: 0
72 final convergedRemote: 0
72 about to throw Linear solver did not converge
72 Newton::solveLinearSystem : Caught exception: "NumericalProblem [solveLinearSystem:/home/m.giraud/DUMUXexud/dumux/dumux/nonlinear/newtonsolver.hh:381]: Linear solver did not converge when getting convergedRemote "
72 Newton: Caught exception: "NumericalProblem [solveLinearSystem:/home/m.giraud/DUMUXexud/dumux/dumux/nonlinear/newtonsolver.hh:381]: Linear solver did not converge"
72 Newton solver did not converge with dt = 0.099 seconds. Retrying with time step of 0.0495 seconds
72 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
d, &status); 18 20
41: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 3 1 -32766 0 0
41: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 19 20
41: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 3 1 -32766 0 0
Newton::solveLinearSystem : Caught exception from the linear solver when getting converged: "BCRSMatrixError [operator++:/home/m.giraud/DUMUXexud/dune-istl/dune/istl/bcrsmatrix.hh:940]: matrix already built up"
41 converged value: 0 121 1 let s go get convergedRemote
41 to  comm_.min(converged) 
41 did  comm_.min(converged) convergedRemote: 0
41 final convergedRemote: 0
41 about to throw Linear solver did not converge
41 Newton::solveLinearSystem : Caught exception: "NumericalProblem [solveLinearSystem:/home/m.giraud/DUMUXexud/dumux/dumux/nonlinear/newtonsolver.hh:381]: Linear solver did not converge when getting convergedRemote "
41 Newton: Caught exception: "NumericalProblem [solveLinearSystem:/home/m.giraud/DUMUXexud/dumux/dumux/nonlinear/newtonsolver.hh:381]: Linear solver did not converge"
41 Newton solver did not converge with dt = 0.099 seconds. Retrying with time step of 0.0495 seconds
41 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
hed, &status); 18 21
58: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 2 1 -32766 0 0
58: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 19 21
58: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 2 1 -32766 0 0
58: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 20 21
58: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 2 1 -32766 0 0
Newton::solveLinearSystem : Caught exception from the linear solver when getting converged: "BCRSMatrixError [operator++:/home/m.giraud/DUMUXexud/dune-istl/dune/istl/bcrsmatrix.hh:940]: matrix already built up"
58 converged value: 0 121 1 let s go get convergedRemote
58 to  comm_.min(converged) 
58 did  comm_.min(converged) convergedRemote: 0
58 final convergedRemote: 0
58 about to throw Linear solver did not converge
58 Newton::solveLinearSystem : Caught exception: "NumericalProblem [solveLinearSystem:/home/m.giraud/DUMUXexud/dumux/dumux/nonlinear/newtonsolver.hh:381]: Linear solver did not converge when getting convergedRemote "
58 Newton: Caught exception: "NumericalProblem [solveLinearSystem:/home/m.giraud/DUMUXexud/dumux/dumux/nonlinear/newtonsolver.hh:381]: Linear solver did not converge"
58 Newton solver did not converge with dt = 0.099 seconds. Retrying with time step of 0.0495 seconds
58 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
106 did  comm_.min(converged) convergedRemote: 0
106 final convergedRemote: 0
106 about to throw Linear solver did not converge
106 Newton::solveLinearSystem : Caught exception: "NumericalProblem [solveLinearSystem:/home/m.giraud/DUMUXexud/dumux/dumux/nonlinear/newtonsolver.hh:381]: Linear solver did not converge when getting convergedRemote "
106 Newton: Caught exception: "NumericalProblem [solveLinearSystem:/home/m.giraud/DUMUXexud/dumux/dumux/nonlinear/newtonsolver.hh:381]: Linear solver did not converge"
106 Newton solver did not converge with dt = 0.099 seconds. Retrying with time step of 0.0495 seconds
106 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
ests, &finished, &status); 6 11
85: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 5 1 -32766 0 0
85: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 11
85: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 5 1 -32766 0 0
85: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 11
85: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 5 1 -32766 0 0
85: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 11
85: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 5 1 -32766 0 0
85: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 10 11
85: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 5 1 -32766 0 0
Newton::solveLinearSystem : Caught exception from the linear solver when getting converged: "BCRSMatrixError [operator++:/home/m.giraud/DUMUXexud/dune-istl/dune/istl/bcrsmatrix.hh:940]: matrix already built up"
85 converged value: 0 121 1 let s go get convergedRemote
85 to  comm_.min(converged) 
85 did  comm_.min(converged) convergedRemote: 0
85 final convergedRemote: 0
85 about to throw Linear solver did not converge
85 Newton::solveLinearSystem : Caught exception: "NumericalProblem [solveLinearSystem:/home/m.giraud/DUMUXexud/dumux/dumux/nonlinear/newtonsolver.hh:381]: Linear solver did not converge when getting convergedRemote "
85 Newton: Caught exception: "NumericalProblem [solveLinearSystem:/home/m.giraud/DUMUXexud/dumux/dumux/nonlinear/newtonsolver.hh:381]: Linear solver did not converge"
85 Newton solver did not converge with dt = 0.099 seconds. Retrying with time step of 0.0495 seconds
85 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
34 did  comm_.min(converged) convergedRemote: 0
34 final convergedRemote: 0
34 about to throw Linear solver did not converge
34 Newton::solveLinearSystem : Caught exception: "NumericalProblem [solveLinearSystem:/home/m.giraud/DUMUXexud/dumux/dumux/nonlinear/newtonsolver.hh:381]: Linear solver did not converge when getting convergedRemote "
34 Newton: Caught exception: "NumericalProblem [solveLinearSystem:/home/m.giraud/DUMUXexud/dumux/dumux/nonlinear/newtonsolver.hh:381]: Linear solver did not converge"
34 Newton solver did not converge with dt = 0.099 seconds. Retrying with time step of 0.0495 seconds
34 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
Requests, &finished, &status); 18 26
82: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 12 1 -32766 0 0
82: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 19 26
82: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 12 1 -32766 0 0
82: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 20 26
82: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 12 1 -32766 0 0
82: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 21 26
82: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 12 1 -32766 0 0
82: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 22 26
82: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 12 1 -32766 0 0
82: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 23 26
82: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 12 1 -32766 0 0
82: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 24 26
82: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 12 1 -32766 0 0
82: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 25 26
82: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 12 1 -32766 0 0
Newton::solveLinearSystem : Caught exception from the linear solver when getting converged: "BCRSMatrixError [operator++:/home/m.giraud/DUMUXexud/dune-istl/dune/istl/bcrsmatrix.hh:940]: matrix already built up"
82 converged value: 0 121 1 let s go get convergedRemote
82 to  comm_.min(converged) 
82 did  comm_.min(converged) convergedRemote: 0
82 final convergedRemote: 0
82 about to throw Linear solver did not converge
82 Newton::solveLinearSystem : Caught exception: "NumericalProblem [solveLinearSystem:/home/m.giraud/DUMUXexud/dumux/dumux/nonlinear/newtonsolver.hh:381]: Linear solver did not converge when getting convergedRemote "
82 Newton: Caught exception: "NumericalProblem [solveLinearSystem:/home/m.giraud/DUMUXexud/dumux/dumux/nonlinear/newtonsolver.hh:381]: Linear solver did not converge"
82 Newton solver did not converge with dt = 0.099 seconds. Retrying with time step of 0.0495 seconds
82 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
115 did  comm_.min(converged) convergedRemote: 0
115 final convergedRemote: 0
115 about to throw Linear solver did not converge
115 Newton::solveLinearSystem : Caught exception: "NumericalProblem [solveLinearSystem:/home/m.giraud/DUMUXexud/dumux/dumux/nonlinear/newtonsolver.hh:381]: Linear solver did not converge when getting convergedRemote "
115 Newton: Caught exception: "NumericalProblem [solveLinearSystem:/home/m.giraud/DUMUXexud/dumux/dumux/nonlinear/newtonsolver.hh:381]: Linear solver did not converge"
115 Newton solver did not converge with dt = 0.099 seconds. Retrying with time step of 0.0495 seconds
115 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
29 did  comm_.min(converged) convergedRemote: 0
29 final convergedRemote: 0
29 about to throw Linear solver did not converge
29 Newton::solveLinearSystem : Caught exception: "NumericalProblem [solveLinearSystem:/home/m.giraud/DUMUXexud/dumux/dumux/nonlinear/newtonsolver.hh:381]: Linear solver did not converge when getting convergedRemote "
29 Newton: Caught exception: "NumericalProblem [solveLinearSystem:/home/m.giraud/DUMUXexud/dumux/dumux/nonlinear/newtonsolver.hh:381]: Linear solver did not converge"
29 Newton solver did not converge with dt = 0.099 seconds. Retrying with time step of 0.0495 seconds
29 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
, &status); 18 22
68: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 0 1 -32766 0 0
68: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 19 22
68: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 0 1 -32766 0 0
68: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 20 22
68: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 0 1 -32766 0 0
68: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 21 22
68: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 0 1 -32766 0 0
Newton::solveLinearSystem : Caught exception from the linear solver when getting converged: "BCRSMatrixError [operator++:/home/m.giraud/DUMUXexud/dune-istl/dune/istl/bcrsmatrix.hh:940]: matrix already built up"
68 converged value: 0 121 1 let s go get convergedRemote
68 to  comm_.min(converged) 
68 did  comm_.min(converged) convergedRemote: 0
68 final convergedRemote: 0
68 about to throw Linear solver did not converge
68 Newton::solveLinearSystem : Caught exception: "NumericalProblem [solveLinearSystem:/home/m.giraud/DUMUXexud/dumux/dumux/nonlinear/newtonsolver.hh:381]: Linear solver did not converge when getting convergedRemote "
68 Newton: Caught exception: "NumericalProblem [solveLinearSystem:/home/m.giraud/DUMUXexud/dumux/dumux/nonlinear/newtonsolver.hh:381]: Linear solver did not converge"
68 Newton solver did not converge with dt = 0.099 seconds. Retrying with time step of 0.0495 seconds
68 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
: r(x^k) = dS/dt + div F - q;   M = grad r
vRequests, &finished, &status); 18 21
88: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 13 1 -32766 0 0
88: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 19 21
88: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 13 1 -32766 0 0
88: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 20 21
88: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 13 1 -32766 0 0
Newton::solveLinearSystem : Caught exception from the linear solver when getting converged: "BCRSMatrixError [operator++:/home/m.giraud/DUMUXexud/dune-istl/dune/istl/bcrsmatrix.hh:940]: matrix already built up"
88 converged value: 0 121 1 let s go get convergedRemote
88 to  comm_.min(converged) 
88 did  comm_.min(converged) convergedRemote: 0
88 final convergedRemote: 0
88 about to throw Linear solver did not converge
88 Newton::solveLinearSystem : Caught exception: "NumericalProblem [solveLinearSystem:/home/m.giraud/DUMUXexud/dumux/dumux/nonlinear/newtonsolver.hh:381]: Linear solver did not converge when getting convergedRemote "
88 Newton: Caught exception: "NumericalProblem [solveLinearSystem:/home/m.giraud/DUMUXexud/dumux/dumux/nonlinear/newtonsolver.hh:381]: Linear solver did not converge"
88 Newton solver did not converge with dt = 0.099 seconds. Retrying with time step of 0.0495 seconds
88 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
= dS/dt + div F - q;   M = grad r
m_.min(converged) convergedRemote: 0
14 final convergedRemote: 0
14 about to throw Linear solver did not converge
14 Newton::solveLinearSystem : Caught exception: "NumericalProblem [solveLinearSystem:/home/m.giraud/DUMUXexud/dumux/dumux/nonlinear/newtonsolver.hh:381]: Linear solver did not converge when getting convergedRemote "
14 Newton: Caught exception: "NumericalProblem [solveLinearSystem:/home/m.giraud/DUMUXexud/dumux/dumux/nonlinear/newtonsolver.hh:381]: Linear solver did not converge"
14 Newton solver did not converge with dt = 0.099 seconds. Retrying with time step of 0.0495 seconds
14 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
rixError [operator++:/home/m.giraud/DUMUXexud/dune-istl/dune/istl/bcrsmatrix.hh:940]: matrix already built up"
98 converged value: 0 121 1 let s go get convergedRemote
98 to  comm_.min(converged) 
98 did  comm_.min(converged) convergedRemote: 0
98 final convergedRemote: 0
98 about to throw Linear solver did not converge
98 Newton::solveLinearSystem : Caught exception: "NumericalProblem [solveLinearSystem:/home/m.giraud/DUMUXexud/dumux/dumux/nonlinear/newtonsolver.hh:381]: Linear solver did not converge when getting convergedRemote "
98 Newton: Caught exception: "NumericalProblem [solveLinearSystem:/home/m.giraud/DUMUXexud/dumux/dumux/nonlinear/newtonsolver.hh:381]: Linear solver did not converge"
98 Newton solver did not converge with dt = 0.099 seconds. Retrying with time step of 0.0495 seconds
98 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
MUXexud/dumux/dumux/nonlinear/newtonsolver.hh:381]: Linear solver did not converge"
49 Newton solver did not converge with dt = 0.099 seconds. Retrying with time step of 0.0495 seconds
49 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
olver did not converge
103 Newton::solveLinearSystem : Caught exception: "NumericalProblem [solveLinearSystem:/home/m.giraud/DUMUXexud/dumux/dumux/nonlinear/newtonsolver.hh:381]: Linear solver did not converge when getting convergedRemote "
103 Newton: Caught exception: "NumericalProblem [solveLinearSystem:/home/m.giraud/DUMUXexud/dumux/dumux/nonlinear/newtonsolver.hh:381]: Linear solver did not converge"
103 Newton solver did not converge with dt = 0.099 seconds. Retrying with time step of 0.0495 seconds
103 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
cvRequests, &finished, &status); 8 9
104: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 6 1 -32766 0 0
Newton::solveLinearSystem : Caught exception from the linear solver when getting converged: "BCRSMatrixError [operator++:/home/m.giraud/DUMUXexud/dune-istl/dune/istl/bcrsmatrix.hh:940]: matrix already built up"
104 converged value: 0 121 1 let s go get convergedRemote
104 to  comm_.min(converged) 
104 did  comm_.min(converged) convergedRemote: 0
104 final convergedRemote: 0
104 about to throw Linear solver did not converge
104 Newton::solveLinearSystem : Caught exception: "NumericalProblem [solveLinearSystem:/home/m.giraud/DUMUXexud/dumux/dumux/nonlinear/newtonsolver.hh:381]: Linear solver did not converge when getting convergedRemote "
104 Newton: Caught exception: "NumericalProblem [solveLinearSystem:/home/m.giraud/DUMUXexud/dumux/dumux/nonlinear/newtonsolver.hh:381]: Linear solver did not converge"
104 Newton solver did not converge with dt = 0.099 seconds. Retrying with time step of 0.0495 seconds
104 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
, &status); 18 21
94: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 0 1 -32766 0 0
94: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 19 21
94: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 0 1 -32766 0 0
94: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 20 21
94: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 0 1 -32766 0 0
Newton::solveLinearSystem : Caught exception from the linear solver when getting converged: "BCRSMatrixError [operator++:/home/m.giraud/DUMUXexud/dune-istl/dune/istl/bcrsmatrix.hh:940]: matrix already built up"
94 converged value: 0 121 1 let s go get convergedRemote
94 to  comm_.min(converged) 
94 did  comm_.min(converged) convergedRemote: 0
94 final convergedRemote: 0
94 about to throw Linear solver did not converge
94 Newton::solveLinearSystem : Caught exception: "NumericalProblem [solveLinearSystem:/home/m.giraud/DUMUXexud/dumux/dumux/nonlinear/newtonsolver.hh:381]: Linear solver did not converge when getting convergedRemote "
94 Newton: Caught exception: "NumericalProblem [solveLinearSystem:/home/m.giraud/DUMUXexud/dumux/dumux/nonlinear/newtonsolver.hh:381]: Linear solver did not converge"
94 Newton solver did not converge with dt = 0.099 seconds. Retrying with time step of 0.0495 seconds
94 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
 = dS/dt + div F - q;   M = grad r
d, &status); 18 20
42: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 2 1 -32766 0 0
42: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 19 20
42: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 2 1 -32766 0 0
Newton::solveLinearSystem : Caught exception from the linear solver when getting converged: "BCRSMatrixError [operator++:/home/m.giraud/DUMUXexud/dune-istl/dune/istl/bcrsmatrix.hh:940]: matrix already built up"
42 converged value: 0 121 1 let s go get convergedRemote
42 to  comm_.min(converged) 
42 did  comm_.min(converged) convergedRemote: 0
42 final convergedRemote: 0
42 about to throw Linear solver did not converge
42 Newton::solveLinearSystem : Caught exception: "NumericalProblem [solveLinearSystem:/home/m.giraud/DUMUXexud/dumux/dumux/nonlinear/newtonsolver.hh:381]: Linear solver did not converge when getting convergedRemote "
42 Newton: Caught exception: "NumericalProblem [solveLinearSystem:/home/m.giraud/DUMUXexud/dumux/dumux/nonlinear/newtonsolver.hh:381]: Linear solver did not converge"
42 Newton solver did not converge with dt = 0.099 seconds. Retrying with time step of 0.0495 seconds
42 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
xception from the linear solver when getting converged: "BCRSMatrixError [operator++:/home/m.giraud/DUMUXexud/dune-istl/dune/istl/bcrsmatrix.hh:940]: matrix already built up"
59 converged value: 0 121 1 let s go get convergedRemote
59 to  comm_.min(converged) 
59 did  comm_.min(converged) convergedRemote: 0
59 final convergedRemote: 0
59 about to throw Linear solver did not converge
59 Newton::solveLinearSystem : Caught exception: "NumericalProblem [solveLinearSystem:/home/m.giraud/DUMUXexud/dumux/dumux/nonlinear/newtonsolver.hh:381]: Linear solver did not converge when getting convergedRemote "
59 Newton: Caught exception: "NumericalProblem [solveLinearSystem:/home/m.giraud/DUMUXexud/dumux/dumux/nonlinear/newtonsolver.hh:381]: Linear solver did not converge"
59 Newton solver did not converge with dt = 0.099 seconds. Retrying with time step of 0.0495 seconds
59 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
d, &status); 18 19
87: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 1 1 -32766 0 0
Newton::solveLinearSystem : Caught exception from the linear solver when getting converged: "BCRSMatrixError [operator++:/home/m.giraud/DUMUXexud/dune-istl/dune/istl/bcrsmatrix.hh:940]: matrix already built up"
87 converged value: 0 121 1 let s go get convergedRemote
87 to  comm_.min(converged) 
87 did  comm_.min(converged) convergedRemote: 0
87 final convergedRemote: 0
87 about to throw Linear solver did not converge
87 Newton::solveLinearSystem : Caught exception: "NumericalProblem [solveLinearSystem:/home/m.giraud/DUMUXexud/dumux/dumux/nonlinear/newtonsolver.hh:381]: Linear solver did not converge when getting convergedRemote "
87 Newton: Caught exception: "NumericalProblem [solveLinearSystem:/home/m.giraud/DUMUXexud/dumux/dumux/nonlinear/newtonsolver.hh:381]: Linear solver did not converge"
87 Newton solver did not converge with dt = 0.099 seconds. Retrying with time step of 0.0495 seconds
87 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
h::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000213973
77: communicator.hh::sendRecv,MPI_Testany 0 1 -32766 0 0 0.000216973
77: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 0 1 -32766 0 0
77: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 6
77: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 0 1 -32766 0 0
77: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 6
77: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 0 1 -32766 0 0
77: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 6
77: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 0 1 -32766 0 0
77: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 6
77: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 0 1 -32766 0 0
77: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 6
77: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 0 1 -32766 0 0
Newton::solveLinearSystem : Caught exception from the linear solver when getting converged: "BCRSMatrixError [operator++:/home/m.giraud/DUMUXexud/dune-istl/dune/istl/bcrsmatrix.hh:940]: matrix already built up"
77 converged value: 0 121 1 let s go get convergedRemote
77 to  comm_.min(converged) 
77 did  comm_.min(converged) convergedRemote: 0
77 final convergedRemote: 0
77 about to throw Linear solver did not converge
77 Newton::solveLinearSystem : Caught exception: "NumericalProblem [solveLinearSystem:/home/m.giraud/DUMUXexud/dumux/dumux/nonlinear/newtonsolver.hh:381]: Linear solver did not converge when getting convergedRemote "
77 Newton: Caught exception: "NumericalProblem [solveLinearSystem:/home/m.giraud/DUMUXexud/dumux/dumux/nonlinear/newtonsolver.hh:381]: Linear solver did not converge"
77 Newton solver did not converge with dt = 0.099 seconds. Retrying with time step of 0.0495 seconds
77 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
[solveLinearSystem:/home/m.giraud/DUMUXexud/dumux/dumux/nonlinear/newtonsolver.hh:381]: Linear solver did not converge when getting convergedRemote "
97 Newton: Caught exception: "NumericalProblem [solveLinearSystem:/home/m.giraud/DUMUXexud/dumux/dumux/nonlinear/newtonsolver.hh:381]: Linear solver did not converge"
97 Newton solver did not converge with dt = 0.099 seconds. Retrying with time step of 0.0495 seconds
97 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
ests, &finished, &status); 5 26
52: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 4 1 -32766 0 0
52: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 26
52: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 4 1 -32766 0 0
52: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 26
52: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 4 1 -32766 0 0
52: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 26
52: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 4 1 -32766 0 0
52: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 26
52: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 4 1 -32766 0 0
52: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 10 26
52: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 4 1 -32766 0 0
52: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 11 26
52: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 4 1 -32766 0 0
52: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 12 26
52: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 4 1 -32766 0 0
52: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 13 26
52: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 4 1 -32766 0 0
52: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 14 26
52: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 4 1 -32766 0 0
52: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 15 26
52: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 4 1 -32766 0 0
52: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 16 26
52: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 4 1 -32766 0 0
52: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 17 26
52: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 4 1 -32766 0 0
52: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 18 26
52: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 4 1 -32766 0 0
52: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 19 26
52: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 4 1 -32766 0 0
52: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 20 26
52: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 4 1 -32766 0 0
52: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 21 26
52: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 4 1 -32766 0 0
52: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 22 26
52: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 4 1 -32766 0 0
52: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 23 26
52: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 4 1 -32766 0 0
52: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 24 26
52: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 4 1 -32766 0 0
52: communicator.hh::sendRecv, before MPI_WaitInformation_.size(), recvRequests, &finished, &status); 4 10
51: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 6 1 -32766 0 0
51: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 10
51: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 6 1 -32766 0 0
51: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 10
51: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 6 1 -32766 0 0
51: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 10
51: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 6 1 -32766 0 0
51: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 10
51: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 6 1 -32766 0 0
51: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 10
51: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 6 1 -32766 0 0
Newton::solveLinearSystem : Caught exception from the linear solver when getting converged: "BCRSMatrixError [operator++:/home/m.giraud/DUMUXexud/dune-istl/dune/istl/bcrsmatrix.hh:940]: matrix already built up"
51 converged value: 0 121 1 let s go get convergedRemote
51 to  comm_.min(converged) 
51 did  comm_.min(converged) convergedRemote: 0
51 final convergedRemote: 0
51 about to throw Linear solver did not converge
51 Newton::solveLinearSystem : Caught exception: "NumericalProblem [solveLinearSystem:/home/m.giraud/DUMUXexud/dumux/dumux/nonlinear/newtonsolver.hh:381]: Linear solver did not converge when getting convergedRemote "
51 Newton: Caught exception: "NumericalProblem [solveLinearSystem:/home/m.giraud/DUMUXexud/dumux/dumux/nonlinear/newtonsolver.hh:381]: Linear solver did not converge"
51 Newton solver did not converge with dt = 0.099 seconds. Retrying with time step of 0.0495 seconds
51 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
m_.min(converged) convergedRemote: 0
19 final convergedRemote: 0
19 about to throw Linear solver did not converge
19 Newton::solveLinearSystem : Caught exception: "NumericalProblem [solveLinearSystem:/home/m.giraud/DUMUXexud/dumux/dumux/nonlinear/newtonsolver.hh:381]: Linear solver did not converge when getting convergedRemote "
19 Newton: Caught exception: "NumericalProblem [solveLinearSystem:/home/m.giraud/DUMUXexud/dumux/dumux/nonlinear/newtonsolver.hh:381]: Linear solver did not converge"
19 Newton solver did not converge with dt = 0.099 seconds. Retrying with time step of 0.0495 seconds
19 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
vRequests, &finished, &status); 18 24
38: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 14 1 -32766 0 0
38: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 19 24
38: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 14 1 -32766 0 0
38: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 20 24
38: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 14 1 -32766 0 0
38: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 21 24
38: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 14 1 -32766 0 0
38: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 22 24
38: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 14 1 -32766 0 0
38: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 23 24
38: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 14 1 -32766 0 0
Newton::solveLinearSystem : Caught exception from the linear solver when getting converged: "BCRSMatrixError [operator++:/home/m.giraud/DUMUXexud/dune-istl/dune/istl/bcrsmatrix.hh:940]: matrix already built up"
38 converged value: 0 121 1 let s go get convergedRemote
38 to  comm_.min(converged) 
38 did  comm_.min(converged) convergedRemote: 0
38 final convergedRemote: 0
38 about to throw Linear solver did not converge
38 Newton::solveLinearSystem : Caught exception: "NumericalProblem [solveLinearSystem:/home/m.giraud/DUMUXexud/dumux/dumux/nonlinear/newtonsolver.hh:381]: Linear solver did not converge when getting convergedRemote "
38 Newton: Caught exception: "NumericalProblem [solveLinearSystem:/home/m.giraud/DUMUXexud/dumux/dumux/nonlinear/newtonsolver.hh:381]: Linear solver did not converge"
38 Newton solver did not converge with dt = 0.099 seconds. Retrying with time step of 0.0495 seconds
38 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
_.min(converged) convergedRemote: 0
83 final convergedRemote: 0
83 about to throw Linear solver did not converge
83 Newton::solveLinearSystem : Caught exception: "NumericalProblem [solveLinearSystem:/home/m.giraud/DUMUXexud/dumux/dumux/nonlinear/newtonsolver.hh:381]: Linear solver did not converge when getting convergedRemote "
83 Newton: Caught exception: "NumericalProblem [solveLinearSystem:/home/m.giraud/DUMUXexud/dumux/dumux/nonlinear/newtonsolver.hh:381]: Linear solver did not converge"
83 Newton solver did not converge with dt = 0.099 seconds. Retrying with time step of 0.0495 seconds
83 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
of 0.0495 seconds
118 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
home/m.giraud/DUMUXexud/dumux/dumux/nonlinear/newtonsolver.hh:381]: Linear solver did not converge"
48 Newton solver did not converge with dt = 0.099 seconds. Retrying with time step of 0.0495 seconds
48 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
dS/dt + div F - q;   M = grad r
 -32766 0 0 0.000437706
2: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000440856
2: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000443706
2: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000446566
2: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000449416
2: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000452236
2: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000469216
2: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000472446
2: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000475846
2: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000478686
2: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000481516
2: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000484356
2: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000486986
2: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000489606
2: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000492436
2: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000495527
2: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000498397
2: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000501227
2: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000504077
2: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000506907
2: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000509737
2: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000512537
2: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000515657
2: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000518507
2: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000521377
2: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000524227
2: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000526847
2: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000529487
2: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000532307
2: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000535397
2: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000538227
2: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000541057
2: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000543927
2: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000546567
2: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000549197
2: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000551817
2: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000554917
2: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000557757
2: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000560577
2: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000563407
2: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000566067
2: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000568697
2: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000571527
2: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000574638
2: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000577478
2: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000580318
2: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000583148
2: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000585788
2: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000588458
2: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000591098
2: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000594208
2: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000718099
2: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.00073773
2: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.0007406
2: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.00074347
2: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.00074659
2: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 verge with dt = 0.099 seconds. Retrying with time step of 0.0495 seconds
3 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
UMUXexud/dumux/dumux/nonlinear/newtonsolver.hh:381]: Linear solver did not converge"
66 Newton solver did not converge with dt = 0.099 seconds. Retrying with time step of 0.0495 seconds
66 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
2766 0 0
55: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 11
55: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 0 1 -32766 0 0
55: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 11
55: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 0 1 -32766 0 0
55: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 11
55: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 0 1 -32766 0 0
55: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 11
55: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 0 1 -32766 0 0
55: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 11
55: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 0 1 -32766 0 0
55: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 11
55: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 0 1 -32766 0 0
55: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 10 11
55: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 0 1 -32766 0 0
Newton::solveLinearSystem : Caught exception from the linear solver when getting converged: "BCRSMatrixError [operator++:/home/m.giraud/DUMUXexud/dune-istl/dune/istl/bcrsmatrix.hh:940]: matrix already built up"
55 converged value: 0 121 1 let s go get convergedRemote
55 to  comm_.min(converged) 
55 did  comm_.min(converged) convergedRemote: 0
55 final convergedRemote: 0
55 about to throw Linear solver did not converge
55 Newton::solveLinearSystem : Caught exception: "NumericalProblem [solveLinearSystem:/home/m.giraud/DUMUXexud/dumux/dumux/nonlinear/newtonsolver.hh:381]: Linear solver did not converge when getting convergedRemote "
55 Newton: Caught exception: "NumericalProblem [solveLinearSystem:/home/m.giraud/DUMUXexud/dumux/dumux/nonlinear/newtonsolver.hh:381]: Linear solver did not converge"
55 Newton solver did not converge with dt = 0.099 seconds. Retrying with time step of 0.0495 seconds
55 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
:sendRecv, before assert(finished != MPI_UNDEFINED); 3 1 -32766 0 0
Newton::solveLinearSystem : Caught exception from the linear solver when getting converged: "BCRSMatrixError [operator++:/home/m.giraud/DUMUXexud/dune-istl/dune/istl/bcrsmatrix.hh:940]: matrix already built up"
30 converged value: 0 121 1 let s go get convergedRemote
30 to  comm_.min(converged) 
30 did  comm_.min(converged) convergedRemote: 0
30 final convergedRemote: 0
30 about to throw Linear solver did not converge
30 Newton::solveLinearSystem : Caught exception: "NumericalProblem [solveLinearSystem:/home/m.giraud/DUMUXexud/dumux/dumux/nonlinear/newtonsolver.hh:381]: Linear solver did not converge when getting convergedRemote "
30 Newton: Caught exception: "NumericalProblem [solveLinearSystem:/home/m.giraud/DUMUXexud/dumux/dumux/nonlinear/newtonsolver.hh:381]: Linear solver did not converge"
30 Newton solver did not converge with dt = 0.099 seconds. Retrying with time step of 0.0495 seconds
30 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
DUMUXexud/dumux/dumux/nonlinear/newtonsolver.hh:381]: Linear solver did not converge"
40 Newton solver did not converge with dt = 0.099 seconds. Retrying with time step of 0.0495 seconds
40 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
UMUXexud/dumux/dumux/nonlinear/newtonsolver.hh:381]: Linear solver did not converge"
37 Newton solver did not converge with dt = 0.099 seconds. Retrying with time step of 0.0495 seconds
37 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
te: 0
70 final convergedRemote: 0
70 about to throw Linear solver did not converge
70 Newton::solveLinearSystem : Caught exception: "NumericalProblem [solveLinearSystem:/home/m.giraud/DUMUXexud/dumux/dumux/nonlinear/newtonsolver.hh:381]: Linear solver did not converge when getting convergedRemote "
70 Newton: Caught exception: "NumericalProblem [solveLinearSystem:/home/m.giraud/DUMUXexud/dumux/dumux/nonlinear/newtonsolver.hh:381]: Linear solver did not converge"
70 Newton solver did not converge with dt = 0.099 seconds. Retrying with time step of 0.0495 seconds
70 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
vRequests, &finished, &status); 18 26
26: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 12 1 -32766 0 0
26: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 19 26
26: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 12 1 -32766 0 0
26: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 20 26
26: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 12 1 -32766 0 0
26: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 21 26
26: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 12 1 -32766 0 0
26: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 22 26
26: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 12 1 -32766 0 0
26: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 23 26
26: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 12 1 -32766 0 0
26: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 24 26
26: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 12 1 -32766 0 0
26: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 25 26
26: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 12 1 -32766 0 0
Newton::solveLinearSystem : Caught exception from the linear solver when getting converged: "BCRSMatrixError [operator++:/home/m.giraud/DUMUXexud/dune-istl/dune/istl/bcrsmatrix.hh:940]: matrix already built up"
26 converged value: 0 121 1 let s go get convergedRemote
26 to  comm_.min(converged) 
26 did  comm_.min(converged) convergedRemote: 0
26 final convergedRemote: 0
26 about to throw Linear solver did not converge
26 Newton::solveLinearSystem : Caught exception: "NumericalProblem [solveLinearSystem:/home/m.giraud/DUMUXexud/dumux/dumux/nonlinear/newtonsolver.hh:381]: Linear solver did not converge when getting convergedRemote "
26 Newton: Caught exception: "NumericalProblem [solveLinearSystem:/home/m.giraud/DUMUXexud/dumux/dumux/nonlinear/newtonsolver.hh:381]: Linear solver did not converge"
26 Newton solver did not converge with dt = 0.099 seconds. Retrying with time step of 0.0495 seconds
26 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
hed != MPI_UNDEFINED); 6 1 -32766 0 0
35: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 9
35: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 6 1 -32766 0 0
Newton::solveLinearSystem : Caught exception from the linear solver when getting converged: "BCRSMatrixError [operator++:/home/m.giraud/DUMUXexud/dune-istl/dune/istl/bcrsmatrix.hh:940]: matrix already built up"
35 converged value: 0 121 1 let s go get convergedRemote
35 to  comm_.min(converged) 
35 did  comm_.min(converged) convergedRemote: 0
35 final convergedRemote: 0
35 about to throw Linear solver did not converge
35 Newton::solveLinearSystem : Caught exception: "NumericalProblem [solveLinearSystem:/home/m.giraud/DUMUXexud/dumux/dumux/nonlinear/newtonsolver.hh:381]: Linear solver did not converge when getting convergedRemote "
35 Newton: Caught exception: "NumericalProblem [solveLinearSystem:/home/m.giraud/DUMUXexud/dumux/dumux/nonlinear/newtonsolver.hh:381]: Linear solver did not converge"
35 Newton solver did not converge with dt = 0.099 seconds. Retrying with time step of 0.0495 seconds
35 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
formation_.size(), recvRequests, &finished, &status); 18 24
101: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 4 1 -32766 0 0
101: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 19 24
101: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 4 1 -32766 0 0
101: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 20 24
101: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 4 1 -32766 0 0
101: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 21 24
101: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 4 1 -32766 0 0
101: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 22 24
101: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 4 1 -32766 0 0
101: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 23 24
101: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 4 1 -32766 0 0
Newton::solveLinearSystem : Caught exception from the linear solver when getting converged: "BCRSMatrixError [operator++:/home/m.giraud/DUMUXexud/dune-istl/dune/istl/bcrsmatrix.hh:940]: matrix already built up"
101 converged value: 0 121 1 let s go get convergedRemote
101 to  comm_.min(converged) 
101 did  comm_.min(converged) convergedRemote: 0
101 final convergedRemote: 0
101 about to throw Linear solver did not converge
101 Newton::solveLinearSystem : Caught exception: "NumericalProblem [solveLinearSystem:/home/m.giraud/DUMUXexud/dumux/dumux/nonlinear/newtonsolver.hh:381]: Linear solver did not converge when getting convergedRemote "
101 Newton: Caught exception: "NumericalProblem [solveLinearSystem:/home/m.giraud/DUMUXexud/dumux/dumux/nonlinear/newtonsolver.hh:381]: Linear solver did not converge"
101 Newton solver did not converge with dt = 0.099 seconds. Retrying with time step of 0.0495 seconds
101 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
.giraud/DUMUXexud/dumux/dumux/nonlinear/newtonsolver.hh:381]: Linear solver did not converge when getting convergedRemote "
79 Newton: Caught exception: "NumericalProblem [solveLinearSystem:/home/m.giraud/DUMUXexud/dumux/dumux/nonlinear/newtonsolver.hh:381]: Linear solver did not converge"
79 Newton solver did not converge with dt = 0.099 seconds. Retrying with time step of 0.0495 seconds
79 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
formation_.size(), recvRequests, &finished, &status); 18 22
117: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 8 1 -32766 0 0
117: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 19 22
117: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 8 1 -32766 0 0
117: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 20 22
117: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 8 1 -32766 0 0
117: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 21 22
117: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 8 1 -32766 0 0
Newton::solveLinearSystem : Caught exception from the linear solver when getting converged: "BCRSMatrixError [operator++:/home/m.giraud/DUMUXexud/dune-istl/dune/istl/bcrsmatrix.hh:940]: matrix already built up"
117 converged value: 0 121 1 let s go get convergedRemote
117 to  comm_.min(converged) 
117 did  comm_.min(converged) convergedRemote: 0
117 final convergedRemote: 0
117 about to throw Linear solver did not converge
117 Newton::solveLinearSystem : Caught exception: "NumericalProblem [solveLinearSystem:/home/m.giraud/DUMUXexud/dumux/dumux/nonlinear/newtonsolver.hh:381]: Linear solver did not converge when getting convergedRemote "
117 Newton: Caught exception: "NumericalProblem [solveLinearSystem:/home/m.giraud/DUMUXexud/dumux/dumux/nonlinear/newtonsolver.hh:381]: Linear solver did not converge"
117 Newton solver did not converge with dt = 0.099 seconds. Retrying with time step of 0.0495 seconds
117 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
ricalProblem [solveLinearSystem:/home/m.giraud/DUMUXexud/dumux/dumux/nonlinear/newtonsolver.hh:381]: Linear solver did not converge"
43 Newton solver did not converge with dt = 0.099 seconds. Retrying with time step of 0.0495 seconds
43 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
ed, &status); 18 26
12: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 0 1 -32766 0 0
12: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 19 26
12: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 0 1 -32766 0 0
12: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 20 26
12: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 0 1 -32766 0 0
12: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 21 26
12: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 0 1 -32766 0 0
12: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 22 26
12: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 0 1 -32766 0 0
12: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 23 26
12: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 0 1 -32766 0 0
12: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 24 26
12: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 0 1 -32766 0 0
12: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 25 26
12: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 0 1 -32766 0 0
Newton::solveLinearSystem : Caught exception from the linear solver when getting converged: "BCRSMatrixError [operator++:/home/m.giraud/DUMUXexud/dune-istl/dune/istl/bcrsmatrix.hh:940]: matrix already built up"
12 converged value: 0 121 1 let s go get convergedRemote
12 to  comm_.min(converged) 
12 did  comm_.min(converged) convergedRemote: 0
12 final convergedRemote: 0
12 about to throw Linear solver did not converge
12 Newton::solveLinearSystem : Caught exception: "NumericalProblem [solveLinearSystem:/home/m.giraud/DUMUXexud/dumux/dumux/nonlinear/newtonsolver.hh:381]: Linear solver did not converge when getting convergedRemote "
12 Newton: Caught exception: "NumericalProblem [solveLinearSystem:/home/m.giraud/DUMUXexud/dumux/dumux/nonlinear/newtonsolver.hh:381]: Linear solver did not converge"
12 Newton solver did not converge with dt = 0.099 seconds. Retrying with time step of 0.0495 seconds
12 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
comm_.min(converged) convergedRemote: 0
27 final convergedRemote: 0
27 about to throw Linear solver did not converge
27 Newton::solveLinearSystem : Caught exception: "NumericalProblem [solveLinearSystem:/home/m.giraud/DUMUXexud/dumux/dumux/nonlinear/newtonsolver.hh:381]: Linear solver did not converge when getting convergedRemote "
27 Newton: Caught exception: "NumericalProblem [solveLinearSystem:/home/m.giraud/DUMUXexud/dumux/dumux/nonlinear/newtonsolver.hh:381]: Linear solver did not converge"
27 Newton solver did not converge with dt = 0.099 seconds. Retrying with time step of 0.0495 seconds
27 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
vRequests, &finished, &status); 18 31
56: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 14 1 -32766 0 0
56: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 19 31
56: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 14 1 -32766 0 0
56: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 20 31
56: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 14 1 -32766 0 0
56: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 21 31
56: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 14 1 -32766 0 0
56: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 22 31
56: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 14 1 -32766 0 0
56: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 23 31
56: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 14 1 -32766 0 0
56: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 24 31
56: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 14 1 -32766 0 0
56: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 25 31
56: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 14 1 -32766 0 0
56: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 26 31
56: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 14 1 -32766 0 0
56: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 27 31
56: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 14 1 -32766 0 0
56: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 28 31
56: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 14 1 -32766 0 0
56: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 29 31
56: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 14 1 -32766 0 0
56: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 30 31
56: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 14 1 -32766 0 0
Newton::solveLinearSystem : Caught exception from the linear solver when getting converged: "BCRSMatrixError [operator++:/home/m.giraud/DUMUXexud/dune-istl/dune/istl/bcrsmatrix.hh:940]: matrix already built up"
56 converged value: 0 121 1 let s go get convergedRemote
56 to  comm_.min(converged) 
56 did  comm_.min(converged) convergedRemote: 0
56 final convergedRemote: 0
56 about to throw Linear solver did not converge
56 Newton::solveLinearSystem : Caught exception: "NumericalProblem [solveLinearSystem:/home/m.giraud/DUMUXexud/dumux/dumux/nonlinear/newtonsolver.hh:381]: Linear solver did not converge when getting convergedRemote "
56 Newton: Caught exception: "NumericalProblem [solveLinearSystem:/home/m.giraud/DUMUXexud/dumux/dumux/nonlinear/newtonsolver.hh:381]: Linear solver did not converge"
56 Newton solver did not converge with dt = 0.099 seconds. Retrying with time step of 0.0495 seconds
56 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
t converge"
107 Newton solver did not converge with dt = 0.099 seconds. Retrying with time step of 0.0495 seconds
107 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
rixError [operator++:/home/m.giraud/DUMUXexud/dune-istl/dune/istl/bcrsmatrix.hh:940]: matrix already built up"
28 converged value: 0 121 1 let s go get convergedRemote
28 to  comm_.min(converged) 
28 did  comm_.min(converged) convergedRemote: 0
28 final convergedRemote: 0
28 about to throw Linear solver did not converge
28 Newton::solveLinearSystem : Caught exception: "NumericalProblem [solveLinearSystem:/home/m.giraud/DUMUXexud/dumux/dumux/nonlinear/newtonsolver.hh:381]: Linear solver did not converge when getting convergedRemote "
28 Newton: Caught exception: "NumericalProblem [solveLinearSystem:/home/m.giraud/DUMUXexud/dumux/dumux/nonlinear/newtonsolver.hh:381]: Linear solver did not converge"
28 Newton solver did not converge with dt = 0.099 seconds. Retrying with time step of 0.0495 seconds
28 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
/dumux/dumux/nonlinear/newtonsolver.hh:381]: Linear solver did not converge when getting convergedRemote "
60 Newton: Caught exception: "NumericalProblem [solveLinearSystem:/home/m.giraud/DUMUXexud/dumux/dumux/nonlinear/newtonsolver.hh:381]: Linear solver did not converge"
60 Newton solver did not converge with dt = 0.099 seconds. Retrying with time step of 0.0495 seconds
60 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
endRecv, before assert(finished != MPI_UNDEFINED); 0 1 -32766 0 0
8: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 19 20
8: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 0 1 -32766 0 0
Newton::solveLinearSystem : Caught exception from the linear solver when getting converged: "BCRSMatrixError [operator++:/home/m.giraud/DUMUXexud/dune-istl/dune/istl/bcrsmatrix.hh:940]: matrix already built up"
8 converged value: 0 121 1 let s go get convergedRemote
8 to  comm_.min(converged) 
8 did  comm_.min(converged) convergedRemote: 0
8 final convergedRemote: 0
8 about to throw Linear solver did not converge
8 Newton::solveLinearSystem : Caught exception: "NumericalProblem [solveLinearSystem:/home/m.giraud/DUMUXexud/dumux/dumux/nonlinear/newtonsolver.hh:381]: Linear solver did not converge when getting convergedRemote "
8 Newton: Caught exception: "NumericalProblem [solveLinearSystem:/home/m.giraud/DUMUXexud/dumux/dumux/nonlinear/newtonsolver.hh:381]: Linear solver did not converge"
8 Newton solver did not converge with dt = 0.099 seconds. Retrying with time step of 0.0495 seconds
8 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
hed, &status); 18 31
86: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 4 1 -32766 0 0
86: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 19 31
86: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 4 1 -32766 0 0
86: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 20 31
86: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 4 1 -32766 0 0
86: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 21 31
86: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 4 1 -32766 0 0
86: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 22 31
86: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 4 1 -32766 0 0
86: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 23 31
86: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 4 1 -32766 0 0
86: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 24 31
86: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 4 1 -32766 0 0
86: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 25 31
86: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 4 1 -32766 0 0
86: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 26 31
86: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 4 1 -32766 0 0
86: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 27 31
86: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 4 1 -32766 0 0
86: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 28 31
86: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 4 1 -32766 0 0
86: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 29 31
86: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 4 1 -32766 0 0
86: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 30 31
86: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 4 1 -32766 0 0
Newton::solveLinearSystem : Caught exception from the linear solver when getting converged: "BCRSMatrixError [operator++:/home/m.giraud/DUMUXexud/dune-istl/dune/istl/bcrsmatrix.hh:940]: matrix already built up"
86 converged value: 0 121 1 let s go get convergedRemote
86 to  comm_.min(converged) 
86 did  comm_.min(converged) convergedRemote: 0
86 final convergedRemote: 0
86 about to throw Linear solver did not converge
86 Newton::solveLinearSystem : Caught exception: "NumericalProblem [solveLinearSystem:/home/m.giraud/DUMUXexud/dumux/dumux/nonlinear/newtonsolver.hh:381]: Linear solver did not converge when getting convergedRemote "
86 Newton: Caught exception: "NumericalProblem [solveLinearSystem:/home/m.giraud/DUMUXexud/dumux/dumux/nonlinear/newtonsolver.hh:381]: Linear solver did not converge"
86 Newton solver did not converge with dt = 0.099 seconds. Retrying with time step of 0.0495 seconds
86 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
recvRequests, &finished, &status); 5 6
100: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 3 1 -32766 0 0
Newton::solveLinearSystem : Caught exception from the linear solver when getting converged: "BCRSMatrixError [operator++:/home/m.giraud/DUMUXexud/dune-istl/dune/istl/bcrsmatrix.hh:940]: matrix already built up"
100 converged value: 0 121 1 let s go get convergedRemote
100 to  comm_.min(converged) 
100 did  comm_.min(converged) convergedRemote: 0
100 final convergedRemote: 0
100 about to throw Linear solver did not converge
100 Newton::solveLinearSystem : Caught exception: "NumericalProblem [solveLinearSystem:/home/m.giraud/DUMUXexud/dumux/dumux/nonlinear/newtonsolver.hh:381]: Linear solver did not converge when getting convergedRemote "
100 Newton: Caught exception: "NumericalProblem [solveLinearSystem:/home/m.giraud/DUMUXexud/dumux/dumux/nonlinear/newtonsolver.hh:381]: Linear solver did not converge"
100 Newton solver did not converge with dt = 0.099 seconds. Retrying with time step of 0.0495 seconds
100 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
.giraud/DUMUXexud/dumux/dumux/nonlinear/newtonsolver.hh:381]: Linear solver did not converge when getting convergedRemote "
92 Newton: Caught exception: "NumericalProblem [solveLinearSystem:/home/m.giraud/DUMUXexud/dumux/dumux/nonlinear/newtonsolver.hh:381]: Linear solver did not converge"
92 Newton solver did not converge with dt = 0.099 seconds. Retrying with time step of 0.0495 seconds
92 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
solver when getting converged: "BCRSMatrixError [operator++:/home/m.giraud/DUMUXexud/dune-istl/dune/istl/bcrsmatrix.hh:940]: matrix already built up"
113 converged value: 0 121 1 let s go get convergedRemote
113 to  comm_.min(converged) 
113 did  comm_.min(converged) convergedRemote: 0
113 final convergedRemote: 0
113 about to throw Linear solver did not converge
113 Newton::solveLinearSystem : Caught exception: "NumericalProblem [solveLinearSystem:/home/m.giraud/DUMUXexud/dumux/dumux/nonlinear/newtonsolver.hh:381]: Linear solver did not converge when getting convergedRemote "
113 Newton: Caught exception: "NumericalProblem [solveLinearSystem:/home/m.giraud/DUMUXexud/dumux/dumux/nonlinear/newtonsolver.hh:381]: Linear solver did not converge"
113 Newton solver did not converge with dt = 0.099 seconds. Retrying with time step of 0.0495 seconds
113 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
veLinearSystem:/home/m.giraud/DUMUXexud/dumux/dumux/nonlinear/newtonsolver.hh:381]: Linear solver did not converge"
23 Newton solver did not converge with dt = 0.099 seconds. Retrying with time step of 0.0495 seconds
23 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
_.min(converged) convergedRemote: 0
11 final convergedRemote: 0
11 about to throw Linear solver did not converge
11 Newton::solveLinearSystem : Caught exception: "NumericalProblem [solveLinearSystem:/home/m.giraud/DUMUXexud/dumux/dumux/nonlinear/newtonsolver.hh:381]: Linear solver did not converge when getting convergedRemote "
11 Newton: Caught exception: "NumericalProblem [solveLinearSystem:/home/m.giraud/DUMUXexud/dumux/dumux/nonlinear/newtonsolver.hh:381]: Linear solver did not converge"
11 Newton solver did not converge with dt = 0.099 seconds. Retrying with time step of 0.0495 seconds
11 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
n(converged) convergedRemote: 0
47 final convergedRemote: 0
47 about to throw Linear solver did not converge
47 Newton::solveLinearSystem : Caught exception: "NumericalProblem [solveLinearSystem:/home/m.giraud/DUMUXexud/dumux/dumux/nonlinear/newtonsolver.hh:381]: Linear solver did not converge when getting convergedRemote "
47 Newton: Caught exception: "NumericalProblem [solveLinearSystem:/home/m.giraud/DUMUXexud/dumux/dumux/nonlinear/newtonsolver.hh:381]: Linear solver did not converge"
47 Newton solver did not converge with dt = 0.099 seconds. Retrying with time step of 0.0495 seconds
47 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
any(messageInformation_.size(), recvRequests, &finished, &status); 25 26
52: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 4 1 -32766 0 0
Newton::solveLinearSystem : Caught exception from the linear solver when getting converged: "BCRSMatrixError [operator++:/home/m.giraud/DUMUXexud/dune-istl/dune/istl/bcrsmatrix.hh:940]: matrix already built up"
52 converged value: 0 121 1 let s go get convergedRemote
52 to  comm_.min(converged) 
52 did  comm_.min(converged) convergedRemote: 0
52 final convergedRemote: 0
52 about to throw Linear solver did not converge
52 Newton::solveLinearSystem : Caught exception: "NumericalProblem [solveLinearSystem:/home/m.giraud/DUMUXexud/dumux/dumux/nonlinear/newtonsolver.hh:381]: Linear solver did not converge when getting convergedRemote "
52 Newton: Caught exception: "NumericalProblem [solveLinearSystem:/home/m.giraud/DUMUXexud/dumux/dumux/nonlinear/newtonsolver.hh:381]: Linear solver did not converge"
52 Newton solver did not converge with dt = 0.099 seconds. Retrying with time step of 0.0495 seconds
52 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
0.0007685
2: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.00077255
2: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000905452
2: communicator.hh::sendRecv,MPI_Testany 1 1 -32766 0 0 0.000929022
2: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 1 1 -32766 0 0
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 8
2: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 1 1 -32766 0 0
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 8
2: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 1 1 -32766 0 0
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 8
2: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 1 1 -32766 0 0
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 8
2: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 1 1 -32766 0 0
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 8
2: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 1 1 -32766 0 0
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 8
2: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 1 1 -32766 0 0
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 8
2: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 1 1 -32766 0 0
Newton::solveLinearSystem : Caught exception from the linear solver when getting converged: "BCRSMatrixError [operator++:/home/m.giraud/DUMUXexud/dune-istl/dune/istl/bcrsmatrix.hh:940]: matrix already built up"
2 converged value: 0 121 1 let s go get convergedRemote
2 to  comm_.min(converged) 
2 did  comm_.min(converged) convergedRemote: 0
2 final convergedRemote: 0
2 about to throw Linear solver did not converge
2 Newton::solveLinearSystem : Caught exception: "NumericalProblem [solveLinearSystem:/home/m.giraud/DUMUXexud/dumux/dumux/nonlinear/newtonsolver.hh:381]: Linear solver did not converge when getting convergedRemote "
2 Newton: Caught exception: "NumericalProblem [solveLinearSystem:/home/m.giraud/DUMUXexud/dumux/dumux/nonlinear/newtonsolver.hh:381]: Linear solver did not converge"
2 Newton solver did not converge with dt = 0.099 seconds. Retrying with time step of 0.0495 seconds
2 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
7 Solve: M deltax^k = r 
7 Newton::solveLinearSystem : entering the try section 0
7to  comm_.sum(norm2) 
7did  comm_.sum(norm2) 
7 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
7 solveLinearSystemImpl_, before converged = ls.solve 
7 121 amgbackend::solve isParallel 1
113 Solve: M deltax^k = r 
113 Newton::solveLinearSystem : entering the try section 0
113to  comm_.sum(norm2) 
113did  comm_.sum(norm2) 
113 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
113 solveLinearSystemImpl_, before converged = ls.solve 
113 121 amgbackend::solve isParallel 1
23 Solve: M deltax^k = r 
23 Newton::solveLinearSystem : entering the try section 0
23to  comm_.sum(norm2) 
23did  comm_.sum(norm2) 
23 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
23 solveLinearSystemImpl_, before converged = ls.solve 
23 121 amgbackend::solve isParallel 1
20 Solve: M deltax^k = r 
20 Newton::solveLinearSystem : entering the try section 0
20to  comm_.sum(norm2) 
20did  comm_.sum(norm2) 
20 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
20 solveLinearSystemImpl_, before converged = ls.solve 
20 121 amgbackend::solve isParallel 1
90 Solve: M deltax^k = r 
90 Newton::solveLinearSystem : entering the try section 0
90to  comm_.sum(norm2) 
90did  comm_.sum(norm2) 
90 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
90 solveLinearSystemImpl_, before converged = ls.solve 
90 121 amgbackend::solve isParallel 1
110 Solve: M deltax^k = r 
110 Newton::solveLinearSystem : entering the try section 0
110to  comm_.sum(norm2) 
110did  comm_.sum(norm2) 
110 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
110 solveLinearSystemImpl_, before converged = ls.solve 
110 121 amgbackend::solve isParallel 1
73 Solve: M deltax^k = r 
73 Newton::solveLinearSystem : entering the try section 0
73to  comm_.sum(norm2) 
73did  comm_.sum(norm2) 
73 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
73 solveLinearSystemImpl_, before converged = ls.solve 
73 121 amgbackend::solve isParallel 1
63 Solve: M deltax^k = r 
63 Newton::solveLinearSystem : entering the try section 0
63to  comm_.sum(norm2) 
63did  comm_.sum(norm2) 
63 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
63 solveLinearSystemImpl_, before converged = ls.solve 
63 121 amgbackend::solve isParallel 1
18 Solve: M deltax^k = r 
18 Newton::solveLinearSystem : entering the try section 0
18to  comm_.sum(norm2) 
18did  comm_.sum(norm2) 
18 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
18 solveLinearSystemImpl_, before converged = ls.solve 
18 121 amgbackend::solve isParallel 1
31 Solve: M deltax^k = r 
31 Newton::solveLinearSystem : entering the try section 0
31to  comm_.sum(norm2) 
31did  comm_.sum(norm2) 
31 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
31 solveLinearSystemImpl_, before converged = ls.solve 
31 121 amgbackend::solve isParallel 1
96 Solve: M deltax^k = r 
96 Newton::solveLinearSystem : entering the try section 0
96to  comm_.sum(norm2) 
96did  comm_.sum(norm2) 
96 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
96 solveLinearSystemImpl_, before converged = ls.solve 
96 121 amgbackend::solve isParallel 1
53 Solve: M deltax^k = r 
53 Newton::solveLinearSystem : entering the try section 0
53to  comm_.sum(norm2) 
53did  comm_.sum(norm2) 
53 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
53 solveLinearSystemImpl_, before converged = ls.solve 
53 121 amgbackend::solve isParallel 1
11 Solve: M deltax^k = r 
11 Newton::solveLinearSystem : entering the try section 0
11to  comm_.sum(norm2) 
11did  comm_.sum(norm2) 
11 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
11 solveLinearSystemImpl_, before converged = ls.solve 
11 121 amgbackend::solve isParallel 1
22 Solve: M deltax^k = r 
22 Newton::solveLinearSystem : entering the try section 0
22to  comm_.sum(norm2) 
22did  comm_.sum(norm2) 
22 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
22 solveLinearSystemImpl_, before converged = ls.solve 
22 121 amgbackend::solve isParallel 1
108 Solve: M deltax^k = r 
108 Newton::solveLinearSystem : entering the try section 0
108to  comm_.sum(norm2) 
108did  comm_.sum(norm2) 
108 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
108 solveLinearSystemImpl_, before converged = ls.solve 
108 121 amgbackend::solve isParallel 1
44 Solve: M deltax^k = r 
44 Newton::solveLinearSystem : entering the try section 0
44to  comm_.sum(norm2) 
44did  comm_.sum(norm2) 
44 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
44 solveLinearSystemImpl_, before converged = ls.solve 
44 121 amgbackend::solve isParallel 1
4 Solve: M deltax^k = r 
4 Newton::solveLinearSystem : entering the try section 0
4to  comm_.sum(norm2) 
4did  comm_.sum(norm2) 
4 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
4 solveLinearSystemImpl_, before converged = ls.solve 
4 121 amgbackend::solve isParallel 1
84 Solve: M deltax^k = r 
84 Newton::solveLinearSystem : entering the try section 0
84to  comm_.sum(norm2) 
84did  comm_.sum(norm2) 
84 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
84 solveLinearSystemImpl_, before converged = ls.solve 
84 121 amgbackend::solve isParallel 1
50 Solve: M deltax^k = r 
50 Newton::solveLinearSystem : entering the try section 0
50to  comm_.sum(norm2) 
50did  comm_.sum(norm2) 
50 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
50 solveLinearSystemImpl_, before converged = ls.solve 
50 121 amgbackend::solve isParallel 1
45 Solve: M deltax^k = r 
45 Newton::solveLinearSystem : entering the try section 0
45to  comm_.sum(norm2) 
45did  comm_.sum(norm2) 
45 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
45 solveLinearSystemImpl_, before converged = ls.solve 
45 121 amgbackend::solve isParallel 1
109 Solve: M deltax^k = r 
109 Newton::solveLinearSystem : entering the try section 0
109to  comm_.sum(norm2) 
109did  comm_.sum(norm2) 
109 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
109 solveLinearSystemImpl_, before converged = ls.solve 
109 121 amgbackend::solve isParallel 1
71 Solve: M deltax^k = r 
71 Newton::solveLinearSystem : entering the try section 0
71to  comm_.sum(norm2) 
71did  comm_.sum(norm2) 
71 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
71 solveLinearSystemImpl_, before converged = ls.solve 
71 121 amgbackend::solve isParallel 1
62 Solve: M deltax^k = r 
62 Newton::solveLinearSystem : entering the try section 0
62to  comm_.sum(norm2) 
62did  comm_.sum(norm2) 
62 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
62 solveLinearSystemImpl_, before converged = ls.solve 
62 121 amgbackend::solve isParallel 1
61 Solve: M deltax^k = r 
61 Newton::solveLinearSystem : entering the try section 0
61to  comm_.sum(norm2) 
61did  comm_.sum(norm2) 
61 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
61 solveLinearSystemImpl_, before converged = ls.solve 
61 121 amgbackend::solve isParallel 1
89 Solve: M deltax^k = r 
89 Newton::solveLinearSystem : entering the try section 0
89to  comm_.sum(norm2) 
89did  comm_.sum(norm2) 
89 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
89 solveLinearSystemImpl_, before converged = ls.solve 
89 121 amgbackend::solve isParallel 1
17 Solve: M deltax^k = r 
17 Newton::solveLinearSystem : entering the try section 0
17to  comm_.sum(norm2) 
17did  comm_.sum(norm2) 
17 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
17 solveLinearSystemImpl_, before converged = ls.solve 
17 121 amgbackend::solve isParallel 1
102 Solve: M deltax^k = r 
102 Newton::solveLinearSystem : entering the try section 0
102to  comm_.sum(norm2) 
102did  comm_.sum(norm2) 
102 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
102 solveLinearSystemImpl_, before converged = ls.solve 
102 121 amgbackend::solve isParallel 1
78 Solve: M deltax^k = r 
78 Newton::solveLinearSystem : entering the try section 0
78to  comm_.sum(norm2) 
78did  comm_.sum(norm2) 
78 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
78 solveLinearSystemImpl_, before converged = ls.solve 
78 121 amgbackend::solve isParallel 1
9 Solve: M deltax^k = r 
9 Newton::solveLinearSystem : entering the try section 0
9to  comm_.sum(norm2) 
9did  comm_.sum(norm2) 
9 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
9 solveLinearSystemImpl_, before converged = ls.solve 
9 121 amgbackend::solve isParallel 1
72 Solve: M deltax^k = r 
72 Newton::solveLinearSystem : entering the try section 0
72to  comm_.sum(norm2) 
72did  comm_.sum(norm2) 
72 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
72 solveLinearSystemImpl_, before converged = ls.solve 
72 121 amgbackend::solve isParallel 1
65 Solve: M deltax^k = r 
65 Newton::solveLinearSystem : entering the try section 0
65to  comm_.sum(norm2) 
65did  comm_.sum(norm2) 
65 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
65 solveLinearSystemImpl_, before converged = ls.solve 
65 121 amgbackend::solve isParallel 1
41 Solve: M deltax^k = r 
41 Newton::solveLinearSystem : entering the try section 0
41to  comm_.sum(norm2) 
41did  comm_.sum(norm2) 
41 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
41 solveLinearSystemImpl_, before converged = ls.solve 
41 121 amgbackend::solve isParallel 1
58 Solve: M deltax^k = r 
58 Newton::solveLinearSystem : entering the try section 0
58to  comm_.sum(norm2) 
58did  comm_.sum(norm2) 
58 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
58 solveLinearSystemImpl_, before converged = ls.solve 
58 121 amgbackend::solve isParallel 1
106 Solve: M deltax^k = r 
106 Newton::solveLinearSystem : entering the try section 0
106to  comm_.sum(norm2) 
106did  comm_.sum(norm2) 
106 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
106 solveLinearSystemImpl_, before converged = ls.solve 
106 121 amgbackend::solve isParallel 1
85 Solve: M deltax^k = r 
85 Newton::solveLinearSystem : entering the try section 0
85to  comm_.sum(norm2) 
85did  comm_.sum(norm2) 
85 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
85 solveLinearSystemImpl_, before converged = ls.solve 
85 121 amgbackend::solve isParallel 1
34 Solve: M deltax^k = r 
34 Newton::solveLinearSystem : entering the try section 0
34to  comm_.sum(norm2) 
34did  comm_.sum(norm2) 
34 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
34 solveLinearSystemImpl_, before converged = ls.solve 
34 121 amgbackend::solve isParallel 1
82 Solve: M deltax^k = r 
82 Newton::solveLinearSystem : entering the try section 0
82to  comm_.sum(norm2) 
82did  comm_.sum(norm2) 
82 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
82 solveLinearSystemImpl_, before converged = ls.solve 
82 121 amgbackend::solve isParallel 1
115 Solve: M deltax^k = r 
115 Newton::solveLinearSystem : entering the try section 0
115to  comm_.sum(norm2) 
115did  comm_.sum(norm2) 
115 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
115 solveLinearSystemImpl_, before converged = ls.solve 
115 121 amgbackend::solve isParallel 1
29 Solve: M deltax^k = r 
29 Newton::solveLinearSystem : entering the try section 0
29to  comm_.sum(norm2) 
29did  comm_.sum(norm2) 
29 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
29 solveLinearSystemImpl_, before converged = ls.solve 
29 121 amgbackend::solve isParallel 1
68 Solve: M deltax^k = r 
68 Newton::solveLinearSystem : entering the try section 0
68to  comm_.sum(norm2) 
68did  comm_.sum(norm2) 
68 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
68 solveLinearSystemImpl_, before converged = ls.solve 
68 121 amgbackend::solve isParallel 1
95 Solve: M deltax^k = r 
95 Newton::solveLinearSystem : entering the try section 0
95to  comm_.sum(norm2) 
95did  comm_.sum(norm2) 
95 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
95 solveLinearSystemImpl_, before converged = ls.solve 
95 121 amgbackend::solve isParallel 1
10 Solve: M deltax^k = r 
10 Newton::solveLinearSystem : entering the try section 0
10to  comm_.sum(norm2) 
10did  comm_.sum(norm2) 
10 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
10 solveLinearSystemImpl_, before converged = ls.solve 
10 121 amgbackend::solve isParallel 1
88 Solve: M deltax^k = r 
88 Newton::solveLinearSystem : entering the try section 0
88to  comm_.sum(norm2) 
88did  comm_.sum(norm2) 
88 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
88 solveLinearSystemImpl_, before converged = ls.solve 
88 121 amgbackend::solve isParallel 1
1 Solve: M deltax^k = r 
1 Newton::solveLinearSystem : entering the try section 0
1to  comm_.sum(norm2) 
1did  comm_.sum(norm2) 
1 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
1 solveLinearSystemImpl_, before converged = ls.solve 
1 121 amgbackend::solve isParallel 1
25 Solve: M deltax^k = r 
25 Newton::solveLinearSystem : entering the try section 0
25to  comm_.sum(norm2) 
25did  comm_.sum(norm2) 
25 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
25 solveLinearSystemImpl_, before converged = ls.solve 
25 121 amgbackend::solve isParallel 1
14 Solve: M deltax^k = r 
14 Newton::solveLinearSystem : entering the try section 0
14to  comm_.sum(norm2) 
14did  comm_.sum(norm2) 
14 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
14 solveLinearSystemImpl_, before converged = ls.solve 
14 121 amgbackend::solve isParallel 1
98 Solve: M deltax^k = r 
98 Newton::solveLinearSystem : entering the try section 0
98to  comm_.sum(norm2) 
98did  comm_.sum(norm2) 
98 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
98 solveLinearSystemImpl_, before converged = ls.solve 
98 121 amgbackend::solve isParallel 1
49 Solve: M deltax^k = r 
49 Newton::solveLinearSystem : entering the try section 0
49to  comm_.sum(norm2) 
49did  comm_.sum(norm2) 
49 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
49 solveLinearSystemImpl_, before converged = ls.solve 
49 121 amgbackend::solve isParallel 1
103 Solve: M deltax^k = r 
103 Newton::solveLinearSystem : entering the try section 0
103to  comm_.sum(norm2) 
103did  comm_.sum(norm2) 
103 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
103 solveLinearSystemImpl_, before converged = ls.solve 
103 121 amgbackend::solve isParallel 1
104 Solve: M deltax^k = r 
104 Newton::solveLinearSystem : entering the try section 0
104to  comm_.sum(norm2) 
104did  comm_.sum(norm2) 
104 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
104 solveLinearSystemImpl_, before converged = ls.solve 
104 121 amgbackend::solve isParallel 1
94 Solve: M deltax^k = r 
94 Newton::solveLinearSystem : entering the try section 0
94to  comm_.sum(norm2) 
94did  comm_.sum(norm2) 
94 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
94 solveLinearSystemImpl_, before converged = ls.solve 
94 121 amgbackend::solve isParallel 1
0 Solve: M deltax^k = r 
0 Newton::solveLinearSystem : entering the try section 0
0to  comm_.sum(norm2) 
0did  comm_.sum(norm2) 
0 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
0 solveLinearSystemImpl_, before converged = ls.solve 
0 121 amgbackend::solve isParallel 1
24 Solve: M deltax^k = r 
24 Newton::solveLinearSystem : entering the try section 0
24to  comm_.sum(norm2) 
24did  comm_.sum(norm2) 
24 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
24 solveLinearSystemImpl_, before converged = ls.solve 
24 121 amgbackend::solve isParallel 1
42 Solve: M deltax^k = r 
42 Newton::solveLinearSystem : entering the try section 0
42to  comm_.sum(norm2) 
42did  comm_.sum(norm2) 
42 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
42 solveLinearSystemImpl_, before converged = ls.solve 
42 121 amgbackend::solve isParallel 1
59 Solve: M deltax^k = r 
59 Newton::solveLinearSystem : entering the try section 0
59to  comm_.sum(norm2) 
59did  comm_.sum(norm2) 
59 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
59 solveLinearSystemImpl_, before converged = ls.solve 
59 121 amgbackend::solve isParallel 1
80 Solve: M deltax^k = r 
80 Newton::solveLinearSystem : entering the try section 0
80to  comm_.sum(norm2) 
80did  comm_.sum(norm2) 
80 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
80 solveLinearSystemImpl_, before converged = ls.solve 
80 121 amgbackend::solve isParallel 1
87 Solve: M deltax^k = r 
87 Newton::solveLinearSystem : entering the try section 0
87to  comm_.sum(norm2) 
87did  comm_.sum(norm2) 
87 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
87 solveLinearSystemImpl_, before converged = ls.solve 
87 121 amgbackend::solve isParallel 1
91 Solve: M deltax^k = r 
91 Newton::solveLinearSystem : entering the try section 0
91to  comm_.sum(norm2) 
91did  comm_.sum(norm2) 
91 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
91 solveLinearSystemImpl_, before converged = ls.solve 
91 121 amgbackend::solve isParallel 1
77 Solve: M deltax^k = r 
77 Newton::solveLinearSystem : entering the try section 0
77to  comm_.sum(norm2) 
77did  comm_.sum(norm2) 
77 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
77 solveLinearSystemImpl_, before converged = ls.solve 
77 121 amgbackend::solve isParallel 1
97 Solve: M deltax^k = r 
97 Newton::solveLinearSystem : entering the try section 0
97to  comm_.sum(norm2) 
97did  comm_.sum(norm2) 
97 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
97 solveLinearSystemImpl_, before converged = ls.solve 
97 121 amgbackend::solve isParallel 1
47 Solve: M deltax^k = r 
47 Newton::solveLinearSystem : entering the try section 0
47to  comm_.sum(norm2) 
47did  comm_.sum(norm2) 
47 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
47 solveLinearSystemImpl_, before converged = ls.solve 
47 121 amgbackend::solve isParallel 1
74 Solve: M deltax^k = r 
74 Newton::solveLinearSystem : entering the try section 0
74to  comm_.sum(norm2) 
74did  comm_.sum(norm2) 
74 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
74 solveLinearSystemImpl_, before converged = ls.solve 
74 121 amgbackend::solve isParallel 1
21 Solve: M deltax^k = r 
21 Newton::solveLinearSystem : entering the try section 0
21to  comm_.sum(norm2) 
21did  comm_.sum(norm2) 
21 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
21 solveLinearSystemImpl_, before converged = ls.solve 
21 121 amgbackend::solve isParallel 1
51 Solve: M deltax^k = r 
51 Newton::solveLinearSystem : entering the try section 0
51to  comm_.sum(norm2) 
51did  comm_.sum(norm2) 
51 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
51 solveLinearSystemImpl_, before converged = ls.solve 
51 121 amgbackend::solve isParallel 1
15 Solve: M deltax^k = r 
15 Newton::solveLinearSystem : entering the try section 0
15to  comm_.sum(norm2) 
15did  comm_.sum(norm2) 
15 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
15 solveLinearSystemImpl_, before converged = ls.solve 
15 121 amgbackend::solve isParallel 1
19 Solve: M deltax^k = r 
19 Newton::solveLinearSystem : entering the try section 0
19to  comm_.sum(norm2) 
19did  comm_.sum(norm2) 
19 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
19 solveLinearSystemImpl_, before converged = ls.solve 
19 121 amgbackend::solve isParallel 1
38 Solve: M deltax^k = r 
38 Newton::solveLinearSystem : entering the try section 0
38to  comm_.sum(norm2) 
38did  comm_.sum(norm2) 
38 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
38 solveLinearSystemImpl_, before converged = ls.solve 
38 121 amgbackend::solve isParallel 1
83 Solve: M deltax^k = r 
83 Newton::solveLinearSystem : entering the try section 0
83to  comm_.sum(norm2) 
83did  comm_.sum(norm2) 
83 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
83 solveLinearSystemImpl_, before converged = ls.solve 
83 121 amgbackend::solve isParallel 1
118 Solve: M deltax^k = r 
118 Newton::solveLinearSystem : entering the try section 0
118to  comm_.sum(norm2) 
118did  comm_.sum(norm2) 
118 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
118 solveLinearSystemImpl_, before converged = ls.solve 
118 121 amgbackend::solve isParallel 1
33 Solve: M deltax^k = r 
33 Newton::solveLinearSystem : entering the try section 0
33to  comm_.sum(norm2) 
33did  comm_.sum(norm2) 
33 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
33 solveLinearSystemImpl_, before converged = ls.solve 
33 121 amgbackend::solve isParallel 1
67 Solve: M deltax^k = r 
67 Newton::solveLinearSystem : entering the try section 0
67to  comm_.sum(norm2) 
67did  comm_.sum(norm2) 
67 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
67 solveLinearSystemImpl_, before converged = ls.solve 
67 121 amgbackend::solve isParallel 1
112 Solve: M deltax^k = r 
112 Newton::solveLinearSystem : entering the try section 0
112to  comm_.sum(norm2) 
112did  comm_.sum(norm2) 
112 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
112 solveLinearSystemImpl_, before converged = ls.solve 
112 121 amgbackend::solve isParallel 1
48 Solve: M deltax^k = r 
48 Newton::solveLinearSystem : entering the try section 0
48to  comm_.sum(norm2) 
48did  comm_.sum(norm2) 
48 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
48 solveLinearSystemImpl_, before converged = ls.solve 
48 121 amgbackend::solve isParallel 1
5 Solve: M deltax^k = r 
5 Newton::solveLinearSystem : entering the try section 0
5to  comm_.sum(norm2) 
5did  comm_.sum(norm2) 
5 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
5 solveLinearSystemImpl_, before converged = ls.solve 
5 121 amgbackend::solve isParallel 1
36 Solve: M deltax^k = r 
36 Newton::solveLinearSystem : entering the try section 0
36to  comm_.sum(norm2) 
36did  comm_.sum(norm2) 
36 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
36 solveLinearSystemImpl_, before converged = ls.solve 
36 121 amgbackend::solve isParallel 1
52 Solve: M deltax^k = r 
52 Newton::solveLinearSystem : entering the try section 0
52to  comm_.sum(norm2) 
52did  comm_.sum(norm2) 
52 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
52 solveLinearSystemImpl_, before converged = ls.solve 
52 121 amgbackend::solve isParallel 1
3 Solve: M deltax^k = r 
3 Newton::solveLinearSystem : entering the try section 0
3to  comm_.sum(norm2) 
3did  comm_.sum(norm2) 
3 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
3 solveLinearSystemImpl_, before converged = ls.solve 
3 121 amgbackend::solve isParallel 1
76 Solve: M deltax^k = r 
76 Newton::solveLinearSystem : entering the try section 0
76to  comm_.sum(norm2) 
76did  comm_.sum(norm2) 
76 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
76 solveLinearSystemImpl_, before converged = ls.solve 
76 121 amgbackend::solve isParallel 1
66 Solve: M deltax^k = r 
66 Newton::solveLinearSystem : entering the try section 0
66to  comm_.sum(norm2) 
66did  comm_.sum(norm2) 
66 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
66 solveLinearSystemImpl_, before converged = ls.solve 
66 121 amgbackend::solve isParallel 1
55 Solve: M deltax^k = r 
55 Newton::solveLinearSystem : entering the try section 0
55to  comm_.sum(norm2) 
55did  comm_.sum(norm2) 
55 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
55 solveLinearSystemImpl_, before converged = ls.solve 
55 121 amgbackend::solve isParallel 1
30 Solve: M deltax^k = r 
30 Newton::solveLinearSystem : entering the try section 0
30to  comm_.sum(norm2) 
30did  comm_.sum(norm2) 
30 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
30 solveLinearSystemImpl_, before converged = ls.solve 
30 121 amgbackend::solve isParallel 1
69 Solve: M deltax^k = r 
69 Newton::solveLinearSystem : entering the try section 0
69to  comm_.sum(norm2) 
69did  comm_.sum(norm2) 
69 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
69 solveLinearSystemImpl_, before converged = ls.solve 
69 121 amgbackend::solve isParallel 1
40 Solve: M deltax^k = r 
40 Newton::solveLinearSystem : entering the try section 0
40to  comm_.sum(norm2) 
40did  comm_.sum(norm2) 
40 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
40 solveLinearSystemImpl_, before converged = ls.solve 
40 121 amgbackend::solve isParallel 1
16 Solve: M deltax^k = r 
16 Newton::solveLinearSystem : entering the try section 0
16to  comm_.sum(norm2) 
16did  comm_.sum(norm2) 
16 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
16 solveLinearSystemImpl_, before converged = ls.solve 
16 121 amgbackend::solve isParallel 1
105 Solve: M deltax^k = r 
105 Newton::solveLinearSystem : entering the try section 0
105to  comm_.sum(norm2) 
105did  comm_.sum(norm2) 
105 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
105 solveLinearSystemImpl_, before converged = ls.solve 
105 121 amgbackend::solve isParallel 1
37 Solve: M deltax^k = r 
37 Newton::solveLinearSystem : entering the try section 0
37to  comm_.sum(norm2) 
37did  comm_.sum(norm2) 
37 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
37 solveLinearSystemImpl_, before converged = ls.solve 
37 121 amgbackend::solve isParallel 1
70 Solve: M deltax^k = r 
70 Newton::solveLinearSystem : entering the try section 0
70to  comm_.sum(norm2) 
70did  comm_.sum(norm2) 
70 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
70 solveLinearSystemImpl_, before converged = ls.solve 
70 121 amgbackend::solve isParallel 1
26 Solve: M deltax^k = r 
26 Newton::solveLinearSystem : entering the try section 0
26to  comm_.sum(norm2) 
26did  comm_.sum(norm2) 
26 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
26 solveLinearSystemImpl_, before converged = ls.solve 
26 121 amgbackend::solve isParallel 1
35 Solve: M deltax^k = r 
35 Newton::solveLinearSystem : entering the try section 0
35to  comm_.sum(norm2) 
35did  comm_.sum(norm2) 
35 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
35 solveLinearSystemImpl_, before converged = ls.solve 
35 121 amgbackend::solve isParallel 1
93 Solve: M deltax^k = r 
93 Newton::solveLinearSystem : entering the try section 0
93to  comm_.sum(norm2) 
93did  comm_.sum(norm2) 
93 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
93 solveLinearSystemImpl_, before converged = ls.solve 
93 121 amgbackend::solve isParallel 1
114 Solve: M deltax^k = r 
114 Newton::solveLinearSystem : entering the try section 0
114to  comm_.sum(norm2) 
114did  comm_.sum(norm2) 
114 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
114 solveLinearSystemImpl_, before converged = ls.solve 
114 121 amgbackend::solve isParallel 1
101 Solve: M deltax^k = r 
101 Newton::solveLinearSystem : entering the try section 0
101to  comm_.sum(norm2) 
101did  comm_.sum(norm2) 
101 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
101 solveLinearSystemImpl_, before converged = ls.solve 
101 121 amgbackend::solve isParallel 1
13 Solve: M deltax^k = r 
13 Newton::solveLinearSystem : entering the try section 0
13to  comm_.sum(norm2) 
13did  comm_.sum(norm2) 
13 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
13 solveLinearSystemImpl_, before converged = ls.solve 
13 121 amgbackend::solve isParallel 1
39 Solve: M deltax^k = r 
39 Newton::solveLinearSystem : entering the try section 0
39to  comm_.sum(norm2) 
39did  comm_.sum(norm2) 
39 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
39 solveLinearSystemImpl_, before converged = ls.solve 
39 121 amgbackend::solve isParallel 1
79 Solve: M deltax^k = r 
79 Newton::solveLinearSystem : entering the try section 0
79to  comm_.sum(norm2) 
79did  comm_.sum(norm2) 
79 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
79 solveLinearSystemImpl_, before converged = ls.solve 
79 121 amgbackend::solve isParallel 1
117 Solve: M deltax^k = r 
117 Newton::solveLinearSystem : entering the try section 0
117to  comm_.sum(norm2) 
117did  comm_.sum(norm2) 
117 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
117 solveLinearSystemImpl_, before converged = ls.solve 
117 121 amgbackend::solve isParallel 1
43 Solve: M deltax^k = r 
43 Newton::solveLinearSystem : entering the try section 0
43to  comm_.sum(norm2) 
43did  comm_.sum(norm2) 
43 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
43 solveLinearSystemImpl_, before converged = ls.solve 
43 121 amgbackend::solve isParallel 1
46 Solve: M deltax^k = r 
46 Newton::solveLinearSystem : entering the try section 0
46to  comm_.sum(norm2) 
46did  comm_.sum(norm2) 
46 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
46 solveLinearSystemImpl_, before converged = ls.solve 
46 121 amgbackend::solve isParallel 1
12 Solve: M deltax^k = r 
12 Newton::solveLinearSystem : entering the try section 0
12to  comm_.sum(norm2) 
12did  comm_.sum(norm2) 
12 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
12 solveLinearSystemImpl_, before converged = ls.solve 
12 121 amgbackend::solve isParallel 1
27 Solve: M deltax^k = r 
27 Newton::solveLinearSystem : entering the try section 0
27to  comm_.sum(norm2) 
27did  comm_.sum(norm2) 
27 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
27 solveLinearSystemImpl_, before converged = ls.solve 
27 121 amgbackend::solve isParallel 1
75 Solve: M deltax^k = r 
75 Newton::solveLinearSystem : entering the try section 0
75to  comm_.sum(norm2) 
75did  comm_.sum(norm2) 
75 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
75 solveLinearSystemImpl_, before converged = ls.solve 
75 121 amgbackend::solve isParallel 1
120 Solve: M deltax^k = r 
120 Newton::solveLinearSystem : entering the try section 0
120to  comm_.sum(norm2) 
120did  comm_.sum(norm2) 
120 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
120 solveLinearSystemImpl_, before converged = ls.solve 
120 121 amgbackend::solve isParallel 1
54 Solve: M deltax^k = r 
54 Newton::solveLinearSystem : entering the try section 0
54to  comm_.sum(norm2) 
54did  comm_.sum(norm2) 
54 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
54 solveLinearSystemImpl_, before converged = ls.solve 
54 121 amgbackend::solve isParallel 1
81 Solve: M deltax^k = r 
81 Newton::solveLinearSystem : entering the try section 0
81to  comm_.sum(norm2) 
81did  comm_.sum(norm2) 
81 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
81 solveLinearSystemImpl_, before converged = ls.solve 
81 121 amgbackend::solve isParallel 1
6 Solve: M deltax^k = r 
6 Newton::solveLinearSystem : entering the try section 0
6to  comm_.sum(norm2) 
6did  comm_.sum(norm2) 
6 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
6 solveLinearSystemImpl_, before converged = ls.solve 
6 121 amgbackend::solve isParallel 1
99 Solve: M deltax^k = r 
99 Newton::solveLinearSystem : entering the try section 0
99to  comm_.sum(norm2) 
99did  comm_.sum(norm2) 
99 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
99 solveLinearSystemImpl_, before converged = ls.solve 
99 121 amgbackend::solve isParallel 1
119 Solve: M deltax^k = r 
119 Newton::solveLinearSystem : entering the try section 0
119to  comm_.sum(norm2) 
119did  comm_.sum(norm2) 
119 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
119 solveLinearSystemImpl_, before converged = ls.solve 
119 121 amgbackend::solve isParallel 1
32 Solve: M deltax^k = r 
32 Newton::solveLinearSystem : entering the try section 0
32to  comm_.sum(norm2) 
32did  comm_.sum(norm2) 
32 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
32 solveLinearSystemImpl_, before converged = ls.solve 
32 121 amgbackend::solve isParallel 1
57 Solve: M deltax^k = r 
57 Newton::solveLinearSystem : entering the try section 0
57to  comm_.sum(norm2) 
57did  comm_.sum(norm2) 
57 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
57 solveLinearSystemImpl_, before converged = ls.solve 
57 121 amgbackend::solve isParallel 1
64 Solve: M deltax^k = r 
64 Newton::solveLinearSystem : entering the try section 0
64to  comm_.sum(norm2) 
64did  comm_.sum(norm2) 
64 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
64 solveLinearSystemImpl_, before converged = ls.solve 
64 121 amgbackend::solve isParallel 1
2 Solve: M deltax^k = r 
2 Newton::solveLinearSystem : entering the try section 0
2to  comm_.sum(norm2) 
2did  comm_.sum(norm2) 
2 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
2 solveLinearSystemImpl_, before converged = ls.solve 
2 121 amgbackend::solve isParallel 1
116 Solve: M deltax^k = r 
116 Newton::solveLinearSystem : entering the try section 0
116to  comm_.sum(norm2) 
116did  comm_.sum(norm2) 
116 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
116 solveLinearSystemImpl_, before converged = ls.solve 
116 121 amgbackend::solve isParallel 1
107 Solve: M deltax^k = r 
107 Newton::solveLinearSystem : entering the try section 0
107to  comm_.sum(norm2) 
107did  comm_.sum(norm2) 
107 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
107 solveLinearSystemImpl_, before converged = ls.solve 
107 121 amgbackend::solve isParallel 1
28 Solve: M deltax^k = r 
28 Newton::solveLinearSystem : entering the try section 0
28to  comm_.sum(norm2) 
28did  comm_.sum(norm2) 
28 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
28 solveLinearSystemImpl_, before converged = ls.solve 
28 121 amgbackend::solve isParallel 1
60 Solve: M deltax^k = r 
60 Newton::solveLinearSystem : entering the try section 0
60to  comm_.sum(norm2) 
60did  comm_.sum(norm2) 
60 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
60 solveLinearSystemImpl_, before converged = ls.solve 
60 121 amgbackend::solve isParallel 1
8 Solve: M deltax^k = r 
8 Newton::solveLinearSystem : entering the try section 0
8to  comm_.sum(norm2) 
8did  comm_.sum(norm2) 
8 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
8 solveLinearSystemImpl_, before converged = ls.solve 
8 121 amgbackend::solve isParallel 1
86 Solve: M deltax^k = r 
86 Newton::solveLinearSystem : entering the try section 0
86to  comm_.sum(norm2) 
86did  comm_.sum(norm2) 
86 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
86 solveLinearSystemImpl_, before converged = ls.solve 
86 121 amgbackend::solve isParallel 1
111 Solve: M deltax^k = r 
111 Newton::solveLinearSystem : entering the try section 0
111to  comm_.sum(norm2) 
111did  comm_.sum(norm2) 
111 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
111 solveLinearSystemImpl_, before converged = ls.solve 
111 121 amgbackend::solve isParallel 1
100 Solve: M deltax^k = r 
100 Newton::solveLinearSystem : entering the try section 0
100to  comm_.sum(norm2) 
100did  comm_.sum(norm2) 
100 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
100 solveLinearSystemImpl_, before converged = ls.solve 
100 121 amgbackend::solve isParallel 1
92 Solve: M deltax^k = r 
92 Newton::solveLinearSystem : entering the try section 0
92to  comm_.sum(norm2) 
92did  comm_.sum(norm2) 
92 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
92 solveLinearSystemImpl_, before converged = ls.solve 
92 121 amgbackend::solve isParallel 1
56 Solve: M deltax^k = r 
56 Newton::solveLinearSystem : entering the try section 0
56to  comm_.sum(norm2) 
56did  comm_.sum(norm2) 
56 in solveLinearSystemImpl_ typename std::enable_if_t<!isMultiTypeBlockVector<V>(), bool>
56 solveLinearSystemImpl_, before converged = ls.solve 
56 121 amgbackend::solve isParallel 1
17 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
17: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 3
17: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 4.2e-07
17: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 6.71e-06
17: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 1.004e-05
17: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000139732
17: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000362345
17: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000368855
17: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000507337
17: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000513557
17: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000516207
17: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000518797
17: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000524447
17: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000653958
17: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000656759
17: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.00079083
17: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.00079362
17: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000937542
17: communicator.hh::sendRecv,MPI_Testany 0 1 -32766 0 0 0.00106425
17: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 0 1 -32766 0 0
17: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 3
17: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 0 1 -32766 0 0
17: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 3
17: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 0 1 -32766 0 0
102 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
102: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 15
102: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 5.01e-06
102: communicator.hh::sendRecv,MPI_Testany 3 1 -32766 0 0 1.173e-05
102: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 3 1 -32766 0 0
102: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 15
102: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 3 1 -32766 0 0
102: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 15
102: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 3 1 -32766 0 0
102: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 15
102: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 3 1 -32766 0 0
102: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 15
102: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 3 1 -32766 0 0
102: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 15
102: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 3 1 -32766 0 0
102: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 15
102: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 3 1 -32766 0 0
102: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 15
102: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 3 1 -32766 0 0
102: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 15
102: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 3 1 -32766 0 0
102: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 15
102: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 3 1 -32766 0 0
102: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 10 15
102: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 3 1 -32766 0 0
102: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 11 15
102: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 3 1 -32766 0 0
102: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 12 15
102: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 3 1 -32766 0 0
102: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 13 15
102: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 3 1 -32766 0 0
102: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 14 15
102: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 3 1 -32766 0 0
78 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
78: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 15
78: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 6.2e-07
78: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 6.76e-06
78: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 9.65e-06
78: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 1.237e-05
78: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 1.5341e-05
78: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 1.8011e-05
78: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 2.0811e-05
78: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 2.3391e-05
78: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 2.5981e-05
78: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 2.8571e-05
78: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 3.1171e-05
78: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 3.4851e-05
78: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 3.8451e-05
78: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 4.2011e-05
78: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 4.5651e-05
78: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 4.9071e-05
78: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 5.2141e-05
78: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 5.5141e-05
78: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 5.8411e-05
78: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 6.1551e-05
78: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 6.4561e-05
78: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 8.8381e-05
78: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 9.2471e-05
78: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 9.5822e-05
78: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 9.9132e-05
78: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000102562
78: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000106012
78: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000109172
78: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000112252
78: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000115312
78: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000118372
78: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000121812
78: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000124992
78: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000128162
78: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000131242
78: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000134292
78: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000137312
78: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000140762
78: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000146022
78: communicator.hh::sendRecv,MPI_Testany 10 1 -32766 0 0 0.000149862
78: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 10 1 -32766 0 0
9 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
9: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 7
9: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 1.34e-06
9: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 1.236e-05
9: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000143111
9: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000151732
9: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000283703
9: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000426235
9: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000554967
9: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000572077
9: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000574897
9: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000577787
9: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000591727
9: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000595147
9: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000598037
9: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000612957
9: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000616307
9: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000619087
9: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000633088
9: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000635858
9: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000651718
9: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000654638
9: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000677708
9: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000681678
9: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000705289
9: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000709079
9: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000712379
9: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000715649
9: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000735309
9: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000739719
9: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000743149
9: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000746189
9: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000749209
9: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000752469
9: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000755499
9: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000758529
9: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.00078233
9: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.00078591
9: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.00078899
9: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.00079202
9: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.00079525
9: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.00079829
9: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.00080129
9: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.00080477
9: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.00082623
9: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.00083036
9: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.00083364
9: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.00083665
9: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.00083964
9: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.00084262
9: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.00084622
9: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.00085007
9: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000873681
9: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000877861
9: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.072 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
72: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 20
72: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 7.991e-06
72: communicator.hh::sendRecv,MPI_Testany 0 1 -32766 0 0 1.3741e-05
72: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 0 1 -32766 0 0
72: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 20
72: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 0 1 -32766 0 0
72: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 20
72: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 0 1 -32766 0 0
72: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 20
72: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 0 1 -32766 0 0
72: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 20
72: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 0 1 -32766 0 0
72: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 20
72: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 0 1 -32766 0 0
72: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 20
72: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 0 1 -32766 0 0
72: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 20
72: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 0 1 -32766 0 0
72: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 20
72: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 0 1 -32766 0 0
72: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 20
72: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 0 1 -32766 0 0
72: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 10 20
72: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 0 1 -32766 0 0
72: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 11 20
72: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 0 1 -32766 0 0
72: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 12 20
72: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 0 1 -32766 0 0
72: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 13 20
72: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 0 1 -32766 0 0
72: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 14 20
72: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 0 1 -32766 0 0
72: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 15 20
72: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 0 1 -32766 0 0
72: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 16 20
72: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 0 1 -32766 0 0
72: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 17 20
72: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 0 1 -32766 0 0
72: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finis65 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
65: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 9
65: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 2.09e-06
65: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 8.71e-06
65: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 1.248e-05
65: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 1.618e-05
65: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 1.996e-05
65: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 2.354e-05
65: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 2.731e-05
65: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 3.066e-05
65: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 3.399e-05
65: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 3.733e-05
65: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 4.066e-05
65: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 4.427e-05
65: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 4.782e-05
65: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 5.118e-05
65: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 8.8201e-05
65: communicator.hh::sendRecv,MPI_Testany 6 1 -32766 0 0 9.2981e-05
65: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 6 1 -32766 0 0
65: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 9
65: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 6 1 -32766 0 0
65: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 9
65: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 6 1 -32766 0 0
65: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 9
65: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 6 1 -32766 0 0
65: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 9
65: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 6 1 -32766 0 0
65: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 9
65: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 6 1 -32766 0 0
65: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 9
65: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 6 1 -32766 0 0
65: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 9
65: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 6 1 -32766 0 0
65: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 9
65: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 6 1 -32766 0 0
41 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
41: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 20
41: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 1.14e-06
41: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 1.061e-05
41: communicator.hh::sendRecv,MPI_Testany 1 1 -32766 0 0 1.5041e-05
41: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 1 1 -32766 0 0
41: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 20
41: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 1 1 -32766 0 0
41: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 20
41: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 1 1 -32766 0 0
41: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 20
41: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 1 1 -32766 0 0
41: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 20
41: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 1 1 -32766 0 0
41: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 20
41: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 1 1 -32766 0 0
41: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 20
41: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 1 1 -32766 0 0
41: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 20
41: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 1 1 -32766 0 0
41: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 20
41: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 1 1 -32766 0 0
41: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 20
41: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 1 1 -32766 0 0
41: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 10 20
41: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 1 1 -32766 0 0
41: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 11 20
41: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 1 1 -32766 0 0
41: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 12 20
41: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 1 1 -32766 0 0
41: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 13 20
41: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 1 1 -32766 0 0
41: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 14 20
41: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 1 1 -32766 0 0
41: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 15 20
41: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 1 1 -32766 0 0
41: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 16 20
41: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 1 1 -32766 0 0
41: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 17 20
41: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 1 1 -32766 0 0
41: communicator.hh::sendRe58 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
58: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 21
58: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 1.003e-05
58: communicator.hh::sendRecv,MPI_Testany 2 1 -32766 0 0 1.604e-05
58: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 2 1 -32766 0 0
58: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 21
58: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 2 1 -32766 0 0
58: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 21
58: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 2 1 -32766 0 0
58: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 21
58: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 2 1 -32766 0 0
58: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 21
58: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 2 1 -32766 0 0
58: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 21
58: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 2 1 -32766 0 0
58: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 21
58: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 2 1 -32766 0 0
58: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 21
58: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 2 1 -32766 0 0
58: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 21
58: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 2 1 -32766 0 0
58: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 21
58: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 2 1 -32766 0 0
58: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 10 21
58: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 2 1 -32766 0 0
58: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 11 21
58: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 2 1 -32766 0 0
58: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 12 21
58: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 2 1 -32766 0 0
58: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 13 21
58: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 2 1 -32766 0 0
58: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 14 21
58: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 2 1 -32766 0 0
58: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 15 21
58: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 2 1 -32766 0 0
58: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 16 21
58: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 2 1 -32766 0 0
58: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 17 21
58: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 2 1 -32766 0 0
58: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finish106 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
106: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 16
106: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 7.15e-06
106: communicator.hh::sendRecv,MPI_Testany 2 1 -32766 0 0 1.3471e-05
106: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 2 1 -32766 0 0
106: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 16
106: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 2 1 -32766 0 0
106: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 16
106: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 2 1 -32766 0 0
106: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 16
106: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 2 1 -32766 0 0
106: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 16
106: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 2 1 -32766 0 0
106: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 16
106: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 2 1 -32766 0 0
106: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 16
106: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 2 1 -32766 0 0
106: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 16
106: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 2 1 -32766 0 0
106: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 16
106: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 2 1 -32766 0 0
106: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 16
106: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 2 1 -32766 0 0
106: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 10 16
106: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 2 1 -32766 0 0
106: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 11 16
106: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 2 1 -32766 0 0
106: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 12 16
106: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 2 1 -32766 0 0
106: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 13 16
106: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 2 1 -32766 0 0
106: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 14 16
106: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 2 1 -32766 0 0
106: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 15 16
106: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 2 1 -32766 0 0
85 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
85: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 11
85: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 1.26e-06
85: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 7.49e-06
85: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 2.443e-05
85: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 2.735e-05
85: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 4.3111e-05
85: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 5.8251e-05
85: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 6.1311e-05
85: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 7.5831e-05
85: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 7.8891e-05
85: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 8.1991e-05
85: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 8.4881e-05
85: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000107321
85: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000110351
85: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000113422
85: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000116222
85: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000119052
85: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000121962
85: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000140872
85: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000143762
85: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000146482
85: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000149082
85: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000151682
85: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000154262
85: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000157062
85: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000159772
85: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000162512
85: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000165232
85: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000168172
85: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000189772
85: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000192863
85: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000195603
85: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000198193
85: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000200923
85: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000203653
85: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000206263
85: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000209183
85: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000211933
85: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000214813
85: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000237183
85: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000240053
85: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000242653
85: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000245583
85: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000248313
85: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000251023
85: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000253733
85: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000256443
85: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000259333
85: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000285024
85: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000288004
85: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000290744
85: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000293334
85: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000295914
85: c34 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
34: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 14
34: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 3.15e-06
34: communicator.hh::sendRecv,MPI_Testany 0 1 -32766 0 0 9.06e-06
34: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 0 1 -32766 0 0
34: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 14
34: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 0 1 -32766 0 0
34: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 14
34: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 0 1 -32766 0 0
34: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 14
34: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 0 1 -32766 0 0
34: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 14
34: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 0 1 -32766 0 0
34: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 14
34: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 0 1 -32766 0 0
34: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 14
34: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 0 1 -32766 0 0
34: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 14
34: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 0 1 -32766 0 0
34: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 14
34: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 0 1 -32766 0 0
34: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 14
34: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 0 1 -32766 0 0
34: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 10 14
34: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 0 1 -32766 0 0
34: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 11 14
34: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 0 1 -32766 0 0
34: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 12 14
34: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 0 1 -32766 0 0
34: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 13 14
34: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 0 1 -32766 0 0
82 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
82: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 26
82: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 6.51e-06
82: communicator.hh::sendRecv,MPI_Testany 14 1 -32766 0 0 1.2231e-05
82: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 14 1 -32766 0 0
82: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 26
82: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 14 1 -32766 0 0
82: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 26
82: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 14 1 -32766 0 0
82: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 26
82: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 14 1 -32766 0 0
82: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 26
82: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 14 1 -32766 0 0
82: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 26
82: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 14 1 -32766 0 0
82: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 26
82: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 14 1 -32766 0 0
82: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 26
82: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 14 1 -32766 0 0
82: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 26
82: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 14 1 -32766 0 0
82: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 26
82: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 14 1 -32766 0 0
82: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 10 26
82: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 14 1 -32766 0 0
82: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 11 26
82: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 14 1 -32766 0 0
82: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 12 26
82: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 14 1 -32766 0 0
82: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 13 26
82: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 14 1 -32766 0 0
82: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 14 26
82: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 14 1 -32766 0 0
82: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 15 26
82: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 14 1 -32766 0 0
82: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 16 26
82: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 14 1 -32766 0 0
82: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 17 26
82: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 14 1 -32766 0 0
82: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), re115 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
115: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 11
115: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 1.34e-06
115: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 1.095e-05
115: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000140921
115: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000144032
115: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000278133
115: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000289023
115: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000430325
115: communicator.hh::sendRecv,MPI_Testany 6 1 -32766 0 0 0.000557707
115: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 6 1 -32766 0 0
115: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 11
115: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 6 1 -32766 0 0
115: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 11
115: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 6 1 -32766 0 0
115: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 11
115: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 6 1 -32766 0 0
115: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 11
115: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 6 1 -32766 0 0
115: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 11
115: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 6 1 -32766 0 0
115: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 11
115: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 6 1 -32766 0 0
115: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 11
115: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 6 1 -32766 0 0
115: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 11
115: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 6 1 -32766 0 0
115: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 11
115: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 6 1 -32766 0 0
115: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 10 11
115: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 6 1 -32766 0 0
Newton::solveLinearSystem : Caught exception from the linear solver when getting converged: "BCRSMatrixError [operator++:/home/m.giraud/DUMUXexud/dune-istl/dune/istl/bcrsmatrix.hh:940]: matrix already built up"
115 converged value: 0 121 1 let s go get convergedRemote
115 to  comm_.min(converged) 
29 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
29: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 11
29: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 8.5e-07
29: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 6.76e-06
29: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 9.83e-06
29: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 1.274e-05
29: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 1.573e-05
29: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 3.777e-05
29: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 4.069e-05
29: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 4.336e-05
29: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 4.605e-05
29: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 4.871e-05
29: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 5.136e-05
29: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 5.432e-05
29: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 5.711e-05
29: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 5.99e-05
29: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 6.269e-05
29: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 8.6071e-05
29: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 8.9291e-05
29: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 9.1971e-05
29: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 9.4921e-05
29: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 9.7701e-05
29: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000100471
29: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000103481
29: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000106201
29: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000126971
29: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000130301
29: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000133041
29: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000135781
29: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000138531
29: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000141161
29: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000143792
29: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000146572
29: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000149682
29: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000152502
29: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000179282
29: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000182572
29: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000185212
29: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000188052
29: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000190812
29: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000193432
29: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000196042
29: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000198962
29: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000201862
29: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000232843
29: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000235753
29: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000238383
29: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000241453
29: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000244083
29: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000246693
29: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000249523
29: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000252263
29: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000254893
29: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000257613
29: communicator.hh::sendRec68 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
68: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 22
68: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 7.59e-06
68: communicator.hh::sendRecv,MPI_Testany 0 1 -32766 0 0 1.307e-05
68: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 0 1 -32766 0 0
68: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 22
68: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 0 1 -32766 0 0
68: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 22
68: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 0 1 -32766 0 0
68: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 22
68: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 0 1 -32766 0 0
68: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 22
68: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 0 1 -32766 0 0
68: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 22
68: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 0 1 -32766 0 0
68: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 22
68: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 0 1 -32766 0 0
68: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 22
68: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 0 1 -32766 0 0
68: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 22
68: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 0 1 -32766 0 0
68: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 22
68: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 0 1 -32766 0 0
68: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 10 22
68: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 0 1 -32766 0 0
68: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 11 22
68: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 0 1 -32766 0 0
68: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 12 22
68: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 0 1 -32766 0 0
68: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 13 22
68: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 0 1 -32766 0 0
68: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 14 22
68: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 0 1 -32766 0 0
68: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 15 22
68: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 0 1 -32766 0 0
68: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 16 22
68: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 0 1 -32766 0 0
68: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 17 22
68: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 0 1 -32766 0 0
68: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finishe95 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
95: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 7
95: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 9.9e-07
95: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 1.655e-05
95: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 1.96e-05
95: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 2.255e-05
95: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 2.576e-05
95: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 3.897e-05
95: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 4.174e-05
95: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 4.449e-05
95: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 5.7491e-05
95: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 6.0571e-05
95: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 7.6861e-05
95: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 7.9861e-05
95: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 9.5971e-05
95: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 9.9021e-05
95: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000102061
95: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000104941
95: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000127132
95: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000130302
95: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000133242
95: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000136182
95: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000138902
95: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000141532
95: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000159082
95: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000162482
95: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000165212
95: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000167942
95: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000170672
95: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000173292
95: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000175882
95: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000178652
95: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000181362
95: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000184072
95: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000206693
95: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000209823
95: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000212533
95: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000215313
95: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000218033
95: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000220743
95: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000223463
95: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000226043
95: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000228753
95: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000252023
95: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000255033
95: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000257793
95: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000260533
95: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000263283
95: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000265893
95: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000268663
95: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000271373
95: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000274103
95: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000276973
95: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000299974
95: communicat10 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
10: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 11
10: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 4.99e-06
10: communicator.hh::sendRecv,MPI_Testany 0 1 -32766 0 0 1.049e-05
10: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 0 1 -32766 0 0
10: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 11
10: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 0 1 -32766 0 0
10: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 11
10: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 0 1 -32766 0 0
10: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 11
10: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 0 1 -32766 0 0
10: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 11
10: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 0 1 -32766 0 0
10: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 11
10: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 0 1 -32766 0 0
10: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 11
10: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 0 1 -32766 0 0
10: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 11
10: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 0 1 -32766 0 0
10: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 11
10: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 0 1 -32766 0 0
10: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 11
10: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 0 1 -32766 0 0
10: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 10 11
10: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 0 1 -32766 0 0
88 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
88: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 21
88: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 5.841e-06
88: communicator.hh::sendRecv,MPI_Testany 2 1 -32766 0 0 1.3041e-05
88: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 2 1 -32766 0 0
88: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 21
88: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 2 1 -32766 0 0
88: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 21
88: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 2 1 -32766 0 0
88: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 21
88: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 2 1 -32766 0 0
88: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 21
88: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 2 1 -32766 0 0
88: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 21
88: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 2 1 -32766 0 0
88: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 21
88: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 2 1 -32766 0 0
88: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 21
88: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 2 1 -32766 0 0
88: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 21
88: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 2 1 -32766 0 0
88: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 21
88: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 2 1 -32766 0 0
88: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 10 21
88: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 2 1 -32766 0 0
88: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 11 21
88: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 2 1 -32766 0 0
88: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 12 21
88: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 2 1 -32766 0 0
88: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 13 21
88: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 2 1 -32766 0 0
88: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 14 21
88: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 2 1 -32766 0 0
88: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 15 21
88: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 2 1 -32766 0 0
88: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 16 21
88: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 2 1 -32766 0 0
88: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 17 21
88: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 2 1 -32766 0 0
88: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finis1 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 5
1: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 8e-07
1: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 6.09e-06
1: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 1.151e-05
1: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000151332
1: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000281353
1: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000284423
1: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000287243
1: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000291003
1: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000294393
1: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000297773
1: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000427655
1: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000649788
1: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000655838
1: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.00079341
1: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.00079916
1: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.00080251
1: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.00080538
1: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.00081148
1: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000941472
1: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.0010746
1: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.00108313
1: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.00122464
1: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.00135197
1: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.00136908
1: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.00137194
1: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.00137496
1: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.00139198
1: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.00139502
1: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.0013978
1: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.00141063
1: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.00141379
1: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.00141658
1: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.00143026
1: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.00143335
1: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.00144828
1: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.0014511
1: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.00145407
1: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.00147409
1: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.00147694
1: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.00148051
1: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.0015031
1: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.00150626
1: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.00150914
1: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.00151207
1: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.00151473
1: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.00151749
1: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.00153507
1: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.00153836
1: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.001541
1: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.00154366
1: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.00154628
1: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.0015489
1: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.00155153
1: communicator.hh::sendRec25 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
25: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 7
25: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 1.64e-06
25: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 7.44e-06
25: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 1.961e-05
25: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 2.266e-05
25: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 4.062e-05
25: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 4.408e-05
25: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 4.681e-05
25: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 6.1941e-05
25: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 7.7421e-05
25: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 8.2301e-05
25: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 9.7501e-05
25: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000100661
25: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000103831
25: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000126601
25: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000130071
25: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000132991
25: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000135641
25: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000155822
25: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000158812
25: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000161642
25: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000164502
25: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000167112
25: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000169762
25: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000173052
25: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000193482
25: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000196342
25: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000199172
25: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000201782
25: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000204622
25: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000207382
25: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000210002
25: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000212572
25: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000215182
25: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000217803
25: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000220543
25: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000243143
25: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000246693
25: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000249283
25: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000251903
25: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000254513
25: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000257133
25: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000260073
25: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000262953
25: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000285313
25: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000288873
25: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000291713
25: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000294303
25: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000297214
25: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000299804
25: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000302684
25: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000305534
25: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000308144
25: commu14 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
14: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 17
14: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 5.45e-06
14: communicator.hh::sendRecv,MPI_Testany 0 1 -32766 0 0 1.079e-05
14: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 0 1 -32766 0 0
14: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 17
14: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 0 1 -32766 0 0
14: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 17
14: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 0 1 -32766 0 0
14: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 17
14: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 0 1 -32766 0 0
14: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 17
14: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 0 1 -32766 0 0
14: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 17
14: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 0 1 -32766 0 0
14: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 17
14: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 0 1 -32766 0 0
14: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 17
14: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 0 1 -32766 0 0
14: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 17
14: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 0 1 -32766 0 0
14: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 17
14: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 0 1 -32766 0 0
14: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 10 17
14: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 0 1 -32766 0 0
14: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 11 17
14: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 0 1 -32766 0 0
14: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 12 17
14: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 0 1 -32766 0 0
14: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 13 17
14: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 0 1 -32766 0 0
14: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 14 17
14: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 0 1 -32766 0 0
14: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 15 17
14: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 0 1 -32766 0 0
14: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 16 17
14: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 0 1 -32766 0 0
Newton::solveLinearSystem : Caught exception from the linear solver when getting converged: "BCRSMatrixError [operator++:/home/m.giraud/DUMUXexud/dune-istl/dune/istl/bcrsmatrix.hh:940]: matrix already built up"
14 converged value: 0 121 1 let s go get convergedRemote
14 to  comm_.min(converged) 
98 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
98: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 18
98: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 5.64e-06
98: communicator.hh::sendRecv,MPI_Testany 14 1 -32766 0 0 1.1981e-05
98: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 14 1 -32766 0 0
98: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 18
98: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 14 1 -32766 0 0
98: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 18
98: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 14 1 -32766 0 0
98: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 18
98: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 14 1 -32766 0 0
98: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 18
98: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 14 1 -32766 0 0
98: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 18
98: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 14 1 -32766 0 0
98: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 18
98: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 14 1 -32766 0 0
98: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 18
98: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 14 1 -32766 0 0
98: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 18
98: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 14 1 -32766 0 0
98: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 18
98: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 14 1 -32766 0 0
98: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 10 18
98: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 14 1 -32766 0 0
98: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 11 18
98: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 14 1 -32766 0 0
98: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 12 18
98: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 14 1 -32766 0 0
98: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 13 18
98: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 14 1 -32766 0 0
98: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 14 18
98: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 14 1 -32766 0 0
98: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 15 18
98: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 14 1 -32766 0 0
98: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 16 18
98: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 14 1 -32766 0 0
98: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 17 18
98: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 14 1 -32766 0 0
Newton::solveLinearSystem : Caught exception from the linear solver when getting49 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
49: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 15
49: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 1.82e-06
49: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 8.65e-06
49: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 1.242e-05
49: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 1.604e-05
49: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 1.979e-05
49: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 2.322e-05
49: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 2.66e-05
49: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 3.006e-05
49: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 3.342e-05
49: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 3.68e-05
49: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 4.018e-05
49: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 7.9631e-05
49: communicator.hh::sendRecv,MPI_Testany 5 1 -32766 0 0 8.3631e-05
49: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 5 1 -32766 0 0
49: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 15
49: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 5 1 -32766 0 0
49: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 15
49: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 5 1 -32766 0 0
49: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 15
49: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 5 1 -32766 0 0
49: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 15
49: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 5 1 -32766 0 0
49: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 15
49: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 5 1 -32766 0 0
49: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 15
49: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 5 1 -32766 0 0
49: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 15
49: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 5 1 -32766 0 0
49: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 15
49: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 5 1 -32766 0 0
49: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 15
49: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 5 1 -32766 0 0
49: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 10 15
49: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 5 1 -32766 0 0
49: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 11 15
49: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 5 1 -32766 0 0
49: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 12 15
49: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 5 1 -32766 0 0
49: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 13 15
49: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 5 1 -32766 0 0
49: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 14 15
49: communicator.h103 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
103: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 16
103: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 5.65e-06
103: communicator.hh::sendRecv,MPI_Testany 5 1 -32766 0 0 1.2721e-05
103: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 5 1 -32766 0 0
103: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 16
103: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 5 1 -32766 0 0
103: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 16
103: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 5 1 -32766 0 0
103: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 16
103: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 5 1 -32766 0 0
103: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 16
103: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 5 1 -32766 0 0
103: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 16
103: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 5 1 -32766 0 0
103: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 16
103: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 5 1 -32766 0 0
103: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 16
103: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 5 1 -32766 0 0
103: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 16
103: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 5 1 -32766 0 0
103: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 16
103: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 5 1 -32766 0 0
103: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 10 16
103: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 5 1 -32766 0 0
103: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 11 16
103: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 5 1 -32766 0 0
103: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 12 16
103: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 5 1 -32766 0 0
103: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 13 16
103: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 5 1 -32766 0 0
103: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 14 16
103: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 5 1 -32766 0 0
103: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 15 16
103: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 5 1 -32766 0 0
Newton::solveLinearSystem : Caught exception from the linear solver when getting converged: "BCRSMatrixError [operator++:/home/m.giraud/DUMUXexud/dune-istl/dune/istl/bcrsmatrix.hh:940]: matrix already built up"
103 converged value: 0 121 1 let s go get convergedRemote
103 to  comm_.min(converged) 
104 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
104: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 9
104: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 4.21e-06
104: communicator.hh::sendRecv,MPI_Testany 1 1 -32766 0 0 1.026e-05
104: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 1 1 -32766 0 0
104: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 9
104: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 1 1 -32766 0 0
104: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 9
104: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 1 1 -32766 0 0
104: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 9
104: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 1 1 -32766 0 0
104: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 9
104: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 1 1 -32766 0 0
104: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 9
104: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 1 1 -32766 0 0
104: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 9
104: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 1 1 -32766 0 0
104: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 9
104: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 1 1 -32766 0 0
104: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 9
104: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 1 1 -32766 0 0
Newton::solveLinearSystem : Caught exception from the linear solver when getting converged: "BCRSMatrixError [operator++:/home/m.giraud/DUMUXexud/dune-istl/dune/istl/bcrsmatrix.hh:940]: matrix already built up"
104 converged value: 0 121 1 let s go get convergedRemote
104 to  comm_.min(converged) 
94 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
94: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 21
94: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 4.621e-06
94: communicator.hh::sendRecv,MPI_Testany 6 1 -32766 0 0 1.0691e-05
94: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 6 1 -32766 0 0
94: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 21
94: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 6 1 -32766 0 0
94: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 21
94: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 6 1 -32766 0 0
94: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 21
94: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 6 1 -32766 0 0
94: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 21
94: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 6 1 -32766 0 0
94: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 21
94: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 6 1 -32766 0 0
94: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 21
94: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 6 1 -32766 0 0
94: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 21
94: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 6 1 -32766 0 0
94: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 21
94: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 6 1 -32766 0 0
94: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 21
94: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 6 1 -32766 0 0
94: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 10 21
94: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 6 1 -32766 0 0
94: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 11 21
94: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 6 1 -32766 0 0
94: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 12 21
94: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 6 1 -32766 0 0
94: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 13 21
94: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 6 1 -32766 0 0
94: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 14 21
94: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 6 1 -32766 0 0
94: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 15 21
94: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 6 1 -32766 0 0
94: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 16 21
94: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 6 1 -32766 0 0
94: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 17 21
94: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 6 1 -32766 0 0
94: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finis0 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 3
0: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 4.5e-07
0: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 8.33e-06
0: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000193683
0: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000329555
0: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000333395
0: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000339615
0: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000448486
0: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000452486
0: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000455976
0: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000460496
0: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000463516
0: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000466276
0: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000468976
0: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000472096
0: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000474866
0: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000477406
0: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000480016
0: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000482586
0: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000485146
0: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000487727
0: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000490357
0: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000493167
0: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000495957
0: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000498527
0: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000501157
0: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000503727
0: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000506547
0: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000509697
0: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000512497
0: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000515067
0: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000517627
0: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000520187
0: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000522797
0: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000525647
0: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000528707
0: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000531527
0: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000534077
0: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000536637
0: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000539177
0: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000541747
0: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000544357
0: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000547427
0: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000550757
0: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000555547
0: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000559037
0: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000562417
0: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000564977
0: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000567548
0: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000570658
0: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000573438
0: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000575998
0: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000578558
0: communicator.hh::sendRecv,MPI_Testany -32766 0 24 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
24: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 14
24: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 1.9e-06
24: communicator.hh::sendRecv,MPI_Testany 1 1 -32766 0 0 7.56e-06
24: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 1 1 -32766 0 0
24: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 14
24: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 1 1 -32766 0 0
24: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 14
24: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 1 1 -32766 0 0
24: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 14
24: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 1 1 -32766 0 0
24: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 14
24: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 1 1 -32766 0 0
24: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 14
24: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 1 1 -32766 0 0
24: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 14
24: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 1 1 -32766 0 0
24: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 14
24: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 1 1 -32766 0 0
24: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 14
24: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 1 1 -32766 0 0
24: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 14
24: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 1 1 -32766 0 0
24: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 10 14
24: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 1 1 -32766 0 0
24: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 11 14
24: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 1 1 -32766 0 0
24: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 12 14
24: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 1 1 -32766 0 0
24: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 13 14
24: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 1 1 -32766 0 0
Newton::solveLinearSystem : Caught exception from the linear solver when getting converged: "BCRSMatrixError [operator++:/home/m.giraud/DUMUXexud/dune-istl/dune/istl/bcrsmatrix.hh:940]: matrix already built up"
24 converged value: 0 121 1 let s go get convergedRemote
24 to  comm_.min(converged) 
42 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
42: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 20
42: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 8.86e-06
42: communicator.hh::sendRecv,MPI_Testany 0 1 -32766 0 0 1.474e-05
42: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 0 1 -32766 0 0
42: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 20
42: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 0 1 -32766 0 0
42: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 20
42: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 0 1 -32766 0 0
42: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 20
42: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 0 1 -32766 0 0
42: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 20
42: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 0 1 -32766 0 0
42: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 20
42: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 0 1 -32766 0 0
42: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 20
42: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 0 1 -32766 0 0
42: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 20
42: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 0 1 -32766 0 0
42: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 20
42: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 0 1 -32766 0 0
42: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 20
42: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 0 1 -32766 0 0
42: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 10 20
42: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 0 1 -32766 0 0
42: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 11 20
42: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 0 1 -32766 0 0
42: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 12 20
42: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 0 1 -32766 0 0
42: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 13 20
42: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 0 1 -32766 0 0
42: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 14 20
42: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 0 1 -32766 0 0
42: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 15 20
42: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 0 1 -32766 0 0
42: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 16 20
42: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 0 1 -32766 0 0
42: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 17 20
42: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 0 1 -32766 0 0
42: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finishe59 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
59: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 11
59: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 3.42e-06
59: communicator.hh::sendRecv,MPI_Testany 1 1 -32766 0 0 1.076e-05
59: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 1 1 -32766 0 0
59: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 11
59: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 1 1 -32766 0 0
59: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 11
59: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 1 1 -32766 0 0
59: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 11
59: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 1 1 -32766 0 0
59: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 11
59: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 1 1 -32766 0 0
59: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 11
59: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 1 1 -32766 0 0
59: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 11
59: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 1 1 -32766 0 0
59: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 11
59: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 1 1 -32766 0 0
59: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 11
59: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 1 1 -32766 0 0
59: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 11
59: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 1 1 -32766 0 0
59: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 10 11
59: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 1 1 -32766 0 0
Newton::solveLinearSystem : Caught exception from the linear solver when getting converged: "BCRSMatrixError [operator++:/home/m.giraud/DUMUXexud/dune-istl/dune/istl/bcrsmatrix.hh:940]: matrix already built up"
59 converged value: 0 121 1 let s go get convergedRemote
59 to  comm_.min(converged) 
80 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
80: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 10
80: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 1.56e-06
80: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 7.78e-06
80: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 1.116e-05
80: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 2.372e-05
80: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 2.747e-05
80: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 3.019e-05
80: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 3.283e-05
80: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 3.546e-05
80: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 5.0651e-05
80: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 5.4141e-05
80: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 5.7271e-05
80: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 7.3931e-05
80: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 7.6881e-05
80: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 9.8351e-05
80: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000119661
80: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000369985
80: communicator.hh::sendRecv,MPI_Testany 2 1 -32766 0 0 0.000431725
80: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 2 1 -32766 0 0
80: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 10
80: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 2 1 -32766 0 0
80: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 10
80: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 2 1 -32766 0 0
80: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 10
80: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 2 1 -32766 0 0
80: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 10
80: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 2 1 -32766 0 0
80: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 10
80: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 2 1 -32766 0 0
80: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 10
80: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 2 1 -32766 0 0
80: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 10
80: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 2 1 -32766 0 0
80: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 10
80: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 2 1 -32766 0 0
80: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 10
80: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 2 1 -32766 0 0
Newton::solveLinearSystem : Caught exception from the linear solver when getting converged: "BCRSMatrixError [operator++:/home/m.giraud/DUMUXexud/dune-istl/dune/istl/bcrsmatrix.hh:940]: matrix already built up"
80 converged value: 0 121 1 let s go get convergedRemote
80 to  comm_.min(converged) 
87 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
87: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 19
87: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 1.08e-06
87: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 7.08e-06
87: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 3.021e-05
87: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 3.326e-05
87: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 3.651e-05
87: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 3.9401e-05
87: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 4.2151e-05
87: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 4.4841e-05
87: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 4.7491e-05
87: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 5.0151e-05
87: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 5.3051e-05
87: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 7.5651e-05
87: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 7.9021e-05
87: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 8.1831e-05
87: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 8.4611e-05
87: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 8.7231e-05
87: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 8.9881e-05
87: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 9.2681e-05
87: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 9.5641e-05
87: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 9.8541e-05
87: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000124942
87: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000128412
87: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000131372
87: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000134072
87: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000136922
87: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000139642
87: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000142262
87: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000144912
87: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000147682
87: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000175282
87: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000178722
87: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000181582
87: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000184212
87: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000186842
87: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000189472
87: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000192122
87: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000194952
87: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000230033
87: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000233023
87: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000235663
87: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000238303
87: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000241513
87: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000245083
87: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000248633
87: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000252143
87: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000255653
87: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000258933
87: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000262233
87: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000265683
87: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000269053
87: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000272343
87: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000275664
87: communicato91 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
91: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 9
91: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 5.9e-07
91: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 6.16e-06
91: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 9.27e-06
91: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 5.279e-05
91: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 5.686e-05
91: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 6.0511e-05
91: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 6.4041e-05
91: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 6.7551e-05
91: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 7.0971e-05
91: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 7.4271e-05
91: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 7.7581e-05
91: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 8.0901e-05
91: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 8.4271e-05
91: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 8.7661e-05
91: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 9.1591e-05
91: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 9.4981e-05
91: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 9.8321e-05
91: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000101891
91: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000105731
91: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000109351
91: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000147712
91: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000151892
91: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000155482
91: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000159042
91: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000162662
91: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000166562
91: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000169902
91: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000174612
91: communicator.hh::sendRecv,MPI_Testany 7 1 -32766 0 0 0.000178032
91: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 7 1 -32766 0 0
91: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 9
91: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 7 1 -32766 0 0
91: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 9
91: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 7 1 -32766 0 0
91: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 9
91: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 7 1 -32766 0 0
91: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 9
91: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 7 1 -32766 0 0
91: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 9
91: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 7 1 -32766 0 0
91: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 9
91: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 7 1 -32766 0 0
91: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 9
91: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 7 1 -32766 0 0
91: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 9
91: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 7 1 -327677 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
77: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 6
77: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 1.73e-06
77: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 1.641e-05
77: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 1.98e-05
77: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 3.78e-05
77: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 4.106e-05
77: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 4.385e-05
77: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 5.9851e-05
77: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 7.6661e-05
77: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 9.2131e-05
77: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 9.5391e-05
77: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 9.8161e-05
77: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000119831
77: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000122831
77: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000125531
77: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000128601
77: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000152702
77: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000155482
77: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000158432
77: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000161042
77: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000163642
77: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000166282
77: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000169042
77: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000190822
77: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000194262
77: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000197012
77: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000199772
77: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000202502
77: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000205212
77: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000207962
77: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000210762
77: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000213472
77: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000216083
77: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000218823
77: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000242413
77: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000245303
77: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000248103
77: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000250893
77: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000253513
77: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000256123
77: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000258733
77: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000261483
77: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000283583
77: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000286623
77: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000289243
77: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000291893
77: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000294613
77: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000297364
77: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000300134
77: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000302864
77: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000305484
77: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000308214
77: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000336454
77: commu97 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
97: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 9
97: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 4.2e-07
97: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 2.09e-05
97: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 2.401e-05
97: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 2.674e-05
97: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 2.99e-05
97: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 3.285e-05
97: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 3.55e-05
97: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 3.825e-05
97: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 4.101e-05
97: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 6.457e-05
97: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 6.794e-05
97: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 7.081e-05
97: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 7.365e-05
97: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 7.625e-05
97: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 7.8861e-05
97: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 8.1541e-05
97: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 8.4571e-05
97: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 8.7241e-05
97: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 9.0201e-05
97: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000110631
97: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000114051
97: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000116801
97: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000119471
97: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000122111
97: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000125021
97: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000127881
97: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000130521
97: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000157042
97: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000160312
97: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000163092
97: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000165982
97: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000168802
97: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000171422
97: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000174042
97: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000176662
97: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000179412
97: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000207802
97: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000211312
97: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000214202
97: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000217362
97: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000220002
97: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000222622
97: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000225532
97: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000228502
97: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000262783
97: communicator.hh::sendRecv,MPI_Testany 8 1 -32766 0 0 0.000266133
97: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 8 1 -32766 0 0
97: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 9
97: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 8 1 -32766 0 0
97: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 9
97: communicator.hh::sendRecv, before assert(finished != MPI_UNDEF47 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
47: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 6
47: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 1.04e-06
47: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 6.91e-06
47: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 3.032e-05
47: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 3.3821e-05
47: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 3.6681e-05
47: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 3.9411e-05
47: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 4.2231e-05
47: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 6.2011e-05
47: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 6.5131e-05
47: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 6.7761e-05
47: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 7.0501e-05
47: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 7.3381e-05
47: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 7.6101e-05
47: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 7.8741e-05
47: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 8.1371e-05
47: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 8.3991e-05
47: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 8.6741e-05
47: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000106961
47: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000110611
47: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000113472
47: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000116542
47: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000119392
47: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000122052
47: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000124702
47: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000127492
47: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000130382
47: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000152882
47: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000156142
47: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000158772
47: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000161412
47: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000164322
47: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000167072
47: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000169692
47: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000172302
47: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000175052
47: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000201463
47: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000204663
47: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000207433
47: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000210043
47: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000212653
47: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000215263
47: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000217893
47: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000220783
47: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000223513
47: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000226423
47: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000256223
47: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000259453
47: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000262093
47: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000265003
47: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000267763
47: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000270354
47: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000272974
47: communi74 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
74: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 11
74: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 5.61e-06
74: communicator.hh::sendRecv,MPI_Testany 4 1 -32766 0 0 1.143e-05
74: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 4 1 -32766 0 0
74: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 11
74: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 4 1 -32766 0 0
74: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 11
74: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 4 1 -32766 0 0
74: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 11
74: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 4 1 -32766 0 0
74: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 11
74: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 4 1 -32766 0 0
74: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 11
74: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 4 1 -32766 0 0
74: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 11
74: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 4 1 -32766 0 0
74: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 11
74: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 4 1 -32766 0 0
74: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 11
74: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 4 1 -32766 0 0
74: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 11
74: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 4 1 -32766 0 0
74: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 10 11
74: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 4 1 -32766 0 0
Newton::solveLinearSystem : Caught exception from the linear solver when getting converged: "BCRSMatrixError [operator++:/home/m.giraud/DUMUXexud/dune-istl/dune/istl/bcrsmatrix.hh:940]: matrix already built up"
74 converged value: 0 121 1 let s go get convergedRemote
74 to  comm_.min(converged) 
21 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
21: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 12
21: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 1.07e-06
21: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 2.271e-05
21: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 2.584e-05
21: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 2.853e-05
21: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 3.146e-05
21: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 3.416e-05
21: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 3.677e-05
21: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 3.937e-05
21: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 4.206e-05
21: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 4.478e-05
21: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 4.762e-05
21: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 6.6921e-05
21: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 7.0091e-05
21: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 7.2801e-05
21: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 7.5591e-05
21: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 7.8311e-05
21: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 8.1021e-05
21: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 8.3771e-05
21: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 8.6581e-05
21: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 8.9261e-05
21: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 9.2111e-05
21: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000112661
21: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000116081
21: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000118771
21: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000121651
21: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000124341
21: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000126901
21: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000129461
21: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000132021
21: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000134601
21: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000137661
21: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000161202
21: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000164252
21: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000166812
21: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000169392
21: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000171952
21: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000175382
21: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000179222
21: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000182762
21: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000186862
21: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000219972
21: communicator.hh::sendRecv,MPI_Testany 9 1 -32766 0 0 0.000223293
21: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 9 1 -32766 0 0
21: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 12
21: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 9 1 -32766 0 0
21: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 12
21: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 9 1 -32766 0 0
21: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 12
21: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 9 1 -32766 0 0
21: communicator.hh::sendRecv, before MPI_Waitany(messageInf51 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
51: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 10
51: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 4.11e-06
51: communicator.hh::sendRecv,MPI_Testany 0 1 -32766 0 0 1.155e-05
51: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 0 1 -32766 0 0
51: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 10
51: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 0 1 -32766 0 0
51: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 10
51: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 0 1 -32766 0 0
51: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 10
51: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 0 1 -32766 0 0
51: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 10
51: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 0 1 -32766 0 0
51: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 10
51: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 0 1 -32766 0 0
51: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 10
51: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 0 1 -32766 0 0
51: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 10
51: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 0 1 -32766 0 0
51: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 10
51: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 0 1 -32766 0 0
51: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 10
51: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 0 1 -32766 0 0
Newton::solveLinearSystem : Caught exception from the linear solver when getting converged: "BCRSMatrixError [operator++:/home/m.giraud/DUMUXexud/dune-istl/dune/istl/bcrsmatrix.hh:940]: matrix already built up"
51 converged value: 0 121 1 let s go get convergedRemote
51 to  comm_.min(converged) 
51 did  comm_.min(converged) convergedRemote: 0
51 final convergedRemote: 0
51 about to throw Linear solver did not converge
51 Newton::solveLinearSystem : Caught exception: "NumericalProblem [solveLinearSystem:/home/m.giraud/DUMUXexud/dumux/dumux/nonlinear/newtonsolver.hh:381]: Linear solver did not converge when getting convergedRemote "
51 Newton: Caught exception: "NumericalProblem [solveLinearSystem:/home/m.giraud/DUMUXexud/dumux/dumux/nonlinear/newtonsolver.hh:381]: Linear solver did not converge"
51 Newton solver did not converge with dt = 0.0495 seconds. Retrying with time step of 0.02475 seconds
51 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
15 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
15: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 8
15: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 2.57e-06
15: communicator.hh::sendRecv,MPI_Testany 3 1 -32766 0 0 1.1811e-05
15: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 3 1 -32766 0 0
15: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 8
15: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 3 1 -32766 0 0
15: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 8
15: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 3 1 -32766 0 0
15: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 8
15: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 3 1 -32766 0 0
15: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 8
15: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 3 1 -32766 0 0
15: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 8
15: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 3 1 -32766 0 0
15: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 8
15: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 3 1 -32766 0 0
15: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 8
15: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 3 1 -32766 0 0
Newton::solveLinearSystem : Caught exception from the linear solver when getting converged: "BCRSMatrixError [operator++:/home/m.giraud/DUMUXexud/dune-istl/dune/istl/bcrsmatrix.hh:940]: matrix already built up"
15 converged value: 0 121 1 let s go get convergedRemote
15 to  comm_.min(converged) 
15 did  comm_.min(converged) convergedRemote: 0
15 final convergedRemote: 0
15 about to throw Linear solver did not converge
15 Newton::solveLinearSystem : Caught exception: "NumericalProblem [solveLinearSystem:/home/m.giraud/DUMUXexud/dumux/dumux/nonlinear/newtonsolver.hh:381]: Linear solver did not converge when getting convergedRemote "
15 Newton: Caught exception: "NumericalProblem [solveLinearSystem:/home/m.giraud/DUMUXexud/dumux/dumux/nonlinear/newtonsolver.hh:381]: Linear solver did not converge"
15 Newton solver did not converge with dt = 0.0495 seconds. Retrying with time step of 0.02475 seconds
15 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
19 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
19: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 17
19: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 2.5e-06
19: communicator.hh::sendRecv,MPI_Testany 8 1 -32766 0 0 1.004e-05
19: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 8 1 -32766 0 0
19: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 17
19: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 8 1 -32766 0 0
19: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 17
19: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 8 1 -32766 0 0
19: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 17
19: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 8 1 -32766 0 0
19: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 17
19: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 8 1 -32766 0 0
19: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 17
19: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 8 1 -32766 0 0
19: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 17
19: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 8 1 -32766 0 0
19: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 17
19: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 8 1 -32766 0 0
19: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 17
19: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 8 1 -32766 0 0
19: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 17
19: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 8 1 -32766 0 0
19: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 10 17
19: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 8 1 -32766 0 0
19: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 11 17
19: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 8 1 -32766 0 0
19: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 12 17
19: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 8 1 -32766 0 0
19: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 13 17
19: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 8 1 -32766 0 0
19: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 14 17
19: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 8 1 -32766 0 0
19: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 15 17
19: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 8 1 -32766 0 0
19: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 16 17
19: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 8 1 -32766 0 0
Newton::solveLinearSystem : Caught exception from the linear solver when getting converged: "BCRSMatrixError [operator++:/home/m.giraud/DUMUXexud/dune-istl/dune/istl/bcrsmatrix.hh:940]: matrix already built up"
19 converged value: 0 121 1 let s go get convergedRemote
19 to  comm_.min(converged) 
19 did  com38 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
38: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 24
38: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 6.77e-06
38: communicator.hh::sendRecv,MPI_Testany 9 1 -32766 0 0 1.269e-05
38: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 9 1 -32766 0 0
38: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 24
38: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 9 1 -32766 0 0
38: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 24
38: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 9 1 -32766 0 0
38: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 24
38: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 9 1 -32766 0 0
38: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 24
38: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 9 1 -32766 0 0
38: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 24
38: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 9 1 -32766 0 0
38: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 24
38: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 9 1 -32766 0 0
38: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 24
38: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 9 1 -32766 0 0
38: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 24
38: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 9 1 -32766 0 0
38: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 24
38: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 9 1 -32766 0 0
38: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 10 24
38: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 9 1 -32766 0 0
38: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 11 24
38: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 9 1 -32766 0 0
38: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 12 24
38: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 9 1 -32766 0 0
38: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 13 24
38: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 9 1 -32766 0 0
38: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 14 24
38: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 9 1 -32766 0 0
38: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 15 24
38: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 9 1 -32766 0 0
38: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 16 24
38: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 9 1 -32766 0 0
38: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 17 24
38: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 9 1 -32766 0 0
38: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finishe83 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
83: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 17
83: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 1.19e-06
83: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 4.804e-05
83: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 5.193e-05
83: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 5.537e-05
83: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 5.9031e-05
83: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 6.2591e-05
83: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 6.7541e-05
83: communicator.hh::sendRecv,MPI_Testany 9 1 -32766 0 0 7.1151e-05
83: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 9 1 -32766 0 0
83: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 17
83: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 9 1 -32766 0 0
83: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 17
83: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 9 1 -32766 0 0
83: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 17
83: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 9 1 -32766 0 0
83: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 17
83: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 9 1 -32766 0 0
83: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 17
83: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 9 1 -32766 0 0
83: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 17
83: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 9 1 -32766 0 0
83: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 17
83: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 9 1 -32766 0 0
83: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 17
83: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 9 1 -32766 0 0
83: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 17
83: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 9 1 -32766 0 0
83: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 10 17
83: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 9 1 -32766 0 0
83: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 11 17
83: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 9 1 -32766 0 0
83: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 12 17
83: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 9 1 -32766 0 0
83: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 13 17
83: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 9 1 -32766 0 0
83: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 14 17
83: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 9 1 -32766 0 0
83: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 15 17
83: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 9 1 -32766 0 0
83: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), 118 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
118: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 14
118: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 6.48e-06
118: communicator.hh::sendRecv,MPI_Testany 2 1 -32766 0 0 1.252e-05
118: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 2 1 -32766 0 0
118: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 14
118: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 2 1 -32766 0 0
118: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 14
118: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 2 1 -32766 0 0
118: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 14
118: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 2 1 -32766 0 0
118: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 14
118: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 2 1 -32766 0 0
118: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 14
118: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 2 1 -32766 0 0
118: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 14
118: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 2 1 -32766 0 0
118: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 14
118: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 2 1 -32766 0 0
118: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 14
118: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 2 1 -32766 0 0
118: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 14
118: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 2 1 -32766 0 0
118: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 10 14
118: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 2 1 -32766 0 0
118: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 11 14
118: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 2 1 -32766 0 0
118: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 12 14
118: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 2 1 -32766 0 0
118: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 13 14
118: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 2 1 -32766 0 0
Newton::solveLinearSystem : Caught exception from the linear solver when getting converged: "BCRSMatrixError [operator++:/home/m.giraud/DUMUXexud/dune-istl/dune/istl/bcrsmatrix.hh:940]: matrix already built up"
118 converged value: 0 121 1 let s go get convergedRemote
118 to  comm_.min(converged) 
118 did  comm_.min(converged) convergedRemote: 0
118 final convergedRemote: 0
118 about to throw Linear solver did not converge
118 Newton::solveLinearSystem : Caught exception: "NumericalProblem [solveLinearSystem:/home/m.giraud/DUMUXexud/dumux/dumux/nonlinear/newtonsolver.hh:381]: Linear solver did not converge when getting convergedRemote "
118 Newton: Caught exception: "NumericalProblem [solveLinearSystem:/home/m.giraud/DUMUXexud/dumux/dumux/nonlinear/newtonsolver.hh:381]: Linear solver did not converge"
118 Newton solver did not converge with dt = 0.0495 seconds. Retrying with time step33 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
33: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 9
33: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 4.6e-07
33: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 5.81e-06
33: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 2.691e-05
33: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 3.041e-05
33: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 3.347e-05
33: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 3.623e-05
33: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 3.883e-05
33: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 4.14e-05
33: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 4.404e-05
33: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 4.679e-05
33: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 4.9571e-05
33: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 5.2491e-05
33: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 5.5391e-05
33: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 7.5841e-05
33: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 7.8811e-05
33: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 8.1551e-05
33: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 8.4151e-05
33: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 8.7401e-05
33: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 9.0201e-05
33: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 9.2791e-05
33: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 9.5381e-05
33: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 9.8111e-05
33: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000122871
33: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000126161
33: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000128982
33: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000131582
33: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000134192
33: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000136792
33: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000139392
33: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000141982
33: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000144972
33: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000172242
33: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000175672
33: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000178532
33: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000181392
33: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000184232
33: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000187012
33: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000189872
33: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000192482
33: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000225233
33: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000228263
33: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000230853
33: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000233993
33: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000236593
33: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000239393
33: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000241993
33: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000244573
33: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000247163
33: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000250303
33: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000252993
33: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000255963
33: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000301864
33: communicator.hh::send67 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
67: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 13
67: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 4.65e-06
67: communicator.hh::sendRecv,MPI_Testany 5 1 -32766 0 0 1.287e-05
67: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 5 1 -32766 0 0
67: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 13
67: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 5 1 -32766 0 0
67: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 13
67: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 5 1 -32766 0 0
67: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 13
67: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 5 1 -32766 0 0
67: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 13
67: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 5 1 -32766 0 0
67: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 13
67: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 5 1 -32766 0 0
67: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 13
67: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 5 1 -32766 0 0
67: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 13
67: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 5 1 -32766 0 0
67: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 13
67: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 5 1 -32766 0 0
67: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 13
67: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 5 1 -32766 0 0
67: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 10 13
67: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 5 1 -32766 0 0
67: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 11 13
67: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 5 1 -32766 0 0
67: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 12 13
67: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 5 1 -32766 0 0
Newton::solveLinearSystem : Caught exception from the linear solver when getting converged: "BCRSMatrixError [operator++:/home/m.giraud/DUMUXexud/dune-istl/dune/istl/bcrsmatrix.hh:940]: matrix already built up"
67 converged value: 0 121 1 let s go get convergedRemote
67 to  comm_.min(converged) 
67 did  comm_.min(converged) convergedRemote: 0
67 final convergedRemote: 0
67 about to throw Linear solver did not converge
67 Newton::solveLinearSystem : Caught exception: "NumericalProblem [solveLinearSystem:/home/m.giraud/DUMUXexud/dumux/dumux/nonlinear/newtonsolver.hh:381]: Linear solver did not converge when getting convergedRemote "
67 Newton: Caught exception: "NumericalProblem [solveLinearSystem:/home/m.giraud/DUMUXexud/dumux/dumux/nonlinear/newtonsolver.hh:381]: Linear solver did not converge"
67 Newton solver did not converge with dt = 0.0495 seconds. Retrying with time step of 0.02475 seconds
67 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
112 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
112: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 5
112: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 9.7e-07
112: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 7.3e-06
112: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 1.774e-05
112: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 2.159e-05
112: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 2.488e-05
112: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 2.81e-05
112: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 3.936e-05
112: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 4.277e-05
112: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 4.544e-05
112: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 4.833e-05
112: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 5.093e-05
112: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 5.438e-05
112: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 6.8901e-05
112: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 7.2351e-05
112: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 7.5361e-05
112: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 9.6541e-05
112: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000113681
112: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000117891
112: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000139042
112: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000390775
112: communicator.hh::sendRecv,MPI_Testany 3 1 -32766 0 0 0.000454305
112: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 3 1 -32766 0 0
112: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 5
112: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 3 1 -32766 0 0
112: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 5
112: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 3 1 -32766 0 0
112: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 5
112: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 3 1 -32766 0 0
112: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 5
112: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 3 1 -32766 0 0
Newton::solveLinearSystem : Caught exception from the linear solver when getting converged: "BCRSMatrixError [operator++:/home/m.giraud/DUMUXexud/dune-istl/dune/istl/bcrsmatrix.hh:940]: matrix already built up"
112 converged value: 0 121 1 let s go get convergedRemote
112 to  comm_.min(converged) 
112 did  comm_.min(converged) convergedRemote: 0
112 final convergedRemote: 0
112 about to throw Linear solver did not converge
112 Newton::solveLinearSystem : Caught exception: "NumericalProblem [solveLinearSystem:/home/m.giraud/DUMUXexud/dumux/dumux/nonlinear/newtonsolver.hh:381]: Linear solver did not converge when getting convergedRemote "
112 Newton: Caught exception: "NumericalProblem [solveLinearSystem:/home/m.giraud/DUMUXexud/dumux/dumux/nonlinear/newtonsolver.hh:381]: Linear solver did not converge"
112 Newton solver did not converge with dt = 0.0495 seconds. Retrying with time step of 0.02475 seconds
112 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
48 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
48: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 15
48: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 1.04e-06
48: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 7.26e-06
48: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 1.061e-05
48: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 2.6991e-05
48: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 3.0301e-05
48: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 3.3161e-05
48: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 5.0511e-05
48: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 5.3791e-05
48: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 5.6621e-05
48: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 7.5531e-05
48: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 9.6141e-05
48: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000230623
48: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000369335
48: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000372395
48: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000375035
48: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000377655
48: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000380245
48: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000383165
48: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000386055
48: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000388775
48: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000391485
48: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000394035
48: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000396595
48: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000399215
48: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000401875
48: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000404435
48: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000407115
48: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000409675
48: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000412465
48: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000439096
48: communicator.hh::sendRecv,MPI_Testany 9 1 -32766 0 0 0.000442556
48: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 9 1 -32766 0 0
48: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 15
48: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 9 1 -32766 0 0
48: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 15
48: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 9 1 -32766 0 0
48: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 15
48: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 9 1 -32766 0 0
48: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 15
48: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 9 1 -32766 0 0
48: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 15
48: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 9 1 -32766 0 0
48: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 15
48: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 9 1 -32766 0 0
48: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 15
48: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 9 1 -32766 0 0
48: communicator.hh::sendRecv, be5 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 11
5: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 2.121e-06
5: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 9.261e-06
5: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 1.2951e-05
5: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 1.6571e-05
5: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 2.0201e-05
5: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 2.3881e-05
5: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 2.7811e-05
5: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 3.2171e-05
5: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 3.6721e-05
5: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 4.1301e-05
5: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 4.7611e-05
5: communicator.hh::sendRecv,MPI_Testany 0 1 -32766 0 0 8.6582e-05
5: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 0 1 -32766 0 0
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 11
5: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 0 1 -32766 0 0
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 11
5: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 0 1 -32766 0 0
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 11
5: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 0 1 -32766 0 0
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 11
5: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 0 1 -32766 0 0
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 11
5: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 0 1 -32766 0 0
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 11
5: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 0 1 -32766 0 0
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 11
5: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 0 1 -32766 0 0
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 11
5: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 0 1 -32766 0 0
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 11
5: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 0 1 -32766 0 0
5: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 10 11
5: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 0 1 -32766 0 0
Newton::solveLinearSystem : Caught exception from the linear solver when getting converged: "BCRSMatrixError [operator++:/home/m.giraud/DUMUXexud/dune-istl/dune/istl/bcrsmatrix.hh:940]: matrix already built up"
5 converged value: 0 121 1 let s go get convergedRemote
5 to  comm_.min(converged) 
5 did  comm_.min(converged) convergedRemote: 0
5 final convergedRemote: 0
5 about to throw Linear solver did not converge
5 Newton::solveLinearSystem : Caught exception: "NumericalProblem [solveLinearSystem:/home/m.giraud/DUMUXexud/dumux/dumux/nonlinear/newtonsolver.hh:381]: Linear solver did not converge when getting convergedRemote "
5 Newton: Caught exception: "NumericalProblem [solveLinearSystem:/home/m.giraud/DUMUXexud/dumux/dumux/nonlinear/newtonsolver.hh:381]: Linear solver did not converge"
5 Newton solver did not converge with dt = 0.0495 seco36 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
36: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 14
36: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 5.22e-06
36: communicator.hh::sendRecv,MPI_Testany 2 1 -32766 0 0 1.16e-05
36: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 2 1 -32766 0 0
36: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 14
36: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 2 1 -32766 0 0
36: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 14
36: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 2 1 -32766 0 0
36: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 14
36: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 2 1 -32766 0 0
36: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 14
36: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 2 1 -32766 0 0
36: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 14
36: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 2 1 -32766 0 0
36: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 14
36: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 2 1 -32766 0 0
36: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 14
36: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 2 1 -32766 0 0
36: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 14
36: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 2 1 -32766 0 0
36: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 14
36: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 2 1 -32766 0 0
36: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 10 14
36: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 2 1 -32766 0 0
36: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 11 14
36: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 2 1 -32766 0 0
36: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 12 14
36: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 2 1 -32766 0 0
36: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 13 14
36: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 2 1 -32766 0 0
Newton::solveLinearSystem : Caught exception from the linear solver when getting converged: "BCRSMatrixError [operator++:/home/m.giraud/DUMUXexud/dune-istl/dune/istl/bcrsmatrix.hh:940]: matrix already built up"
36 converged value: 0 121 1 let s go get convergedRemote
36 to  comm_.min(converged) 
36 did  comm_.min(converged) convergedRemote: 0
36 final convergedRemote: 0
36 about to throw Linear solver did not converge
36 Newton::solveLinearSystem : Caught exception: "NumericalProblem [solveLinearSystem:/home/m.giraud/DUMUXexud/dumux/dumux/nonlinear/newtonsolver.hh:381]: Linear solver did not converge when getting convergedRemote "
36 Newton: Caught exception: "NumericalProblem [solveLinearSystem:/home/m.giraud/DUMUXexud/dumux/dumux/nonlinear/newtonsolver.hh:381]: Linear solver did not converge"
36 Newton solver did not converge with dt = 0.0495 seconds. Retrying with time step of 0.02475 seconds
36 Assemble: r(x^k) 52 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
52: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 26
52: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 5.96e-06
52: communicator.hh::sendRecv,MPI_Testany 4 1 -32766 0 0 1.167e-05
52: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 4 1 -32766 0 0
52: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 26
52: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 4 1 -32766 0 0
52: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 26
52: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 4 1 -32766 0 0
52: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 26
52: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 4 1 -32766 0 0
52: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 26
52: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 4 1 -32766 0 0
52: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 26
52: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 4 1 -32766 0 0
52: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 26
52: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 4 1 -32766 0 0
52: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 26
52: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 4 1 -32766 0 0
52: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 26
52: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 4 1 -32766 0 0
52: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 26
52: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 4 1 -32766 0 0
52: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 10 26
52: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 4 1 -32766 0 0
52: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 11 26
52: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 4 1 -32766 0 0
52: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 12 26
52: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 4 1 -32766 0 0
52: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 13 26
52: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 4 1 -32766 0 0
52: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 14 26
52: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 4 1 -32766 0 0
52: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 15 26
52: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 4 1 -32766 0 0
52: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 16 26
52: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 4 1 -32766 0 0
52: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 17 26
52: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 4 1 -32766 0 0
52: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finishe3 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 9
3: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 1.3e-06
3: communicator.hh::sendRecv,MPI_Testany 1 1 -32766 0 0 6.69e-06
3: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 1 1 -32766 0 0
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 9
3: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 1 1 -32766 0 0
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 9
3: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 1 1 -32766 0 0
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 9
3: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 1 1 -32766 0 0
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 9
3: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 1 1 -32766 0 0
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 9
3: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 1 1 -32766 0 0
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 9
3: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 1 1 -32766 0 0
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 9
3: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 1 1 -32766 0 0
3: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 9
3: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 1 1 -32766 0 0
Newton::solveLinearSystem : Caught exception from the linear solver when getting converged: "BCRSMatrixError [operator++:/home/m.giraud/DUMUXexud/dune-istl/dune/istl/bcrsmatrix.hh:940]: matrix already built up"
3 converged value: 0 121 1 let s go get convergedRemote
3 to  comm_.min(converged) 
3 did  comm_.min(converged) convergedRemote: 0
3 final convergedRemote: 0
3 about to throw Linear solver did not converge
3 Newton::solveLinearSystem : Caught exception: "NumericalProblem [solveLinearSystem:/home/m.giraud/DUMUXexud/dumux/dumux/nonlinear/newtonsolver.hh:381]: Linear solver did not converge when getting convergedRemote "
3 Newton: Caught exception: "NumericalProblem [solveLinearSystem:/home/m.giraud/DUMUXexud/dumux/dumux/nonlinear/newtonsolver.hh:381]: Linear solver did not converge"
3 Newton solver did not converge with dt = 0.0495 seconds. Retrying with time step of 0.02475 seconds
3 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
76 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
76: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 12
76: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 2.08e-06
76: communicator.hh::sendRecv,MPI_Testany 7 1 -32766 0 0 7.8e-06
76: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 7 1 -32766 0 0
76: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 12
76: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 7 1 -32766 0 0
76: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 12
76: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 7 1 -32766 0 0
76: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 12
76: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 7 1 -32766 0 0
76: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 12
76: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 7 1 -32766 0 0
76: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 12
76: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 7 1 -32766 0 0
76: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 12
76: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 7 1 -32766 0 0
76: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 12
76: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 7 1 -32766 0 0
76: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 12
76: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 7 1 -32766 0 0
76: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 12
76: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 7 1 -32766 0 0
76: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 10 12
76: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 7 1 -32766 0 0
76: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 11 12
76: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 7 1 -32766 0 0
Newton::solveLinearSystem : Caught exception from the linear solver when getting converged: "BCRSMatrixError [operator++:/home/m.giraud/DUMUXexud/dune-istl/dune/istl/bcrsmatrix.hh:940]: matrix already built up"
76 converged value: 0 121 1 let s go get convergedRemote
76 to  comm_.min(converged) 
76 did  comm_.min(converged) convergedRemote: 0
76 final convergedRemote: 0
76 about to throw Linear solver did not converge
76 Newton::solveLinearSystem : Caught exception: "NumericalProblem [solveLinearSystem:/home/m.giraud/DUMUXexud/dumux/dumux/nonlinear/newtonsolver.hh:381]: Linear solver did not converge when getting convergedRemote "
76 Newton: Caught exception: "NumericalProblem [solveLinearSystem:/home/m.giraud/DUMUXexud/dumux/dumux/nonlinear/newtonsolver.hh:381]: Linear solver did not converge"
76 Newton solver did not converge with dt = 0.0495 seconds. Retrying with time step of 0.02475 seconds
76 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
66 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
66: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 15
66: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 4.14e-06
66: communicator.hh::sendRecv,MPI_Testany 10 1 -32766 0 0 9.84e-06
66: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 10 1 -32766 0 0
66: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 15
66: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 10 1 -32766 0 0
66: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 15
66: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 10 1 -32766 0 0
66: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 15
66: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 10 1 -32766 0 0
66: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 15
66: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 10 1 -32766 0 0
66: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 15
66: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 10 1 -32766 0 0
66: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 15
66: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 10 1 -32766 0 0
66: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 15
66: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 10 1 -32766 0 0
66: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 15
66: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 10 1 -32766 0 0
66: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 15
66: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 10 1 -32766 0 0
66: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 10 15
66: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 10 1 -32766 0 0
66: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 11 15
66: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 10 1 -32766 0 0
66: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 12 15
66: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 10 1 -32766 0 0
66: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 13 15
66: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 10 1 -32766 0 0
66: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 14 15
66: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 10 1 -32766 0 0
Newton::solveLinearSystem : Caught exception from the linear solver when getting converged: "BCRSMatrixError [operator++:/home/m.giraud/DUMUXexud/dune-istl/dune/istl/bcrsmatrix.hh:940]: matrix already built up"
66 converged value: 0 121 1 let s go get convergedRemote
66 to  comm_.min(converged) 
66 did  comm_.min(converged) convergedRemote: 0
66 final convergedRemote: 0
66 about to throw Linear solver did not converge
66 Newton::solveLinearSystem : Caught exception: "NumericalProblem [solveLinearSystem:/home/m.giraud/DUMUXexud/dumux/dumux/nonlinear/newtonsolver.hh:381]: Linear solver did not converge when getting convergedRemote "
66 Newton: Caught exception: "NumericalProblem [solveLinearSystem:/55 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
55: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 11
55: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 1.72e-06
55: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 1.0221e-05
55: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 2.9521e-05
55: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 3.3121e-05
55: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 3.6771e-05
55: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 4.0121e-05
55: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 4.3441e-05
55: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 6.6631e-05
55: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 7.0401e-05
55: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 7.3601e-05
55: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 7.7531e-05
55: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 8.1351e-05
55: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 8.5181e-05
55: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 8.8242e-05
55: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 9.1562e-05
55: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 9.4902e-05
55: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000117882
55: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000121852
55: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000125302
55: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000128752
55: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000131892
55: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000134992
55: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000138292
55: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000164602
55: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000168013
55: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000171603
55: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000174683
55: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000178113
55: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000181193
55: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000184413
55: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000214523
55: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000218693
55: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000222013
55: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000225323
55: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000228903
55: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000232233
55: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000235993
55: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000268374
55: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000272394
55: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000275684
55: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000278994
55: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000282804
55: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000286654
55: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000290814
55: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000294874
55: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000298934
55: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000303014
55: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000354555
55: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000359335
55: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000363485
55: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000367615
55: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000372055
55: co30 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
30: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 6
30: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 6.6e-07
30: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000255934
30: communicator.hh::sendRecv,MPI_Testany 3 1 -32766 0 0 0.000320934
30: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 3 1 -32766 0 0
30: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 6
30: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 3 1 -32766 0 0
30: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 6
30: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 3 1 -32766 0 0
30: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 6
30: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 3 1 -32766 0 0
30: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 6
30: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 3 1 -32766 0 0
30: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 6
30: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 3 1 -32766 0 0
Newton::solveLinearSystem : Caught exception from the linear solver when getting converged: "BCRSMatrixError [operator++:/home/m.giraud/DUMUXexud/dune-istl/dune/istl/bcrsmatrix.hh:940]: matrix already built up"
30 converged value: 0 121 1 let s go get convergedRemote
30 to  comm_.min(converged) 
30 did  comm_.min(converged) convergedRemote: 0
30 final convergedRemote: 0
30 about to throw Linear solver did not converge
30 Newton::solveLinearSystem : Caught exception: "NumericalProblem [solveLinearSystem:/home/m.giraud/DUMUXexud/dumux/dumux/nonlinear/newtonsolver.hh:381]: Linear solver did not converge when getting convergedRemote "
30 Newton: Caught exception: "NumericalProblem [solveLinearSystem:/home/m.giraud/DUMUXexud/dumux/dumux/nonlinear/newtonsolver.hh:381]: Linear solver did not converge"
30 Newton solver did not converge with dt = 0.0495 seconds. Retrying with time step of 0.02475 seconds
30 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
69 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
69: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 7
69: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 5.5e-07
69: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 7.81e-06
69: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 1.144e-05
69: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 1.482e-05
69: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 6.1831e-05
69: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 6.5651e-05
69: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 6.9101e-05
69: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 7.2521e-05
69: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 7.5851e-05
69: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 7.9281e-05
69: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 8.3521e-05
69: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 8.8001e-05
69: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 9.1971e-05
69: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 9.5421e-05
69: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 9.9361e-05
69: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000102601
69: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000106011
69: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000110311
69: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000113881
69: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000117141
69: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000120471
69: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000124261
69: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000160482
69: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000165162
69: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000169032
69: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000172322
69: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000175862
69: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000179172
69: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000184192
69: communicator.hh::sendRecv,MPI_Testany 2 1 -32766 0 0 0.000187932
69: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 2 1 -32766 0 0
69: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 7
69: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 2 1 -32766 0 0
69: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 7
69: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 2 1 -32766 0 0
69: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 7
69: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 2 1 -32766 0 0
69: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 7
69: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 2 1 -32766 0 0
69: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 7
69: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 2 1 -32766 0 0
69: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 7
69: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 2 1 -32766 0 0
Newton::solveLinearSystem : Caught exception from the linear solver when getting converged: "BCRSMatrixError [operator++:/home/m.giraud/DUMUXexud/dune-istl/dune/istl/bcrsmatrix.hh:940]: matrix already built up"
69 converged value: 0 121 1 let s go get convergedRemote
69 to  comm_.min(converged) 
69 did  comm_.min(converged) 40 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
40: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 15
40: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 6.17e-06
40: communicator.hh::sendRecv,MPI_Testany 9 1 -32766 0 0 1.367e-05
40: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 9 1 -32766 0 0
40: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 15
40: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 9 1 -32766 0 0
40: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 15
40: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 9 1 -32766 0 0
40: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 15
40: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 9 1 -32766 0 0
40: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 15
40: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 9 1 -32766 0 0
40: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 15
40: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 9 1 -32766 0 0
40: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 15
40: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 9 1 -32766 0 0
40: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 15
40: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 9 1 -32766 0 0
40: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 15
40: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 9 1 -32766 0 0
40: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 15
40: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 9 1 -32766 0 0
40: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 10 15
40: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 9 1 -32766 0 0
40: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 11 15
40: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 9 1 -32766 0 0
40: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 12 15
40: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 9 1 -32766 0 0
40: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 13 15
40: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 9 1 -32766 0 0
40: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 14 15
40: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 9 1 -32766 0 0
Newton::solveLinearSystem : Caught exception from the linear solver when getting converged: "BCRSMatrixError [operator++:/home/m.giraud/DUMUXexud/dune-istl/dune/istl/bcrsmatrix.hh:940]: matrix already built up"
40 converged value: 0 121 1 let s go get convergedRemote
40 to  comm_.min(converged) 
40 did  comm_.min(converged) convergedRemote: 0
40 final convergedRemote: 0
40 about to throw Linear solver did not converge
40 Newton::solveLinearSystem : Caught exception: "NumericalProblem [solveLinearSystem:/home/m.giraud/DUMUXexud/dumux/dumux/nonlinear/newtonsolver.hh:381]: Linear solver did not converge when getting convergedRemote "
40 Newton: Caught exception: "NumericalProblem [solveLinearSystem:/home/m.giraud/D16 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
16: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 6
16: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 1.03e-06
16: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 1.277e-05
16: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000124741
16: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000205282
16: communicator.hh::sendRecv,MPI_Testany 1 1 -32766 0 0 0.000247353
16: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 1 1 -32766 0 0
16: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 6
16: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 1 1 -32766 0 0
16: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 6
16: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 1 1 -32766 0 0
16: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 6
16: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 1 1 -32766 0 0
16: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 6
16: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 1 1 -32766 0 0
16: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 6
16: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 1 1 -32766 0 0
Newton::solveLinearSystem : Caught exception from the linear solver when getting converged: "BCRSMatrixError [operator++:/home/m.giraud/DUMUXexud/dune-istl/dune/istl/bcrsmatrix.hh:940]: matrix already built up"
16 converged value: 0 121 1 let s go get convergedRemote
16 to  comm_.min(converged) 
16 did  comm_.min(converged) convergedRemote: 0
16 final convergedRemote: 0
16 about to throw Linear solver did not converge
16 Newton::solveLinearSystem : Caught exception: "NumericalProblem [solveLinearSystem:/home/m.giraud/DUMUXexud/dumux/dumux/nonlinear/newtonsolver.hh:381]: Linear solver did not converge when getting convergedRemote "
16 Newton: Caught exception: "NumericalProblem [solveLinearSystem:/home/m.giraud/DUMUXexud/dumux/dumux/nonlinear/newtonsolver.hh:381]: Linear solver did not converge"
16 Newton solver did not converge with dt = 0.0495 seconds. Retrying with time step of 0.02475 seconds
16 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
105 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
105: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 9
105: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 2.7e-06
105: communicator.hh::sendRecv,MPI_Testany 7 1 -32766 0 0 8.95e-06
105: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 7 1 -32766 0 0
105: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 9
105: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 7 1 -32766 0 0
105: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 9
105: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 7 1 -32766 0 0
105: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 9
105: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 7 1 -32766 0 0
105: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 9
105: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 7 1 -32766 0 0
105: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 9
105: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 7 1 -32766 0 0
105: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 9
105: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 7 1 -32766 0 0
105: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 9
105: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 7 1 -32766 0 0
105: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 9
105: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 7 1 -32766 0 0
Newton::solveLinearSystem : Caught exception from the linear solver when getting converged: "BCRSMatrixError [operator++:/home/m.giraud/DUMUXexud/dune-istl/dune/istl/bcrsmatrix.hh:940]: matrix already built up"
105 converged value: 0 121 1 let s go get convergedRemote
105 to  comm_.min(converged) 
105 did  comm_.min(converged) convergedRemote: 0
105 final convergedRemote: 0
105 about to throw Linear solver did not converge
105 Newton::solveLinearSystem : Caught exception: "NumericalProblem [solveLinearSystem:/home/m.giraud/DUMUXexud/dumux/dumux/nonlinear/newtonsolver.hh:381]: Linear solver did not converge when getting convergedRemote "
105 Newton: Caught exception: "NumericalProblem [solveLinearSystem:/home/m.giraud/DUMUXexud/dumux/dumux/nonlinear/newtonsolver.hh:381]: Linear solver did not converge"
105 Newton solver did not converge with dt = 0.0495 seconds. Retrying with time step of 0.02475 seconds
105 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
37 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
37: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 15
37: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 2.36e-06
37: communicator.hh::sendRecv,MPI_Testany 7 1 -32766 0 0 1.121e-05
37: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 7 1 -32766 0 0
37: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 15
37: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 7 1 -32766 0 0
37: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 15
37: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 7 1 -32766 0 0
37: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 15
37: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 7 1 -32766 0 0
37: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 15
37: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 7 1 -32766 0 0
37: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 15
37: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 7 1 -32766 0 0
37: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 15
37: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 7 1 -32766 0 0
37: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 15
37: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 7 1 -32766 0 0
37: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 15
37: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 7 1 -32766 0 0
37: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 15
37: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 7 1 -32766 0 0
37: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 10 15
37: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 7 1 -32766 0 0
37: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 11 15
37: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 7 1 -32766 0 0
37: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 12 15
37: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 7 1 -32766 0 0
37: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 13 15
37: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 7 1 -32766 0 0
37: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 14 15
37: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 7 1 -32766 0 0
Newton::solveLinearSystem : Caught exception from the linear solver when getting converged: "BCRSMatrixError [operator++:/home/m.giraud/DUMUXexud/dune-istl/dune/istl/bcrsmatrix.hh:940]: matrix already built up"
37 converged value: 0 121 1 let s go get convergedRemote
37 to  comm_.min(converged) 
37 did  comm_.min(converged) convergedRemote: 0
37 final convergedRemote: 0
37 about to throw Linear solver did not converge
37 Newton::solveLinearSystem : Caught exception: "NumericalProblem [solveLinearSystem:/home/m.giraud/DUMUXexud/dumux/dumux/nonlinear/newtonsolver.hh:381]: Linear solver did not converge when getting convergedRemote "
37 Newton: Caught exception: "NumericalProblem [solveLinearSystem:/home/m.giraud/D70 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
70: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 13
70: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 4.29e-06
70: communicator.hh::sendRecv,MPI_Testany 8 1 -32766 0 0 9.9e-06
70: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 8 1 -32766 0 0
70: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 13
70: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 8 1 -32766 0 0
70: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 13
70: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 8 1 -32766 0 0
70: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 13
70: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 8 1 -32766 0 0
70: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 13
70: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 8 1 -32766 0 0
70: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 13
70: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 8 1 -32766 0 0
70: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 13
70: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 8 1 -32766 0 0
70: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 13
70: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 8 1 -32766 0 0
70: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 13
70: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 8 1 -32766 0 0
70: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 13
70: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 8 1 -32766 0 0
70: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 10 13
70: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 8 1 -32766 0 0
70: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 11 13
70: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 8 1 -32766 0 0
70: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 12 13
70: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 8 1 -32766 0 0
Newton::solveLinearSystem : Caught exception from the linear solver when getting converged: "BCRSMatrixError [operator++:/home/m.giraud/DUMUXexud/dune-istl/dune/istl/bcrsmatrix.hh:940]: matrix already built up"
70 converged value: 0 121 1 let s go get convergedRemote
70 to  comm_.min(converged) 
70 did  comm_.min(converged) convergedRemote: 0
70 final convergedRemote: 0
70 about to throw Linear solver did not converge
70 Newton::solveLinearSystem : Caught exception: "NumericalProblem [solveLinearSystem:/home/m.giraud/DUMUXexud/dumux/dumux/nonlinear/newtonsolver.hh:381]: Linear solver did not converge when getting convergedRemote "
70 Newton: Caught exception: "NumericalProblem [solveLinearSystem:/home/m.giraud/DUMUXexud/dumux/dumux/nonlinear/newtonsolver.hh:381]: Linear solver did not converge"
70 Newton solver did not converge with dt = 0.0495 seconds. Retrying with time step of 0.02475 seconds
70 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
26 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
26: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 26
26: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 7.33e-06
26: communicator.hh::sendRecv,MPI_Testany 9 1 -32766 0 0 1.281e-05
26: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 9 1 -32766 0 0
26: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 26
26: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 9 1 -32766 0 0
26: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 26
26: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 9 1 -32766 0 0
26: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 26
26: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 9 1 -32766 0 0
26: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 26
26: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 9 1 -32766 0 0
26: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 26
26: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 9 1 -32766 0 0
26: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 26
26: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 9 1 -32766 0 0
26: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 26
26: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 9 1 -32766 0 0
26: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 26
26: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 9 1 -32766 0 0
26: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 26
26: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 9 1 -32766 0 0
26: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 10 26
26: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 9 1 -32766 0 0
26: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 11 26
26: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 9 1 -32766 0 0
26: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 12 26
26: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 9 1 -32766 0 0
26: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 13 26
26: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 9 1 -32766 0 0
26: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 14 26
26: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 9 1 -32766 0 0
26: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 15 26
26: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 9 1 -32766 0 0
26: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 16 26
26: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 9 1 -32766 0 0
26: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 17 26
26: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 9 1 -32766 0 0
26: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finishe35 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
35: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 9
35: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 1.41e-06
35: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 6.79e-06
35: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 1.073e-05
35: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 1.424e-05
35: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 1.823e-05
35: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 2.158e-05
35: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 2.525e-05
35: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 7.5031e-05
35: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 7.9091e-05
35: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 8.2431e-05
35: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 8.6031e-05
35: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 8.9621e-05
35: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 9.3111e-05
35: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 9.6511e-05
35: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 9.9861e-05
35: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000103191
35: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000107031
35: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000111031
35: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000116061
35: communicator.hh::sendRecv,MPI_Testany 3 1 -32766 0 0 0.000119451
35: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 3 1 -32766 0 0
35: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 9
35: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 3 1 -32766 0 0
35: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 9
35: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 3 1 -32766 0 0
35: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 9
35: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 3 1 -32766 0 0
35: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 9
35: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 3 1 -32766 0 0
35: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 9
35: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 3 1 -32766 0 0
35: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 9
35: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 3 1 -32766 0 0
35: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 9
35: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 3 1 -32766 0 0
35: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 9
35: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 3 1 -32766 0 0
Newton::solveLinearSystem : Caught exception from the linear solver when getting converged: "BCRSMatrixError [operator++:/home/m.giraud/DUMUXexud/dune-istl/dune/istl/bcrsmatrix.hh:940]: matrix already built up"
35 converged value: 0 121 1 let s go get convergedRemote
35 to  comm_.min(converged) 
35 did  comm_.min(converged) convergedRemote: 0
35 final convergedRemote: 0
35 about to throw Linear solver did not converge
35 Newton::solveLinearSystem : Caught exception: "NumericalProblem [solveLinearSystem:/home/m.giraud/DUMUXexud/dumux/dumux/nonlinear/newtonsolver.hh:381]: Linear solver did not converge when getting convergedRemote "
35 Newton: Caught e93 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
93: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 13
93: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 5.2e-07
93: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 6.35e-06
93: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 9.3e-06
93: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 1.213e-05
93: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 1.523e-05
93: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 3.705e-05
93: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 3.979e-05
93: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 4.239e-05
93: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 4.509e-05
93: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 4.78e-05
93: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 5.058e-05
93: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 5.336e-05
93: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 5.605e-05
93: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 5.862e-05
93: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 6.133e-05
93: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 8.5421e-05
93: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 8.8461e-05
93: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 9.1071e-05
93: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 9.3971e-05
93: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 9.6671e-05
93: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 9.9251e-05
93: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000101841
93: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000104621
93: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000107571
93: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000128561
93: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000131651
93: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000134291
93: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000137021
93: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000139601
93: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000142211
93: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000144982
93: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000147712
93: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000150432
93: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000153152
93: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000155912
93: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000183412
93: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000186232
93: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000188962
93: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000191972
93: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000194702
93: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000197282
93: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000199872
93: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000202752
93: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000233103
93: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000236353
93: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000239173
93: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000241763
93: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000244593
93: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000247353
93: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000250073
93: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000253363
93: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000256353
93: communicator.hh::sendRecv,114 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
114: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 13
114: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 1.78e-06
114: communicator.hh::sendRecv,MPI_Testany 7 1 -32766 0 0 2.315e-05
114: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 7 1 -32766 0 0
114: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 13
114: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 7 1 -32766 0 0
114: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 13
114: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 7 1 -32766 0 0
114: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 13
114: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 7 1 -32766 0 0
114: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 13
114: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 7 1 -32766 0 0
114: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 13
114: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 7 1 -32766 0 0
114: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 13
114: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 7 1 -32766 0 0
114: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 13
114: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 7 1 -32766 0 0
114: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 13
114: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 7 1 -32766 0 0
114: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 13
114: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 7 1 -32766 0 0
114: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 10 13
114: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 7 1 -32766 0 0
114: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 11 13
114: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 7 1 -32766 0 0
114: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 12 13
114: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 7 1 -32766 0 0
Newton::solveLinearSystem : Caught exception from the linear solver when getting converged: "BCRSMatrixError [operator++:/home/m.giraud/DUMUXexud/dune-istl/dune/istl/bcrsmatrix.hh:940]: matrix already built up"
114 converged value: 0 121 1 let s go get convergedRemote
114 to  comm_.min(converged) 
114 did  comm_.min(converged) convergedRemote: 0
114 final convergedRemote: 0
114 about to throw Linear solver did not converge
114 Newton::solveLinearSystem : Caught exception: "NumericalProblem [solveLinearSystem:/home/m.giraud/DUMUXexud/dumux/dumux/nonlinear/newtonsolver.hh:381]: Linear solver did not converge when getting convergedRemote "
114 Newton: Caught exception: "NumericalProblem [solveLinearSystem:/home/m.giraud/DUMUXexud/dumux/dumux/nonlinear/newtonsolver.hh:381]: Linear solver did not converge"
114 Newton solver did not converge with dt = 0.0495 seconds. Retrying with time step of 0.02475 seconds
114 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
101 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
101: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 24
101: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 8.29e-06
101: communicator.hh::sendRecv,MPI_Testany 6 1 -32766 0 0 2.262e-05
101: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 6 1 -32766 0 0
101: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 24
101: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 6 1 -32766 0 0
101: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 24
101: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 6 1 -32766 0 0
101: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 24
101: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 6 1 -32766 0 0
101: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 24
101: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 6 1 -32766 0 0
101: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 24
101: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 6 1 -32766 0 0
101: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 24
101: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 6 1 -32766 0 0
101: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 24
101: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 6 1 -32766 0 0
101: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 24
101: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 6 1 -32766 0 0
101: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 24
101: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 6 1 -32766 0 0
101: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 10 24
101: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 6 1 -32766 0 0
101: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 11 24
101: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 6 1 -32766 0 0
101: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 12 24
101: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 6 1 -32766 0 0
101: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 13 24
101: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 6 1 -32766 0 0
101: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 14 24
101: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 6 1 -32766 0 0
101: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 15 24
101: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 6 1 -32766 0 0
101: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 16 24
101: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 6 1 -32766 0 0
101: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 17 24
101: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 6 1 -32766 0 0
101: communicator.hh::sendRecv, before MPI_Waitany(messageInf13 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
13: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 11
13: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 2.78e-06
13: communicator.hh::sendRecv,MPI_Testany 2 1 -32766 0 0 1.214e-05
13: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 2 1 -32766 0 0
13: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 11
13: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 2 1 -32766 0 0
13: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 11
13: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 2 1 -32766 0 0
13: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 11
13: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 2 1 -32766 0 0
13: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 11
13: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 2 1 -32766 0 0
13: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 11
13: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 2 1 -32766 0 0
13: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 11
13: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 2 1 -32766 0 0
13: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 11
13: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 2 1 -32766 0 0
13: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 11
13: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 2 1 -32766 0 0
13: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 11
13: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 2 1 -32766 0 0
13: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 10 11
13: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 2 1 -32766 0 0
Newton::solveLinearSystem : Caught exception from the linear solver when getting converged: "BCRSMatrixError [operator++:/home/m.giraud/DUMUXexud/dune-istl/dune/istl/bcrsmatrix.hh:940]: matrix already built up"
13 converged value: 0 121 1 let s go get convergedRemote
13 to  comm_.min(converged) 
13 did  comm_.min(converged) convergedRemote: 0
13 final convergedRemote: 0
13 about to throw Linear solver did not converge
13 Newton::solveLinearSystem : Caught exception: "NumericalProblem [solveLinearSystem:/home/m.giraud/DUMUXexud/dumux/dumux/nonlinear/newtonsolver.hh:381]: Linear solver did not converge when getting convergedRemote "
13 Newton: Caught exception: "NumericalProblem [solveLinearSystem:/home/m.giraud/DUMUXexud/dumux/dumux/nonlinear/newtonsolver.hh:381]: Linear solver did not converge"
13 Newton solver did not converge with dt = 0.0495 seconds. Retrying with time step of 0.02475 seconds
13 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
39 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
39: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 9
39: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 9.9e-07
39: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 7.49e-06
39: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 1.102e-05
39: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 1.451e-05
39: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 1.79e-05
39: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 2.134e-05
39: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 2.462e-05
39: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 2.826e-05
39: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 7.6241e-05
39: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 8.0171e-05
39: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 8.3611e-05
39: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 8.7121e-05
39: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 9.0511e-05
39: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 9.3761e-05
39: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 9.7381e-05
39: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000100661
39: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000104241
39: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000108031
39: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000111611
39: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000115111
39: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000118612
39: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000122012
39: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000125262
39: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000128722
39: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000132812
39: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000136112
39: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000139372
39: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000142692
39: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000146372
39: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000150352
39: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000153792
39: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000188792
39: communicator.hh::sendRecv,MPI_Testany 7 1 -32766 0 0 0.000192842
39: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 7 1 -32766 0 0
39: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 9
39: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 7 1 -32766 0 0
39: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 9
39: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 7 1 -32766 0 0
39: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 9
39: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 7 1 -32766 0 0
39: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 9
39: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 7 1 -32766 0 0
39: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 9
39: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 7 1 -32766 0 0
39: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 9
39: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 7 1 -32766 0 0
39: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &statu79 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
79: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 16
79: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 2.41e-06
79: communicator.hh::sendRecv,MPI_Testany 11 1 -32766 0 0 9.23e-06
79: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 11 1 -32766 0 0
79: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 16
79: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 11 1 -32766 0 0
79: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 16
79: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 11 1 -32766 0 0
79: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 16
79: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 11 1 -32766 0 0
79: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 16
79: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 11 1 -32766 0 0
79: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 16
79: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 11 1 -32766 0 0
79: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 16
79: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 11 1 -32766 0 0
79: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 16
79: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 11 1 -32766 0 0
79: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 16
79: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 11 1 -32766 0 0
79: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 16
79: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 11 1 -32766 0 0
79: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 10 16
79: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 11 1 -32766 0 0
79: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 11 16
79: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 11 1 -32766 0 0
79: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 12 16
79: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 11 1 -32766 0 0
79: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 13 16
79: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 11 1 -32766 0 0
79: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 14 16
79: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 11 1 -32766 0 0
79: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 15 16
79: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 11 1 -32766 0 0
Newton::solveLinearSystem : Caught exception from the linear solver when getting converged: "BCRSMatrixError [operator++:/home/m.giraud/DUMUXexud/dune-istl/dune/istl/bcrsmatrix.hh:940]: matrix already built up"
79 converged value: 0 121 1 let s go get convergedRemote
79 to  comm_.min(converged) 
79 did  comm_.min(converged) convergedRemote: 0
79 final convergedRemote: 0
79 about to throw Linear solver did not converge
79 Newton::solveLinearSystem : Caught exception: "NumericalProblem [solveLin117 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
117: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 22
117: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 1.94e-06
117: communicator.hh::sendRecv,MPI_Testany 17 1 -32766 0 0 7.93e-06
117: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 17 1 -32766 0 0
117: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 22
117: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 17 1 -32766 0 0
117: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 22
117: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 17 1 -32766 0 0
117: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 22
117: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 17 1 -32766 0 0
117: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 22
117: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 17 1 -32766 0 0
117: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 22
117: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 17 1 -32766 0 0
117: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 22
117: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 17 1 -32766 0 0
117: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 22
117: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 17 1 -32766 0 0
117: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 22
117: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 17 1 -32766 0 0
117: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 22
117: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 17 1 -32766 0 0
117: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 10 22
117: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 17 1 -32766 0 0
117: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 11 22
117: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 17 1 -32766 0 0
117: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 12 22
117: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 17 1 -32766 0 0
117: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 13 22
117: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 17 1 -32766 0 0
117: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 14 22
117: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 17 1 -32766 0 0
117: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 15 22
117: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 17 1 -32766 0 0
117: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 16 22
117: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 17 1 -32766 0 0
117: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 17 22
117: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 17 1 -32766 0 0
117: communicator.hh::sendRecv, before MPI_43 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
43: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 11
43: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 2.08e-06
43: communicator.hh::sendRecv,MPI_Testany 8 1 -32766 0 0 9.06e-06
43: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 8 1 -32766 0 0
43: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 11
43: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 8 1 -32766 0 0
43: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 11
43: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 8 1 -32766 0 0
43: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 11
43: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 8 1 -32766 0 0
43: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 11
43: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 8 1 -32766 0 0
43: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 11
43: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 8 1 -32766 0 0
43: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 11
43: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 8 1 -32766 0 0
43: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 11
43: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 8 1 -32766 0 0
43: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 11
43: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 8 1 -32766 0 0
43: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 11
43: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 8 1 -32766 0 0
43: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 10 11
43: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 8 1 -32766 0 0
Newton::solveLinearSystem : Caught exception from the linear solver when getting converged: "BCRSMatrixError [operator++:/home/m.giraud/DUMUXexud/dune-istl/dune/istl/bcrsmatrix.hh:940]: matrix already built up"
43 converged value: 0 121 1 let s go get convergedRemote
43 to  comm_.min(converged) 
43 did  comm_.min(converged) convergedRemote: 0
43 final convergedRemote: 0
43 about to throw Linear solver did not converge
43 Newton::solveLinearSystem : Caught exception: "NumericalProblem [solveLinearSystem:/home/m.giraud/DUMUXexud/dumux/dumux/nonlinear/newtonsolver.hh:381]: Linear solver did not converge when getting convergedRemote "
43 Newton: Caught exception: "NumericalProblem [solveLinearSystem:/home/m.giraud/DUMUXexud/dumux/dumux/nonlinear/newtonsolver.hh:381]: Linear solver did not converge"
43 Newton solver did not converge with dt = 0.0495 seconds. Retrying with time step of 0.02475 seconds
43 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
46 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
46: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 12
46: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 3.071e-06
46: communicator.hh::sendRecv,MPI_Testany 1 1 -32766 0 0 1.0851e-05
46: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 1 1 -32766 0 0
46: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 12
46: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 1 1 -32766 0 0
46: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 12
46: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 1 1 -32766 0 0
46: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 12
46: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 1 1 -32766 0 0
46: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 12
46: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 1 1 -32766 0 0
46: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 12
46: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 1 1 -32766 0 0
46: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 12
46: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 1 1 -32766 0 0
46: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 12
46: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 1 1 -32766 0 0
46: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 12
46: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 1 1 -32766 0 0
46: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 12
46: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 1 1 -32766 0 0
46: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 10 12
46: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 1 1 -32766 0 0
46: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 11 12
46: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 1 1 -32766 0 0
Newton::solveLinearSystem : Caught exception from the linear solver when getting converged: "BCRSMatrixError [operator++:/home/m.giraud/DUMUXexud/dune-istl/dune/istl/bcrsmatrix.hh:940]: matrix already built up"
46 converged value: 0 121 1 let s go get convergedRemote
46 to  comm_.min(converged) 
46 did  comm_.min(converged) convergedRemote: 0
46 final convergedRemote: 0
46 about to throw Linear solver did not converge
46 Newton::solveLinearSystem : Caught exception: "NumericalProblem [solveLinearSystem:/home/m.giraud/DUMUXexud/dumux/dumux/nonlinear/newtonsolver.hh:381]: Linear solver did not converge when getting convergedRemote "
46 Newton: Caught exception: "NumericalProblem [solveLinearSystem:/home/m.giraud/DUMUXexud/dumux/dumux/nonlinear/newtonsolver.hh:381]: Linear solver did not converge"
46 Newton solver did not converge with dt = 0.0495 seconds. Retrying with time step of 0.02475 seconds
46 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
12 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
12: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 26
12: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 1.24e-05
12: communicator.hh::sendRecv,MPI_Testany 0 1 -32766 0 0 2.0291e-05
12: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 0 1 -32766 0 0
12: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 26
12: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 0 1 -32766 0 0
12: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 26
12: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 0 1 -32766 0 0
12: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 26
12: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 0 1 -32766 0 0
12: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 26
12: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 0 1 -32766 0 0
12: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 26
12: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 0 1 -32766 0 0
12: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 26
12: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 0 1 -32766 0 0
12: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 26
12: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 0 1 -32766 0 0
12: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 26
12: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 0 1 -32766 0 0
12: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 26
12: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 0 1 -32766 0 0
12: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 10 26
12: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 0 1 -32766 0 0
12: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 11 26
12: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 0 1 -32766 0 0
12: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 12 26
12: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 0 1 -32766 0 0
12: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 13 26
12: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 0 1 -32766 0 0
12: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 14 26
12: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 0 1 -32766 0 0
12: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 15 26
12: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 0 1 -32766 0 0
12: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 16 26
12: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 0 1 -32766 0 0
12: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 17 26
12: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 0 1 -32766 0 0
12: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finish27 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
27: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 17
27: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 9.74e-06
27: communicator.hh::sendRecv,MPI_Testany 5 1 -32766 0 0 1.867e-05
27: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 5 1 -32766 0 0
27: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 17
27: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 5 1 -32766 0 0
27: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 17
27: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 5 1 -32766 0 0
27: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 17
27: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 5 1 -32766 0 0
27: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 17
27: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 5 1 -32766 0 0
27: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 17
27: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 5 1 -32766 0 0
27: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 17
27: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 5 1 -32766 0 0
27: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 17
27: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 5 1 -32766 0 0
27: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 17
27: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 5 1 -32766 0 0
27: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 17
27: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 5 1 -32766 0 0
27: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 10 17
27: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 5 1 -32766 0 0
27: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 11 17
27: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 5 1 -32766 0 0
27: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 12 17
27: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 5 1 -32766 0 0
27: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 13 17
27: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 5 1 -32766 0 0
27: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 14 17
27: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 5 1 -32766 0 0
27: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 15 17
27: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 5 1 -32766 0 0
27: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 16 17
27: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 5 1 -32766 0 0
Newton::solveLinearSystem : Caught exception from the linear solver when getting converged: "BCRSMatrixError [operator++:/home/m.giraud/DUMUXexud/dune-istl/dune/istl/bcrsmatrix.hh:940]: matrix already built up"
27 converged value: 0 121 1 let s go get convergedRemote
27 to  comm_.min(converged) 
27 did  co75 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
75: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 17
75: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 2.77e-06
75: communicator.hh::sendRecv,MPI_Testany 12 1 -32766 0 0 9.99e-06
75: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 12 1 -32766 0 0
75: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 17
75: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 12 1 -32766 0 0
75: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 17
75: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 12 1 -32766 0 0
75: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 17
75: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 12 1 -32766 0 0
75: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 17
75: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 12 1 -32766 0 0
75: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 17
75: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 12 1 -32766 0 0
75: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 17
75: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 12 1 -32766 0 0
75: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 17
75: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 12 1 -32766 0 0
75: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 17
75: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 12 1 -32766 0 0
75: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 17
75: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 12 1 -32766 0 0
75: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 10 17
75: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 12 1 -32766 0 0
75: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 11 17
75: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 12 1 -32766 0 0
75: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 12 17
75: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 12 1 -32766 0 0
75: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 13 17
75: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 12 1 -32766 0 0
75: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 14 17
75: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 12 1 -32766 0 0
75: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 15 17
75: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 12 1 -32766 0 0
75: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 16 17
75: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 12 1 -32766 0 0
Newton::solveLinearSystem : Caught exception from the linear solver when getting converged: "BCRSMatrixError [operator++:/home/m.giraud/DUMUXexud/dune-istl/dune/istl/bcrsmatrix.hh:940]: matrix already built up"
75 converged value: 0 121 1 let s go get convergedRemote
75 to  comm_.min(conve120 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
120: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 7
120: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 2.53e-06
120: communicator.hh::sendRecv,MPI_Testany 3 1 -32766 0 0 9.77e-06
120: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 3 1 -32766 0 0
120: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 7
120: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 3 1 -32766 0 0
120: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 7
120: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 3 1 -32766 0 0
120: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 7
120: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 3 1 -32766 0 0
120: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 7
120: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 3 1 -32766 0 0
120: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 7
120: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 3 1 -32766 0 0
120: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 7
120: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 3 1 -32766 0 0
Newton::solveLinearSystem : Caught exception from the linear solver when getting converged: "BCRSMatrixError [operator++:/home/m.giraud/DUMUXexud/dune-istl/dune/istl/bcrsmatrix.hh:940]: matrix already built up"
120 converged value: 0 121 1 let s go get convergedRemote
120 to  comm_.min(converged) 
120 did  comm_.min(converged) convergedRemote: 0
120 final convergedRemote: 0
120 about to throw Linear solver did not converge
120 Newton::solveLinearSystem : Caught exception: "NumericalProblem [solveLinearSystem:/home/m.giraud/DUMUXexud/dumux/dumux/nonlinear/newtonsolver.hh:381]: Linear solver did not converge when getting convergedRemote "
120 Newton: Caught exception: "NumericalProblem [solveLinearSystem:/home/m.giraud/DUMUXexud/dumux/dumux/nonlinear/newtonsolver.hh:381]: Linear solver did not converge"
120 Newton solver did not converge with dt = 0.0495 seconds. Retrying with time step of 0.02475 seconds
120 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
54 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
54: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 18
54: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 9.98e-06
54: communicator.hh::sendRecv,MPI_Testany 1 1 -32766 0 0 4.3261e-05
54: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 1 1 -32766 0 0
54: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 18
54: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 1 1 -32766 0 0
54: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 18
54: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 1 1 -32766 0 0
54: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 18
54: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 1 1 -32766 0 0
54: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 18
54: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 1 1 -32766 0 0
54: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 18
54: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 1 1 -32766 0 0
54: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 18
54: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 1 1 -32766 0 0
54: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 18
54: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 1 1 -32766 0 0
54: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 18
54: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 1 1 -32766 0 0
54: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 18
54: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 1 1 -32766 0 0
54: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 10 18
54: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 1 1 -32766 0 0
54: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 11 18
54: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 1 1 -32766 0 0
54: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 12 18
54: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 1 1 -32766 0 0
54: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 13 18
54: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 1 1 -32766 0 0
54: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 14 18
54: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 1 1 -32766 0 0
54: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 15 18
54: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 1 1 -32766 0 0
54: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 16 18
54: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 1 1 -32766 0 0
54: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 17 18
54: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 1 1 -32766 0 0
Newton::solveLinearSystem : Caught exception from the linear solver when getting converged: "BCRSMa81 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
81: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 11
81: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 1.77e-06
81: communicator.hh::sendRecv,MPI_Testany 8 1 -32766 0 0 8.62e-06
81: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 8 1 -32766 0 0
81: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 11
81: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 8 1 -32766 0 0
81: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 11
81: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 8 1 -32766 0 0
81: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 11
81: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 8 1 -32766 0 0
81: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 11
81: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 8 1 -32766 0 0
81: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 11
81: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 8 1 -32766 0 0
81: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 11
81: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 8 1 -32766 0 0
81: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 11
81: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 8 1 -32766 0 0
81: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 11
81: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 8 1 -32766 0 0
81: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 11
81: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 8 1 -32766 0 0
81: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 10 11
81: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 8 1 -32766 0 0
Newton::solveLinearSystem : Caught exception from the linear solver when getting converged: "BCRSMatrixError [operator++:/home/m.giraud/DUMUXexud/dune-istl/dune/istl/bcrsmatrix.hh:940]: matrix already built up"
81 converged value: 0 121 1 let s go get convergedRemote
81 to  comm_.min(converged) 
81 did  comm_.min(converged) convergedRemote: 0
81 final convergedRemote: 0
81 about to throw Linear solver did not converge
81 Newton::solveLinearSystem : Caught exception: "NumericalProblem [solveLinearSystem:/home/m.giraud/DUMUXexud/dumux/dumux/nonlinear/newtonsolver.hh:381]: Linear solver did not converge when getting convergedRemote "
81 Newton: Caught exception: "NumericalProblem [solveLinearSystem:/home/m.giraud/DUMUXexud/dumux/dumux/nonlinear/newtonsolver.hh:381]: Linear solver did not converge"
81 Newton solver did not converge with dt = 0.0495 seconds. Retrying with time step of 0.02475 seconds
81 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
6 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 17
6: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 4.39e-06
6: communicator.hh::sendRecv,MPI_Testany 0 1 -32766 0 0 1.016e-05
6: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 0 1 -32766 0 0
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 17
6: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 0 1 -32766 0 0
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 17
6: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 0 1 -32766 0 0
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 17
6: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 0 1 -32766 0 0
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 17
6: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 0 1 -32766 0 0
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 17
6: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 0 1 -32766 0 0
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 17
6: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 0 1 -32766 0 0
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 17
6: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 0 1 -32766 0 0
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 17
6: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 0 1 -32766 0 0
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 17
6: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 0 1 -32766 0 0
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 10 17
6: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 0 1 -32766 0 0
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 11 17
6: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 0 1 -32766 0 0
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 12 17
6: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 0 1 -32766 0 0
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 13 17
6: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 0 1 -32766 0 0
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 14 17
6: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 0 1 -32766 0 0
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 15 17
6: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 0 1 -32766 0 0
6: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 16 17
6: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 0 1 -32766 0 0
Newton::solveLinearSystem : Caught exception from the linear solver when getting converged: "BCRSMatrixError [operator++:/home/m.giraud/DUMUXexud/dune-istl/dune/istl/bcrsmatrix.hh:940]: matrix already built up"
6 converged value: 0 121 1 let s go get convergedRemote
6 to  comm_.min(converged) 
6 did  comm_.min(converged) convergedRemote: 0
6 99 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
99: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 13
99: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 3.81e-06
99: communicator.hh::sendRecv,MPI_Testany 9 1 -32766 0 0 1.079e-05
99: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 9 1 -32766 0 0
99: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 13
99: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 9 1 -32766 0 0
99: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 13
99: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 9 1 -32766 0 0
99: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 13
99: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 9 1 -32766 0 0
99: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 13
99: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 9 1 -32766 0 0
99: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 13
99: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 9 1 -32766 0 0
99: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 13
99: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 9 1 -32766 0 0
99: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 13
99: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 9 1 -32766 0 0
99: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 13
99: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 9 1 -32766 0 0
99: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 13
99: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 9 1 -32766 0 0
99: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 10 13
99: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 9 1 -32766 0 0
99: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 11 13
99: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 9 1 -32766 0 0
99: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 12 13
99: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 9 1 -32766 0 0
Newton::solveLinearSystem : Caught exception from the linear solver when getting converged: "BCRSMatrixError [operator++:/home/m.giraud/DUMUXexud/dune-istl/dune/istl/bcrsmatrix.hh:940]: matrix already built up"
99 converged value: 0 121 1 let s go get convergedRemote
99 to  comm_.min(converged) 
99 did  comm_.min(converged) convergedRemote: 0
99 final convergedRemote: 0
99 about to throw Linear solver did not converge
99 Newton::solveLinearSystem : Caught exception: "NumericalProblem [solveLinearSystem:/home/m.giraud/DUMUXexud/dumux/dumux/nonlinear/newtonsolver.hh:381]: Linear solver did not converge when getting convergedRemote "
99 Newton: Caught exception: "NumericalProblem [solveLinearSystem:/home/m.giraud/DUMUXexud/dumux/dumux/nonlinear/newtonsolver.hh:381]: Linear solver did not converge"
99 Newton solver did not converge with dt = 0.0495 seconds. Retrying with time step of 0.02475 seconds
99 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
119 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
119: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 13
119: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 1.13e-06
119: communicator.hh::sendRecv,MPI_Testany 8 1 -32766 0 0 2.114e-05
119: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 8 1 -32766 0 0
119: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 13
119: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 8 1 -32766 0 0
119: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 13
119: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 8 1 -32766 0 0
119: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 13
119: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 8 1 -32766 0 0
119: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 13
119: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 8 1 -32766 0 0
119: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 13
119: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 8 1 -32766 0 0
119: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 13
119: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 8 1 -32766 0 0
119: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 13
119: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 8 1 -32766 0 0
119: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 13
119: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 8 1 -32766 0 0
119: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 13
119: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 8 1 -32766 0 0
119: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 10 13
119: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 8 1 -32766 0 0
119: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 11 13
119: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 8 1 -32766 0 0
119: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 12 13
119: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 8 1 -32766 0 0
Newton::solveLinearSystem : Caught exception from the linear solver when getting converged: "BCRSMatrixError [operator++:/home/m.giraud/DUMUXexud/dune-istl/dune/istl/bcrsmatrix.hh:940]: matrix already built up"
119 converged value: 0 121 1 let s go get convergedRemote
119 to  comm_.min(converged) 
119 did  comm_.min(converged) convergedRemote: 0
119 final convergedRemote: 0
119 about to throw Linear solver did not converge
119 Newton::solveLinearSystem : Caught exception: "NumericalProblem [solveLinearSystem:/home/m.giraud/DUMUXexud/dumux/dumux/nonlinear/newtonsolver.hh:381]: Linear solver did not converge when getting convergedRemote "
119 Newton: Caught exception: "NumericalProblem [solveLinearSystem:/home/m.giraud/DUMUXexud/dumux/dumux/nonlinear/newtonsolver.hh:381]: Linear solver did not converge"
119 Newton solver did not converge with dt = 0.0495 seconds. Retrying with time step of 0.02475 seconds
119 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
32 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
32: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 16
32: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 3.51e-06
32: communicator.hh::sendRecv,MPI_Testany 13 1 -32766 0 0 1.025e-05
32: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 13 1 -32766 0 0
32: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 16
32: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 13 1 -32766 0 0
32: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 16
32: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 13 1 -32766 0 0
32: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 16
32: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 13 1 -32766 0 0
32: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 16
32: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 13 1 -32766 0 0
32: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 16
32: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 13 1 -32766 0 0
32: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 16
32: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 13 1 -32766 0 0
32: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 16
32: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 13 1 -32766 0 0
32: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 16
32: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 13 1 -32766 0 0
32: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 16
32: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 13 1 -32766 0 0
32: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 10 16
32: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 13 1 -32766 0 0
32: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 11 16
32: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 13 1 -32766 0 0
32: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 12 16
32: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 13 1 -32766 0 0
32: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 13 16
32: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 13 1 -32766 0 0
32: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 14 16
32: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 13 1 -32766 0 0
32: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 15 16
32: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 13 1 -32766 0 0
Newton::solveLinearSystem : Caught exception from the linear solver when getting converged: "BCRSMatrixError [operator++:/home/m.giraud/DUMUXexud/dune-istl/dune/istl/bcrsmatrix.hh:940]: matrix already built up"
32 converged value: 0 121 1 let s go get convergedRemote
32 to  comm_.min(converged) 
32 did  comm_.min(converged) convergedRemote: 0
32 final convergedRemote: 0
32 about to throw Linear solver did not converge
32 Newton::solveLinearSystem : Caught exception: "NumericalProblem [solveLi57 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
57: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 19
57: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 2.26e-06
57: communicator.hh::sendRecv,MPI_Testany 18 1 -32766 0 0 9.77e-06
57: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 18 1 -32766 0 0
57: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 19
57: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 18 1 -32766 0 0
57: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 19
57: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 18 1 -32766 0 0
57: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 19
57: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 18 1 -32766 0 0
57: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 19
57: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 18 1 -32766 0 0
57: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 19
57: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 18 1 -32766 0 0
57: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 19
57: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 18 1 -32766 0 0
57: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 19
57: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 18 1 -32766 0 0
57: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 19
57: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 18 1 -32766 0 0
57: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 19
57: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 18 1 -32766 0 0
57: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 10 19
57: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 18 1 -32766 0 0
57: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 11 19
57: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 18 1 -32766 0 0
57: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 12 19
57: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 18 1 -32766 0 0
57: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 13 19
57: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 18 1 -32766 0 0
57: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 14 19
57: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 18 1 -32766 0 0
57: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 15 19
57: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 18 1 -32766 0 0
57: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 16 19
57: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 18 1 -32766 0 0
57: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 17 19
57: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 18 1 -32766 0 0
57: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recv56 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
56: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 31
56: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 7.15e-06
56: communicator.hh::sendRecv,MPI_Testany 11 1 -32766 0 0 1.32e-05
56: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 11 1 -32766 0 0
56: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 31
56: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 11 1 -32766 0 0
56: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 31
56: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 11 1 -32766 0 0
56: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 31
56: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 11 1 -32766 0 0
56: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 31
56: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 11 1 -32766 0 0
56: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 31
56: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 11 1 -32766 0 0
56: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 31
56: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 11 1 -32766 0 0
56: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 31
56: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 11 1 -32766 0 0
56: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 31
56: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 11 1 -32766 0 0
56: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 31
56: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 11 1 -32766 0 0
56: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 10 31
56: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 11 1 -32766 0 0
56: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 11 31
56: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 11 1 -32766 0 0
56: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 12 31
56: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 11 1 -32766 0 0
56: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 13 31
56: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 11 1 -32766 0 0
56: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 14 31
56: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 11 1 -32766 0 0
56: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 15 31
56: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 11 1 -32766 0 0
56: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 16 31
56: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 11 1 -32766 0 0
56: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 17 31
56: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 11 1 -32766 0 0
56: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recv116 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
116: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 7
116: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 4.4e-07
116: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 9.5341e-05
116: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000102922
116: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000106942
116: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000109732
116: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000112432
116: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000115132
116: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000118312
116: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000121182
116: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000123802
116: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000126422
116: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000129032
116: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000131632
116: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000134782
116: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000137892
116: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000140502
116: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000143102
116: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000145702
116: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000148312
116: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000151182
116: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000154022
116: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000156632
116: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000159252
116: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000161862
116: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000164462
116: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000167352
116: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000170182
116: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000172782
116: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000175393
116: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000178003
116: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000180623
116: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000183493
116: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000186323
116: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000188933
116: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000191553
116: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000194153
116: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000196773
116: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000199653
116: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000202483
116: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000205093
116: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000207673
116: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000210273
116: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000212883
116: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000216003
116: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000218613
116: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000221243
116: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000223873
116: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000310164
116: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000313524
116: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000387935
116: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000469006
116: communicat60 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
60: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 6
60: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 5e-07
60: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000111811
60: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000115181
60: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000119651
60: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000214742
60: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000217872
60: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000307903
60: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000311784
60: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000438285
60: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000523186
60: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000597707
60: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000679908
60: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.00080759
60: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.00081514
60: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.00081854
60: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.00082776
60: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000939441
60: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.00101866
60: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.0010594
60: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.00118038
60: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.00119516
60: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.00119793
60: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.00120082
60: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.00120378
60: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.00120649
60: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.00120937
60: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.00121221
60: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.00122422
60: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.00122772
60: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.00123043
60: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.0012334
60: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.00124909
60: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.00125223
60: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.00125507
60: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.00125766
60: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.00126051
60: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.00127686
60: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.00127983
60: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.0012994
60: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.00130289
60: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.00132145
60: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.00134061
60: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.00159119
60: communicator.hh::sendRecv,MPI_Testany 3 1 -32766 0 0 0.00159433
60: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 3 1 -32766 0 0
60: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 6
60: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 3 1 -32766 0 0
60: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 6
60: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 3 1 -32766 0 0
60: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 6
60: communi53 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
53: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 17
53: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 1.32e-06
53: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 7.07e-06
53: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 9.99e-06
53: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 1.28e-05
53: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 1.581e-05
53: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 1.883e-05
53: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 2.156e-05
53: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 2.432e-05
53: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 4.8701e-05
53: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 5.2071e-05
53: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 5.5001e-05
53: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 5.8161e-05
53: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 6.1021e-05
53: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 6.3671e-05
53: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 6.6311e-05
53: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 6.8941e-05
53: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 8.8571e-05
53: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 9.1831e-05
53: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 9.4991e-05
53: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 9.7891e-05
53: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000101101
53: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000104181
53: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000106891
53: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000109522
53: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000112782
53: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000135912
53: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000139192
53: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000142062
53: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000145242
53: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000147902
53: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000150812
53: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000153442
53: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000156072
53: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000158692
53: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000161442
53: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000193573
53: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000197283
53: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000199923
53: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000202553
53: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000205183
53: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000207803
53: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000210433
53: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000213753
53: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000256073
53: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000259803
53: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000263083
53: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000266343
53: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000269614
53: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000273204
53: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000276504
53: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000279784
53: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000283064
53: communicator.hh:108 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
108: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 9
108: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 1.04e-06
108: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 1.809e-05
108: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 2.126e-05
108: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 2.407e-05
108: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 2.717e-05
108: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 4.496e-05
108: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 4.928e-05
108: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 5.277e-05
108: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 6.7911e-05
108: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 7.2411e-05
108: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 9.3851e-05
108: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000111211
108: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000115901
108: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000136622
108: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000390785
108: communicator.hh::sendRecv,MPI_Testany 4 1 -32766 0 0 0.000451186
108: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 4 1 -32766 0 0
108: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 9
108: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 4 1 -32766 0 0
108: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 9
108: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 4 1 -32766 0 0
108: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 9
108: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 4 1 -32766 0 0
108: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 9
108: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 4 1 -32766 0 0
108: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 9
108: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 4 1 -32766 0 0
108: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 9
108: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 4 1 -32766 0 0
108: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 9
108: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 4 1 -32766 0 0
108: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 9
108: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 4 1 -32766 0 0
Newton::solveLinearSystem : Caught exception from the linear solver when getting converged: "BCRSMatrixError [operator++:/home/m.giraud/DUMUXexud/dune-istl/dune/istl/bcrsmatrix.hh:940]: matrix already built up"
108 converged value: 0 121 1 let s go get convergedRemote
108 to  comm_.min(converged) 
108 did  comm_.min(converged) convergedRemote: 0
108 final convergedRemote: 0
108 about to throw Linear solver did not converge
108 Newton::solveLinearSystem : Caught exception: "NumericalProblem [solveLinearSystem:/home/m.giraud/DUMUXexud/dumux/dumux/nonlinear/newtonsolver.hh:381]: Linear solver did not converge when getting convergedRemote "
108 Newton: Caught exception: "NumericalProblem [solveLinearSystem:/home/m.giraud/DUMUXexud/dumux/dumux/nonlinear/newtonsolver.hh:381]: Linear solver did not converge"
108 Newton solver did not converge with dt = 0.0495 seconds. Retrying with time step of 0.02475 second4 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 14
4: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 3.9e-06
4: communicator.hh::sendRecv,MPI_Testany 0 1 -32766 0 0 1.112e-05
4: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 0 1 -32766 0 0
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 14
4: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 0 1 -32766 0 0
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 14
4: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 0 1 -32766 0 0
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 14
4: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 0 1 -32766 0 0
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 14
4: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 0 1 -32766 0 0
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 14
4: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 0 1 -32766 0 0
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 14
4: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 0 1 -32766 0 0
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 14
4: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 0 1 -32766 0 0
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 14
4: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 0 1 -32766 0 0
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 14
4: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 0 1 -32766 0 0
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 10 14
4: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 0 1 -32766 0 0
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 11 14
4: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 0 1 -32766 0 0
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 12 14
4: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 0 1 -32766 0 0
4: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 13 14
4: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 0 1 -32766 0 0
Newton::solveLinearSystem : Caught exception from the linear solver when getting converged: "BCRSMatrixError [operator++:/home/m.giraud/DUMUXexud/dune-istl/dune/istl/bcrsmatrix.hh:940]: matrix already built up"
4 converged value: 0 121 1 let s go get convergedRemote
4 to  comm_.min(converged) 
4 did  comm_.min(converged) convergedRemote: 0
4 final convergedRemote: 0
4 about to throw Linear solver did not converge
4 Newton::solveLinearSystem : Caught exception: "NumericalProblem [solveLinearSystem:/home/m.giraud/DUMUXexud/dumux/dumux/nonlinear/newtonsolver.hh:381]: Linear solver did not converge when getting convergedRemote "
4 Newton: Caught exception: "NumericalProblem [solveLinearSystem:/home/m.giraud/DUMUXexud/dumux/dumux/nonlinear/newtonsolver.hh:381]: Linear solver did not converge"
4 Newton solver did not converge with dt = 0.0495 seconds. Retrying with time step of 0.02475 seconds
4 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
84 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
84: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 18
84: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 6.46e-06
84: communicator.hh::sendRecv,MPI_Testany 8 1 -32766 0 0 1.403e-05
84: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 8 1 -32766 0 0
84: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 18
84: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 8 1 -32766 0 0
84: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 18
84: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 8 1 -32766 0 0
84: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 18
84: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 8 1 -32766 0 0
84: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 18
84: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 8 1 -32766 0 0
84: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 18
84: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 8 1 -32766 0 0
84: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 18
84: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 8 1 -32766 0 0
84: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 18
84: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 8 1 -32766 0 0
84: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 18
84: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 8 1 -32766 0 0
84: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 18
84: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 8 1 -32766 0 0
84: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 10 18
84: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 8 1 -32766 0 0
84: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 11 18
84: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 8 1 -32766 0 0
84: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 12 18
84: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 8 1 -32766 0 0
84: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 13 18
84: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 8 1 -32766 0 0
84: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 14 18
84: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 8 1 -32766 0 0
84: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 15 18
84: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 8 1 -32766 0 0
84: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 16 18
84: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 8 1 -32766 0 0
84: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 17 18
84: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 8 1 -32766 0 0
Newton::solveLinearSystem : Caught exception from the linear solver when getting converged: "BCRSMat50 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
50: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 10
50: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 8.5e-07
50: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 2.145e-05
50: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 2.469e-05
50: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 2.736e-05
50: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 3.013e-05
50: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 4.5331e-05
50: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 4.8491e-05
50: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 5.1651e-05
50: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 7.0951e-05
50: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 8.9561e-05
50: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 9.2431e-05
50: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 9.5681e-05
50: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000116372
50: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000367325
50: communicator.hh::sendRecv,MPI_Testany 2 1 -32766 0 0 0.000370895
50: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 2 1 -32766 0 0
50: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 10
50: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 2 1 -32766 0 0
50: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 10
50: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 2 1 -32766 0 0
50: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 10
50: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 2 1 -32766 0 0
50: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 10
50: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 2 1 -32766 0 0
50: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 10
50: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 2 1 -32766 0 0
50: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 10
50: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 2 1 -32766 0 0
50: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 10
50: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 2 1 -32766 0 0
50: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 10
50: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 2 1 -32766 0 0
50: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 10
50: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 2 1 -32766 0 0
Newton::solveLinearSystem : Caught exception from the linear solver when getting converged: "BCRSMatrixError [operator++:/home/m.giraud/DUMUXexud/dune-istl/dune/istl/bcrsmatrix.hh:940]: matrix already built up"
50 converged value: 0 121 1 let s go get convergedRemote
50 to  comm_.min(converged) 
50 did  comm_.min(converged) convergedRemote: 0
50 final convergedRemote: 0
50 about to throw Linear solver did not converge
50 Newton::solveLinearSystem : Caught exception: "NumericalProblem [solveLinearSystem:/home/m.giraud/DUMUXexud/dumux/dumux/nonlinear/newtonsolver.hh:381]: Linear solver did not converge when getting convergedRemote "
50 Newton: Caught exception: "NumericalProblem [solveLinearSystem:/home/m.giraud/DUMUXexud/dumux/dumux/nonlinear/newtonsolver.hh:381]: Linear solver did not converge"
5045 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
45: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 17
45: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 4.43e-06
45: communicator.hh::sendRecv,MPI_Testany 3 1 -32766 0 0 1.074e-05
45: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 3 1 -32766 0 0
45: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 17
45: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 3 1 -32766 0 0
45: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 17
45: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 3 1 -32766 0 0
45: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 17
45: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 3 1 -32766 0 0
45: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 17
45: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 3 1 -32766 0 0
45: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 17
45: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 3 1 -32766 0 0
45: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 17
45: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 3 1 -32766 0 0
45: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 17
45: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 3 1 -32766 0 0
45: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 17
45: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 3 1 -32766 0 0
45: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 17
45: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 3 1 -32766 0 0
45: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 10 17
45: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 3 1 -32766 0 0
45: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 11 17
45: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 3 1 -32766 0 0
45: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 12 17
45: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 3 1 -32766 0 0
45: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 13 17
45: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 3 1 -32766 0 0
45: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 14 17
45: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 3 1 -32766 0 0
45: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 15 17
45: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 3 1 -32766 0 0
45: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 16 17
45: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 3 1 -32766 0 0
Newton::solveLinearSystem : Caught exception from the linear solver when getting converged: "BCRSMatrixError [operator++:/home/m.giraud/DUMUXexud/dune-istl/dune/istl/bcrsmatrix.hh:940]: matrix already built up"
45 converged value: 0 121 1 let s go get convergedRemote
45 to  comm_.min(converged) 
45 did  co109 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
109: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 20
109: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 5.13e-06
109: communicator.hh::sendRecv,MPI_Testany 5 1 -32766 0 0 1.351e-05
109: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 5 1 -32766 0 0
109: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 20
109: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 5 1 -32766 0 0
109: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 20
109: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 5 1 -32766 0 0
109: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 20
109: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 5 1 -32766 0 0
109: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 20
109: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 5 1 -32766 0 0
109: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 20
109: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 5 1 -32766 0 0
109: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 20
109: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 5 1 -32766 0 0
109: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 20
109: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 5 1 -32766 0 0
109: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 20
109: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 5 1 -32766 0 0
109: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 20
109: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 5 1 -32766 0 0
109: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 10 20
109: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 5 1 -32766 0 0
109: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 11 20
109: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 5 1 -32766 0 0
109: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 12 20
109: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 5 1 -32766 0 0
109: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 13 20
109: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 5 1 -32766 0 0
109: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 14 20
109: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 5 1 -32766 0 0
109: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 15 20
109: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 5 1 -32766 0 0
109: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 16 20
109: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 5 1 -32766 0 0
109: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 17 20
109: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 5 1 -32766 0 0
109: communicator.hh::sendRecv, before MPI_Waitany(messageInf71 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
71: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 20
71: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 2.76e-06
71: communicator.hh::sendRecv,MPI_Testany 3 1 -32766 0 0 1.039e-05
71: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 3 1 -32766 0 0
71: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 20
71: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 3 1 -32766 0 0
71: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 20
71: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 3 1 -32766 0 0
71: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 20
71: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 3 1 -32766 0 0
71: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 20
71: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 3 1 -32766 0 0
71: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 20
71: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 3 1 -32766 0 0
71: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 20
71: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 3 1 -32766 0 0
71: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 20
71: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 3 1 -32766 0 0
71: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 20
71: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 3 1 -32766 0 0
71: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 20
71: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 3 1 -32766 0 0
71: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 10 20
71: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 3 1 -32766 0 0
71: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 11 20
71: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 3 1 -32766 0 0
71: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 12 20
71: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 3 1 -32766 0 0
71: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 13 20
71: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 3 1 -32766 0 0
71: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 14 20
71: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 3 1 -32766 0 0
71: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 15 20
71: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 3 1 -32766 0 0
71: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 16 20
71: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 3 1 -32766 0 0
71: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 17 20
71: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 3 1 -32766 0 0
71: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finishe62 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
62: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 15
62: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 2.36e-06
62: communicator.hh::sendRecv,MPI_Testany 0 1 -32766 0 0 8.231e-06
62: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 0 1 -32766 0 0
62: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 15
62: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 0 1 -32766 0 0
62: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 15
62: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 0 1 -32766 0 0
62: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 15
62: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 0 1 -32766 0 0
62: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 15
62: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 0 1 -32766 0 0
62: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 15
62: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 0 1 -32766 0 0
62: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 15
62: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 0 1 -32766 0 0
62: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 15
62: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 0 1 -32766 0 0
62: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 15
62: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 0 1 -32766 0 0
62: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 15
62: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 0 1 -32766 0 0
62: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 10 15
62: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 0 1 -32766 0 0
62: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 11 15
62: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 0 1 -32766 0 0
62: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 12 15
62: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 0 1 -32766 0 0
62: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 13 15
62: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 0 1 -32766 0 0
62: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 14 15
62: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 0 1 -32766 0 0
Newton::solveLinearSystem : Caught exception from the linear solver when getting converged: "BCRSMatrixError [operator++:/home/m.giraud/DUMUXexud/dune-istl/dune/istl/bcrsmatrix.hh:940]: matrix already built up"
62 converged value: 0 121 1 let s go get convergedRemote
62 to  comm_.min(converged) 
62 did  comm_.min(converged) convergedRemote: 0
62 final convergedRemote: 0
62 about to throw Linear solver did not converge
62 Newton::solveLinearSystem : Caught exception: "NumericalProblem [solveLinearSystem:/home/m.giraud/DUMUXexud/dumux/dumux/nonlinear/newtonsolver.hh:381]: Linear solver did not converge when getting convergedRemote "
62 Newton: Caught exception: "NumericalProblem [solveLinearSystem:/home/m.giraud/D61 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
61: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 9
61: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 4.9e-07
61: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 6.09e-06
61: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 9.03e-06
61: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 1.193e-05
61: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 3.7421e-05
61: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 4.0501e-05
61: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 4.3141e-05
61: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 4.5751e-05
61: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 4.8381e-05
61: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 5.0981e-05
61: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 5.3601e-05
61: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 5.6391e-05
61: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 5.9261e-05
61: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 8.6711e-05
61: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 8.9671e-05
61: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 9.2291e-05
61: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 9.4891e-05
61: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 9.7641e-05
61: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000100481
61: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000103311
61: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000106181
61: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000142082
61: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000145262
61: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000148022
61: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000150962
61: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000153712
61: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000156732
61: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000159552
61: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000162532
61: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000165782
61: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000169182
61: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000172412
61: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000175662
61: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000179042
61: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000226873
61: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000230743
61: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000234263
61: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000237593
61: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000240923
61: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000244213
61: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000247523
61: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000250843
61: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000254573
61: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000257913
61: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000261233
61: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000264743
61: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000268083
61: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000271414
61: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000274814
61: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000278114
61: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000281474
61: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000286474
61: communicato89 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
89: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 11
89: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 8.4e-07
89: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 2.327e-05
89: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 2.638e-05
89: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 2.92e-05
89: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 3.223e-05
89: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 3.495e-05
89: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 3.791e-05
89: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 4.058e-05
89: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 4.325e-05
89: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 4.593e-05
89: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 4.924e-05
89: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 7.1861e-05
89: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 7.4791e-05
89: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 7.7511e-05
89: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 8.0191e-05
89: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 8.2851e-05
89: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 8.5531e-05
89: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 8.8221e-05
89: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 9.1071e-05
89: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 9.3891e-05
89: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000115331
89: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000118901
89: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000121831
89: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000124691
89: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000127651
89: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000130341
89: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000133041
89: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000135841
89: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000161152
89: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000164332
89: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000167212
89: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000169872
89: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000172542
89: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000175372
89: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000178192
89: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000181262
89: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000184532
89: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000212552
89: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000215882
89: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000218812
89: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000221732
89: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000224412
89: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000227282
89: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000229943
89: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000266493
89: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000269583
89: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000272403
89: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000275373
89: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000278863
89: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000281653
89: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000284703
89: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000287853
89: communicator.hh::s111 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
111: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 11
111: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 9.4e-07
111: communicator.hh::sendRecv,MPI_Testany 8 1 -32766 0 0 2.3e-05
111: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 8 1 -32766 0 0
111: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 11
111: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 8 1 -32766 0 0
111: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 11
111: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 8 1 -32766 0 0
111: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 11
111: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 8 1 -32766 0 0
111: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 11
111: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 8 1 -32766 0 0
111: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 11
111: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 8 1 -32766 0 0
111: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 11
111: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 8 1 -32766 0 0
111: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 11
111: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 8 1 -32766 0 0
111: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 11
111: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 8 1 -32766 0 0
111: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 11
111: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 8 1 -32766 0 0
111: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 10 11
111: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 8 1 -32766 0 0
Newton::solveLinearSystem : Caught exception from the linear solver when getting converged: "BCRSMatrixError [operator++:/home/m.giraud/DUMUXexud/dune-istl/dune/istl/bcrsmatrix.hh:940]: matrix already built up"
111 converged value: 0 121 1 let s go get convergedRemote
111 to  comm_.min(converged) 
111 did  comm_.min(converged) convergedRemote: 0
111 final convergedRemote: 0
111 about to throw Linear solver did not converge
111 Newton::solveLinearSystem : Caught exception: "NumericalProblem [solveLinearSystem:/home/m.giraud/DUMUXexud/dumux/dumux/nonlinear/newtonsolver.hh:381]: Linear solver did not converge when getting convergedRemote "
111 Newton: Caught exception: "NumericalProblem [solveLinearSystem:/home/m.giraud/DUMUXexud/dumux/dumux/nonlinear/newtonsolver.hh:381]: Linear solver did not converge"
111 Newton solver did not converge with dt = 0.0495 seconds. Retrying with time step of 0.02475 seconds
111 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
100 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
100: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 6
100: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 9.4e-07
100: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 6.98e-06
100: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 9.91e-06
100: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 1.295e-05
100: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 2.44e-05
100: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 2.75e-05
100: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 3.014e-05
100: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 3.301e-05
100: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 4.902e-05
100: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 5.2081e-05
100: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 5.4761e-05
100: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 5.8261e-05
100: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 7.2551e-05
100: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 7.6161e-05
100: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 9.7051e-05
100: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000117451
100: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000120501
100: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000254983
100: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000396335
100: communicator.hh::sendRecv,MPI_Testany 4 1 -32766 0 0 0.000399255
100: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 4 1 -32766 0 0
100: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 6
100: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 4 1 -32766 0 0
100: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 6
100: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 4 1 -32766 0 0
100: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 6
100: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 4 1 -32766 0 0
100: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 6
100: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 4 1 -32766 0 0
100: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 6
100: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 4 1 -32766 0 0
Newton::solveLinearSystem : Caught exception from the linear solver when getting converged: "BCRSMatrixError [operator++:/home/m.giraud/DUMUXexud/dune-istl/dune/istl/bcrsmatrix.hh:940]: matrix already built up"
100 converged value: 0 121 1 let s go get convergedRemote
100 to  comm_.min(converged) 
100 did  comm_.min(converged) convergedRemote: 0
100 final convergedRemote: 0
100 about to throw Linear solver did not converge
100 Newton::solveLinearSystem : Caught exception: "NumericalProblem [solveLinearSystem:/home/m.giraud/DUMUXexud/dumux/dumux/nonlinear/newtonsolver.hh:381]: Linear solver did not converge when getting convergedRemote "
100 Newton: Caught exception: "NumericalProblem [solveLinearSystem:/home/m.giraud/DUMUXexud/dumux/dumux/nonlinear/newtonsolver.hh:381]: Linear solver did not converge"
100 Newton solver did not converge with dt = 0.0495 seconds. Retrying with time step of 0.02475 seconds
100 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
64 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
64: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 14
64: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 2.18e-06
64: communicator.hh::sendRecv,MPI_Testany 0 1 -32766 0 0 7.75e-06
64: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 0 1 -32766 0 0
64: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 14
64: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 0 1 -32766 0 0
64: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 14
64: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 0 1 -32766 0 0
64: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 14
64: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 0 1 -32766 0 0
64: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 14
64: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 0 1 -32766 0 0
64: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 14
64: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 0 1 -32766 0 0
64: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 14
64: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 0 1 -32766 0 0
64: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 14
64: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 0 1 -32766 0 0
64: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 14
64: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 0 1 -32766 0 0
64: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 14
64: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 0 1 -32766 0 0
64: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 10 14
64: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 0 1 -32766 0 0
64: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 11 14
64: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 0 1 -32766 0 0
64: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 12 14
64: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 0 1 -32766 0 0
64: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 13 14
64: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 0 1 -32766 0 0
Newton::solveLinearSystem : Caught exception from the linear solver when getting converged: "BCRSMatrixError [operator++:/home/m.giraud/DUMUXexud/dune-istl/dune/istl/bcrsmatrix.hh:940]: matrix already built up"
64 converged value: 0 121 1 let s go get convergedRemote
64 to  comm_.min(converged) 
64 did  comm_.min(converged) convergedRemote: 0
64 final convergedRemote: 0
64 about to throw Linear solver did not converge
64 Newton::solveLinearSystem : Caught exception: "NumericalProblem [solveLinearSystem:/home/m.giraud/DUMUXexud/dumux/dumux/nonlinear/newtonsolver.hh:381]: Linear solver did not converge when getting convergedRemote "
64 Newton: Caught exception: "NumericalProblem [solveLinearSystem:/home/m.giraud/DUMUXexud/dumux/dumux/nonlinear/newtonsolver.hh:381]: Linear solver did not converge"
64 Newton solver did not converge with dt = 0.0495 seconds. Retrying with time step of 0.02475 seconds
64 Assemble: r(x^k) 7 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
7: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 13
7: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 2.55e-06
7: communicator.hh::sendRecv,MPI_Testany 1 1 -32766 0 0 9.64e-06
7: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 1 1 -32766 0 0
7: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 13
7: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 1 1 -32766 0 0
7: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 13
7: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 1 1 -32766 0 0
7: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 13
7: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 1 1 -32766 0 0
7: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 13
7: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 1 1 -32766 0 0
7: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 13
7: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 1 1 -32766 0 0
7: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 13
7: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 1 1 -32766 0 0
7: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 13
7: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 1 1 -32766 0 0
7: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 13
7: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 1 1 -32766 0 0
7: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 13
7: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 1 1 -32766 0 0
7: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 10 13
7: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 1 1 -32766 0 0
7: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 11 13
7: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 1 1 -32766 0 0
7: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 12 13
7: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 1 1 -32766 0 0
Newton::solveLinearSystem : Caught exception from the linear solver when getting converged: "BCRSMatrixError [operator++:/home/m.giraud/DUMUXexud/dune-istl/dune/istl/bcrsmatrix.hh:940]: matrix already built up"
7 converged value: 0 121 1 let s go get convergedRemote
7 to  comm_.min(converged) 
7 did  comm_.min(converged) convergedRemote: 0
7 final convergedRemote: 0
7 about to throw Linear solver did not converge
7 Newton::solveLinearSystem : Caught exception: "NumericalProblem [solveLinearSystem:/home/m.giraud/DUMUXexud/dumux/dumux/nonlinear/newtonsolver.hh:381]: Linear solver did not converge when getting convergedRemote "
7 Newton: Caught exception: "NumericalProblem [solveLinearSystem:/home/m.giraud/DUMUXexud/dumux/dumux/nonlinear/newtonsolver.hh:381]: Linear solver did not converge"
7 Newton solver did not converge with dt = 0.0495 seconds. Retrying with time step of 0.02475 seconds
7 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
113 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
113: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 18
113: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 1.041e-06
113: communicator.hh::sendRecv,MPI_Testany 12 1 -32766 0 0 7.041e-06
113: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 12 1 -32766 0 0
113: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 18
113: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 12 1 -32766 0 0
113: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 18
113: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 12 1 -32766 0 0
113: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 18
113: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 12 1 -32766 0 0
113: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 18
113: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 12 1 -32766 0 0
113: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 18
113: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 12 1 -32766 0 0
113: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 18
113: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 12 1 -32766 0 0
113: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 18
113: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 12 1 -32766 0 0
113: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 18
113: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 12 1 -32766 0 0
113: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 18
113: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 12 1 -32766 0 0
113: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 10 18
113: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 12 1 -32766 0 0
113: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 11 18
113: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 12 1 -32766 0 0
113: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 12 18
113: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 12 1 -32766 0 0
113: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 13 18
113: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 12 1 -32766 0 0
113: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 14 18
113: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 12 1 -32766 0 0
113: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 15 18
113: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 12 1 -32766 0 0
113: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 16 18
113: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 12 1 -32766 0 0
113: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 17 18
113: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 12 1 -32766 0 0
Newton::solveLinearSystem : Caught except23 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
23: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 11
23: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 1.05e-06
23: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 1.965e-05
23: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 2.275e-05
23: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 3.997e-05
23: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 4.345e-05
23: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 4.624e-05
23: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 6.956e-05
23: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 7.269e-05
23: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 7.5441e-05
23: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 7.8261e-05
23: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 8.1051e-05
23: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 9.9341e-05
23: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000102521
23: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000105461
23: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000108291
23: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000110981
23: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000113641
23: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000116601
23: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000119351
23: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000121961
23: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000143911
23: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000147031
23: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000150891
23: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000153792
23: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000156562
23: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000159172
23: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000161812
23: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000164432
23: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000167192
23: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000188672
23: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000191922
23: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000194582
23: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000197192
23: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000199822
23: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000202462
23: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000205272
23: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000208102
23: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000210852
23: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000236963
23: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000239883
23: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000242513
23: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000245313
23: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000248063
23: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000250773
23: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000253503
23: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000256253
23: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000259313
23: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000289173
23: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000292413
23: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000295053
23: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000297683
23: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000300333
23: commu20 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
20: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 10
20: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 2.99e-06
20: communicator.hh::sendRecv,MPI_Testany 1 1 -32766 0 0 1.078e-05
20: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 1 1 -32766 0 0
20: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 10
20: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 1 1 -32766 0 0
20: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 10
20: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 1 1 -32766 0 0
20: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 10
20: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 1 1 -32766 0 0
20: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 10
20: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 1 1 -32766 0 0
20: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 10
20: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 1 1 -32766 0 0
20: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 10
20: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 1 1 -32766 0 0
20: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 10
20: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 1 1 -32766 0 0
20: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 10
20: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 1 1 -32766 0 0
20: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 10
20: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 1 1 -32766 0 0
Newton::solveLinearSystem : Caught exception from the linear solver when getting converged: "BCRSMatrixError [operator++:/home/m.giraud/DUMUXexud/dune-istl/dune/istl/bcrsmatrix.hh:940]: matrix already built up"
20 converged value: 0 121 1 let s go get convergedRemote
20 to  comm_.min(converged) 
20 did  comm_.min(converged) convergedRemote: 0
20 final convergedRemote: 0
20 about to throw Linear solver did not converge
20 Newton::solveLinearSystem : Caught exception: "NumericalProblem [solveLinearSystem:/home/m.giraud/DUMUXexud/dumux/dumux/nonlinear/newtonsolver.hh:381]: Linear solver did not converge when getting convergedRemote "
20 Newton: Caught exception: "NumericalProblem [solveLinearSystem:/home/m.giraud/DUMUXexud/dumux/dumux/nonlinear/newtonsolver.hh:381]: Linear solver did not converge"
20 Newton solver did not converge with dt = 0.0495 seconds. Retrying with time step of 0.02475 seconds
20 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
90 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
90: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 6
90: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 3.52e-06
90: communicator.hh::sendRecv,MPI_Testany 3 1 -32766 0 0 1.083e-05
90: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 3 1 -32766 0 0
90: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 6
90: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 3 1 -32766 0 0
90: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 6
90: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 3 1 -32766 0 0
90: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 6
90: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 3 1 -32766 0 0
90: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 6
90: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 3 1 -32766 0 0
90: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 6
90: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 3 1 -32766 0 0
Newton::solveLinearSystem : Caught exception from the linear solver when getting converged: "BCRSMatrixError [operator++:/home/m.giraud/DUMUXexud/dune-istl/dune/istl/bcrsmatrix.hh:940]: matrix already built up"
90 converged value: 0 121 1 let s go get convergedRemote
90 to  comm_.min(converged) 
90 did  comm_.min(converged) convergedRemote: 0
90 final convergedRemote: 0
90 about to throw Linear solver did not converge
90 Newton::solveLinearSystem : Caught exception: "NumericalProblem [solveLinearSystem:/home/m.giraud/DUMUXexud/dumux/dumux/nonlinear/newtonsolver.hh:381]: Linear solver did not converge when getting convergedRemote "
90 Newton: Caught exception: "NumericalProblem [solveLinearSystem:/home/m.giraud/DUMUXexud/dumux/dumux/nonlinear/newtonsolver.hh:381]: Linear solver did not converge"
90 Newton solver did not converge with dt = 0.0495 seconds. Retrying with time step of 0.02475 seconds
90 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
110 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
110: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 11
110: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 6.8e-06
110: communicator.hh::sendRecv,MPI_Testany 0 1 -32766 0 0 1.3371e-05
110: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 0 1 -32766 0 0
110: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 11
110: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 0 1 -32766 0 0
110: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 11
110: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 0 1 -32766 0 0
110: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 11
110: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 0 1 -32766 0 0
110: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 11
110: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 0 1 -32766 0 0
110: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 11
110: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 0 1 -32766 0 0
110: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 11
110: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 0 1 -32766 0 0
110: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 11
110: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 0 1 -32766 0 0
110: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 11
110: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 0 1 -32766 0 0
110: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 11
110: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 0 1 -32766 0 0
110: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 10 11
110: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 0 1 -32766 0 0
Newton::solveLinearSystem : Caught exception from the linear solver when getting converged: "BCRSMatrixError [operator++:/home/m.giraud/DUMUXexud/dune-istl/dune/istl/bcrsmatrix.hh:940]: matrix already built up"
110 converged value: 0 121 1 let s go get convergedRemote
110 to  comm_.min(converged) 
110 did  comm_.min(converged) convergedRemote: 0
110 final convergedRemote: 0
110 about to throw Linear solver did not converge
110 Newton::solveLinearSystem : Caught exception: "NumericalProblem [solveLinearSystem:/home/m.giraud/DUMUXexud/dumux/dumux/nonlinear/newtonsolver.hh:381]: Linear solver did not converge when getting convergedRemote "
110 Newton: Caught exception: "NumericalProblem [solveLinearSystem:/home/m.giraud/DUMUXexud/dumux/dumux/nonlinear/newtonsolver.hh:381]: Linear solver did not converge"
110 Newton solver did not converge with dt = 0.0495 seconds. Retrying with time step of 0.02475 seconds
110 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
73 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
73: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 11
73: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 9e-07
73: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 2.205e-05
73: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 2.528e-05
73: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 2.815e-05
73: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 3.12e-05
73: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 3.412e-05
73: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 3.675e-05
73: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 3.953e-05
73: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 4.23e-05
73: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 4.527e-05
73: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 4.805e-05
73: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 6.6681e-05
73: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 6.9651e-05
73: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 7.2341e-05
73: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 7.4971e-05
73: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 7.7601e-05
73: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 8.0241e-05
73: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 8.2991e-05
73: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 8.5861e-05
73: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 8.8601e-05
73: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 9.1321e-05
73: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000111451
73: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000114661
73: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000117481
73: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000120561
73: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000123371
73: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000126151
73: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000128881
73: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000131901
73: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000134611
73: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000137531
73: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000160512
73: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000163862
73: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000166632
73: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000169242
73: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000171862
73: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000174922
73: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000177732
73: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000180362
73: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000182972
73: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000185742
73: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000215442
73: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000218892
73: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000221653
73: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000224283
73: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000226913
73: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000229533
73: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000232163
73: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000235173
73: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000277673
73: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000281503
73: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000284883
73: communicator.hh::sendR63 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
63: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 9
63: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 7.1e-07
63: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 6.36e-06
63: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 9.26e-06
63: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 1.206e-05
63: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 1.485e-05
63: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 1.7651e-05
63: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 4.0981e-05
63: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 4.3921e-05
63: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 4.6671e-05
63: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 4.9251e-05
63: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 5.2021e-05
63: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 5.4851e-05
63: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 5.7581e-05
63: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 6.0301e-05
63: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 8.2831e-05
63: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 8.5901e-05
63: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 8.8631e-05
63: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 9.1371e-05
63: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 9.4261e-05
63: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 9.7012e-05
63: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 9.9592e-05
63: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000102172
63: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000104912
63: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000107762
63: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000135022
63: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000137932
63: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000140842
63: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000143482
63: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000146072
63: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000148662
63: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000151432
63: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000154452
63: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000157212
63: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000187353
63: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000190373
63: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000192993
63: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000195753
63: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000198463
63: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000201083
63: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000203663
63: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000206533
63: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000209413
63: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000212353
63: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000215613
63: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000261944
63: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000265184
63: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000267934
63: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000270734
63: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000273754
63: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000276454
63: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000279334
63: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000282154
63: communicator.hh18 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
18: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 14
18: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 5.49e-06
18: communicator.hh::sendRecv,MPI_Testany 6 1 -32766 0 0 1.273e-05
18: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 6 1 -32766 0 0
18: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 14
18: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 6 1 -32766 0 0
18: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 14
18: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 6 1 -32766 0 0
18: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 14
18: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 6 1 -32766 0 0
18: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 14
18: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 6 1 -32766 0 0
18: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 14
18: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 6 1 -32766 0 0
18: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 14
18: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 6 1 -32766 0 0
18: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 14
18: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 6 1 -32766 0 0
18: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 14
18: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 6 1 -32766 0 0
18: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 14
18: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 6 1 -32766 0 0
18: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 10 14
18: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 6 1 -32766 0 0
18: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 11 14
18: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 6 1 -32766 0 0
18: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 12 14
18: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 6 1 -32766 0 0
18: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 13 14
18: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 6 1 -32766 0 0
Newton::solveLinearSystem : Caught exception from the linear solver when getting converged: "BCRSMatrixError [operator++:/home/m.giraud/DUMUXexud/dune-istl/dune/istl/bcrsmatrix.hh:940]: matrix already built up"
18 converged value: 0 121 1 let s go get convergedRemote
18 to  comm_.min(converged) 
18 did  comm_.min(converged) convergedRemote: 0
18 final convergedRemote: 0
18 about to throw Linear solver did not converge
18 Newton::solveLinearSystem : Caught exception: "NumericalProblem [solveLinearSystem:/home/m.giraud/DUMUXexud/dumux/dumux/nonlinear/newtonsolver.hh:381]: Linear solver did not converge when getting convergedRemote "
18 Newton: Caught exception: "NumericalProblem [solveLinearSystem:/home/m.giraud/DUMUXexud/dumux/dumux/nonlinear/newtonsolver.hh:381]: Linear solver did not converge"
18 Newton solver did not converge with dt = 0.0495 seconds. Retrying with time step of 0.02475 seconds
18 Assemble: r(x^k)31 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
31: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 10
31: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 8.9e-07
31: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 6.571e-06
31: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 9.481e-06
31: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 1.2331e-05
31: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 3.5011e-05
31: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 3.8411e-05
31: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 4.1031e-05
31: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 4.3641e-05
31: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 4.6281e-05
31: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 4.8941e-05
31: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 5.1551e-05
31: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 5.4381e-05
31: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 8.0832e-05
31: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 8.4192e-05
31: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 8.6932e-05
31: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 8.9552e-05
31: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 9.2152e-05
31: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 9.4912e-05
31: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 9.7712e-05
31: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000100462
31: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000128432
31: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000131842
31: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000134492
31: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000137142
31: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000139972
31: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000142762
31: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000145522
31: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000148412
31: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000181943
31: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000185283
31: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000188203
31: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000191233
31: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000193873
31: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000196503
31: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000199143
31: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000201823
31: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000205233
31: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000208153
31: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000211133
31: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000214403
31: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000217793
31: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000265444
31: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000270084
31: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000273464
31: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000276894
31: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000280304
31: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000283674
31: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000287004
31: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000290674
31: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000294354
31: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000299534
31: communicator.hh::sendRecv,MPI_Testany 6 1 -32766 0 0 0.000303144
31: communicator.96 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
96: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 12
96: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 6.06e-06
96: communicator.hh::sendRecv,MPI_Testany 3 1 -32766 0 0 1.255e-05
96: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 3 1 -32766 0 0
96: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 12
96: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 3 1 -32766 0 0
96: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 12
96: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 3 1 -32766 0 0
96: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 12
96: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 3 1 -32766 0 0
96: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 12
96: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 3 1 -32766 0 0
96: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 12
96: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 3 1 -32766 0 0
96: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 12
96: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 3 1 -32766 0 0
96: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 12
96: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 3 1 -32766 0 0
96: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 12
96: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 3 1 -32766 0 0
96: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 12
96: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 3 1 -32766 0 0
96: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 10 12
96: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 3 1 -32766 0 0
96: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 11 12
96: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 3 1 -32766 0 0
Newton::solveLinearSystem : Caught exception from the linear solver when getting converged: "BCRSMatrixError [operator++:/home/m.giraud/DUMUXexud/dune-istl/dune/istl/bcrsmatrix.hh:940]: matrix already built up"
96 converged value: 0 121 1 let s go get convergedRemote
96 to  comm_.min(converged) 
96 did  comm_.min(converged) convergedRemote: 0
96 final convergedRemote: 0
96 about to throw Linear solver did not converge
96 Newton::solveLinearSystem : Caught exception: "NumericalProblem [solveLinearSystem:/home/m.giraud/DUMUXexud/dumux/dumux/nonlinear/newtonsolver.hh:381]: Linear solver did not converge when getting convergedRemote "
96 Newton: Caught exception: "NumericalProblem [solveLinearSystem:/home/m.giraud/DUMUXexud/dumux/dumux/nonlinear/newtonsolver.hh:381]: Linear solver did not converge"
96 Newton solver did not converge with dt = 0.0495 seconds. Retrying with time step of 0.02475 seconds
96 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
cator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 3 1 -32766 0 0
60: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 6
60: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 3 1 -32766 0 0
60: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 6
60: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 3 1 -32766 0 0
Newton::solveLinearSystem : Caught exception from the linear solver when getting converged: "BCRSMatrixError [operator++:/home/m.giraud/DUMUXexud/dune-istl/dune/istl/bcrsmatrix.hh:940]: matrix already built up"
60 converged value: 0 121 1 let s go get convergedRemote
60 to  comm_.min(converged) 
60 did  comm_.min(converged) convergedRemote: 0
60 final convergedRemote: 0
60 about to throw Linear solver did not converge
60 Newton::solveLinearSystem : Caught exception: "NumericalProblem [solveLinearSystem:/home/m.giraud/DUMUXexud/dumux/dumux/nonlinear/newtonsolver.hh:381]: Linear solver did not converge when getting convergedRemote "
60 Newton: Caught exception: "NumericalProblem [solveLinearSystem:/home/m.giraud/DUMUXexud/dumux/dumux/nonlinear/newtonsolver.hh:381]: Linear solver did not converge"
60 Newton solver did not converge with dt = 0.0495 seconds. Retrying with time step of 0.02475 seconds
60 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
11 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
11: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 17
11: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 3.61e-06
11: communicator.hh::sendRecv,MPI_Testany 6 1 -32766 0 0 1.052e-05
11: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 6 1 -32766 0 0
11: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 17
11: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 6 1 -32766 0 0
11: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 17
11: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 6 1 -32766 0 0
11: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 17
11: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 6 1 -32766 0 0
11: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 17
11: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 6 1 -32766 0 0
11: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 17
11: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 6 1 -32766 0 0
11: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 17
11: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 6 1 -32766 0 0
11: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 17
11: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 6 1 -32766 0 0
11: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 17
11: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 6 1 -32766 0 0
11: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 17
11: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 6 1 -32766 0 0
11: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 10 17
11: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 6 1 -32766 0 0
11: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 11 17
11: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 6 1 -32766 0 0
11: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 12 17
11: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 6 1 -32766 0 0
11: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 13 17
11: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 6 1 -32766 0 0
11: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 14 17
11: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 6 1 -32766 0 0
11: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 15 17
11: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 6 1 -32766 0 0
11: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 16 17
11: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 6 1 -32766 0 0
Newton::solveLinearSystem : Caught exception from the linear solver when getting converged: "BCRSMatrixError [operator++:/home/m.giraud/DUMUXexud/dune-istl/dune/istl/bcrsmatrix.hh:940]: matrix already built up"
11 converged value: 0 121 1 let s go get convergedRemote
11 to  comm_.min(converged) 
11 did  co22 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
22: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 20
22: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 5.48e-06
22: communicator.hh::sendRecv,MPI_Testany 8 1 -32766 0 0 1.2941e-05
22: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 8 1 -32766 0 0
22: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 20
22: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 8 1 -32766 0 0
22: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 20
22: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 8 1 -32766 0 0
22: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 20
22: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 8 1 -32766 0 0
22: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 20
22: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 8 1 -32766 0 0
22: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 20
22: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 8 1 -32766 0 0
22: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 20
22: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 8 1 -32766 0 0
22: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 20
22: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 8 1 -32766 0 0
22: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 20
22: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 8 1 -32766 0 0
22: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 20
22: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 8 1 -32766 0 0
22: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 10 20
22: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 8 1 -32766 0 0
22: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 11 20
22: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 8 1 -32766 0 0
22: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 12 20
22: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 8 1 -32766 0 0
22: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 13 20
22: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 8 1 -32766 0 0
22: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 14 20
22: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 8 1 -32766 0 0
22: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 15 20
22: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 8 1 -32766 0 0
22: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 16 20
22: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 8 1 -32766 0 0
22: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 17 20
22: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 8 1 -32766 0 0
22: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finish:sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000286364
53: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000289644
53: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000293494
53: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000297104
53: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000300424
53: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000305214
53: communicator.hh::sendRecv,MPI_Testany 5 1 -32766 0 0 0.000308884
53: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 5 1 -32766 0 0
53: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 17
53: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 5 1 -32766 0 0
53: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 17
53: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 5 1 -32766 0 0
53: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 17
53: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 5 1 -32766 0 0
53: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 17
53: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 5 1 -32766 0 0
53: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 17
53: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 5 1 -32766 0 0
53: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 17
53: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 5 1 -32766 0 0
53: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 17
53: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 5 1 -32766 0 0
53: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 17
53: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 5 1 -32766 0 0
53: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 17
53: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 5 1 -32766 0 0
53: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 10 17
53: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 5 1 -32766 0 0
53: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 11 17
53: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 5 1 -32766 0 0
53: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 12 17
53: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 5 1 -32766 0 0
53: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 13 17
53: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 5 1 -32766 0 0
53: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 14 17
53: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 5 1 -32766 0 0
53: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 15 17
53: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 5 1 -32766 0 0
53: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 16 17
53: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 5 1 -32766 0 0
Newton::solveLinearSystem : Caught exception from the linear solver when getting converged: "BCRSMatrixError [operator++:/home/m.giraud/DUMUXexud/dune-istl/dune/istl/bcrsmatrix.hh:940]: matrix already built up"
44 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
44: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 11
44: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 2.51e-06
44: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 8.45e-06
44: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 1.156e-05
44: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 1.462e-05
44: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 1.7431e-05
44: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 2.0301e-05
44: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 2.3111e-05
44: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 2.6341e-05
44: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 2.8991e-05
44: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 3.1561e-05
44: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 3.4331e-05
44: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 3.6941e-05
44: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 3.9541e-05
44: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 4.2151e-05
44: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 4.4751e-05
44: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 4.7331e-05
44: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 5.0171e-05
44: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 5.2981e-05
44: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 5.5611e-05
44: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 5.8221e-05
44: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 6.2171e-05
44: communicator.hh::sendRecv,MPI_Testany 0 1 -32766 0 0 6.4841e-05
44: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 0 1 -32766 0 0
44: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 11
44: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 0 1 -32766 0 0
44: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 11
44: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 0 1 -32766 0 0
44: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 11
44: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 0 1 -32766 0 0
44: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 11
44: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 0 1 -32766 0 0
44: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 11
44: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 0 1 -32766 0 0
44: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 11
44: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 0 1 -32766 0 0
44: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 11
44: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 0 1 -32766 0 0
44: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 11
44: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 0 1 -32766 0 0
44: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 11
44: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 0 1 -32766 0 0
44: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 10 11
44: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 0 1 -32766 0 0
Newton::solveLinearSystem : Caught exception from the linear solver when getting converged: s
108 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
rixError [operator++:/home/m.giraud/DUMUXexud/dune-istl/dune/istl/bcrsmatrix.hh:940]: matrix already built up"
84 converged value: 0 121 1 let s go get convergedRemote
84 to  comm_.min(converged) 
84 did  comm_.min(converged) convergedRemote: 0
84 final convergedRemote: 0
84 about to throw Linear solver did not converge
84 Newton::solveLinearSystem : Caught exception: "NumericalProblem [solveLinearSystem:/home/m.giraud/DUMUXexud/dumux/dumux/nonlinear/newtonsolver.hh:381]: Linear solver did not converge when getting convergedRemote "
84 Newton: Caught exception: "NumericalProblem [solveLinearSystem:/home/m.giraud/DUMUXexud/dumux/dumux/nonlinear/newtonsolver.hh:381]: Linear solver did not converge"
84 Newton solver did not converge with dt = 0.0495 seconds. Retrying with time step of 0.02475 seconds
84 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
 Newton solver did not converge with dt = 0.0495 seconds. Retrying with time step of 0.02475 seconds
50 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
mm_.min(converged) convergedRemote: 0
45 final convergedRemote: 0
45 about to throw Linear solver did not converge
45 Newton::solveLinearSystem : Caught exception: "NumericalProblem [solveLinearSystem:/home/m.giraud/DUMUXexud/dumux/dumux/nonlinear/newtonsolver.hh:381]: Linear solver did not converge when getting convergedRemote "
45 Newton: Caught exception: "NumericalProblem [solveLinearSystem:/home/m.giraud/DUMUXexud/dumux/dumux/nonlinear/newtonsolver.hh:381]: Linear solver did not converge"
45 Newton solver did not converge with dt = 0.0495 seconds. Retrying with time step of 0.02475 seconds
45 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
ormation_.size(), recvRequests, &finished, &status); 18 20
109: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 5 1 -32766 0 0
109: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 19 20
109: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 5 1 -32766 0 0
Newton::solveLinearSystem : Caught exception from the linear solver when getting converged: "BCRSMatrixError [operator++:/home/m.giraud/DUMUXexud/dune-istl/dune/istl/bcrsmatrix.hh:940]: matrix already built up"
109 converged value: 0 121 1 let s go get convergedRemote
109 to  comm_.min(converged) 
109 did  comm_.min(converged) convergedRemote: 0
109 final convergedRemote: 0
109 about to throw Linear solver did not converge
109 Newton::solveLinearSystem : Caught exception: "NumericalProblem [solveLinearSystem:/home/m.giraud/DUMUXexud/dumux/dumux/nonlinear/newtonsolver.hh:381]: Linear solver did not converge when getting convergedRemote "
109 Newton: Caught exception: "NumericalProblem [solveLinearSystem:/home/m.giraud/DUMUXexud/dumux/dumux/nonlinear/newtonsolver.hh:381]: Linear solver did not converge"
109 Newton solver did not converge with dt = 0.0495 seconds. Retrying with time step of 0.02475 seconds
109 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
d, &status); 18 20
71: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 3 1 -32766 0 0
71: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 19 20
71: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 3 1 -32766 0 0
Newton::solveLinearSystem : Caught exception from the linear solver when getting converged: "BCRSMatrixError [operator++:/home/m.giraud/DUMUXexud/dune-istl/dune/istl/bcrsmatrix.hh:940]: matrix already built up"
71 converged value: 0 121 1 let s go get convergedRemote
71 to  comm_.min(converged) 
71 did  comm_.min(converged) convergedRemote: 0
71 final convergedRemote: 0
71 about to throw Linear solver did not converge
71 Newton::solveLinearSystem : Caught exception: "NumericalProblem [solveLinearSystem:/home/m.giraud/DUMUXexud/dumux/dumux/nonlinear/newtonsolver.hh:381]: Linear solver did not converge when getting convergedRemote "
71 Newton: Caught exception: "NumericalProblem [solveLinearSystem:/home/m.giraud/DUMUXexud/dumux/dumux/nonlinear/newtonsolver.hh:381]: Linear solver did not converge"
71 Newton solver did not converge with dt = 0.0495 seconds. Retrying with time step of 0.02475 seconds
71 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
UMUXexud/dumux/dumux/nonlinear/newtonsolver.hh:381]: Linear solver did not converge"
62 Newton solver did not converge with dt = 0.0495 seconds. Retrying with time step of 0.02475 seconds
62 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
r.hh::sendRecv,MPI_Testany 0 1 -32766 0 0 0.000289854
61: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 0 1 -32766 0 0
61: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 9
61: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 0 1 -32766 0 0
61: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 9
61: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 0 1 -32766 0 0
61: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 9
61: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 0 1 -32766 0 0
61: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 9
61: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 0 1 -32766 0 0
61: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 9
61: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 0 1 -32766 0 0
61: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 9
61: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 0 1 -32766 0 0
61: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 9
61: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 0 1 -32766 0 0
61: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 9
61: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 0 1 -32766 0 0
Newton::solveLinearSystem : Caught exception from the linear solver when getting converged: "BCRSMatrixError [operator++:/home/m.giraud/DUMUXexud/dune-istl/dune/istl/bcrsmatrix.hh:940]: matrix already built up"
61 converged value: 0 121 1 let s go get convergedRemote
61 to  comm_.min(converged) 
61 did  comm_.min(converged) convergedRemote: 0
61 final convergedRemote: 0
61 about to throw Linear solver did not converge
61 Newton::solveLinearSystem : Caught exception: "NumericalProblem [solveLinearSystem:/home/m.giraud/DUMUXexud/dumux/dumux/nonlinear/newtonsolver.hh:381]: Linear solver did not converge when getting convergedRemote "
61 Newton: Caught exception: "NumericalProblem [solveLinearSystem:/home/m.giraud/DUMUXexud/dumux/dumux/nonlinear/newtonsolver.hh:381]: Linear solver did not converge"
61 Newton solver did not converge with dt = 0.0495 seconds. Retrying with time step of 0.02475 seconds
61 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
Requests, &finished, &status); 18 19
57: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 18 1 -32766 0 0
Newton::solveLinearSystem : Caught exception from the linear solver when getting converged: "BCRSMatrixError [operator++:/home/m.giraud/DUMUXexud/dune-istl/dune/istl/bcrsmatrix.hh:940]: matrix already built up"
57 converged value: 0 121 1 let s go get convergedRemote
57 to  comm_.min(converged) 
57 did  comm_.min(converged) convergedRemote: 0
57 final convergedRemote: 0
57 about to throw Linear solver did not converge
57 Newton::solveLinearSystem : Caught exception: "NumericalProblem [solveLinearSystem:/home/m.giraud/DUMUXexud/dumux/dumux/nonlinear/newtonsolver.hh:381]: Linear solver did not converge when getting convergedRemote "
57 Newton: Caught exception: "NumericalProblem [solveLinearSystem:/home/m.giraud/DUMUXexud/dumux/dumux/nonlinear/newtonsolver.hh:381]: Linear solver did not converge"
57 Newton solver did not converge with dt = 0.0495 seconds. Retrying with time step of 0.02475 seconds
57 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
Newton::solveLinearSystem : Caught exception from the linear solver when getting converged: "BCRSMatrixError [operator++:/home/m.giraud/DUMUXexud/dune-istl/dune/istl/bcrsmatrix.hh:940]: matrix already built up"
17 converged value: 0 121 1 let s go get convergedRemote
17 to  comm_.min(converged) 
17 did  comm_.min(converged) convergedRemote: 0
17 final convergedRemote: 0
17 about to throw Linear solver did not converge
17 Newton::solveLinearSystem : Caught exception: "NumericalProblem [solveLinearSystem:/home/m.giraud/DUMUXexud/dumux/dumux/nonlinear/newtonsolver.hh:381]: Linear solver did not converge when getting convergedRemote "
17 Newton: Caught exception: "NumericalProblem [solveLinearSystem:/home/m.giraud/DUMUXexud/dumux/dumux/nonlinear/newtonsolver.hh:381]: Linear solver did not converge"
17 Newton solver did not converge with dt = 0.0495 seconds. Retrying with time step of 0.02475 seconds
17 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
Newton::solveLinearSystem : Caught exception from the linear solver when getting converged: "BCRSMatrixError [operator++:/home/m.giraud/DUMUXexud/dune-istl/dune/istl/bcrsmatrix.hh:940]: matrix already built up"
102 converged value: 0 121 1 let s go get convergedRemote
102 to  comm_.min(converged) 
102 did  comm_.min(converged) convergedRemote: 0
102 final convergedRemote: 0
102 about to throw Linear solver did not converge
102 Newton::solveLinearSystem : Caught exception: "NumericalProblem [solveLinearSystem:/home/m.giraud/DUMUXexud/dumux/dumux/nonlinear/newtonsolver.hh:381]: Linear solver did not converge when getting convergedRemote "
102 Newton: Caught exception: "NumericalProblem [solveLinearSystem:/home/m.giraud/DUMUXexud/dumux/dumux/nonlinear/newtonsolver.hh:381]: Linear solver did not converge"
102 Newton solver did not converge with dt = 0.0495 seconds. Retrying with time step of 0.02475 seconds
102 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
78: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 15
78: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 10 1 -32766 0 0
78: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 15
78: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 10 1 -32766 0 0
78: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 15
78: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 10 1 -32766 0 0
78: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 15
78: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 10 1 -32766 0 0
78: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 15
78: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 10 1 -32766 0 0
78: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 15
78: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 10 1 -32766 0 0
78: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 15
78: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 10 1 -32766 0 0
78: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 15
78: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 10 1 -32766 0 0
78: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 15
78: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 10 1 -32766 0 0
78: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 10 15
78: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 10 1 -32766 0 0
78: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 11 15
78: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 10 1 -32766 0 0
78: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 12 15
78: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 10 1 -32766 0 0
78: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 13 15
78: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 10 1 -32766 0 0
78: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 14 15
78: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 10 1 -32766 0 0
Newton::solveLinearSystem : Caught exception from the linear solver when getting converged: "BCRSMatrixError [operator++:/home/m.giraud/DUMUXexud/dune-istl/dune/istl/bcrsmatrix.hh:940]: matrix already built up"
78 converged value: 0 121 1 let s go get convergedRemote
78 to  comm_.min(converged) 
78 did  comm_.min(converged) convergedRemote: 0
78 final convergedRemote: 0
78 about to throw Linear solver did not converge
78 Newton::solveLinearSystem : Caught exception: "NumericalProblem [solveLinearSystem:/home/m.giraud/DUMUXexud/dumux/dumux/nonlinear/newtonsolver.hh:381]: Linear solver did not converge when getting convergedRemote "
78 Newton: Caught exception: "NumericalProblem [solveLinearSystem:/home/m.giraud/DUMUXexud/dumux/dumux/nonlinear/newtonsolver.hh:381]: Linear solver did not converge"
78 Newton solver did not converge with dt = 0.0495 seconds. Retrying with time step of 0.02475 seconds
78 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
00880941
9: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000884191
9: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000887211
9: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000890771
9: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000894031
9: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000897031
9: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000900111
9: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000927711
9: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000931191
9: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000935151
9: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000938532
9: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000941682
9: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000944832
9: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000948122
9: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000991902
9: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000996622
9: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.0010005
9: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.00100518
9: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.00100907
9: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.00101284
9: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.00101658
9: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.00102056
9: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.00102433
9: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.00102809
9: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.00103247
9: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.00103648
9: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.00104043
9: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.00107793
9: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.00108243
9: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.00108638
9: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.00109053
9: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.00109487
9: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.00109893
9: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.00110271
9: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.00110943
9: communicator.hh::sendRecv,MPI_Testany 0 1 -32766 0 0 0.00111357
9: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 0 1 -32766 0 0
9: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 7
9: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 0 1 -32766 0 0
9: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 7
9: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 0 1 -32766 0 0
9: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 7
9: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 0 1 -32766 0 0
9: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 7
9: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 0 1 -32766 0 0
9: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 7
9: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 0 1 -32766 0 0
9: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 7
9: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 0 1 -32766 0 0
Newton::solveLinearSystem : Caught exception from the linear solver when getting converged: "BCRSMatrixError [operator++:/home/m.giraud/DUMUXexud/dune-istl/dune/istl/bcrsmatrix.hh:940]: matrix already built up"
9 converged value: 0 121 1 let s go get ched, &status); 18 20
72: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 0 1 -32766 0 0
72: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 19 20
72: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 0 1 -32766 0 0
Newton::solveLinearSystem : Caught exception from the linear solver when getting converged: "BCRSMatrixError [operator++:/home/m.giraud/DUMUXexud/dune-istl/dune/istl/bcrsmatrix.hh:940]: matrix already built up"
72 converged value: 0 121 1 let s go get convergedRemote
72 to  comm_.min(converged) 
72 did  comm_.min(converged) convergedRemote: 0
72 final convergedRemote: 0
72 about to throw Linear solver did not converge
72 Newton::solveLinearSystem : Caught exception: "NumericalProblem [solveLinearSystem:/home/m.giraud/DUMUXexud/dumux/dumux/nonlinear/newtonsolver.hh:381]: Linear solver did not converge when getting convergedRemote "
72 Newton: Caught exception: "NumericalProblem [solveLinearSystem:/home/m.giraud/DUMUXexud/dumux/dumux/nonlinear/newtonsolver.hh:381]: Linear solver did not converge"
72 Newton solver did not converge with dt = 0.0495 seconds. Retrying with time step of 0.02475 seconds
72 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
Newton::solveLinearSystem : Caught exception from the linear solver when getting converged: "BCRSMatrixError [operator++:/home/m.giraud/DUMUXexud/dune-istl/dune/istl/bcrsmatrix.hh:940]: matrix already built up"
65 converged value: 0 121 1 let s go get convergedRemote
65 to  comm_.min(converged) 
65 did  comm_.min(converged) convergedRemote: 0
65 final convergedRemote: 0
65 about to throw Linear solver did not converge
65 Newton::solveLinearSystem : Caught exception: "NumericalProblem [solveLinearSystem:/home/m.giraud/DUMUXexud/dumux/dumux/nonlinear/newtonsolver.hh:381]: Linear solver did not converge when getting convergedRemote "
65 Newton: Caught exception: "NumericalProblem [solveLinearSystem:/home/m.giraud/DUMUXexud/dumux/dumux/nonlinear/newtonsolver.hh:381]: Linear solver did not converge"
65 Newton solver did not converge with dt = 0.0495 seconds. Retrying with time step of 0.02475 seconds
65 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
cv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 18 20
41: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 1 1 -32766 0 0
41: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 19 20
41: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 1 1 -32766 0 0
Newton::solveLinearSystem : Caught exception from the linear solver when getting converged: "BCRSMatrixError [operator++:/home/m.giraud/DUMUXexud/dune-istl/dune/istl/bcrsmatrix.hh:940]: matrix already built up"
41 converged value: 0 121 1 let s go get convergedRemote
41 to  comm_.min(converged) 
41 did  comm_.min(converged) convergedRemote: 0
41 final convergedRemote: 0
41 about to throw Linear solver did not converge
41 Newton::solveLinearSystem : Caught exception: "NumericalProblem [solveLinearSystem:/home/m.giraud/DUMUXexud/dumux/dumux/nonlinear/newtonsolver.hh:381]: Linear solver did not converge when getting convergedRemote "
41 Newton: Caught exception: "NumericalProblem [solveLinearSystem:/home/m.giraud/DUMUXexud/dumux/dumux/nonlinear/newtonsolver.hh:381]: Linear solver did not converge"
41 Newton solver did not converge with dt = 0.0495 seconds. Retrying with time step of 0.02475 seconds
41 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
ed, &status); 18 21
58: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 2 1 -32766 0 0
58: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 19 21
58: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 2 1 -32766 0 0
58: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 20 21
58: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 2 1 -32766 0 0
Newton::solveLinearSystem : Caught exception from the linear solver when getting converged: "BCRSMatrixError [operator++:/home/m.giraud/DUMUXexud/dune-istl/dune/istl/bcrsmatrix.hh:940]: matrix already built up"
58 converged value: 0 121 1 let s go get convergedRemote
58 to  comm_.min(converged) 
58 did  comm_.min(converged) convergedRemote: 0
58 final convergedRemote: 0
58 about to throw Linear solver did not converge
58 Newton::solveLinearSystem : Caught exception: "NumericalProblem [solveLinearSystem:/home/m.giraud/DUMUXexud/dumux/dumux/nonlinear/newtonsolver.hh:381]: Linear solver did not converge when getting convergedRemote "
58 Newton: Caught exception: "NumericalProblem [solveLinearSystem:/home/m.giraud/DUMUXexud/dumux/dumux/nonlinear/newtonsolver.hh:381]: Linear solver did not converge"
58 Newton solver did not converge with dt = 0.0495 seconds. Retrying with time step of 0.02475 seconds
58 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
Newton::solveLinearSystem : Caught exception from the linear solver when getting converged: "BCRSMatrixError [operator++:/home/m.giraud/DUMUXexud/dune-istl/dune/istl/bcrsmatrix.hh:940]: matrix already built up"
106 converged value: 0 121 1 let s go get convergedRemote
106 to  comm_.min(converged) 
106 did  comm_.min(converged) convergedRemote: 0
106 final convergedRemote: 0
106 about to throw Linear solver did not converge
106 Newton::solveLinearSystem : Caught exception: "NumericalProblem [solveLinearSystem:/home/m.giraud/DUMUXexud/dumux/dumux/nonlinear/newtonsolver.hh:381]: Linear solver did not converge when getting convergedRemote "
106 Newton: Caught exception: "NumericalProblem [solveLinearSystem:/home/m.giraud/DUMUXexud/dumux/dumux/nonlinear/newtonsolver.hh:381]: Linear solver did not converge"
106 Newton solver did not converge with dt = 0.0495 seconds. Retrying with time step of 0.02475 seconds
106 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
ommunicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000298514
85: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000301324
85: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000304044
85: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000306624
85: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000334424
85: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000337494
85: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000340234
85: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000343184
85: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000345924
85: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000348644
85: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000351375
85: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000354075
85: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000393245
85: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000396465
85: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000399565
85: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000402525
85: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000405895
85: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000409875
85: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000413125
85: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000416655
85: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000420125
85: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000423375
85: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000426615
85: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000429846
85: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000433076
85: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000436506
85: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000440126
85: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000443516
85: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000447026
85: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000488746
85: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000492086
85: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000495676
85: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000499126
85: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000502566
85: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000505886
85: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000510127
85: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000513457
85: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000516987
85: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000520417
85: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000524627
85: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000528047
85: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000531367
85: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000534667
85: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000538177
85: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000541617
85: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000545067
85: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000548487
85: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000551777
85: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000555077
85: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000558547
85: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000561927
85: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000594858
85: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000598338
85: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000601678
85: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000605498
85: communicator.hh::sendRecv,Newton::solveLinearSystem : Caught exception from the linear solver when getting converged: "BCRSMatrixError [operator++:/home/m.giraud/DUMUXexud/dune-istl/dune/istl/bcrsmatrix.hh:940]: matrix already built up"
34 converged value: 0 121 1 let s go get convergedRemote
34 to  comm_.min(converged) 
34 did  comm_.min(converged) convergedRemote: 0
34 final convergedRemote: 0
34 about to throw Linear solver did not converge
34 Newton::solveLinearSystem : Caught exception: "NumericalProblem [solveLinearSystem:/home/m.giraud/DUMUXexud/dumux/dumux/nonlinear/newtonsolver.hh:381]: Linear solver did not converge when getting convergedRemote "
34 Newton: Caught exception: "NumericalProblem [solveLinearSystem:/home/m.giraud/DUMUXexud/dumux/dumux/nonlinear/newtonsolver.hh:381]: Linear solver did not converge"
34 Newton solver did not converge with dt = 0.0495 seconds. Retrying with time step of 0.02475 seconds
34 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
cvRequests, &finished, &status); 18 26
82: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 14 1 -32766 0 0
82: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 19 26
82: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 14 1 -32766 0 0
82: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 20 26
82: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 14 1 -32766 0 0
82: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 21 26
82: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 14 1 -32766 0 0
82: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 22 26
82: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 14 1 -32766 0 0
82: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 23 26
82: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 14 1 -32766 0 0
82: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 24 26
82: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 14 1 -32766 0 0
82: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 25 26
82: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 14 1 -32766 0 0
Newton::solveLinearSystem : Caught exception from the linear solver when getting converged: "BCRSMatrixError [operator++:/home/m.giraud/DUMUXexud/dune-istl/dune/istl/bcrsmatrix.hh:940]: matrix already built up"
82 converged value: 0 121 1 let s go get convergedRemote
82 to  comm_.min(converged) 
82 did  comm_.min(converged) convergedRemote: 0
82 final convergedRemote: 0
82 about to throw Linear solver did not converge
82 Newton::solveLinearSystem : Caught exception: "NumericalProblem [solveLinearSystem:/home/m.giraud/DUMUXexud/dumux/dumux/nonlinear/newtonsolver.hh:381]: Linear solver did not converge when getting convergedRemote "
82 Newton: Caught exception: "NumericalProblem [solveLinearSystem:/home/m.giraud/DUMUXexud/dumux/dumux/nonlinear/newtonsolver.hh:381]: Linear solver did not converge"
82 Newton solver did not converge with dt = 0.0495 seconds. Retrying with time step of 0.02475 seconds
82 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
115 did  comm_.min(converged) convergedRemote: 0
115 final convergedRemote: 0
115 about to throw Linear solver did not converge
115 Newton::solveLinearSystem : Caught exception: "NumericalProblem [solveLinearSystem:/home/m.giraud/DUMUXexud/dumux/dumux/nonlinear/newtonsolver.hh:381]: Linear solver did not converge when getting convergedRemote "
115 Newton: Caught exception: "NumericalProblem [solveLinearSystem:/home/m.giraud/DUMUXexud/dumux/dumux/nonlinear/newtonsolver.hh:381]: Linear solver did not converge"
115 Newton solver did not converge with dt = 0.0495 seconds. Retrying with time step of 0.02475 seconds
115 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
v,MPI_Testany -32766 0 -32766 0 0 0.000299563
29: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000303354
29: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000306844
29: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000312064
29: communicator.hh::sendRecv,MPI_Testany 2 1 -32766 0 0 0.000315444
29: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 2 1 -32766 0 0
29: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 11
29: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 2 1 -32766 0 0
29: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 11
29: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 2 1 -32766 0 0
29: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 11
29: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 2 1 -32766 0 0
29: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 11
29: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 2 1 -32766 0 0
29: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 11
29: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 2 1 -32766 0 0
29: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 11
29: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 2 1 -32766 0 0
29: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 11
29: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 2 1 -32766 0 0
29: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 11
29: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 2 1 -32766 0 0
29: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 11
29: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 2 1 -32766 0 0
29: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 10 11
29: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 2 1 -32766 0 0
Newton::solveLinearSystem : Caught exception from the linear solver when getting converged: "BCRSMatrixError [operator++:/home/m.giraud/DUMUXexud/dune-istl/dune/istl/bcrsmatrix.hh:940]: matrix already built up"
29 converged value: 0 121 1 let s go get convergedRemote
29 to  comm_.min(converged) 
29 did  comm_.min(converged) convergedRemote: 0
29 final convergedRemote: 0
29 about to throw Linear solver did not converge
29 Newton::solveLinearSystem : Caught exception: "NumericalProblem [solveLinearSystem:/home/m.giraud/DUMUXexud/dumux/dumux/nonlinear/newtonsolver.hh:381]: Linear solver did not converge when getting convergedRemote "
29 Newton: Caught exception: "NumericalProblem [solveLinearSystem:/home/m.giraud/DUMUXexud/dumux/dumux/nonlinear/newtonsolver.hh:381]: Linear solver did not converge"
29 Newton solver did not converge with dt = 0.0495 seconds. Retrying with time step of 0.02475 seconds
29 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
d, &status); 18 22
68: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 0 1 -32766 0 0
68: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 19 22
68: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 0 1 -32766 0 0
68: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 20 22
68: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 0 1 -32766 0 0
68: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 21 22
68: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 0 1 -32766 0 0
Newton::solveLinearSystem : Caught exception from the linear solver when getting converged: "BCRSMatrixError [operator++:/home/m.giraud/DUMUXexud/dune-istl/dune/istl/bcrsmatrix.hh:940]: matrix already built up"
68 converged value: 0 121 1 let s go get convergedRemote
68 to  comm_.min(converged) 
68 did  comm_.min(converged) convergedRemote: 0
68 final convergedRemote: 0
68 about to throw Linear solver did not converge
68 Newton::solveLinearSystem : Caught exception: "NumericalProblem [solveLinearSystem:/home/m.giraud/DUMUXexud/dumux/dumux/nonlinear/newtonsolver.hh:381]: Linear solver did not converge when getting convergedRemote "
68 Newton: Caught exception: "NumericalProblem [solveLinearSystem:/home/m.giraud/DUMUXexud/dumux/dumux/nonlinear/newtonsolver.hh:381]: Linear solver did not converge"
68 Newton solver did not converge with dt = 0.0495 seconds. Retrying with time step of 0.02475 seconds
68 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
or.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000303054
95: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000305824
95: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000308574
95: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000311164
95: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000313764
95: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000316354
95: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000318944
95: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000322054
95: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000324904
95: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000353514
95: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000356234
95: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000358814
95: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000361404
95: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000364305
95: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000366895
95: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000369505
95: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000372505
95: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000416335
95: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000419865
95: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000423705
95: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000427105
95: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000430485
95: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000433845
95: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000437105
95: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000440355
95: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000443866
95: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000447216
95: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000450466
95: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000453686
95: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000456936
95: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000460186
95: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000504766
95: communicator.hh::sendRecv,MPI_Testany 2 1 -32766 0 0 0.000508586
95: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 2 1 -32766 0 0
95: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 7
95: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 2 1 -32766 0 0
95: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 7
95: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 2 1 -32766 0 0
95: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 7
95: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 2 1 -32766 0 0
95: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 7
95: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 2 1 -32766 0 0
95: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 7
95: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 2 1 -32766 0 0
95: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 7
95: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 2 1 -32766 0 0
Newton::solveLinearSystem : Caught exception from the linear solver when getting converged: "BCRSMatrixError [operator++:/home/m.giraud/DUMUXexud/dune-istl/dune/istl/bcrsmatrix.hh:940]: matrix already built up"
95 converged value: 0 121 1 let s go get convergedRemote
95 to  comm_.min(converged) 
95 did  comm_.min(converged) convergedRemote: 0
95 final Newton::solveLinearSystem : Caught exception from the linear solver when getting converged: "BCRSMatrixError [operator++:/home/m.giraud/DUMUXexud/dune-istl/dune/istl/bcrsmatrix.hh:940]: matrix already built up"
10 converged value: 0 121 1 let s go get convergedRemote
10 to  comm_.min(converged) 
10 did  comm_.min(converged) convergedRemote: 0
10 final convergedRemote: 0
10 about to throw Linear solver did not converge
10 Newton::solveLinearSystem : Caught exception: "NumericalProblem [solveLinearSystem:/home/m.giraud/DUMUXexud/dumux/dumux/nonlinear/newtonsolver.hh:381]: Linear solver did not converge when getting convergedRemote "
10 Newton: Caught exception: "NumericalProblem [solveLinearSystem:/home/m.giraud/DUMUXexud/dumux/dumux/nonlinear/newtonsolver.hh:381]: Linear solver did not converge"
10 Newton solver did not converge with dt = 0.0495 seconds. Retrying with time step of 0.02475 seconds
10 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
hed, &status); 18 21
88: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 2 1 -32766 0 0
88: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 19 21
88: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 2 1 -32766 0 0
88: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 20 21
88: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 2 1 -32766 0 0
Newton::solveLinearSystem : Caught exception from the linear solver when getting converged: "BCRSMatrixError [operator++:/home/m.giraud/DUMUXexud/dune-istl/dune/istl/bcrsmatrix.hh:940]: matrix already built up"
88 converged value: 0 121 1 let s go get convergedRemote
88 to  comm_.min(converged) 
88 did  comm_.min(converged) convergedRemote: 0
88 final convergedRemote: 0
88 about to throw Linear solver did not converge
88 Newton::solveLinearSystem : Caught exception: "NumericalProblem [solveLinearSystem:/home/m.giraud/DUMUXexud/dumux/dumux/nonlinear/newtonsolver.hh:381]: Linear solver did not converge when getting convergedRemote "
88 Newton: Caught exception: "NumericalProblem [solveLinearSystem:/home/m.giraud/DUMUXexud/dumux/dumux/nonlinear/newtonsolver.hh:381]: Linear solver did not converge"
88 Newton solver did not converge with dt = 0.0495 seconds. Retrying with time step of 0.02475 seconds
88 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
v,MPI_Testany -32766 0 -32766 0 0 0.00155451
1: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.00155728
1: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.00156002
1: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.00156278
1: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.00158309
1: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.00158625
1: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.00158887
1: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.001592
1: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.00159476
1: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.00159749
1: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.00160024
1: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.00160303
1: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.00160567
1: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.00160828
1: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.00163098
1: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.00163415
1: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.00163676
1: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.00163938
1: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.00164198
1: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.00164461
1: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.00164722
1: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.00165017
1: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.00165292
1: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.00167663
1: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.00167977
1: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.00168238
1: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.001685
1: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.00168766
1: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.00169048
1: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.00169309
1: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.0016957
1: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.00169833
1: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.00170121
1: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.00173109
1: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.00173408
1: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.00173769
1: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.00174047
1: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.0017437
1: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.00174634
1: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.00174894
1: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.00175166
1: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.00179488
1: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.00179842
1: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.00180184
1: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.00180508
1: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.00180835
1: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.0018116
1: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.00181485
1: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.0018181
1: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.00182167
1: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.00182502
1: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.0018283
1: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.00183157
1: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.00183481
1: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.00183808
1: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.00184132
1: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.00184475
1: communicator.hh::sendRecnicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000310904
25: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000337814
25: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000341644
25: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000344494
25: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000347334
25: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000349934
25: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000352524
25: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000355404
25: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000357984
25: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000360824
25: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000393535
25: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000396385
25: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000399225
25: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000402165
25: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000404785
25: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000407665
25: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000410515
25: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000413085
25: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000415685
25: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000419095
25: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000422115
25: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000425415
25: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000428705
25: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000475796
25: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000479246
25: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000482756
25: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000485986
25: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000489446
25: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000492866
25: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000496316
25: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000499746
25: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000503306
25: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000507016
25: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000510406
25: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000513756
25: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000517116
25: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000520346
25: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000523656
25: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000526886
25: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000530116
25: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000533357
25: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000536567
25: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000539817
25: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000543087
25: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000546307
25: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000549567
25: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000552787
25: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000587547
25: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000590957
25: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000594447
25: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000599217
25: communicator.hh::sendRecv,MPI_Testany 6 1 -32766 0 0 0.000602737
25: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 6 1 -32766 0 0
25: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 7
25: communicator.hh::sendRecv, before assert(finished !=14 did  comm_.min(converged) convergedRemote: 0
14 final convergedRemote: 0
14 about to throw Linear solver did not converge
14 Newton::solveLinearSystem : Caught exception: "NumericalProblem [solveLinearSystem:/home/m.giraud/DUMUXexud/dumux/dumux/nonlinear/newtonsolver.hh:381]: Linear solver did not converge when getting convergedRemote "
14 Newton: Caught exception: "NumericalProblem [solveLinearSystem:/home/m.giraud/DUMUXexud/dumux/dumux/nonlinear/newtonsolver.hh:381]: Linear solver did not converge"
14 Newton solver did not converge with dt = 0.0495 seconds. Retrying with time step of 0.02475 seconds
14 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
 converged: "BCRSMatrixError [operator++:/home/m.giraud/DUMUXexud/dune-istl/dune/istl/bcrsmatrix.hh:940]: matrix already built up"
98 converged value: 0 121 1 let s go get convergedRemote
98 to  comm_.min(converged) 
98 did  comm_.min(converged) convergedRemote: 0
98 final convergedRemote: 0
98 about to throw Linear solver did not converge
98 Newton::solveLinearSystem : Caught exception: "NumericalProblem [solveLinearSystem:/home/m.giraud/DUMUXexud/dumux/dumux/nonlinear/newtonsolver.hh:381]: Linear solver did not converge when getting convergedRemote "
98 Newton: Caught exception: "NumericalProblem [solveLinearSystem:/home/m.giraud/DUMUXexud/dumux/dumux/nonlinear/newtonsolver.hh:381]: Linear solver did not converge"
98 Newton solver did not converge with dt = 0.0495 seconds. Retrying with time step of 0.02475 seconds
98 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
h::sendRecv, before assert(finished != MPI_UNDEFINED); 5 1 -32766 0 0
Newton::solveLinearSystem : Caught exception from the linear solver when getting converged: "BCRSMatrixError [operator++:/home/m.giraud/DUMUXexud/dune-istl/dune/istl/bcrsmatrix.hh:940]: matrix already built up"
49 converged value: 0 121 1 let s go get convergedRemote
49 to  comm_.min(converged) 
49 did  comm_.min(converged) convergedRemote: 0
49 final convergedRemote: 0
49 about to throw Linear solver did not converge
49 Newton::solveLinearSystem : Caught exception: "NumericalProblem [solveLinearSystem:/home/m.giraud/DUMUXexud/dumux/dumux/nonlinear/newtonsolver.hh:381]: Linear solver did not converge when getting convergedRemote "
49 Newton: Caught exception: "NumericalProblem [solveLinearSystem:/home/m.giraud/DUMUXexud/dumux/dumux/nonlinear/newtonsolver.hh:381]: Linear solver did not converge"
49 Newton solver did not converge with dt = 0.0495 seconds. Retrying with time step of 0.02475 seconds
49 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
103 did  comm_.min(converged) convergedRemote: 0
103 final convergedRemote: 0
103 about to throw Linear solver did not converge
103 Newton::solveLinearSystem : Caught exception: "NumericalProblem [solveLinearSystem:/home/m.giraud/DUMUXexud/dumux/dumux/nonlinear/newtonsolver.hh:381]: Linear solver did not converge when getting convergedRemote "
103 Newton: Caught exception: "NumericalProblem [solveLinearSystem:/home/m.giraud/DUMUXexud/dumux/dumux/nonlinear/newtonsolver.hh:381]: Linear solver did not converge"
103 Newton solver did not converge with dt = 0.0495 seconds. Retrying with time step of 0.02475 seconds
103 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
104 did  comm_.min(converged) convergedRemote: 0
104 final convergedRemote: 0
104 about to throw Linear solver did not converge
104 Newton::solveLinearSystem : Caught exception: "NumericalProblem [solveLinearSystem:/home/m.giraud/DUMUXexud/dumux/dumux/nonlinear/newtonsolver.hh:381]: Linear solver did not converge when getting convergedRemote "
104 Newton: Caught exception: "NumericalProblem [solveLinearSystem:/home/m.giraud/DUMUXexud/dumux/dumux/nonlinear/newtonsolver.hh:381]: Linear solver did not converge"
104 Newton solver did not converge with dt = 0.0495 seconds. Retrying with time step of 0.02475 seconds
104 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
hed, &status); 18 21
94: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 6 1 -32766 0 0
94: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 19 21
94: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 6 1 -32766 0 0
94: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 20 21
94: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 6 1 -32766 0 0
Newton::solveLinearSystem : Caught exception from the linear solver when getting converged: "BCRSMatrixError [operator++:/home/m.giraud/DUMUXexud/dune-istl/dune/istl/bcrsmatrix.hh:940]: matrix already built up"
94 converged value: 0 121 1 let s go get convergedRemote
94 to  comm_.min(converged) 
94 did  comm_.min(converged) convergedRemote: 0
94 final convergedRemote: 0
94 about to throw Linear solver did not converge
94 Newton::solveLinearSystem : Caught exception: "NumericalProblem [solveLinearSystem:/home/m.giraud/DUMUXexud/dumux/dumux/nonlinear/newtonsolver.hh:381]: Linear solver did not converge when getting convergedRemote "
94 Newton: Caught exception: "NumericalProblem [solveLinearSystem:/home/m.giraud/DUMUXexud/dumux/dumux/nonlinear/newtonsolver.hh:381]: Linear solver did not converge"
94 Newton solver did not converge with dt = 0.0495 seconds. Retrying with time step of 0.02475 seconds
94 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
-32766 0 0 0.000581138
0: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000583698
0: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000586278
0: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000589128
0: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000591858
0: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000594438
0: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000596998
0: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000599558
0: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000602118
0: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000604678
0: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000607688
0: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000610438
0: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000613258
0: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000616048
0: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000618838
0: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000621598
0: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000624368
0: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000627388
0: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000630158
0: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000632918
0: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000635718
0: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000638288
0: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000640858
0: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000643418
0: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000646389
0: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.0007735
0: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000857671
0: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000933052
0: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.00101601
0: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.00114473
0: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.00115194
0: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.00115522
0: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.00116365
0: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.00127488
0: communicator.hh::sendRecv,MPI_Testany 0 1 -32766 0 0 0.00135301
0: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 0 1 -32766 0 0
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 3
0: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 0 1 -32766 0 0
0: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 3
0: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 0 1 -32766 0 0
Newton::solveLinearSystem : Caught exception from the linear solver when getting converged: "BCRSMatrixError [operator++:/home/m.giraud/DUMUXexud/dune-istl/dune/istl/bcrsmatrix.hh:940]: matrix already built up"
0 converged value: 0 121 1 let s go get convergedRemote
0 to  comm_.min(converged) 
0 did  comm_.min(converged) convergedRemote: 0
0 final convergedRemote: 0
0 about to throw Linear solver did not converge
0 Newton::solveLinearSystem : Caught exception: "NumericalProblem [solveLinearSystem:/home/m.giraud/DUMUXexud/dumux/dumux/nonlinear/newtonsolver.hh:381]: Linear solver did not converge when getting convergedRemote "
0 Newton: Caught exception: "NumericalProblem [solveLinearSystem:/home/m.giraud/DUMUXexud/dumux/dumux/nonlinear/newtonsolver.hh:381]: Linear solver did not converge"
0 Newton solver did not converge with dt = 0.0495 seconds. Retrying with time step of 0.02475 seconds
0 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
24 did  comm_.min(converged) convergedRemote: 0
24 final convergedRemote: 0
24 about to throw Linear solver did not converge
24 Newton::solveLinearSystem : Caught exception: "NumericalProblem [solveLinearSystem:/home/m.giraud/DUMUXexud/dumux/dumux/nonlinear/newtonsolver.hh:381]: Linear solver did not converge when getting convergedRemote "
24 Newton: Caught exception: "NumericalProblem [solveLinearSystem:/home/m.giraud/DUMUXexud/dumux/dumux/nonlinear/newtonsolver.hh:381]: Linear solver did not converge"
24 Newton solver did not converge with dt = 0.0495 seconds. Retrying with time step of 0.02475 seconds
24 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
d, &status); 18 20
42: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 0 1 -32766 0 0
42: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 19 20
42: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 0 1 -32766 0 0
Newton::solveLinearSystem : Caught exception from the linear solver when getting converged: "BCRSMatrixError [operator++:/home/m.giraud/DUMUXexud/dune-istl/dune/istl/bcrsmatrix.hh:940]: matrix already built up"
42 converged value: 0 121 1 let s go get convergedRemote
42 to  comm_.min(converged) 
42 did  comm_.min(converged) convergedRemote: 0
42 final convergedRemote: 0
42 about to throw Linear solver did not converge
42 Newton::solveLinearSystem : Caught exception: "NumericalProblem [solveLinearSystem:/home/m.giraud/DUMUXexud/dumux/dumux/nonlinear/newtonsolver.hh:381]: Linear solver did not converge when getting convergedRemote "
42 Newton: Caught exception: "NumericalProblem [solveLinearSystem:/home/m.giraud/DUMUXexud/dumux/dumux/nonlinear/newtonsolver.hh:381]: Linear solver did not converge"
42 Newton solver did not converge with dt = 0.0495 seconds. Retrying with time step of 0.02475 seconds
42 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
59 did  comm_.min(converged) convergedRemote: 0
59 final convergedRemote: 0
59 about to throw Linear solver did not converge
59 Newton::solveLinearSystem : Caught exception: "NumericalProblem [solveLinearSystem:/home/m.giraud/DUMUXexud/dumux/dumux/nonlinear/newtonsolver.hh:381]: Linear solver did not converge when getting convergedRemote "
59 Newton: Caught exception: "NumericalProblem [solveLinearSystem:/home/m.giraud/DUMUXexud/dumux/dumux/nonlinear/newtonsolver.hh:381]: Linear solver did not converge"
59 Newton solver did not converge with dt = 0.0495 seconds. Retrying with time step of 0.02475 seconds
59 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
80 did  comm_.min(converged) convergedRemote: 0
80 final convergedRemote: 0
80 about to throw Linear solver did not converge
80 Newton::solveLinearSystem : Caught exception: "NumericalProblem [solveLinearSystem:/home/m.giraud/DUMUXexud/dumux/dumux/nonlinear/newtonsolver.hh:381]: Linear solver did not converge when getting convergedRemote "
80 Newton: Caught exception: "NumericalProblem [solveLinearSystem:/home/m.giraud/DUMUXexud/dumux/dumux/nonlinear/newtonsolver.hh:381]: Linear solver did not converge"
80 Newton solver did not converge with dt = 0.0495 seconds. Retrying with time step of 0.02475 seconds
80 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
r.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000279064
87: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000324284
87: communicator.hh::sendRecv,MPI_Testany 4 1 -32766 0 0 0.000328254
87: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 4 1 -32766 0 0
87: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 19
87: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 4 1 -32766 0 0
87: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 19
87: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 4 1 -32766 0 0
87: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 19
87: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 4 1 -32766 0 0
87: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 19
87: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 4 1 -32766 0 0
87: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 19
87: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 4 1 -32766 0 0
87: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 19
87: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 4 1 -32766 0 0
87: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 19
87: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 4 1 -32766 0 0
87: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 19
87: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 4 1 -32766 0 0
87: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 19
87: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 4 1 -32766 0 0
87: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 10 19
87: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 4 1 -32766 0 0
87: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 11 19
87: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 4 1 -32766 0 0
87: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 12 19
87: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 4 1 -32766 0 0
87: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 13 19
87: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 4 1 -32766 0 0
87: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 14 19
87: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 4 1 -32766 0 0
87: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 15 19
87: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 4 1 -32766 0 0
87: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 16 19
87: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 4 1 -32766 0 0
87: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 17 19
87: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 4 1 -32766 0 0
87: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 18 19
87: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 4 1 -32766 0 0
Newton::solveLinearSystem : Caught exception from the linear solver when getting converg6 0 0
Newton::solveLinearSystem : Caught exception from the linear solver when getting converged: "BCRSMatrixError [operator++:/home/m.giraud/DUMUXexud/dune-istl/dune/istl/bcrsmatrix.hh:940]: matrix already built up"
91 converged value: 0 121 1 let s go get convergedRemote
91 to  comm_.min(converged) 
91 did  comm_.min(converged) convergedRemote: 0
91 final convergedRemote: 0
91 about to throw Linear solver did not converge
91 Newton::solveLinearSystem : Caught exception: "NumericalProblem [solveLinearSystem:/home/m.giraud/DUMUXexud/dumux/dumux/nonlinear/newtonsolver.hh:381]: Linear solver did not converge when getting convergedRemote "
91 Newton: Caught exception: "NumericalProblem [solveLinearSystem:/home/m.giraud/DUMUXexud/dumux/dumux/nonlinear/newtonsolver.hh:381]: Linear solver did not converge"
91 Newton solver did not converge with dt = 0.0495 seconds. Retrying with time step of 0.02475 seconds
91 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
nicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000339424
77: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000342204
77: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000344954
77: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000347564
77: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000350174
77: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000352794
77: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000355414
77: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000358294
77: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000387795
77: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000390705
77: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000393325
77: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000395945
77: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000398545
77: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000401455
77: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000404205
77: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000406965
77: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000409755
77: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000412475
77: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000415525
77: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000418625
77: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000467146
77: communicator.hh::sendRecv,MPI_Testany 3 1 -32766 0 0 0.000470826
77: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 3 1 -32766 0 0
77: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 6
77: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 3 1 -32766 0 0
77: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 6
77: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 3 1 -32766 0 0
77: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 6
77: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 3 1 -32766 0 0
77: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 6
77: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 3 1 -32766 0 0
77: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 6
77: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 3 1 -32766 0 0
Newton::solveLinearSystem : Caught exception from the linear solver when getting converged: "BCRSMatrixError [operator++:/home/m.giraud/DUMUXexud/dune-istl/dune/istl/bcrsmatrix.hh:940]: matrix already built up"
77 converged value: 0 121 1 let s go get convergedRemote
77 to  comm_.min(converged) 
77 did  comm_.min(converged) convergedRemote: 0
77 final convergedRemote: 0
77 about to throw Linear solver did not converge
77 Newton::solveLinearSystem : Caught exception: "NumericalProblem [solveLinearSystem:/home/m.giraud/DUMUXexud/dumux/dumux/nonlinear/newtonsolver.hh:381]: Linear solver did not converge when getting convergedRemote "
77 Newton: Caught exception: "NumericalProblem [solveLinearSystem:/home/m.giraud/DUMUXexud/dumux/dumux/nonlinear/newtonsolver.hh:381]: Linear solver did not converge"
77 Newton solver did not converge with dt = 0.0495 seconds. Retrying with time step of 0.02475 seconds
77 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
INED); 8 1 -32766 0 0
97: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 9
97: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 8 1 -32766 0 0
97: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 9
97: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 8 1 -32766 0 0
97: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 9
97: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 8 1 -32766 0 0
97: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 9
97: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 8 1 -32766 0 0
97: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 9
97: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 8 1 -32766 0 0
97: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 9
97: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 8 1 -32766 0 0
Newton::solveLinearSystem : Caught exception from the linear solver when getting converged: "BCRSMatrixError [operator++:/home/m.giraud/DUMUXexud/dune-istl/dune/istl/bcrsmatrix.hh:940]: matrix already built up"
97 converged value: 0 121 1 let s go get convergedRemote
97 to  comm_.min(converged) 
97 did  comm_.min(converged) convergedRemote: 0
97 final convergedRemote: 0
97 about to throw Linear solver did not converge
97 Newton::solveLinearSystem : Caught exception: "NumericalProblem [solveLinearSystem:/home/m.giraud/DUMUXexud/dumux/dumux/nonlinear/newtonsolver.hh:381]: Linear solver did not converge when getting convergedRemote "
97 Newton: Caught exception: "NumericalProblem [solveLinearSystem:/home/m.giraud/DUMUXexud/dumux/dumux/nonlinear/newtonsolver.hh:381]: Linear solver did not converge"
97 Newton solver did not converge with dt = 0.0495 seconds. Retrying with time step of 0.02475 seconds
97 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
cator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000276024
47: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000317594
47: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000321454
47: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000324544
47: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000327484
47: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000330314
47: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000333254
47: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000335964
47: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000339244
47: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000342274
47: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000344944
47: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000347834
47: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000351085
47: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000354345
47: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000359265
47: communicator.hh::sendRecv,MPI_Testany 3 1 -32766 0 0 0.000362775
47: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 3 1 -32766 0 0
47: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 6
47: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 3 1 -32766 0 0
47: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 6
47: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 3 1 -32766 0 0
47: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 6
47: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 3 1 -32766 0 0
47: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 6
47: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 3 1 -32766 0 0
47: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 6
47: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 3 1 -32766 0 0
Newton::solveLinearSystem : Caught exception from the linear solver when getting converged: "BCRSMatrixError [operator++:/home/m.giraud/DUMUXexud/dune-istl/dune/istl/bcrsmatrix.hh:940]: matrix already built up"
47 converged value: 0 121 1 let s go get convergedRemote
47 to  comm_.min(converged) 
47 did  comm_.min(converged) convergedRemote: 0
47 final convergedRemote: 0
47 about to throw Linear solver did not converge
47 Newton::solveLinearSystem : Caught exception: "NumericalProblem [solveLinearSystem:/home/m.giraud/DUMUXexud/dumux/dumux/nonlinear/newtonsolver.hh:381]: Linear solver did not converge when getting convergedRemote "
47 Newton: Caught exception: "NumericalProblem [solveLinearSystem:/home/m.giraud/DUMUXexud/dumux/dumux/nonlinear/newtonsolver.hh:381]: Linear solver did not converge"
47 Newton solver did not converge with dt = 0.0495 seconds. Retrying with time step of 0.02475 seconds
47 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
74 did  comm_.min(converged) convergedRemote: 0
74 final convergedRemote: 0
74 about to throw Linear solver did not converge
74 Newton::solveLinearSystem : Caught exception: "NumericalProblem [solveLinearSystem:/home/m.giraud/DUMUXexud/dumux/dumux/nonlinear/newtonsolver.hh:381]: Linear solver did not converge when getting convergedRemote "
74 Newton: Caught exception: "NumericalProblem [solveLinearSystem:/home/m.giraud/DUMUXexud/dumux/dumux/nonlinear/newtonsolver.hh:381]: Linear solver did not converge"
74 Newton solver did not converge with dt = 0.0495 seconds. Retrying with time step of 0.02475 seconds
74 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
ormation_.size(), recvRequests, &finished, &status); 4 12
21: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 9 1 -32766 0 0
21: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 12
21: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 9 1 -32766 0 0
21: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 12
21: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 9 1 -32766 0 0
21: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 12
21: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 9 1 -32766 0 0
21: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 12
21: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 9 1 -32766 0 0
21: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 12
21: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 9 1 -32766 0 0
21: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 10 12
21: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 9 1 -32766 0 0
21: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 11 12
21: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 9 1 -32766 0 0
Newton::solveLinearSystem : Caught exception from the linear solver when getting converged: "BCRSMatrixError [operator++:/home/m.giraud/DUMUXexud/dune-istl/dune/istl/bcrsmatrix.hh:940]: matrix already built up"
21 converged value: 0 121 1 let s go get convergedRemote
21 to  comm_.min(converged) 
21 did  comm_.min(converged) convergedRemote: 0
21 final convergedRemote: 0
21 about to throw Linear solver did not converge
21 Newton::solveLinearSystem : Caught exception: "NumericalProblem [solveLinearSystem:/home/m.giraud/DUMUXexud/dumux/dumux/nonlinear/newtonsolver.hh:381]: Linear solver did not converge when getting convergedRemote "
21 Newton: Caught exception: "NumericalProblem [solveLinearSystem:/home/m.giraud/DUMUXexud/dumux/dumux/nonlinear/newtonsolver.hh:381]: Linear solver did not converge"
21 Newton solver did not converge with dt = 0.0495 seconds. Retrying with time step of 0.02475 seconds
21 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
m_.min(converged) convergedRemote: 0
19 final convergedRemote: 0
19 about to throw Linear solver did not converge
19 Newton::solveLinearSystem : Caught exception: "NumericalProblem [solveLinearSystem:/home/m.giraud/DUMUXexud/dumux/dumux/nonlinear/newtonsolver.hh:381]: Linear solver did not converge when getting convergedRemote "
19 Newton: Caught exception: "NumericalProblem [solveLinearSystem:/home/m.giraud/DUMUXexud/dumux/dumux/nonlinear/newtonsolver.hh:381]: Linear solver did not converge"
19 Newton solver did not converge with dt = 0.0495 seconds. Retrying with time step of 0.02475 seconds
19 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
d, &status); 18 24
38: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 9 1 -32766 0 0
38: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 19 24
38: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 9 1 -32766 0 0
38: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 20 24
38: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 9 1 -32766 0 0
38: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 21 24
38: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 9 1 -32766 0 0
38: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 22 24
38: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 9 1 -32766 0 0
38: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 23 24
38: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 9 1 -32766 0 0
Newton::solveLinearSystem : Caught exception from the linear solver when getting converged: "BCRSMatrixError [operator++:/home/m.giraud/DUMUXexud/dune-istl/dune/istl/bcrsmatrix.hh:940]: matrix already built up"
38 converged value: 0 121 1 let s go get convergedRemote
38 to  comm_.min(converged) 
38 did  comm_.min(converged) convergedRemote: 0
38 final convergedRemote: 0
38 about to throw Linear solver did not converge
38 Newton::solveLinearSystem : Caught exception: "NumericalProblem [solveLinearSystem:/home/m.giraud/DUMUXexud/dumux/dumux/nonlinear/newtonsolver.hh:381]: Linear solver did not converge when getting convergedRemote "
38 Newton: Caught exception: "NumericalProblem [solveLinearSystem:/home/m.giraud/DUMUXexud/dumux/dumux/nonlinear/newtonsolver.hh:381]: Linear solver did not converge"
38 Newton solver did not converge with dt = 0.0495 seconds. Retrying with time step of 0.02475 seconds
38 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
recvRequests, &finished, &status); 16 17
83: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 9 1 -32766 0 0
Newton::solveLinearSystem : Caught exception from the linear solver when getting converged: "BCRSMatrixError [operator++:/home/m.giraud/DUMUXexud/dune-istl/dune/istl/bcrsmatrix.hh:940]: matrix already built up"
83 converged value: 0 121 1 let s go get convergedRemote
83 to  comm_.min(converged) 
83 did  comm_.min(converged) convergedRemote: 0
83 final convergedRemote: 0
83 about to throw Linear solver did not converge
83 Newton::solveLinearSystem : Caught exception: "NumericalProblem [solveLinearSystem:/home/m.giraud/DUMUXexud/dumux/dumux/nonlinear/newtonsolver.hh:381]: Linear solver did not converge when getting convergedRemote "
83 Newton: Caught exception: "NumericalProblem [solveLinearSystem:/home/m.giraud/DUMUXexud/dumux/dumux/nonlinear/newtonsolver.hh:381]: Linear solver did not converge"
83 Newton solver did not converge with dt = 0.0495 seconds. Retrying with time step of 0.02475 seconds
83 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
 of 0.02475 seconds
118 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
Recv,MPI_Testany -32766 0 -32766 0 0 0.000305174
33: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000307954
33: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000310974
33: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000313704
33: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000316454
33: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000319244
33: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000322224
33: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000325024
33: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000328204
33: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000330914
33: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000334854
33: communicator.hh::sendRecv,MPI_Testany 1 1 -32766 0 0 0.000337674
33: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 1 1 -32766 0 0
33: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 9
33: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 1 1 -32766 0 0
33: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 9
33: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 1 1 -32766 0 0
33: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 9
33: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 1 1 -32766 0 0
33: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 9
33: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 1 1 -32766 0 0
33: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 9
33: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 1 1 -32766 0 0
33: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 9
33: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 1 1 -32766 0 0
33: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 9
33: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 1 1 -32766 0 0
33: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 9
33: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 1 1 -32766 0 0
Newton::solveLinearSystem : Caught exception from the linear solver when getting converged: "BCRSMatrixError [operator++:/home/m.giraud/DUMUXexud/dune-istl/dune/istl/bcrsmatrix.hh:940]: matrix already built up"
33 converged value: 0 121 1 let s go get convergedRemote
33 to  comm_.min(converged) 
33 did  comm_.min(converged) convergedRemote: 0
33 final convergedRemote: 0
33 about to throw Linear solver did not converge
33 Newton::solveLinearSystem : Caught exception: "NumericalProblem [solveLinearSystem:/home/m.giraud/DUMUXexud/dumux/dumux/nonlinear/newtonsolver.hh:381]: Linear solver did not converge when getting convergedRemote "
33 Newton: Caught exception: "NumericalProblem [solveLinearSystem:/home/m.giraud/DUMUXexud/dumux/dumux/nonlinear/newtonsolver.hh:381]: Linear solver did not converge"
33 Newton solver did not converge with dt = 0.0495 seconds. Retrying with time step of 0.02475 seconds
33 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
fore MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 15
48: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 9 1 -32766 0 0
48: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 15
48: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 9 1 -32766 0 0
48: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 10 15
48: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 9 1 -32766 0 0
48: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 11 15
48: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 9 1 -32766 0 0
48: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 12 15
48: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 9 1 -32766 0 0
48: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 13 15
48: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 9 1 -32766 0 0
48: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 14 15
48: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 9 1 -32766 0 0
Newton::solveLinearSystem : Caught exception from the linear solver when getting converged: "BCRSMatrixError [operator++:/home/m.giraud/DUMUXexud/dune-istl/dune/istl/bcrsmatrix.hh:940]: matrix already built up"
48 converged value: 0 121 1 let s go get convergedRemote
48 to  comm_.min(converged) 
48 did  comm_.min(converged) convergedRemote: 0
48 final convergedRemote: 0
48 about to throw Linear solver did not converge
48 Newton::solveLinearSystem : Caught exception: "NumericalProblem [solveLinearSystem:/home/m.giraud/DUMUXexud/dumux/dumux/nonlinear/newtonsolver.hh:381]: Linear solver did not converge when getting convergedRemote "
48 Newton: Caught exception: "NumericalProblem [solveLinearSystem:/home/m.giraud/DUMUXexud/dumux/dumux/nonlinear/newtonsolver.hh:381]: Linear solver did not converge"
48 Newton solver did not converge with dt = 0.0495 seconds. Retrying with time step of 0.02475 seconds
48 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
nds. Retrying with time step of 0.02475 seconds
5 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
= dS/dt + div F - q;   M = grad r
d, &status); 18 26
52: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 4 1 -32766 0 0
52: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 19 26
52: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 4 1 -32766 0 0
52: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 20 26
52: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 4 1 -32766 0 0
52: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 21 26
52: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 4 1 -32766 0 0
52: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 22 26
52: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 4 1 -32766 0 0
52: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 23 26
52: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 4 1 -32766 0 0
52: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 24 26
52: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 4 1 -32766 0 0
52: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 25 26
52: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 4 1 -32766 0 0
Newton::solveLinearSystem : Caught exception from the linear solver when getting converged: "BCRSMatrixError [operator++:/home/m.giraud/DUMUXexud/dune-istl/dune/istl/bcrsmatrix.hh:940]: matrix already built up"
52 converged value: 0 121 1 let s go get convergedRemote
52 to  comm_.min(converged) 
52 did  comm_.min(converged) convergedRemote: 0
52 final convergedRemote: 0
52 about to throw Linear solver did not converge
52 Newton::solveLinearSystem : Caught exception: "NumericalProblem [solveLinearSystem:/home/m.giraud/DUMUXexud/dumux/dumux/nonlinear/newtonsolver.hh:381]: Linear solver did not converge when getting convergedRemote "
52 Newton: Caught exception: "NumericalProblem [solveLinearSystem:/home/m.giraud/DUMUXexud/dumux/dumux/nonlinear/newtonsolver.hh:381]: Linear solver did not converge"
52 Newton solver did not converge with dt = 0.0495 seconds. Retrying with time step of 0.02475 seconds
52 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
home/m.giraud/DUMUXexud/dumux/dumux/nonlinear/newtonsolver.hh:381]: Linear solver did not converge"
66 Newton solver did not converge with dt = 0.0495 seconds. Retrying with time step of 0.02475 seconds
66 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
mmunicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000378565
55: communicator.hh::sendRecv,MPI_Testany 3 1 -32766 0 0 0.000382825
55: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 3 1 -32766 0 0
55: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 11
55: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 3 1 -32766 0 0
55: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 11
55: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 3 1 -32766 0 0
55: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 11
55: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 3 1 -32766 0 0
55: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 11
55: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 3 1 -32766 0 0
55: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 11
55: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 3 1 -32766 0 0
55: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 11
55: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 3 1 -32766 0 0
55: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 11
55: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 3 1 -32766 0 0
55: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 11
55: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 3 1 -32766 0 0
55: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 11
55: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 3 1 -32766 0 0
55: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 10 11
55: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 3 1 -32766 0 0
Newton::solveLinearSystem : Caught exception from the linear solver when getting converged: "BCRSMatrixError [operator++:/home/m.giraud/DUMUXexud/dune-istl/dune/istl/bcrsmatrix.hh:940]: matrix already built up"
55 converged value: 0 121 1 let s go get convergedRemote
55 to  comm_.min(converged) 
55 did  comm_.min(converged) convergedRemote: 0
55 final convergedRemote: 0
55 about to throw Linear solver did not converge
55 Newton::solveLinearSystem : Caught exception: "NumericalProblem [solveLinearSystem:/home/m.giraud/DUMUXexud/dumux/dumux/nonlinear/newtonsolver.hh:381]: Linear solver did not converge when getting convergedRemote "
55 Newton: Caught exception: "NumericalProblem [solveLinearSystem:/home/m.giraud/DUMUXexud/dumux/dumux/nonlinear/newtonsolver.hh:381]: Linear solver did not converge"
55 Newton solver did not converge with dt = 0.0495 seconds. Retrying with time step of 0.02475 seconds
55 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
convergedRemote: 0
69 final convergedRemote: 0
69 about to throw Linear solver did not converge
69 Newton::solveLinearSystem : Caught exception: "NumericalProblem [solveLinearSystem:/home/m.giraud/DUMUXexud/dumux/dumux/nonlinear/newtonsolver.hh:381]: Linear solver did not converge when getting convergedRemote "
69 Newton: Caught exception: "NumericalProblem [solveLinearSystem:/home/m.giraud/DUMUXexud/dumux/dumux/nonlinear/newtonsolver.hh:381]: Linear solver did not converge"
69 Newton solver did not converge with dt = 0.0495 seconds. Retrying with time step of 0.02475 seconds
69 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
UMUXexud/dumux/dumux/nonlinear/newtonsolver.hh:381]: Linear solver did not converge"
40 Newton solver did not converge with dt = 0.0495 seconds. Retrying with time step of 0.02475 seconds
40 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
UMUXexud/dumux/dumux/nonlinear/newtonsolver.hh:381]: Linear solver did not converge"
37 Newton solver did not converge with dt = 0.0495 seconds. Retrying with time step of 0.02475 seconds
37 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
d, &status); 18 26
26: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 9 1 -32766 0 0
26: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 19 26
26: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 9 1 -32766 0 0
26: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 20 26
26: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 9 1 -32766 0 0
26: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 21 26
26: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 9 1 -32766 0 0
26: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 22 26
26: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 9 1 -32766 0 0
26: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 23 26
26: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 9 1 -32766 0 0
26: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 24 26
26: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 9 1 -32766 0 0
26: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 25 26
26: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 9 1 -32766 0 0
Newton::solveLinearSystem : Caught exception from the linear solver when getting converged: "BCRSMatrixError [operator++:/home/m.giraud/DUMUXexud/dune-istl/dune/istl/bcrsmatrix.hh:940]: matrix already built up"
26 converged value: 0 121 1 let s go get convergedRemote
26 to  comm_.min(converged) 
26 did  comm_.min(converged) convergedRemote: 0
26 final convergedRemote: 0
26 about to throw Linear solver did not converge
26 Newton::solveLinearSystem : Caught exception: "NumericalProblem [solveLinearSystem:/home/m.giraud/DUMUXexud/dumux/dumux/nonlinear/newtonsolver.hh:381]: Linear solver did not converge when getting convergedRemote "
26 Newton: Caught exception: "NumericalProblem [solveLinearSystem:/home/m.giraud/DUMUXexud/dumux/dumux/nonlinear/newtonsolver.hh:381]: Linear solver did not converge"
26 Newton solver did not converge with dt = 0.0495 seconds. Retrying with time step of 0.02475 seconds
26 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
xception: "NumericalProblem [solveLinearSystem:/home/m.giraud/DUMUXexud/dumux/dumux/nonlinear/newtonsolver.hh:381]: Linear solver did not converge"
35 Newton solver did not converge with dt = 0.0495 seconds. Retrying with time step of 0.02475 seconds
35 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
MPI_Testany -32766 0 -32766 0 0 0.000258943
93: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000301173
93: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000305024
93: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000308514
93: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000311994
93: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000315454
93: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000318954
93: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000322434
93: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000325984
93: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000329384
93: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000332804
93: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000336214
93: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000339524
93: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000342814
93: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000346394
93: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000349894
93: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000353524
93: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000357034
93: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000360524
93: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000364074
93: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000404315
93: communicator.hh::sendRecv,MPI_Testany 8 1 -32766 0 0 0.000408025
93: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 8 1 -32766 0 0
93: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 13
93: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 8 1 -32766 0 0
93: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 13
93: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 8 1 -32766 0 0
93: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 13
93: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 8 1 -32766 0 0
93: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 13
93: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 8 1 -32766 0 0
93: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 13
93: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 8 1 -32766 0 0
93: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 13
93: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 8 1 -32766 0 0
93: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 13
93: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 8 1 -32766 0 0
93: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 13
93: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 8 1 -32766 0 0
93: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 13
93: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 8 1 -32766 0 0
93: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 10 13
93: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 8 1 -32766 0 0
93: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 11 13
93: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 8 1 -32766 0 0
93: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 12 13
93: communicator.hh::sendReormation_.size(), recvRequests, &finished, &status); 18 24
101: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 6 1 -32766 0 0
101: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 19 24
101: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 6 1 -32766 0 0
101: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 20 24
101: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 6 1 -32766 0 0
101: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 21 24
101: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 6 1 -32766 0 0
101: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 22 24
101: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 6 1 -32766 0 0
101: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 23 24
101: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 6 1 -32766 0 0
Newton::solveLinearSystem : Caught exception from the linear solver when getting converged: "BCRSMatrixError [operator++:/home/m.giraud/DUMUXexud/dune-istl/dune/istl/bcrsmatrix.hh:940]: matrix already built up"
101 converged value: 0 121 1 let s go get convergedRemote
101 to  comm_.min(converged) 
101 did  comm_.min(converged) convergedRemote: 0
101 final convergedRemote: 0
101 about to throw Linear solver did not converge
101 Newton::solveLinearSystem : Caught exception: "NumericalProblem [solveLinearSystem:/home/m.giraud/DUMUXexud/dumux/dumux/nonlinear/newtonsolver.hh:381]: Linear solver did not converge when getting convergedRemote "
101 Newton: Caught exception: "NumericalProblem [solveLinearSystem:/home/m.giraud/DUMUXexud/dumux/dumux/nonlinear/newtonsolver.hh:381]: Linear solver did not converge"
101 Newton solver did not converge with dt = 0.0495 seconds. Retrying with time step of 0.02475 seconds
101 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
s); 7 9
39: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 7 1 -32766 0 0
39: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 9
39: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 7 1 -32766 0 0
Newton::solveLinearSystem : Caught exception from the linear solver when getting converged: "BCRSMatrixError [operator++:/home/m.giraud/DUMUXexud/dune-istl/dune/istl/bcrsmatrix.hh:940]: matrix already built up"
39 converged value: 0 121 1 let s go get convergedRemote
39 to  comm_.min(converged) 
39 did  comm_.min(converged) convergedRemote: 0
39 final convergedRemote: 0
39 about to throw Linear solver did not converge
39 Newton::solveLinearSystem : Caught exception: "NumericalProblem [solveLinearSystem:/home/m.giraud/DUMUXexud/dumux/dumux/nonlinear/newtonsolver.hh:381]: Linear solver did not converge when getting convergedRemote "
39 Newton: Caught exception: "NumericalProblem [solveLinearSystem:/home/m.giraud/DUMUXexud/dumux/dumux/nonlinear/newtonsolver.hh:381]: Linear solver did not converge"
39 Newton solver did not converge with dt = 0.0495 seconds. Retrying with time step of 0.02475 seconds
39 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
earSystem:/home/m.giraud/DUMUXexud/dumux/dumux/nonlinear/newtonsolver.hh:381]: Linear solver did not converge when getting convergedRemote "
79 Newton: Caught exception: "NumericalProblem [solveLinearSystem:/home/m.giraud/DUMUXexud/dumux/dumux/nonlinear/newtonsolver.hh:381]: Linear solver did not converge"
79 Newton solver did not converge with dt = 0.0495 seconds. Retrying with time step of 0.02475 seconds
79 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
Waitany(messageInformation_.size(), recvRequests, &finished, &status); 18 22
117: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 17 1 -32766 0 0
117: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 19 22
117: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 17 1 -32766 0 0
117: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 20 22
117: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 17 1 -32766 0 0
117: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 21 22
117: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 17 1 -32766 0 0
Newton::solveLinearSystem : Caught exception from the linear solver when getting converged: "BCRSMatrixError [operator++:/home/m.giraud/DUMUXexud/dune-istl/dune/istl/bcrsmatrix.hh:940]: matrix already built up"
117 converged value: 0 121 1 let s go get convergedRemote
117 to  comm_.min(converged) 
117 did  comm_.min(converged) convergedRemote: 0
117 final convergedRemote: 0
117 about to throw Linear solver did not converge
117 Newton::solveLinearSystem : Caught exception: "NumericalProblem [solveLinearSystem:/home/m.giraud/DUMUXexud/dumux/dumux/nonlinear/newtonsolver.hh:381]: Linear solver did not converge when getting convergedRemote "
117 Newton: Caught exception: "NumericalProblem [solveLinearSystem:/home/m.giraud/DUMUXexud/dumux/dumux/nonlinear/newtonsolver.hh:381]: Linear solver did not converge"
117 Newton solver did not converge with dt = 0.0495 seconds. Retrying with time step of 0.02475 seconds
117 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
ed, &status); 18 26
12: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 0 1 -32766 0 0
12: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 19 26
12: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 0 1 -32766 0 0
12: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 20 26
12: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 0 1 -32766 0 0
12: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 21 26
12: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 0 1 -32766 0 0
12: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 22 26
12: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 0 1 -32766 0 0
12: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 23 26
12: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 0 1 -32766 0 0
12: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 24 26
12: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 0 1 -32766 0 0
12: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 25 26
12: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 0 1 -32766 0 0
Newton::solveLinearSystem : Caught exception from the linear solver when getting converged: "BCRSMatrixError [operator++:/home/m.giraud/DUMUXexud/dune-istl/dune/istl/bcrsmatrix.hh:940]: matrix already built up"
12 converged value: 0 121 1 let s go get convergedRemote
12 to  comm_.min(converged) 
12 did  comm_.min(converged) convergedRemote: 0
12 final convergedRemote: 0
12 about to throw Linear solver did not converge
12 Newton::solveLinearSystem : Caught exception: "NumericalProblem [solveLinearSystem:/home/m.giraud/DUMUXexud/dumux/dumux/nonlinear/newtonsolver.hh:381]: Linear solver did not converge when getting convergedRemote "
12 Newton: Caught exception: "NumericalProblem [solveLinearSystem:/home/m.giraud/DUMUXexud/dumux/dumux/nonlinear/newtonsolver.hh:381]: Linear solver did not converge"
12 Newton solver did not converge with dt = 0.0495 seconds. Retrying with time step of 0.02475 seconds
12 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
mm_.min(converged) convergedRemote: 0
27 final convergedRemote: 0
27 about to throw Linear solver did not converge
27 Newton::solveLinearSystem : Caught exception: "NumericalProblem [solveLinearSystem:/home/m.giraud/DUMUXexud/dumux/dumux/nonlinear/newtonsolver.hh:381]: Linear solver did not converge when getting convergedRemote "
27 Newton: Caught exception: "NumericalProblem [solveLinearSystem:/home/m.giraud/DUMUXexud/dumux/dumux/nonlinear/newtonsolver.hh:381]: Linear solver did not converge"
27 Newton solver did not converge with dt = 0.0495 seconds. Retrying with time step of 0.02475 seconds
27 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
rged) 
75 did  comm_.min(converged) convergedRemote: 0
75 final convergedRemote: 0
75 about to throw Linear solver did not converge
75 Newton::solveLinearSystem : Caught exception: "NumericalProblem [solveLinearSystem:/home/m.giraud/DUMUXexud/dumux/dumux/nonlinear/newtonsolver.hh:381]: Linear solver did not converge when getting convergedRemote "
75 Newton: Caught exception: "NumericalProblem [solveLinearSystem:/home/m.giraud/DUMUXexud/dumux/dumux/nonlinear/newtonsolver.hh:381]: Linear solver did not converge"
75 Newton solver did not converge with dt = 0.0495 seconds. Retrying with time step of 0.02475 seconds
75 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
trixError [operator++:/home/m.giraud/DUMUXexud/dune-istl/dune/istl/bcrsmatrix.hh:940]: matrix already built up"
54 converged value: 0 121 1 let s go get convergedRemote
54 to  comm_.min(converged) 
54 did  comm_.min(converged) convergedRemote: 0
54 final convergedRemote: 0
54 about to throw Linear solver did not converge
54 Newton::solveLinearSystem : Caught exception: "NumericalProblem [solveLinearSystem:/home/m.giraud/DUMUXexud/dumux/dumux/nonlinear/newtonsolver.hh:381]: Linear solver did not converge when getting convergedRemote "
54 Newton: Caught exception: "NumericalProblem [solveLinearSystem:/home/m.giraud/DUMUXexud/dumux/dumux/nonlinear/newtonsolver.hh:381]: Linear solver did not converge"
54 Newton solver did not converge with dt = 0.0495 seconds. Retrying with time step of 0.02475 seconds
54 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
final convergedRemote: 0
6 about to throw Linear solver did not converge
6 Newton::solveLinearSystem : Caught exception: "NumericalProblem [solveLinearSystem:/home/m.giraud/DUMUXexud/dumux/dumux/nonlinear/newtonsolver.hh:381]: Linear solver did not converge when getting convergedRemote "
6 Newton: Caught exception: "NumericalProblem [solveLinearSystem:/home/m.giraud/DUMUXexud/dumux/dumux/nonlinear/newtonsolver.hh:381]: Linear solver did not converge"
6 Newton solver did not converge with dt = 0.0495 seconds. Retrying with time step of 0.02475 seconds
6 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
nearSystem:/home/m.giraud/DUMUXexud/dumux/dumux/nonlinear/newtonsolver.hh:381]: Linear solver did not converge when getting convergedRemote "
32 Newton: Caught exception: "NumericalProblem [solveLinearSystem:/home/m.giraud/DUMUXexud/dumux/dumux/nonlinear/newtonsolver.hh:381]: Linear solver did not converge"
32 Newton solver did not converge with dt = 0.0495 seconds. Retrying with time step of 0.02475 seconds
32 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
endRecv,MPI_Testany -32766 0 -32766 0 0 0.000291073
89: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000293903
89: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000297113
89: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000299933
89: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000303343
89: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000306433
89: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000309384
89: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000312334
89: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000355384
89: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000359064
89: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000362474
89: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000365864
89: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000369254
89: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000372824
89: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000376624
89: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000380204
89: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000383634
89: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000387824
89: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000391255
89: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000394695
89: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000398405
89: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000401845
89: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000405245
89: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000410235
89: communicator.hh::sendRecv,MPI_Testany 5 1 -32766 0 0 0.000413765
89: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 5 1 -32766 0 0
89: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 11
89: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 5 1 -32766 0 0
89: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 11
89: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 5 1 -32766 0 0
89: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 11
89: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 5 1 -32766 0 0
89: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 11
89: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 5 1 -32766 0 0
89: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 11
89: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 5 1 -32766 0 0
89: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 11
89: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 5 1 -32766 0 0
89: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 11
89: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 5 1 -32766 0 0
89: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 11
89: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 5 1 -32766 0 0
89: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 11
89: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 5 1 -32766 0 0
89: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 10 11
89: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 5 1 -32766 0 0
Newton::solveLinearSystem : Caught exception from the linear solver when getting converged: "BCRSMatrixError [operator++:/h92 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
92: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 16
92: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 4.39e-06
92: communicator.hh::sendRecv,MPI_Testany 8 1 -32766 0 0 9.94e-06
92: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 8 1 -32766 0 0
92: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 16
92: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 8 1 -32766 0 0
92: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 16
92: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 8 1 -32766 0 0
92: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 16
92: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 8 1 -32766 0 0
92: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 16
92: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 8 1 -32766 0 0
92: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 16
92: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 8 1 -32766 0 0
92: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 16
92: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 8 1 -32766 0 0
92: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 16
92: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 8 1 -32766 0 0
92: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 16
92: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 8 1 -32766 0 0
92: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 16
92: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 8 1 -32766 0 0
92: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 10 16
92: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 8 1 -32766 0 0
92: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 11 16
92: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 8 1 -32766 0 0
92: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 12 16
92: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 8 1 -32766 0 0
92: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 13 16
92: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 8 1 -32766 0 0
92: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 14 16
92: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 8 1 -32766 0 0
92: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 15 16
92: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 8 1 -32766 0 0
Newton::solveLinearSystem : Caught exception from the linear solver when getting converged: "BCRSMatrixError [operator++:/home/m.giraud/DUMUXexud/dune-istl/dune/istl/bcrsmatrix.hh:940]: matrix already built up"
92 converged value: 0 121 1 let s go get convergedRemote
92 to  comm_.min(converged) 
92 did  comm_.min(converged) convergedRemote: 0
92 final convergedRemote: 0
92 about to throw Linear solver did not converge
92 Newton::solveLinearSystem : Caught exception: "NumericalProblem [solveLinearSystem:/home/m2 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 8
2: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 2.84e-06
2: communicator.hh::sendRecv,MPI_Testany 0 1 -32766 0 0 8.05e-06
2: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 0 1 -32766 0 0
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 8
2: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 0 1 -32766 0 0
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 8
2: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 0 1 -32766 0 0
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 8
2: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 0 1 -32766 0 0
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 8
2: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 0 1 -32766 0 0
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 8
2: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 0 1 -32766 0 0
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 8
2: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 0 1 -32766 0 0
2: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 8
2: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 0 1 -32766 0 0
Newton::solveLinearSystem : Caught exception from the linear solver when getting converged: "BCRSMatrixError [operator++:/home/m.giraud/DUMUXexud/dune-istl/dune/istl/bcrsmatrix.hh:940]: matrix already built up"
2 converged value: 0 121 1 let s go get convergedRemote
2 to  comm_.min(converged) 
2 did  comm_.min(converged) convergedRemote: 0
2 final convergedRemote: 0
2 about to throw Linear solver did not converge
2 Newton::solveLinearSystem : Caught exception: "NumericalProblem [solveLinearSystem:/home/m.giraud/DUMUXexud/dumux/dumux/nonlinear/newtonsolver.hh:381]: Linear solver did not converge when getting convergedRemote "
2 Newton: Caught exception: "NumericalProblem [solveLinearSystem:/home/m.giraud/DUMUXexud/dumux/dumux/nonlinear/newtonsolver.hh:381]: Linear solver did not converge"
2 Newton solver did not converge with dt = 0.0495 seconds. Retrying with time step of 0.02475 seconds
2 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
Requests, &finished, &status); 18 31
56: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 11 1 -32766 0 0
56: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 19 31
56: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 11 1 -32766 0 0
56: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 20 31
56: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 11 1 -32766 0 0
56: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 21 31
56: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 11 1 -32766 0 0
56: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 22 31
56: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 11 1 -32766 0 0
56: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 23 31
56: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 11 1 -32766 0 0
56: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 24 31
56: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 11 1 -32766 0 0
56: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 25 31
56: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 11 1 -32766 0 0
56: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 26 31
56: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 11 1 -32766 0 0
56: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 27 31
56: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 11 1 -32766 0 0
56: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 28 31
56: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 11 1 -32766 0 0
56: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 29 31
56: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 11 1 -32766 0 0
56: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 30 31
56: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 11 1 -32766 0 0
Newton::solveLinearSystem : Caught exception from the linear solver when getting converged: "BCRSMatrixError [operator++:/home/m.giraud/DUMUXexud/dune-istl/dune/istl/bcrsmatrix.hh:940]: matrix already built up"
56 converged value: 0 121 1 let s go get convergedRemote
56 to  comm_.min(converged) 
56 did  comm_.min(converged) convergedRemote: 0
56 final convergedRemote: 0
56 about to throw Linear solver did not converge
56 Newton::solveLinearSystem : Caught exception: "NumericalProblem [solveLinearSystem:/home/m.giraud/DUMUXexud/dumux/dumux/nonlinear/newtonsolver.hh:381]: Linear solver did not converge when getting convergedRemote "
56 Newton: Caught exception: "NumericalProblem [solveLinearSystem:/home/m.giraud/DUMUXexud/dumux/dumux/nonlinear/newtonsolver.hh:381]: Linear solver did not converge"
56 Newton solver did not converge with dt = 0.0495 seconds. Retrying with time step of 0.02475 seconds
56 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
107 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
107: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 5
107: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 5e-07
107: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 7.21e-06
107: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 1.091e-05
107: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 1.451e-05
107: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 1.8041e-05
107: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 2.1381e-05
107: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 2.4741e-05
107: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 2.8181e-05
107: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 3.1671e-05
107: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 3.4961e-05
107: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 3.8281e-05
107: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 4.1751e-05
107: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 8.0241e-05
107: communicator.hh::sendRecv,MPI_Testany 2 1 -32766 0 0 8.4031e-05
107: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 2 1 -32766 0 0
107: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 5
107: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 2 1 -32766 0 0
107: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 5
107: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 2 1 -32766 0 0
107: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 5
107: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 2 1 -32766 0 0
107: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 5
107: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 2 1 -32766 0 0
Newton::solveLinearSystem : Caught exception from the linear solver when getting converged: "BCRSMatrixError [operator++:/home/m.giraud/DUMUXexud/dune-istl/dune/istl/bcrsmatrix.hh:940]: matrix already built up"
107 converged value: 0 121 1 let s go get convergedRemote
107 to  comm_.min(converged) 
107 did  comm_.min(converged) convergedRemote: 0
107 final convergedRemote: 0
107 about to throw Linear solver did not converge
107 Newton::solveLinearSystem : Caught exception: "NumericalProblem [solveLinearSystem:/home/m.giraud/DUMUXexud/dumux/dumux/nonlinear/newtonsolver.hh:381]: Linear solver did not converge when getting convergedRemote "
107 Newton: Caught exception: "NumericalProblem [solveLinearSystem:/home/m.giraud/DUMUXexud/dumux/dumux/nonlinear/newtonsolver.hh:381]: Linear solver did not converge"
107 Newton solver did not converge with dt = 0.0495 seconds. Retrying with time step of 0.02475 seconds
107 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
28 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
28: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 18
28: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 9.52e-06
28: communicator.hh::sendRecv,MPI_Testany 3 1 -32766 0 0 4.5711e-05
28: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 3 1 -32766 0 0
28: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 18
28: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 3 1 -32766 0 0
28: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 18
28: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 3 1 -32766 0 0
28: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 18
28: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 3 1 -32766 0 0
28: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 18
28: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 3 1 -32766 0 0
28: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 18
28: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 3 1 -32766 0 0
28: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 18
28: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 3 1 -32766 0 0
28: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 18
28: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 3 1 -32766 0 0
28: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 18
28: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 3 1 -32766 0 0
28: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 18
28: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 3 1 -32766 0 0
28: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 10 18
28: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 3 1 -32766 0 0
28: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 11 18
28: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 3 1 -32766 0 0
28: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 12 18
28: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 3 1 -32766 0 0
28: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 13 18
28: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 3 1 -32766 0 0
28: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 14 18
28: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 3 1 -32766 0 0
28: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 15 18
28: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 3 1 -32766 0 0
28: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 16 18
28: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 3 1 -32766 0 0
28: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 17 18
28: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 3 1 -32766 0 0
Newton::solveLinearSystem : Caught exception from the linear solver when getting converged: "BCRSMaor.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000596588
116: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000599918
116: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000609428
116: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000618668
116: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.00072995
116: communicator.hh::sendRecv,MPI_Testany 2 1 -32766 0 0 0.000808631
116: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 2 1 -32766 0 0
116: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 7
116: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 2 1 -32766 0 0
116: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 7
116: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 2 1 -32766 0 0
116: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 7
116: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 2 1 -32766 0 0
116: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 7
116: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 2 1 -32766 0 0
116: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 7
116: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 2 1 -32766 0 0
116: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 7
116: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 2 1 -32766 0 0
Newton::solveLinearSystem : Caught exception from the linear solver when getting converged: "BCRSMatrixError [operator++:/home/m.giraud/DUMUXexud/dune-istl/dune/istl/bcrsmatrix.hh:940]: matrix already built up"
116 converged value: 0 121 1 let s go get convergedRemote
116 to  comm_.min(converged) 
116 did  comm_.min(converged) convergedRemote: 0
116 final convergedRemote: 0
116 about to throw Linear solver did not converge
116 Newton::solveLinearSystem : Caught exception: "NumericalProblem [solveLinearSystem:/home/m.giraud/DUMUXexud/dumux/dumux/nonlinear/newtonsolver.hh:381]: Linear solver did not converge when getting convergedRemote "
116 Newton: Caught exception: "NumericalProblem [solveLinearSystem:/home/m.giraud/DUMUXexud/dumux/dumux/nonlinear/newtonsolver.hh:381]: Linear solver did not converge"
116 Newton solver did not converge with dt = 0.0495 seconds. Retrying with time step of 0.02475 seconds
116 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
8 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
8: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 20
8: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 6.62e-06
8: communicator.hh::sendRecv,MPI_Testany 0 1 -32766 0 0 1.252e-05
8: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 0 1 -32766 0 0
8: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 20
8: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 0 1 -32766 0 0
8: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 20
8: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 0 1 -32766 0 0
8: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 20
8: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 0 1 -32766 0 0
8: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 20
8: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 0 1 -32766 0 0
8: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 20
8: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 0 1 -32766 0 0
8: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 20
8: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 0 1 -32766 0 0
8: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 20
8: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 0 1 -32766 0 0
8: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 20
8: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 0 1 -32766 0 0
8: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 20
8: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 0 1 -32766 0 0
8: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 10 20
8: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 0 1 -32766 0 0
8: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 11 20
8: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 0 1 -32766 0 0
8: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 12 20
8: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 0 1 -32766 0 0
8: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 13 20
8: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 0 1 -32766 0 0
8: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 14 20
8: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 0 1 -32766 0 0
8: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 15 20
8: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 0 1 -32766 0 0
8: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 16 20
8: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 0 1 -32766 0 0
8: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 17 20
8: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 0 1 -32766 0 0
8: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 18 20
8: communicator.hh::s86 owneroverlapcopy.hh::copyOwnerToAll (const T& source, T& dest), before communicator.template forward<CopyGatherScatter<T> >(source,dest);
86: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 0 31
86: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 7.69e-06
86: communicator.hh::sendRecv,MPI_Testany 7 1 -32766 0 0 1.333e-05
86: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 7 1 -32766 0 0
86: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 31
86: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 7 1 -32766 0 0
86: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 31
86: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 7 1 -32766 0 0
86: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 31
86: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 7 1 -32766 0 0
86: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 31
86: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 7 1 -32766 0 0
86: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 31
86: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 7 1 -32766 0 0
86: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 31
86: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 7 1 -32766 0 0
86: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 31
86: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 7 1 -32766 0 0
86: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 31
86: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 7 1 -32766 0 0
86: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 31
86: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 7 1 -32766 0 0
86: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 10 31
86: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 7 1 -32766 0 0
86: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 11 31
86: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 7 1 -32766 0 0
86: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 12 31
86: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 7 1 -32766 0 0
86: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 13 31
86: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 7 1 -32766 0 0
86: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 14 31
86: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 7 1 -32766 0 0
86: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 15 31
86: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 7 1 -32766 0 0
86: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 16 31
86: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 7 1 -32766 0 0
86: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 17 31
86: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 7 1 -32766 0 0
86: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finishe MPI_UNDEFINED); 6 1 -32766 0 0
25: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 7
25: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 6 1 -32766 0 0
25: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 7
25: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 6 1 -32766 0 0
25: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 7
25: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 6 1 -32766 0 0
25: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 7
25: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 6 1 -32766 0 0
25: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 7
25: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 6 1 -32766 0 0
Newton::solveLinearSystem : Caught exception from the linear solver when getting converged: "BCRSMatrixError [operator++:/home/m.giraud/DUMUXexud/dune-istl/dune/istl/bcrsmatrix.hh:940]: matrix already built up"
25 converged value: 0 121 1 let s go get convergedRemote
25 to  comm_.min(converged) 
25 did  comm_.min(converged) convergedRemote: 0
25 final convergedRemote: 0
25 about to throw Linear solver did not converge
25 Newton::solveLinearSystem : Caught exception: "NumericalProblem [solveLinearSystem:/home/m.giraud/DUMUXexud/dumux/dumux/nonlinear/newtonsolver.hh:381]: Linear solver did not converge when getting convergedRemote "
25 Newton: Caught exception: "NumericalProblem [solveLinearSystem:/home/m.giraud/DUMUXexud/dumux/dumux/nonlinear/newtonsolver.hh:381]: Linear solver did not converge"
25 Newton solver did not converge with dt = 0.0495 seconds. Retrying with time step of 0.02475 seconds
25 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
ed: "BCRSMatrixError [operator++:/home/m.giraud/DUMUXexud/dune-istl/dune/istl/bcrsmatrix.hh:940]: matrix already built up"
87 converged value: 0 121 1 let s go get convergedRemote
87 to  comm_.min(converged) 
87 did  comm_.min(converged) convergedRemote: 0
87 final convergedRemote: 0
87 about to throw Linear solver did not converge
87 Newton::solveLinearSystem : Caught exception: "NumericalProblem [solveLinearSystem:/home/m.giraud/DUMUXexud/dumux/dumux/nonlinear/newtonsolver.hh:381]: Linear solver did not converge when getting convergedRemote "
87 Newton: Caught exception: "NumericalProblem [solveLinearSystem:/home/m.giraud/DUMUXexud/dumux/dumux/nonlinear/newtonsolver.hh:381]: Linear solver did not converge"
87 Newton solver did not converge with dt = 0.0495 seconds. Retrying with time step of 0.02475 seconds
87 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
cv, before assert(finished != MPI_UNDEFINED); 8 1 -32766 0 0
Newton::solveLinearSystem : Caught exception from the linear solver when getting converged: "BCRSMatrixError [operator++:/home/m.giraud/DUMUXexud/dune-istl/dune/istl/bcrsmatrix.hh:940]: matrix already built up"
93 converged value: 0 121 1 let s go get convergedRemote
93 to  comm_.min(converged) 
93 did  comm_.min(converged) convergedRemote: 0
93 final convergedRemote: 0
93 about to throw Linear solver did not converge
93 Newton::solveLinearSystem : Caught exception: "NumericalProblem [solveLinearSystem:/home/m.giraud/DUMUXexud/dumux/dumux/nonlinear/newtonsolver.hh:381]: Linear solver did not converge when getting convergedRemote "
93 Newton: Caught exception: "NumericalProblem [solveLinearSystem:/home/m.giraud/DUMUXexud/dumux/dumux/nonlinear/newtonsolver.hh:381]: Linear solver did not converge"
93 Newton solver did not converge with dt = 0.0495 seconds. Retrying with time step of 0.02475 seconds
93 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
d, &status); 18 31
86: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 7 1 -32766 0 0
86: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 19 31
86: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 7 1 -32766 0 0
86: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 20 31
86: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 7 1 -32766 0 0
86: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 21 31
86: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 7 1 -32766 0 0
86: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 22 31
86: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 7 1 -32766 0 0
86: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 23 31
86: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 7 1 -32766 0 0
86: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 24 31
86: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 7 1 -32766 0 0
86: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 25 31
86: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 7 1 -32766 0 0
86: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 26 31
86: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 7 1 -32766 0 0
86: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 27 31
86: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 7 1 -32766 0 0
86: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 28 31
86: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 7 1 -32766 0 0
86: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 29 31
86: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 7 1 -32766 0 0
86: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 30 31
86: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 7 1 -32766 0 0
Newton::solveLinearSystem : Caught exception from the linear solver when getting converged: "BCRSMatrixError [operator++:/home/m.giraud/DUMUXexud/dune-istl/dune/istl/bcrsmatrix.hh:940]: matrix already built up"
86 converged value: 0 121 1 let s go get convergedRemote
86 to  comm_.min(converged) 
86 did  comm_.min(converged) convergedRemote: 0
86 final convergedRemote: 0
86 about to throw Linear solver did not converge
86 Newton::solveLinearSystem : Caught exception: "NumericalProblem [solveLinearSystem:/home/m.giraud/DUMUXexud/dumux/dumux/nonlinear/newtonsolver.hh:381]: Linear solver did not converge when getting convergedRemote "
86 Newton: Caught exception: "NumericalProblem [solveLinearSystem:/home/m.giraud/DUMUXexud/dumux/dumux/nonlinear/newtonsolver.hh:381]: Linear solver did not converge"
86 Newton solver did not converge with dt = 0.0495 seconds. Retrying with time step of 0.02475 seconds
86 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
ome/m.giraud/DUMUXexud/dune-istl/dune/istl/bcrsmatrix.hh:940]: matrix already built up"
89 converged value: 0 121 1 let s go get convergedRemote
89 to  comm_.min(converged) 
89 did  comm_.min(converged) convergedRemote: 0
89 final convergedRemote: 0
89 about to throw Linear solver did not converge
89 Newton::solveLinearSystem : Caught exception: "NumericalProblem [solveLinearSystem:/home/m.giraud/DUMUXexud/dumux/dumux/nonlinear/newtonsolver.hh:381]: Linear solver did not converge when getting convergedRemote "
89 Newton: Caught exception: "NumericalProblem [solveLinearSystem:/home/m.giraud/DUMUXexud/dumux/dumux/nonlinear/newtonsolver.hh:381]: Linear solver did not converge"
89 Newton solver did not converge with dt = 0.0495 seconds. Retrying with time step of 0.02475 seconds
89 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
.giraud/DUMUXexud/dumux/dumux/nonlinear/newtonsolver.hh:381]: Linear solver did not converge when getting convergedRemote "
92 Newton: Caught exception: "NumericalProblem [solveLinearSystem:/home/m.giraud/DUMUXexud/dumux/dumux/nonlinear/newtonsolver.hh:381]: Linear solver did not converge"
92 Newton solver did not converge with dt = 0.0495 seconds. Retrying with time step of 0.02475 seconds
92 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
trixError [operator++:/home/m.giraud/DUMUXexud/dune-istl/dune/istl/bcrsmatrix.hh:940]: matrix already built up"
28 converged value: 0 121 1 let s go get convergedRemote
28 to  comm_.min(converged) 
28 did  comm_.min(converged) convergedRemote: 0
28 final convergedRemote: 0
28 about to throw Linear solver did not converge
28 Newton::solveLinearSystem : Caught exception: "NumericalProblem [solveLinearSystem:/home/m.giraud/DUMUXexud/dumux/dumux/nonlinear/newtonsolver.hh:381]: Linear solver did not converge when getting convergedRemote "
28 Newton: Caught exception: "NumericalProblem [solveLinearSystem:/home/m.giraud/DUMUXexud/dumux/dumux/nonlinear/newtonsolver.hh:381]: Linear solver did not converge"
28 Newton solver did not converge with dt = 0.0495 seconds. Retrying with time step of 0.02475 seconds
28 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
endRecv, before assert(finished != MPI_UNDEFINED); 0 1 -32766 0 0
8: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 19 20
8: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 0 1 -32766 0 0
Newton::solveLinearSystem : Caught exception from the linear solver when getting converged: "BCRSMatrixError [operator++:/home/m.giraud/DUMUXexud/dune-istl/dune/istl/bcrsmatrix.hh:940]: matrix already built up"
8 converged value: 0 121 1 let s go get convergedRemote
8 to  comm_.min(converged) 
8 did  comm_.min(converged) convergedRemote: 0
8 final convergedRemote: 0
8 about to throw Linear solver did not converge
8 Newton::solveLinearSystem : Caught exception: "NumericalProblem [solveLinearSystem:/home/m.giraud/DUMUXexud/dumux/dumux/nonlinear/newtonsolver.hh:381]: Linear solver did not converge when getting convergedRemote "
8 Newton: Caught exception: "NumericalProblem [solveLinearSystem:/home/m.giraud/DUMUXexud/dumux/dumux/nonlinear/newtonsolver.hh:381]: Linear solver did not converge"
8 Newton solver did not converge with dt = 0.0495 seconds. Retrying with time step of 0.02475 seconds
8 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
= dS/dt + div F - q;   M = grad r
ion from the linear solver when getting converged: "BCRSMatrixError [operator++:/home/m.giraud/DUMUXexud/dune-istl/dune/istl/bcrsmatrix.hh:940]: matrix already built up"
113 converged value: 0 121 1 let s go get convergedRemote
113 to  comm_.min(converged) 
113 did  comm_.min(converged) convergedRemote: 0
113 final convergedRemote: 0
113 about to throw Linear solver did not converge
113 Newton::solveLinearSystem : Caught exception: "NumericalProblem [solveLinearSystem:/home/m.giraud/DUMUXexud/dumux/dumux/nonlinear/newtonsolver.hh:381]: Linear solver did not converge when getting convergedRemote "
113 Newton: Caught exception: "NumericalProblem [solveLinearSystem:/home/m.giraud/DUMUXexud/dumux/dumux/nonlinear/newtonsolver.hh:381]: Linear solver did not converge"
113 Newton solver did not converge with dt = 0.0495 seconds. Retrying with time step of 0.02475 seconds
113 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
nicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000302973
23: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000305783
23: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000308583
23: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000344364
23: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000347424
23: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000350064
23: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000352704
23: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000355494
23: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000358654
23: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000361554
23: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000364474
23: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000367304
23: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000370184
23: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000373124
23: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000376434
23: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000379704
23: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000384464
23: communicator.hh::sendRecv,MPI_Testany 10 1 -32766 0 0 0.000387944
23: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 10 1 -32766 0 0
23: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 11
23: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 10 1 -32766 0 0
23: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 11
23: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 10 1 -32766 0 0
23: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 11
23: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 10 1 -32766 0 0
23: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 11
23: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 10 1 -32766 0 0
23: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 11
23: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 10 1 -32766 0 0
23: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 11
23: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 10 1 -32766 0 0
23: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 11
23: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 10 1 -32766 0 0
23: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 11
23: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 10 1 -32766 0 0
23: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 11
23: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 10 1 -32766 0 0
23: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 10 11
23: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 10 1 -32766 0 0
Newton::solveLinearSystem : Caught exception from the linear solver when getting converged: "BCRSMatrixError [operator++:/home/m.giraud/DUMUXexud/dune-istl/dune/istl/bcrsmatrix.hh:940]: matrix already built up"
23 converged value: 0 121 1 let s go get convergedRemote
23 to  comm_.min(converged) 
23 did  comm_.min(converged) convergedRemote: 0
23 final convergedRemote: 0
23 about to throw Linear solver did not converge
23 Newton::solveLinearSystem : Caught exception: "NumericalProblem [solveLinearSystem:/home/m.giraud/DUMUXexud/dumux/dumux/nonlinear/newtonsolver.hh:381]: Linear solver did not converge when getecv,MPI_Testany -32766 0 -32766 0 0 0.000288243
73: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000291523
73: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000294993
73: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000298394
73: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000301784
73: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000305164
73: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000308554
73: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000311954
73: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000317004
73: communicator.hh::sendRecv,MPI_Testany 10 1 -32766 0 0 0.000320534
73: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 10 1 -32766 0 0
73: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 11
73: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 10 1 -32766 0 0
73: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 11
73: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 10 1 -32766 0 0
73: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 11
73: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 10 1 -32766 0 0
73: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 11
73: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 10 1 -32766 0 0
73: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 11
73: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 10 1 -32766 0 0
73: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 11
73: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 10 1 -32766 0 0
73: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 11
73: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 10 1 -32766 0 0
73: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 11
73: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 10 1 -32766 0 0
73: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 11
73: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 10 1 -32766 0 0
73: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 10 11
73: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 10 1 -32766 0 0
Newton::solveLinearSystem : Caught exception from the linear solver when getting converged: "BCRSMatrixError [operator++:/home/m.giraud/DUMUXexud/dune-istl/dune/istl/bcrsmatrix.hh:940]: matrix already built up"
73 converged value: 0 121 1 let s go get convergedRemote
73 to  comm_.min(converged) 
73 did  comm_.min(converged) convergedRemote: 0
73 final convergedRemote: 0
73 about to throw Linear solver did not converge
73 Newton::solveLinearSystem : Caught exception: "NumericalProblem [solveLinearSystem:/home/m.giraud/DUMUXexud/dumux/dumux/nonlinear/newtonsolver.hh:381]: Linear solver did not converge when getting convergedRemote "
73 Newton: Caught exception: "NumericalProblem [solveLinearSystem:/home/m.giraud/DUMUXexud/dumux/dumux/nonlinear/newtonsolver.hh:381]: Linear solver did not converge"
73 Newton solver did not converge with dt = 0.0495 seconds. Retrying with time step of 0.02475 seconds
73 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000284804
63: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000287394
63: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000290704
63: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000294094
63: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000297474
63: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000300844
63: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000304084
63: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000307364
63: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000310754
63: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000313994
63: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000317324
63: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000320554
63: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000323784
63: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000327274
63: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000365145
63: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000368765
63: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000372075
63: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000375515
63: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000378815
63: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000382085
63: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000386635
63: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000390025
63: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000393415
63: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000396865
63: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000400165
63: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000403485
63: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000407205
63: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000410875
63: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000414206
63: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000417546
63: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000420886
63: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000424206
63: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000452036
63: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000455646
63: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000832681
63: communicator.hh::sendRecv,MPI_Testany 1 1 -32766 0 0 0.00109293
63: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 1 1 -32766 0 0
63: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 9
63: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 1 1 -32766 0 0
63: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 9
63: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 1 1 -32766 0 0
63: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 9
63: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 1 1 -32766 0 0
63: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 9
63: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 1 1 -32766 0 0
63: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 9
63: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 1 1 -32766 0 0
63: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 9
63: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 1 1 -32766 0 0
63: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 9
63: communicator.hh:: = dS/dt + div F - q;   M = grad r
hh::sendRecv, before assert(finished != MPI_UNDEFINED); 6 1 -32766 0 0
31: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 10
31: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 6 1 -32766 0 0
31: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 10
31: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 6 1 -32766 0 0
31: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 10
31: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 6 1 -32766 0 0
31: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 10
31: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 6 1 -32766 0 0
31: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 10
31: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 6 1 -32766 0 0
31: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 10
31: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 6 1 -32766 0 0
31: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 10
31: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 6 1 -32766 0 0
31: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 10
31: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 6 1 -32766 0 0
31: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 10
31: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 6 1 -32766 0 0
Newton::solveLinearSystem : Caught exception from the linear solver when getting converged: "BCRSMatrixError [operator++:/home/m.giraud/DUMUXexud/dune-istl/dune/istl/bcrsmatrix.hh:940]: matrix already built up"
31 converged value: 0 121 1 let s go get convergedRemote
31 to  comm_.min(converged) 
31 did  comm_.min(converged) convergedRemote: 0
31 final convergedRemote: 0
31 about to throw Linear solver did not converge
31 Newton::solveLinearSystem : Caught exception: "NumericalProblem [solveLinearSystem:/home/m.giraud/DUMUXexud/dumux/dumux/nonlinear/newtonsolver.hh:381]: Linear solver did not converge when getting convergedRemote "
31 Newton: Caught exception: "NumericalProblem [solveLinearSystem:/home/m.giraud/DUMUXexud/dumux/dumux/nonlinear/newtonsolver.hh:381]: Linear solver did not converge"
31 Newton solver did not converge with dt = 0.0495 seconds. Retrying with time step of 0.02475 seconds
31 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
mm_.min(converged) convergedRemote: 0
11 final convergedRemote: 0
11 about to throw Linear solver did not converge
11 Newton::solveLinearSystem : Caught exception: "NumericalProblem [solveLinearSystem:/home/m.giraud/DUMUXexud/dumux/dumux/nonlinear/newtonsolver.hh:381]: Linear solver did not converge when getting convergedRemote "
11 Newton: Caught exception: "NumericalProblem [solveLinearSystem:/home/m.giraud/DUMUXexud/dumux/dumux/nonlinear/newtonsolver.hh:381]: Linear solver did not converge"
11 Newton solver did not converge with dt = 0.0495 seconds. Retrying with time step of 0.02475 seconds
11 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
ed, &status); 18 20
22: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 8 1 -32766 0 0
22: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 19 20
22: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 8 1 -32766 0 0
Newton::solveLinearSystem : Caught exception from the linear solver when getting converged: "BCRSMatrixError [operator++:/home/m.giraud/DUMUXexud/dune-istl/dune/istl/bcrsmatrix.hh:940]: matrix already built up"
22 converged value: 0 121 1 let s go get convergedRemote
22 to  comm_.min(converged) 
22 did  comm_.min(converged) convergedRemote: 0
22 final convergedRemote: 0
22 about to throw Linear solver did not converge
22 Newton::solveLinearSystem : Caught exception: "NumericalProblem [solveLinearSystem:/home/m.giraud/DUMUXexud/dumux/dumux/nonlinear/newtonsolver.hh:381]: Linear solver did not converge when getting convergedRemote "
22 Newton: Caught exception: "NumericalProblem [solveLinearSystem:/home/m.giraud/DUMUXexud/dumux/dumux/nonlinear/newtonsolver.hh:381]: Linear solver did not converge"
22 Newton solver did not converge with dt = 0.0495 seconds. Retrying with time step of 0.02475 seconds
22 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
53 converged value: 0 121 1 let s go get convergedRemote
53 to  comm_.min(converged) 
53 did  comm_.min(converged) convergedRemote: 0
53 final convergedRemote: 0
53 about to throw Linear solver did not converge
53 Newton::solveLinearSystem : Caught exception: "NumericalProblem [solveLinearSystem:/home/m.giraud/DUMUXexud/dumux/dumux/nonlinear/newtonsolver.hh:381]: Linear solver did not converge when getting convergedRemote "
53 Newton: Caught exception: "NumericalProblem [solveLinearSystem:/home/m.giraud/DUMUXexud/dumux/dumux/nonlinear/newtonsolver.hh:381]: Linear solver did not converge"
53 Newton solver did not converge with dt = 0.0495 seconds. Retrying with time step of 0.02475 seconds
53 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
"BCRSMatrixError [operator++:/home/m.giraud/DUMUXexud/dune-istl/dune/istl/bcrsmatrix.hh:940]: matrix already built up"
44 converged value: 0 121 1 let s go get convergedRemote
44 to  comm_.min(converged) 
44 did  comm_.min(converged) convergedRemote: 0
44 final convergedRemote: 0
44 about to throw Linear solver did not converge
44 Newton::solveLinearSystem : Caught exception: "NumericalProblem [solveLinearSystem:/home/m.giraud/DUMUXexud/dumux/dumux/nonlinear/newtonsolver.hh:381]: Linear solver did not converge when getting convergedRemote "
44 Newton: Caught exception: "NumericalProblem [solveLinearSystem:/home/m.giraud/DUMUXexud/dumux/dumux/nonlinear/newtonsolver.hh:381]: Linear solver did not converge"
44 Newton solver did not converge with dt = 0.0495 seconds. Retrying with time step of 0.02475 seconds
44 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
onvergedRemote
9 to  comm_.min(converged) 
9 did  comm_.min(converged) convergedRemote: 0
9 final convergedRemote: 0
9 about to throw Linear solver did not converge
9 Newton::solveLinearSystem : Caught exception: "NumericalProblem [solveLinearSystem:/home/m.giraud/DUMUXexud/dumux/dumux/nonlinear/newtonsolver.hh:381]: Linear solver did not converge when getting convergedRemote "
9 Newton: Caught exception: "NumericalProblem [solveLinearSystem:/home/m.giraud/DUMUXexud/dumux/dumux/nonlinear/newtonsolver.hh:381]: Linear solver did not converge"
9 Newton solver did not converge with dt = 0.0495 seconds. Retrying with time step of 0.02475 seconds
9 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
MPI_Testany -32766 0 -32766 0 0 0.000609038
85: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000612358
85: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000615758
85: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000619188
85: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000622488
85: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000625818
85: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000629648
85: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.000917982
85: communicator.hh::sendRecv,MPI_Testany 0 1 -32766 0 0 0.00116741
85: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 0 1 -32766 0 0
85: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 11
85: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 0 1 -32766 0 0
85: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 11
85: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 0 1 -32766 0 0
85: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 11
85: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 0 1 -32766 0 0
85: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 11
85: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 0 1 -32766 0 0
85: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 5 11
85: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 0 1 -32766 0 0
85: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 6 11
85: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 0 1 -32766 0 0
85: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 7 11
85: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 0 1 -32766 0 0
85: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 11
85: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 0 1 -32766 0 0
85: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 9 11
85: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 0 1 -32766 0 0
85: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 10 11
85: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 0 1 -32766 0 0
Newton::solveLinearSystem : Caught exception from the linear solver when getting converged: "BCRSMatrixError [operator++:/home/m.giraud/DUMUXexud/dune-istl/dune/istl/bcrsmatrix.hh:940]: matrix already built up"
85 converged value: 0 121 1 let s go get convergedRemote
85 to  comm_.min(converged) 
85 did  comm_.min(converged) convergedRemote: 0
85 final convergedRemote: 0
85 about to throw Linear solver did not converge
85 Newton::solveLinearSystem : Caught exception: "NumericalProblem [solveLinearSystem:/home/m.giraud/DUMUXexud/dumux/dumux/nonlinear/newtonsolver.hh:381]: Linear solver did not converge when getting convergedRemote "
85 Newton: Caught exception: "NumericalProblem [solveLinearSystem:/home/m.giraud/DUMUXexud/dumux/dumux/nonlinear/newtonsolver.hh:381]: Linear solver did not converge"
85 Newton solver did not converge with dt = 0.0495 seconds. Retrying with time step of 0.02475 seconds
85 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
convergedRemote: 0
95 about to throw Linear solver did not converge
95 Newton::solveLinearSystem : Caught exception: "NumericalProblem [solveLinearSystem:/home/m.giraud/DUMUXexud/dumux/dumux/nonlinear/newtonsolver.hh:381]: Linear solver did not converge when getting convergedRemote "
95 Newton: Caught exception: "NumericalProblem [solveLinearSystem:/home/m.giraud/DUMUXexud/dumux/dumux/nonlinear/newtonsolver.hh:381]: Linear solver did not converge"
95 Newton solver did not converge with dt = 0.0495 seconds. Retrying with time step of 0.02475 seconds
95 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
v,MPI_Testany -32766 0 -32766 0 0 0.00184825
1: communicator.hh::sendRecv,MPI_Testany -32766 0 -32766 0 0 0.00188767
1: communicator.hh::sendRecv,MPI_Testany 4 1 -32766 0 0 0.00189111
1: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 4 1 -32766 0 0
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 1 5
1: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 4 1 -32766 0 0
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 2 5
1: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 4 1 -32766 0 0
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 3 5
1: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 4 1 -32766 0 0
1: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 4 5
1: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 4 1 -32766 0 0
Newton::solveLinearSystem : Caught exception from the linear solver when getting converged: "BCRSMatrixError [operator++:/home/m.giraud/DUMUXexud/dune-istl/dune/istl/bcrsmatrix.hh:940]: matrix already built up"
1 converged value: 0 121 1 let s go get convergedRemote
1 to  comm_.min(converged) 
1 did  comm_.min(converged) convergedRemote: 0
1 final convergedRemote: 0
1 about to throw Linear solver did not converge
1 Newton::solveLinearSystem : Caught exception: "NumericalProblem [solveLinearSystem:/home/m.giraud/DUMUXexud/dumux/dumux/nonlinear/newtonsolver.hh:381]: Linear solver did not converge when getting convergedRemote "
1 Newton: Caught exception: "NumericalProblem [solveLinearSystem:/home/m.giraud/DUMUXexud/dumux/dumux/nonlinear/newtonsolver.hh:381]: Linear solver did not converge"
1 Newton solver did not converge with dt = 0.0495 seconds. Retrying with time step of 0.02475 seconds
1 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
ting convergedRemote "
23 Newton: Caught exception: "NumericalProblem [solveLinearSystem:/home/m.giraud/DUMUXexud/dumux/dumux/nonlinear/newtonsolver.hh:381]: Linear solver did not converge"
23 Newton solver did not converge with dt = 0.0495 seconds. Retrying with time step of 0.02475 seconds
23 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
sendRecv, before assert(finished != MPI_UNDEFINED); 1 1 -32766 0 0
63: communicator.hh::sendRecv, before MPI_Waitany(messageInformation_.size(), recvRequests, &finished, &status); 8 9
63: communicator.hh::sendRecv, before assert(finished != MPI_UNDEFINED); 1 1 -32766 0 0
Newton::solveLinearSystem : Caught exception from the linear solver when getting converged: "BCRSMatrixError [operator++:/home/m.giraud/DUMUXexud/dune-istl/dune/istl/bcrsmatrix.hh:940]: matrix already built up"
63 converged value: 0 121 1 let s go get convergedRemote
63 to  comm_.min(converged) 
63 did  comm_.min(converged) convergedRemote: 0
63 final convergedRemote: 0
63 about to throw Linear solver did not converge
63 Newton::solveLinearSystem : Caught exception: "NumericalProblem [solveLinearSystem:/home/m.giraud/DUMUXexud/dumux/dumux/nonlinear/newtonsolver.hh:381]: Linear solver did not converge when getting convergedRemote "
63 Newton: Caught exception: "NumericalProblem [solveLinearSystem:/home/m.giraud/DUMUXexud/dumux/dumux/nonlinear/newtonsolver.hh:381]: Linear solver did not converge"
63 Newton solver did not converge with dt = 0.0495 seconds. Retrying with time step of 0.02475 seconds
63 Assemble: r(x^k) = dS/dt + div F - q;   M = grad r
--------------------------------------------------------------------------
Primary job  terminated normally, but 1 process returned
a non-zero exit code. Per user-direction, the job has been aborted.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
mpirun noticed that process rank 30 with PID 2789481 on node node03 exited on signal 11 (Segmentation fault).
--------------------------------------------------------------------------
