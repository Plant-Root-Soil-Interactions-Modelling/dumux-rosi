{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d64aee40-f73c-4ec0-ac21-f29c377deb10",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "pathresults = \"../../results/\"\n",
    "#pathresults = \"/DUMUXDune27/DUMUX/dumux-rosi/python/paperSc/results/\"\n",
    "data_file_delimiter = \",\"\n",
    "import re\n",
    "#countryRoad = r'testFpit/none_1476_{}_10to14_20mn_0s_10/'\n",
    "countryRoad = r'suc2cExudTip/none_1476_{}_10to14_20mn_0s_10/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "98ffa35c-a38f-45a9-a077-737e302404b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " 0\n"
     ]
    }
   ],
   "source": [
    "def getValT(mypath, filename, header=\"infer\",names = None):\n",
    "    fullpath = pathresults +mypath+filename\n",
    "    if names is None:\n",
    "        cst = pd.read_csv(fullpath,delimiter=data_file_delimiter, header = header).dropna(how= \"all\", axis=1)\n",
    "    else:\n",
    "        cst = pd.read_csv(fullpath,delimiter=data_file_delimiter, header = header, names = names).dropna(how= \"all\", axis=1)\n",
    "    return cst\n",
    "\n",
    "exceptPset = []\n",
    "exceptPsetTime = []\n",
    "for i in range(99):\n",
    "    # path2file = pathresults + 'param99none'+str(i)+'0dx_2dumux_10c_10.0to11.0_20mn_0s_192_100/'\n",
    "    path2file = countryRoad.format(i)#'newMucil/none_1476_'+str(i)+'_10to14_20mn_0s_10/'\n",
    "    #'none_55_'+str(i)+'_10to11_20mn_0s_5/'# 'fixeUpdatenone'+str(i)+'0dx_2dumux_10c_10.0to25.0_20mn_0s_192_100/'\n",
    "    tt = getValT(path2file, \"time.txt\",names= ['time','Qlight'])\n",
    "    time = tt.iloc[:,0]\n",
    "    if max(time) < 14: \n",
    "        exceptPset.append(i)\n",
    "        exceptPsetTime.append(max(time))\n",
    "        print(i, (int((max(time)-10.)*24*10)/10), end =\"h, \")\n",
    "print('\\n',len(exceptPset))#, (int((min(exceptPsetTime)-10.)*24*10)/10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "23bf9002-f55d-48bf-80d9-137cbf405b5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getVal(mypath, filename, header=\"infer\",names = None):\n",
    "    fullpath = pathresults +mypath+filename\n",
    "    if names is None:\n",
    "        cst = pd.read_csv(fullpath,delimiter=data_file_delimiter, header = header).dropna(how= \"all\", axis=1)\n",
    "    else:\n",
    "        cst = pd.read_csv(fullpath,delimiter=data_file_delimiter, header = header, names = names).dropna(how= \"all\", axis=1)\n",
    "    return cst\n",
    "def get_last_line_as_numpy_array(file_path, dtype = float):\n",
    "    with open(pathresults +file_path, 'r') as file:\n",
    "        # Read all lines\n",
    "        lines = file.readlines()\n",
    "\n",
    "        # Check if there are any lines in the file\n",
    "        if not lines:\n",
    "            raise ValueError(\"The file is empty\")\n",
    "\n",
    "        # Get the last line\n",
    "        last_line = lines[-1].strip()\n",
    "\n",
    "        # Split the last line by commas and convert to NumPy array\n",
    "        array_from_last_line = np.array(last_line.split(','), dtype=dtype)\n",
    "\n",
    "        return array_from_last_line\n",
    "\n",
    "def list_files_with_prefix(folder_path, prefix):\n",
    "    file_names = []\n",
    "    for file_name in os.listdir(pathresults+folder_path):\n",
    "        if file_name.startswith(prefix):\n",
    "            file_names.append(file_name)\n",
    "    return file_names\n",
    "def getCylIdx(folder_path):\n",
    "    namelist = list_files_with_prefix(folder_path+\"cyl_val/\", \"Cyl_watercontent_\")\n",
    "    idxlist = np.full(len(namelist),-1,dtype = int)\n",
    "    for nn, nl in enumerate(namelist):\n",
    "        match = re.search(r'\\d+', nl)\n",
    "        number_str = match.group()\n",
    "        number_int = int(number_str)\n",
    "        idxlist[nn] = number_int\n",
    "    return idxlist\n",
    "# cylinder max konz (per cell)\n",
    "def getData_(cid,gId,path2file):\n",
    "    fileName = \"cyl_val/Cyl_cellVol_\"+str(gId)+\".txt\"\n",
    "    cVol = get_last_line_as_numpy_array(path2file+fileName, dtype = float)\n",
    "    if cid == volIdx:\n",
    "        return sum(cVol)\n",
    "    elif cid <= 9:\n",
    "        if cid <= 2:# == 0:\n",
    "            fileName = \"cyl_val/Cyl_watercontent_\"+str(gId)+\".txt\"\n",
    "            theta = get_last_line_as_numpy_array(path2file+fileName, dtype = float)\n",
    "            cVol *= theta#cm3 scv to cm3 water\n",
    "            if cid == 0:\n",
    "                return sum(cVol)\n",
    "        fileName = \"cyl_val/Cyl_content\"+str(cid)+\"_\"+str(gId)+\".txt\"\n",
    "        Q1 = get_last_line_as_numpy_array(path2file+fileName, dtype = float)\n",
    "        konz = sum(Q1)/sum(cVol) #mol/cm3 or cm3/cm3\n",
    "        return konz\n",
    "    raise Exception\n",
    "\n",
    "numPset = 99\n",
    "nKonz = 9\n",
    "dictXYZ = {9:'X',10:'Y',11:'Z'}\n",
    "volIdx=12\n",
    "lenIdx=13\n",
    "stIdx=14\n",
    "orgIdId=15\n",
    "relLenId=16\n",
    "\n",
    "nToGet = relLenId+1 #elements + 3d coordinates of y , node+vol+len+st \n",
    "\n",
    "exceptPset =[]\n",
    "allPSets = [ i for i in range(numPset) if i not in exceptPset]\n",
    "\n",
    "def getData():\n",
    "    \n",
    "    GiniAll = [[[] for i in range(numPset)] for ii in range(nToGet)]\n",
    "    for pSet in allPSets:\n",
    "        print(pSet,end =\", \")\n",
    "\n",
    "        #path2file = 'none_55_'+str(pSet)+'_10to11_20mn_0s_5/'\n",
    "        path2file =  countryRoad.format(pSet)#'newMucil/none_1476_'+str(i)+'_10to14_20mn_0s_10/'\n",
    "        \n",
    "        rr = getCylIdx(path2file)\n",
    "        rr.sort()\n",
    "        print('(',len(rr),')',end =\", \")\n",
    "\n",
    "        for cid in range(nKonz):\n",
    "            Ginits = np.array([getData_( cid, gId,path2file) for gId in rr])          \n",
    "            GiniAll[cid][pSet] = Ginits\n",
    "        for cid in range(nKonz,volIdx):\n",
    "            nodes_ = get_last_line_as_numpy_array(path2file+\"nodes_\"+dictXYZ[cid]+\".txt\", dtype =float)\n",
    "            GiniAll[cid][pSet] = nodes_[rr+1]#seg idx to y-idx\n",
    "        #cid = nToGet -1\n",
    "        Ginits = np.array([getData_( volIdx, gId,path2file) for gId in rr])\n",
    "        GiniAll[volIdx][pSet] = Ginits\n",
    "        lens = get_last_line_as_numpy_array(path2file+\"length_Segs\"+\".txt\", dtype =float)\n",
    "        GiniAll[lenIdx][pSet] = lens[rr]\n",
    "        sts = get_last_line_as_numpy_array(path2file+\"subTypes\"+\".txt\", dtype =float)\n",
    "        assert len(sts) == len(lens)\n",
    "        GiniAll[stIdx][pSet] = sts[rr]\n",
    "        \n",
    "        orgidPerNode = get_last_line_as_numpy_array(path2file+\"orgidPerNode.txt\", dtype =int)\n",
    "        globalNodeId = get_last_line_as_numpy_array(path2file+\"globalNodeId.txt\", dtype =int)\n",
    "        parentNidx = get_last_line_as_numpy_array(path2file+\"parentNidx.txt\", dtype =int)\n",
    "        ot_orgs = get_last_line_as_numpy_array(path2file+\"ot_orgs.txt\", dtype =int)\n",
    "        id_orgs = get_last_line_as_numpy_array(path2file+\"id_orgs.txt\", dtype =int)\n",
    "        id_roots = id_orgs[np.where(ot_orgs==2)]\n",
    "        \n",
    "        org2basenode = dict(zip(id_orgs, parentNidx))\n",
    "        #org2ot = dict(zip(id_orgs, ot_orgs))\n",
    "        isRoot = np.array([oo in id_roots for oo in orgidPerNode])\n",
    "        \n",
    "        orgidPerRoot = orgidPerNode[np.where(isRoot)]\n",
    "        globalNodeIdroot =  globalNodeId[np.where(isRoot)]        \n",
    "        segIdxRoot = np.array([rr for _, rr in enumerate(globalNodeIdroot) if rr != org2basenode[orgidPerRoot[_]]]) -1\n",
    "        orgIdxRoot = np.array([orgidPerRoot[_] for _, rr in enumerate(globalNodeIdroot) if rr != org2basenode[orgidPerRoot[_]]])\n",
    "        #segIdx2ot = dict(zip(segIdxRoot,orgIdxRoot))\n",
    "        \n",
    "        relLens = np.zeros(len(segIdxRoot))\n",
    "        #lens = get_last_line_as_numpy_array(path2file+\"length_Segs\"+\".txt\", dtype =float)\n",
    "        for myorg in orgIdxRoot:\n",
    "            thensegs = np.where(orgIdxRoot == myorg)#segIdxRoot[] \n",
    "            lenSegs = GiniAll[lenIdx][pSet]\n",
    "            relLens[thensegs] = np.cumsum(lenSegs[thensegs])\n",
    "            \n",
    "            \n",
    "        GiniAll[orgIdId][pSet] = orgIdxRoot\n",
    "        GiniAll[relLenId][pSet] = relLens\n",
    "\n",
    "    return GiniAll #cid pSet rr\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7924bb88-81aa-435d-a182-eaddbd611fdb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0, ( 323 ), 1, ( 328 ), 2, ( 323 ), 3, ( 328 ), 4, ( 353 ), 5, ( 323 ), 6, ( 322 ), 7, ( 355 ), 8, ( 323 ), 9, ( 329 ), 10, ( 323 ), 11, ( 322 ), 12, ( 323 ), 13, ( 323 ), 14, ( 323 ), 15, ( 339 ), 16, ( 325 ), 17, ( 323 ), 18, ( 328 ), 19, ( 338 ), 20, ( 322 ), 21, ( 323 ), 22, ( 324 ), 23, ( 329 ), 24, ( 342 ), 25, ( 319 ), 26, ( 328 ), 27, ( 322 ), 28, ( 325 ), 29, ( 328 ), 30, ( 324 ), 31, ( 323 ), 32, ( 323 ), 33, ( 324 ), 34, ( 319 ), 35, ( 331 ), 36, ( 328 ), 37, ( 336 ), 38, ( 323 ), 39, ( 323 ), 40, ( 325 ), 41, ( 338 ), 42, ( 319 ), 43, ( 342 ), 44, ( 323 ), 45, ( 327 ), 46, ( 339 ), 47, ( 337 ), 48, ( 323 ), 49, ( 342 ), 50, ( 342 ), 51, ( 358 ), 52, ( 328 ), 53, ( 324 ), 54, ( 323 ), 55, ( 341 ), 56, ( 323 ), 57, ( 328 ), 58, ( 344 ), 59, ( 328 ), 60, ( 345 ), 61, ( 328 ), 62, ( 354 ), 63, ( 328 ), 64, ( 341 ), 65, ( 323 ), 66, ( 329 ), 67, ( 327 ), 68, ( 323 ), 69, ( 328 ), 70, ( 358 ), 71, ( 324 ), 72, ( 346 ), 73, ( 356 ), 74, ( 323 ), 75, ( 323 ), 76, ( 326 ), 77, ( 323 ), 78, ( 329 ), 79, ( 334 ), 80, ( 336 ), 81, ( 328 ), 82, ( 331 ), 83, ( 344 ), 84, ( 328 ), 85, ( 370 ), 86, ( 359 ), 87, ( 329 ), 88, ( 354 ), 89, ( 323 ), 90, ( 336 ), 91, ( 323 ), 92, ( 356 ), 93, ( 348 ), 94, ( 367 ), 95, ( 329 ), 96, ( 323 ), 97, ( 333 ), 98, ( 333 ), "
     ]
    }
   ],
   "source": [
    "GiniAll = getData()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "10efd9e7-4f14-49bd-b978-30e688e1384f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1126/1273250247.py:1: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  assert np.array(GiniAll).shape == (nToGet, numPset, 232) #all same shape\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[131], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m np\u001b[38;5;241m.\u001b[39marray(GiniAll)\u001b[38;5;241m.\u001b[39mshape \u001b[38;5;241m==\u001b[39m (nToGet, numPset, \u001b[38;5;241m232\u001b[39m) \u001b[38;5;66;03m#all same shape\u001b[39;00m\n",
      "\u001b[0;31mAssertionError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#assert np.array(GiniAll).shape == (nToGet, numPset, 232) # some have a couple of segments more"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "80bd6820-d8bd-40b6-9753-0df6c58e4069",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_324427/2873905747.py:1: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  np.array(GiniAll).shape\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(17, 99)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(GiniAll).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e3d081bb-59cd-418b-a452-8a7ca1620904",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get param set\n",
    "paramsets =  pd.read_csv('../../output_random_rows.csv')\n",
    "paramsets = paramsets.drop(paramsets.columns.tolist()[-1],axis=1)\n",
    "paramsets['k_C,S'] /=paramsets['theta']\n",
    "paramsets['k_O,S'] /=paramsets['theta']\n",
    "paramsets.drop(labels='theta', inplace=True, axis=1)\n",
    "outputvalsname =  ['wat','cs','cl','coa','cod','cca',\n",
    "                   'ccd','css2','co2','yX','yY','yZ','vol','lenSeg',\n",
    "                   'st','orgId','relLen']\n",
    "assert len(outputvalsname) == nToGet \n",
    "column_names = outputvalsname + paramsets.columns.tolist()+['pSet']\n",
    "df = pd.DataFrame(columns=column_names)\n",
    "\n",
    "for pset in allPSets:#range(numPset):\n",
    "    N = GiniAll[0][pset].shape[0]\n",
    "    #assert N == 232\n",
    "    rows_to_duplicate_df = pd.DataFrame(paramsets.iloc[pset]).T\n",
    "    duplicated_rows_df = rows_to_duplicate_df.loc[np.repeat(rows_to_duplicate_df.index.values, N)]\n",
    "    # Duplicate the specific rows N times\n",
    "    #print('init dupl row shape',duplicated_rows_df.shape)\n",
    "    for jj, outVn in enumerate(outputvalsname):\n",
    "        duplicated_rows_df[outVn] = GiniAll[jj][pset]\n",
    "        #print('adding',jj,outVn,duplicated_rows_df.shape,len(GiniAll[jj][pset]))\n",
    "    duplicated_rows_df['pSet'] = pset\n",
    "    \n",
    "    path2file =countryRoad.format(pset)#'newMucil/none_1476_'+str(pset)+'_10to11_20mn_0s_10/'\n",
    "    simTime = get_last_line_as_numpy_array(path2file+\"totalComputetime.txt\", dtype =float)[0]\n",
    "    \n",
    "    duplicated_rows_df['simTime'] = simTime\n",
    "    # Concatenate the original DataFrame with the duplicated specific rows\n",
    "    df = pd.concat([df, duplicated_rows_df], ignore_index=True)\n",
    "df['co'] = df['coa']+df['cod']\n",
    "df['cc'] = df['cca']+df['ccd']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c06e6c67-fce5-46e5-b0c3-c2d2b50c732a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_pickle('./cyl10to11_4d.pkl')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py38b",
   "language": "python",
   "name": "py38b"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
